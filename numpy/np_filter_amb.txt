Sentinel values aren't necessarily a bad thing -- and using otherwise nearly-useless Python singletons as sentinels isn't so horrible IMO ( Ellipsis and ( ) are the ones I've used where None would be confusing ) .
How to truncate matrix using NumPy ( Python )
FYI : [ Getting the indices of several elements in a NumPy array at once ] ( #URL )
This will return indices of elements equal to item : #CODE
wouldn't that give the indices of all elements that are * not * equal to item ?
The result is a tuple with first all the row indices , then all the column indices .
If you're going to use this as an index into something else , you can use boolean indices if the arrays are broadcastable ; you don't need explicit indices .
The absolute simplest way to do this is to simply index based on a truth value .
The nonzero method takes booleans , too : #CODE
The two zeros are for the tuple of indices ( assuming first_array is 1D ) and then the first item in the array of indices .
If you need the index of the first occurrence of only one value , you can use ` nonzero ` ( or ` where ` , which amounts to the same thing in this case ): #CODE
The following finds the indices of the first element of each subsequence : #CODE
If the result must have the same shape as in your example just add a reshape : #CODE
If the matrix is not symmetric be careful about the order in dot .
I read in the manual of Numpy that there is function ` det ( M )` that can calculate the determinant .
where ` sign ` is the sign and ` logdet ` the logarithm of the determinant , such that you can calculate the determinant ` det ` doing : #CODE
This can be achieved efficiently in Numpy by combining ` lexsort ` and ` unique ` as follows #CODE
Seems to me that this approach is easier than special-casing out some function to determine absolute type certainty .
As others have answered , there could be other numeric types besides the ones you mention .
You want something like " ( a + b ) / 2 " or " max ( a + b , 256 )" .
Yes , matlab clamps values when doing arithmetic on uint8 values ( e.g. it implicitly does the equivalent to max ( double ( a ) +double ( b ) , 256 ) )"
When I try to do max ( im1arr+im2arr , 256 ) I get the error :
I do ( im1arr+im2arr ) / 2 I get funky colors , only dimmer , max value 127 so I did :
Your choices are divide by 2 , scale by 255 / array.max() or do a clip .
It seems that I had to divide both images first before I can add them , even if I do a clip ( min=0 , max=255 ) on the result , the overflow already happened so the funky colors are still there .
This means each element of the array can only hold values up to 255 , so when your sum exceeds 255 , it loops back around 0 : #CODE
` lexsort ` returns the permutation of the array indices which puts the rows in sorted order .
You can use the append function .
Of course , as mentioned in other answers , unless you're doing some processing ( ex . inversion ) on the matrix / array EVERY time you append something to it , I would just create a list , append to it then convert it to an array .
( The ` append ` function will have the same issue . ) If you want to build up your matrix one column at a time , you might be best off to keep it in a list until it is finished , and only then convert it into an array .
Are numpy arrays / matrices fundamentally different from Matlab ones ?
To create an empty multidimensional array in NumPy ( e.g. a 2D array ` m*n ` to store your matrix ) , in case you don't know ` m ` how many rows you will append and don't care about the computational cost Stephen Simmons mentioned ( namely re-buildinging the array at each append ) , you can squeeze to 0 the dimension to which you want to append to : ` X = np.empty ( shape =[ 0 , n ])` .
You can apply it to build any kind of array , like zeros : #CODE
OP asks ` numpy.matrix ` but ` zeros ` returns ` ndarray ` .
But I guess this should be fast enough : you will still compute std at C speed , and you will save a lot of memory as you won't have to store 0 values where you have variable size arrays .
Or simply program the whole std function in Cython ( see this for benchmarks and examples with sum function )
Filling missing data with any default value would spoil the statistical characteristics ( std , etc ) .
It doesn't look like there's any function to do this in scipy's fft functions ( see #URL ) .
Scipy 0.8 will have single precision support for almost all the fft code ( The code is already in the trunk , so you can install scipy from svn if you need the feature now ) .
I've tried it using numpy's correlate function , but I don't believe the result , as it almost always gives a vector where the first number is not the largest , as it ought to be .
2 ) If you don't believe the numpy results , you might try SciPy's correlate function .
The scipy and numpy correlate functions are both in C . numpy multiarray source code is somewhere in here : #URL and scipy correlate source code is here : #URL
If you define the autocorrelation as ` a ( x , L ) = sum ( k=0 , N-L-1 ) (( xk-xbar ) * ( x ( k+L ) -xbar )) / sum ( k=0 , N-1 ) (( xk-xbar ) **2 )` [ this is the definition given in IDL's a_correlate function and it agrees with what I see in answer 2 of question #12269834 ] , then the following seems to give the correct results : #CODE
As you see I have tested this with a sin curve and a uniform random distribution , and both results look like I would expect them .
z.dumps() reveals something like " \x00\x00 " ( actual zeros in string ) , but pickle seems to be using the string's repr() function , yielding "' \x00\x00 '" ( zeros being ascii zeros ) .
' ascii ' codec can't encode character u ' \xda ' in position 1 : ordinal not in range ( 128 )
Try compiling the whole Python stack with MinGW32 .
If you've been doing any amount of scientific code , ultimately you'd want things like ` max ( a )` to work on matrices of all sizes , even scalars .
You can build up frequencies quite easily and stack them together , add a little noise , and you'll get something that at least doesn't sound anything like a pure tone .
Let's say I have a set of vectors ( readings from sensor 1 , readings from sensor 2 , readings from sensor 3 -- indexed first by timestamp and then by sensor id ) that I'd like to correlate to a separate set of vectors ( temperature , humidity , etc -- also all indexed first by timestamp and secondly by type ) .
Experimental data , especially large ones , tend to have large uncertainties anyways .
or the svd function :
The SVD of matrix A gives you orthogonal matrices U and V and diagonal matrix such that
It's fairly obvious when you look at the values in x U being the dot products of x and the columns of U , and the values in U T x being the dot products of the x and the rows of U T , and the relation of rows and columns in transposition
` insert ` may be slower than ` append ` but allows you to fill the whole row / column with one value easily .
As you can see , we are simply converting the sequence type into an explicit mapping from indices that are nonzero to their values .
Then , you calculate the " regression sum of squares " , which is how much your FITTED values differ from the mean
EDIT : The current ` scipy ` version started to normalize all images so that min ( data ) become black and max ( data ) become white .
For the fourth query if the only data stored in your array is the cells type , you could encode it as integers : #CODE
3 . append new tuple .
You would have to resize the 3d array in that case .
Of course , with only two dimensions , you can compress this down to a single loop using a list comprehension or generator expression , but that's not very scalable or readable : #CODE
If you need to scale this to multiple dimensions and really want a flat list , you can write a ` flatten ` function .
A NumPy array is an array of uniform values -- single-precision numbers takes 4 bytes each , double-precision ones , 8 bytes .
It's super alex , here to answer NumPy questions in the blink of an eye :)
Speed : Here's a test on doing a sum over a list and a numpy array , showing that the sum on the numpy array is 10x faster ( in this test -- mileage may vary ) .
First off , numpy does just fine with integers but you told it to start out with floats then change them to ints , instead use ` zeros ( x , dtype=int32 )` ( or even better : ` empty `) .
I need to translate it into Python .
If " a " is 1-by-18 , you don't need the transpose .
@USER : Then " a " has to be 18-by-1 ( before the transpose ) , not 1-by-18 .
How to go about implementing matrix division in Python ( or any language ) ( Note : Let's go over a simple division of the form ` A / B ` ; for your example you would need to do A T first and then A T / B next , and it's pretty easy to do the transpose operation in Python | left-as-an-exercise :) | )
Matrix transpose
Applying a transpose to both sides gives you the equation B T c T = a T , which is a more standard form ( i.e. Ax = b ) .
I tried to use it after I read the help for lstsq , but I am getting getting values in the matrix that are different from the ones in Matlab , though they are the same dimensions .
Instead you should solve b T c T = a for c T and transpose the result : #CODE
I have tried this with other inputs and receive similar results , usually just a different first element and zeros filling the remainder of the matrix .
if b is invertible , this is a*inv ( b ) , but if it isn't , the it is the x which minimises norm ( bx-a )
In this case the moore-penrose inverse ` b2 = b ( 1 , :) ; inv ( b2*b2 ') *b2*a '` is defined and gives the same answer
solution x , that has the minimum norm ( min NORM ( x )) .
In both cases the approximation error of ` xb-a ` is non-negligible ( non-exact solution ) and the same , i.e. ` norm ( x1* b-a )` and ` norm ( x2* b-a )` will return the same result .
to a.astype ( ' S ' +str ( i )) where i is the max ( len ( str ( a ))) , then pad
Sorry , but after thorough investigation I can't find any way to perform the task you require without a minimum of post-processing ( to strip off the trailing zeros you don't want to see ); something like : #CODE
is speedy and concise , but breaks your constraint of being " off-the-shelf " -- it is , instead , a modular combination of general formatting ( which almost does what you want but leaves trailing zero you want to hide ) and a RE to remove undesired trailing zeros .
For example , as others have already pointed out , there are efficient data structures for sparse matrices ( matrices with lots of zeros ) , like ` scipy.sparse.csc_matrix ` .
There's Matplotlib for plots and the csv module for reading Excel data ( assuming you can dump to CSV ) .
SAGE ( mentionend in the tutorial , see #URL ) is a pretty cool stack based on Python .
If not , I can try to arrange some example code myself ( though the ones I have are over six years old ... ) .
Below is some python code that illustrates my problem : the fourth dot takes a very long time to appear ( if not , increase the size of the array ) #CODE
The diff / where solution is exactly what I had in mind ( not to mention it is about 10 times faster than the other solutions ) .
@USER , I don't do matlab ( funny enough my daughter , now a PhD candidate in advanced radio engineering , does ;-) , but now looking at your answer I do see the analogies -- get the end-of-runs minus the start-of-runs , get those by locating < 0 and > 0 spot in the differences , and pad the bits with zeros to make sure all runs-of-ones do end .
ImportError : No module named convolve
The convolve package is also available from the stsci_python repository .
Also , the 1-D convolve / correlate is not FFT-accelerated .
It shows - surprisingly - that numpy's fft is faster than scipy's , at least on my machine .
The issue is the time required to do all these multiplies -- ( ` f [ unsigned int ( i+j )] * g [ j ]`) In effect , you're filling a big matrix with all these products and picking the row with the maximum sum .
Just enough of the products to be sure you've found the maximum sum .
The issue with finding maxima is that you have to sum everything to see if it's biggest .
If you used ` max ( g ) -g [ j ]` to work with negative numbers , you'd be looking for the smallest , not the biggest .
Using FFTs and the convolution theorem will give you dramatic speed gains by converting the problem from O ( n^2 ) to O ( n log n ) .
iterator , in the order of ascending indices .
" Serialization + persistance :: in a few lines of code , compress and annotate Python objects into SQLite ; then later retrieve them chronologically by keywords without any SQL .
The python_list sum is correct .
Is there an upper limit to the length of the Numpy array or is it with the Numpy sum function ?
Make ` np_array = numpy.arange ( 1000000 , dtype= numpy.uint64 )` , for example , and your sum will come out OK ( although of course there are still limits , with any finite-size number type ) .
Is this the dot square ( ie , equivalent to m . *m ) or the matrix square ( ie m must be a square matrix ) ?
In particular , I'd like to know whether I can get rid of the transpose in this code : #CODE
where ` axes ` is a ` numpy.ndarray ` of AxesSubplot objects , making it very convenient to go through the different subplots just using array indices ` [ i , j ]` .
I've been using it for almost one year in the financial industry where missing and badly aligned data is the norm and it really made my life easier .
to get ` sin ( x )` for ` x ` in ` [ -5 , 5 ]`
How can I do the same to get , for instance ` sin ( x+y+z )` ?
can you use numpy's sqrt and / or sum implementations ?
I found this on the other side of the interwebs ` norm = lambda x : N.sqrt ( N.square ( x ) .sum() )` ; ` norm ( x-y )`
My only real comment was sort of pointing out the connection between a norm ( in this case the Frobenius norm / 2-norm which is the default for norm function ) and a metric ( in this case Euclidean distance ) .
I found that using the ` math ` library's ` sqrt ` with the ` ** ` operator for the square is much faster on my machine than the one line , numpy solution .
What you are calculating is the sum of the distance from every point in p1 to every point in p2 .
Then you can get the total sum in one step , ` scipy.spatial.distance.cdist ( p1 , p2 ) .sum() ` .
Or use ` numpy.linalg.norm ( p1-p2 ) .sum() ` to get the sum between each point in p1 and the corresponding point in p2 ( i.e. not every point in p1 to every point in p2 ) .
Previous versions of NumPy had very slow norm implementations .
you're missing a sqrt here .
How can I generate a complete histogram with numpy ?
I want to generate a histogram for it .
However , Numpy's built in histogram requires a pre-defined number of bins .
What's the best way to generate a full histogram with one bin for each value ?
If you have an array of integers and the max value isn't too large you can use numpy.bincount : #CODE
The approach I have taken so far consists in suggesting that the user define ` sin = numpy.vectorize ( math.sin )` , so that the new ` math.sin ` function ( which works on numbers with uncertainties ) is broadcast to the elements of any Numpy array .
Furthermore , what would be the most efficient and correct way of defining ` sin ` so that it works both for simple numbers , numbers with uncertainties , and Numpy arrays ?
It looks like following what NumPy itself does keeps things clean : " extended " mathematical operations ( sin ) that work on new objects can be put in a separate name space .
The ` stack ` functions are promising , but ideally I would want to grow the array in place .
To study the above problem , I've used the usual memory profiling tools - heapy and pympler - but am only getting the size of the outer array objects ( 80 bytes ) and not the data they are containing .
You will only re-alloc memory log ( n ) times where n is the number of rows .
the problem with ` [ 16 ]` seems to be that the transpose has no effect for an array .
( transpose works for 2D arrays , e.g. for the square one in the example , or when turning into a ` ( N , 1 )` -shape array using ` .reshape ( -1 , 1 )`)
On my system , for a vector with 10000 elements repeated 1000 times , the ` tile ` method is 19.5 times faster than the method in the currently accepted answer ( using the multiplication-operator-method ) .
I think using the broadcast in numpy is the best , and faster
about 15 times faster using broadcast
I am trying to get rid of rows with zeros .
` max ( a , key=len )` gives you the longest string ( and ` len ( max ( a , key=len ))` gives you its length ) without requiring you to code an explicit loop , but of course ` max ` will do its own looping internally , as it couldn't possibly identify " the longest string " in any other way .
If you already have zeros in your real-world problem where your example does , then this solves it .
accumulate is designed to do what you seem to want ; that is , to proprigate an operation along an array .
ValueError : cannot resize this array : it does not own its data
You may be meeting issues discussed in this thread : #URL -- then you can fix them , as Travis Oliphant mentions there , by adding the refcheck=0 argument to the resize call ( unless you HAVE shared the data , in which case there can be no resizing in-place any more ( note that what Travis mentions as a feature of the SVN head of numpy has been part of regularly released numpy for a long time by now -- that thread is 3+ years old ;-) .
translate dates into x values and use first day as x=0 for your problem the values shoul be aproximatly
The idea is quite simple , that for each row in the binary NxM matrix I take only the minimal and maximal indices .
In fact , that is how I got to these binary images . the filter does not give indices however .
The filter will give you the bitmap / matrix where you can find all the indices like you did in your code .
Index-like unique integers like the example ?
Otherwise the searchsorted will return false indices .
The first array contains some zeros ( which are distributed randomly over the length of the array ) , which I would like to remove .
For example , I have a 1GB binary file that's just a bunch of single precision floats packed as a big endian and I want to convert it to little endian and dump into a numpy array .
SAGE provides bindings for it , and that's one of the more powerful free ones .
You can use ` numpy.iinfo ( arg ) .max ` to find the max value for integer types of ` arg ` , and ` numpy.finfo ( arg ) .max ` to find the max value for float types of ` arg ` .
` iinfo ` only offers ` min ` and ` max ` , but ` finfo ` also offers useful values such as ` eps ` ( the smallest number > 0 representable ) and ` resolution ` ( the approximate decimal number resolution of the type of ` arg `) .
If you don't know in advance which columns are strings , how comes you DO know which ones are int8 vs int16 vs int32 , since you do say you need to control _that_ " manually " ?!
Point is , you can either do your own discovery of types and sizes , or just let numpy do it , or let it do it ( on part or all of the data ) and then overrule its opinions by reparsing the data with a different dtype -- I'm not sure what further option you're yearning for ( as you say you want to control the types of some columns but not others BUT you don't know which ones in advance ?! )
I'm sorry , you are correct - I meant that the solution you suggested would be too simple as it will only work for two-element tuples with the string column in the second place , but of course I can just code a loop over columns and for ones I know contain strings , find the maximum length .
You can also reshape existing arrays : #CODE
I want to calculate the sum of every combination of elements in a and b ( sum_ij_ = a_i_ + b_j_ for all i , j ) .
N is max element in a and n is number of
pv is a little helper function I wrote to debug the value of variables .
print() except when you say pv ( x ) it prints both the literal variable name ( or expression string ) , a colon , and then the variable's value .
The modest advantage of using pv over print is that it saves you typing .
The pv function works by using the traceback module to peek at the line of code
used to call the pv function itself .
You " mod " the answer ( modulo N ) .
N is max element in a and n is number of elements in a .
What I really meant is size ( a )= 2^n ( not n as I wrote in the first post ) , max ( a )= 3^n (= N ) , with n as high as posible .
I would like to have n~20 => max ( a )= 3^20 < 2^32 , so I need a clever way to deal with the data .
It creates a width x height x 9 matrix filled with zeros .
What is the easiest way to combine them into one ( i.e. append one table on to the other ) ?
I only know that when I try the same thing with recarrays I get a ValueError : cannot resize this array : it does not own its own data .
The numpy equivalent of ` repmat ( a , m , n )` is ` tile ( a , ( m , n ))` .
Problem is tile promotes ` a ` to the dimension of the tile argument by * prepending * new axes as necessary .
Know both ` tile ` and ` repeat ` .
I'm still learning Python myself , but I think the way that slicing works is that indices point to in-between locations , therefore ` 0:1 ` only gets you the first column .
to think of the indices as pointing
Agree its not clear - I read the question as asking about extracting a column - in matlab m (: , 1 ) gives a 2d column vector - the same as m [: , 0:1 ] in numpy , but m [: , 0 ] in numpy gives a 1d array - I think the question was why this is different to matlab , but the slices translate as you say ...
In fact , ` d1 [ newaxis ,... ] .shape == ( 1 , 18 , 1 8) ` , and you can stack both 3D arrays directly and get the new 3D array ( ` d3 `) that you wanted .
Is there a way to append a row to a NumPy rec.array() ?
You can resize numpy arrays in-place .
That , naturally , is rather slow so you should try to avoid using ` resize ` or ` append ` if possible .
Does numpy or scipy already have it , or do I have to roll my own using ` numpy.linalg.eigh ` ?
scipy and its sparse module provide generic linear algrebra functions working on both sparse and dense matrices , among which there is the eig * family of functions :
For the same reason , you don't need to compute the covariance matrix but only to provide the operation dot ( A.T , dot ( A , x )) for A .
If I use your example where A is a 1000*1000 matrix then ` eigsh ` and ` svds ` are faster than ` eigh ` and ` svd ` by a factor of ~3 , but if A is smaller , say 100*100 , then ` eigh ` and ` svd ` are quicker by factors of ~4 and ~ 1.5 respectively .
` svd ` already returns ` s ` as sorted in descending order , as far as the documentation goes .
You can quite easily " roll " your own using ` scipy.linalg ` ( assuming a pre-centered dataset ` data `) : #CODE
cov matrix is np.dot ( data.T , data , out=covmat ) , where data must be centered matrix .
thanks alot , what you have said is exactly the same as my tutor said which is really important and i think i have already know clearly about the physics , just dont know how to translate to python code , to add a bit more :
You need to use append to add to the list .
thanks for replay , yes , i am going to use append , but it doesnt really solve the question seems ..
If you have specific functions you use a lot , and you are tired of writing np.sin() , np.cos , etc , you should import those specifically ( " from numpy import sin ") . cheers .
update_r calls update_v , update_v calls force , and force calls update_r - you will get a stack overflow :)
i seem to remember that a convolution with a laplace kernel is only an approximation of a laplace transform ...
did you try another laplace convolution kernel , like ` [[ 1 , 1 , 1 ] [ 1 , -8 , 1 ] [ 1 , 1 , 1 ]]` ?
I can also resize the shorter runs to the length of the longest , but all the new fields are filled with " NoneType " .
Well , one way to do it would be to iterate over each row of each data set and append a given column value to an array that's stored in a dictionary , where the time index is used for its key value .
One thing : the reduce / lambda construct can fail when an early value is the highest : ' int ' has no method ' shape ' . replaced with : maxLen = max ([ a.shape [ 0 ] for a in dSets ])
NumPy ' s array doesn't have the append function .
NumPy actually does have an append function , which it seems might do what you want , e.g. , #CODE
` hstack ` gives the same result as ` concatenate (( my_data , new_col ) , axis=1 )` , i'm not sure how they compare performance-wise .
While that's the most direct answer to your question , i should mention that looping through a data source to populate a target via append , while just fine in python , is not idiomatic NumPy .
initializing a NumPy array is relatively expensive , and with this conventional python pattern , you incur that cost , more or less , at each loop iteration ( i.e. , each append to a NumPy array is roughly like initializing a new array with a different size ) .
Suppose I make two recarrays with the same dtype and stack them : #CODE
I'm not 100% sure about fft , how can I pass numpy a chunk and get back three values from - 1.0 to 1.0 for bass , mid and treble ?
How to translate ( say ) a thousand intensities ( one per each 10-Hertz slice of the spectrum , say ) into just three numbers , as you desire , is of course quite a heuristic issue -- for example you could just decide which ranges of frequencies correspond to " bass " and " treble " , with everything in-between being " mid " , and compute the average intensities in each .
but in the first vector would not have to be given to absolute zero ` [ 0.143 , 0.250 , 0.333 ]`
Both are filled with zeros and ones .
Whe you use teh " big matrix indices " x , y to address an element on the small matrix - is this correct ?
Here's a function that takes advantage of the fact that the arrays contain only ones and zeroes : #CODE
The running time depends on the number of ones in the smaller array , being 20ms per each nonzero element .
ValueError : shape mismatch : objects cannot be broadcast to a single shape
Performing operations on a NumPy arrray but masking values along the diagonal from these operations
as I can perform operations on arrays so that does nothing on the diagonal
is calculated such that all but the diagonal #CODE
to avoid the NaN value , but retained the value zero on the diagonal in all responses
Can you just do the calculation as normal , then afterwards set the diagonal back to zero ?
If you have a tuple ` MYTUPLE =( 1 , 2 , 3 )` then the only possible indices are ` MYTUPLE [ 0 ]` , ` MYTUPLE [ 1 ]` and ` MYTUPLE [ 2 ]` .
Floyd Warshall is O ( |V| 3 ) and Dikstra is O ( |E| + |V| log |V| ) but you'll have to run it V times to find all pairs which gives a complexity of O ( |E * V| + |V 2 | log |V| ) I guess .
efficient way to compress a numpy array ( python )
I am looking for an efficient way to compress a numpy array .
The best way to compress a numpy array is to use pytables .
Say I concatenate ` xy1 ` ( length m ) and ` xy2 ` ( length p ) into ` xy ` ( length n ) , and I store the lengths of the original arrays .
Edit : Instead of calling ` sqrt ` , doing squares , etc ., you can use ` numpy.hypot ` : #CODE
For a list of points in a contour detected by opencv2 , I needed to use reshape function of numpy to reshape it first ...
I don't know why they put the coordinates in 2 square bracket , but if you apply cdist directly to the list , each element will only have length 1 ( a list , containing 1 list inside ) , I have to reshape it so the contour become the list of length-2 element ( which means flat out the double bracket )
The context of this is that I want to set the centre of a zeros array to another array in a single line , and the only solution I could come up with is to use reduce and lambda functions .
how well they approximate the sin function over a range of values .
dot product and scaling takes 2 sec
the result of the dot product will exceed the uint8 range .
If so , how do I append series one by one to the timeseries , since I don't want to create a massive non-numpy structure to feed to the time_series() constructor in one shot ?
First you need to find a permutation that sorts a .
Now apply the same permutation to b .
I would want the data already in the 1000x1500 piece of the array not to be changed , only blank data ( zeros ) added to the bottom and right , basically .
ValueError : cannot resize an array references or is referenced
Use the resize function
Also , take a look at other presentation from EuroScipy , there are some more Physics-related ones .
Therefore ` a [ 1 :] * alpha [: -1 ]` is just an array of zeros .
where you set ` first_br_row ` , ` last_br_row ` ` first_br_col ` , ` last_br_col ` to address the subimage where you want to add the brush and ` first_row ` , ` last_row ` , ` first_col ` , ` last_col ` to clip the brush ( normally set them to 0 and # rows / cols - 1 , but adjust when you're near enough to the image boundary to only want to paint part of the brush ) .
Then , a sum of such strokes is the sum of these operations .
Something with your system ( filesystem ? ) ; I would try pickling in binary mode ; use ` dump ( idMap , idmapfile , protocol=2 )`
The previous answers are the correct ones , but as a weeker alternative you can transform into a list : #CODE
Increment Numpy array with repeated indices
I have a Numpy array and a list of indices whose values I would like to increment by one .
This list may contain repeated indices , and I would like the increment to scale with the number of repeats of each index .
For addition ufunc , this method is equivalent to a [ indices ] += b , except that results are accumulated for elements that are indexed more than once .
Selecting indices for a 2d array in numpy
This is because you need two indices to index ` bar ` .
My C++ program is not effective when trying to use std :: cout .
I tried to store my numbers array in a std :: complex [ ] instead of a double [ ] but it is not compiling .
My question is : how can I dump the result so I can check that my python code is doing the same as my cpp / opencv code ?
But if you only want to dump the data , you can probably save the real and imaginary parts as images and read them in python ( i'm not very familiar with OpenCV , you have to check its support - and python's - for float images ) .
For two dimensions , it is easy to find the matrix indices of the maximum element : #CODE
Does Numpy / Scipy have a simple way of returning the indices in multiple dimensions given an index in one ( collapsed ) dimension ?
As mentioned above , AppEngine will not allow you to install C extensions ( but pure python ones do work ) so it's unlikely numpy will work for you on there , since I suspect some of its features use C speedups .
The broadcast needed to do 4 x 3 vector plus a 1D vector with 3 elements succeeds .
The broadcast needed to do 4 x 3 vector plus a 1D vector with 4 elements fails .
Similar to numpy_all_the way , but you dynamically resize if you have an index error .
The first example in the comment looks similar at first , but the indices are slices .
So it seems that slices are different , but except for that , regardless of how indices are constructed , you can specify a tuple or list of ( x-values , y-values ) , and recover those specific elements from the array .
What I am looking for is a good representation of bins , ideally ones that are half closed half open ( so that there is no way of assigning one point to two bins ) , i.e. #CODE
Since the interval is open on the upper limit , the indices are correct : #CODE
thanks for your answer -- but I think histogram is still different from what I want .
It seems like histogram does not return that information , right ?
' bin_assignments ' is a 1d array of indices comprised of integer values from 0 to 4 , corresponding to the five bins -- the bin assignments for each of the 30 original points in the ' data ' matrix above .
I have practically no knowledge of Matlab , and need to translate some parsing routines into Python .
Read n items ( as machine values ) from the file object f and append them to the end of the array .
You can handle this scenario in Python using Numpy's ` shape ` and ` transpose ` .
This is because the bitwise representation of each element in ` x ` is of the form ` 1 followed by ` n ` zeros , and the corresponding element in ` y ` is ` n ` 1s .
If True , the input arrays are both assumed to be unique , which
Then you can access rows as usual with numeric indices , such as ` arr [ 1 ]` ,
It is often faster to compute the FFT , then the product , and the iFFT of the result than convolve the usual way .
Thank you for pointing that out , I hadn't considered that the scipy convolve could be that inefficient .
It looks like , though I havn't checked that closely , that scipy convolve is doing quite a bit of memory manipulation operations and has a number of if statements slowing things down .
Yes , convolve2d is quite inefficient , as it deals with the general case ( it deals with arbitrary objects - you should be able to convolve with array of Decimal objects , for example ) .
Before going to say C with ctypes , I'd suggest running a standalone convolve in C , to see where the limit is .
By the way , google theano convolve =>
I translate the variables to the corresponding dimension in the cpt .
and ( b . ) to put it in a list and translate it again to array so I'll have my new array .
Thing is , stuff in bold means that I create new objects , instead of using just the references to the old ones and this , if d is very large ( which happens to me ) and methods that use d are called many times ( which , again , happens to me ) the whole result is very slow .
Subclassing ( which does involve overriding , as the term is generally used ) is generally much preferable to " monkey-patching " ( stuffing altered methods into existing classes or modules ) , even when the latter is available ( built-in types , meaning ones implemented in C , can protect themselves against monkey-patching , and most of them do ) .
Are the indices always non-negative ?
Yes , the indices are always non-negative .
How can I translate the following Matlab code to Python ?
I'm just reading manageable blocks of data ( ~2mb max ) , calculating stuff with them , writing the result to file then repeating that .
ones I'm finished with aren't being disposed of -- try a ` del xx ` when done ?
I swear it shouldn't be that large , 2MB max , probably under 1MB , that really shouldn't be an issue should it ...
But why do I have zeros in the beginning and not in the end ?
So you get 3 zeros in the beginning .
You don't get any zeros in the end because at least one of the conditions is true ( both are true , in fact ) .
How to get data in a histogram bin
I want to get a list of the data contained in a histogram bin .
However , I want to do this for a 2D histogram and the code to do this is rather ugly .
pyplot.hist in matplotlib creates a histogram ( but also draws it to the screen , which you might not want ) .
` digitize ` , from core NumPy , will give you the index of the bin to which each value in your histogram belongs : #CODE
If there are any numpy devs here , this function should really go in the " see also " part of the histogram docs .
The profiler tells me a lot of time is being spent in the following function ( called often ) , which tells me the sum of square differences between two images #CODE
( which I expect is likely just ` sum (( A-B ) **2 )` if the shape is always ( , , 3 ))
You can also use the sum method : ` (( A-B ) **2 ) .sum() `
It's worth noting that for this you will have to use ` numpy.sum ` , not the builtin ` sum ` , which will find the sum over the first dimension and return a new array of one-dimension-lower .
This way you can do one operation instead of three and using ` numpy.sum ` may be able to optimize the addition better than the builtin ` sum ` .
+1 This is how ` scipy.stats.stats.ss ` ( sum of squares ) does it .
If your data is fairly well behaved then it should be sufficient to find the least squares sum of the component distances .
That said , ` eig ( cov ( data ))` is a really bad way to calculate it , since it does a lot of needless computation and copying and is potentially less accurate than using ` svd ` .
How would I produce the gradient of this line ?
I have written several functions that do some form of gradient descent in numpy / scipy .
Currently I have a code that checks if given element in array is equal = 0 and if so then set the value to ' level ' value ( temp_board is 2D numpy array , indices_to_watch contains 2D coordinates that should be watched for zeros ) .
But unfortunately masked array when doing put() wants to have 1D dimensions ( totally strange ! ) , is there some other way of updating array elements that are equal to 0 and have concrete indices ?
The code below checks the array ( A ) for zeros , and where it finds them , it replaces them with ' level ' .
Then , figure out the actual indices to set : #CODE
You could combine strip and split if the empty lines contain a lot of whitespace .
Here's a variant that might be faster for few indices .
I use np.fromstring instead of np.mat , I needan an array and make a reshape is not expensive .
So np.fromstring + reshape is faster than np.array ( np.mat ( s )) ?
You can also reduce one of the two outer loops by using np.ndenumerate .
So setting up the ones (( 3 , 4 )) is right ... now imagine for each of those 1's in the array I want there to be an array ( for arguments sake , [ 1 , 2 , 3 ]) ... how would I do that ?
No cos now I need a 4-d one and thats a shame cos the 4th dimension affects the other 3 , and I only want it to affect the 3rd dimension :(
Before I run all this code I scan the file and make a list of index locations at ReadFile.fileIndex that I browse using this function and then ' seek ' to the absolute location - is this efficient ?
Then use the supreme vector powers of numpy to calculate your magnitude by dot product .
Just read in big chunks of data , and reshape the data into an array that reflects the ( receiver , channel , sample ) structure , and then use the appropriate indexing to multiply and add things for Pythagoras , and the ' sum ' command to add up the terms in the resulting array .
Then reshape the array ( very fast , only affects metadata ) and take use the ` np.hypot ` ufunc : #CODE
Just because it takes values between 0 and 1 ( or its absolute values do ) doesn't mean it's a probability .
This may not exist , but I'm betting that in a program this simple , most of the ones you need will .
Return common element indices between two numpy arrays
I would like a quick way to return the a2 indices of all elements in a1 .
The example I gave used a matrix that wasn't very big or very sparse , but the ones I am dealing with are quite sparse and potentially very large so it doesn't seem prudent to just list off the elements one by one .
How can this be generalized to an arbitrary series of indices ?
If you need to create an array out of them at the end ( or even occasionally in the midst of this computation ) , it will be far more efficient to accumulate in a list first .
Is it possible to get the area below the diagonal easily when working with numpy matrixs ?
I am mainly trying to just sum those elements .
Quick NB : This avoids the diagonal ...
Otherwise , you'll need a transpose in there .
The only problem is that I'm unable to find a suitably fast enough way to append data into a numpy array .
Here is an incremental improvement that beats the list append solution ( but only slightly ) by eliminating the iteration over 4096 elements .
Tex coords are generated based on tile index but they could be cached as numpy array .
If my dubious 3D diagram makes sense ( the diagonal is a z-axis ) #CODE
Thanks again Steve , was getting confused because I didn't realise you had to reshape the array before using the repeat method - it kept telling me I was trying to use an axis that wasn't defined .
If you don't like the axes ordered this way , you can then transpose : #CODE
However , the line ` if min ( col_list ) = W.shape [ 1 ]: ` doesn't make sense to me in many ways .
Second , why are you checking the min of col_list with the shape ?
I'm thinking that maybe you meant to do ` if max ( col_list ) > = W.shape [ 1 ]: ` ?
Of course , that means that you can't use negative indices too and there aren't any checks for indices being below 0 and what to do with them .
Lets say your lil_matrix is called mat and that you want to remove the i-th column : #CODE
Ok , I understand that this will have to create the two matrices inside the hstack before it does the assignment to the mat variable so it would be like having the original matrix plus one more at the same time but I guess if the sparsity is big enough then I think there shouldn't be any memory problems ( since memory ( and time ) is the whole reason of using sparse matrices ) .
For a sparse csr matrix ( X ) and a list of indices to drop ( index_to_drop ): #CODE
If you just need to find outliers , why not find the point that is the average of the distribution ( average x , average y , average z ) and use the std deviation of the distance away from this point to determine outliers .
Just calculate the sum ( and sum of squares if you need standard deviation as well ) and throw away each value as you calculate it .
With that , you should be able to find outliers ( e.g. excluding points further from the mean than some fraction of the std . dev . ) , and that filtering process should be possible to perform in O ( nd ) time for a total of O ( nd ) .
For small sample sizes , you may want to re-scale the variance ( before sqrt ) to get the unbiased variance .
@USER - I think the answer needs an update for a commit that was made 2012-01-23 ( git log - Sself._r_label1_position ) .
Unpacking tuples / arrays / lists as indices for Numpy Arrays
Well that really depends on the implementation of the convolve and also your kernel .
Note that any decent 8b it convolution algorithm should work with ( at least ) 16bit temporary values because the summing during the convolve can easily overfloat 8b it values , depending on the kernel .
whenever I see ` zip ( * ` , I think " transpose " .
But your screen can't show ( and your eye can't really process ) that much information anyway , so you should probably think of a way to smooth your data to something more reasonable like 1024*1024 before you do this step .
Numpy histogram of large arrays
Wouldn't be hard to roll my own , but wondering if someone already invented this wheel .
As you said , it's not that hard to roll your own .
I'm guessing performance will be an issue with such large files , and the overhead of calling histogram on each line might be too slow .
If you want to make it a tad faster , you can do ` myhist += htemp ` ( I guess that it's faster because it updates the histogram in place ) .
For a tensor , the rank tells you the number of indices ( e.g. a scalar is a rank-0 tensor , a vector rank-1 and a matrix rank-2 ) .
Things that are built into the basic classes tend to be things that can be performed in a unique and straightforward manner , such as matrix multiplications at the most complex .
It's not just numerically-unstable ones .
Calculate all the l2 norm ( euclidian distance ) between a certain point and all other points
` aargsort ` is holding a permutation of ` range ( len ( a ))` telling you where the items of ` aSort ` came from -- much like , in pure Python : #CODE
numpy provides a function ` diff ` for this basic use case #CODE
It seems to me that using the zip function in combination with the numpy broadcast rules might be the most efficient way , but zip doesn't seem to be overloaded in numpy .
The others are simply broadcast .
` arr ` is returned as a ` matrix ` type , which may not be an iterable object that plays nicely with ` join ` .
You could convert ` arr ` to a ` list ` with ` tolist() ` and then perform your ` join ` .
I'd have to transpose each array to Nx1 , and that's pretty lame , since I'd be doing it for every dot-product calculation .
Next up , I tried to create an NxN matrix where column 1 == row 1 ( such that you can multiply two matrices and just take the top-left corner as the dot product ) , but that turned out to be really inefficient .
Unfortunately , the issue with that is that you have to transpose the dang things on the fly , which doesn't make a lot of sense when you're doing millions of comparisons .
I tried caching the dot products , but unfortunately , we don't do the same dot products very often , so that didn't help much .
I'm not sure that it is really much better or faster , but you could do this to avoid using the transpose : #CODE
You could make a subclass of whatever matrix format you are using that has the above statement as the dot product .
Taking two of these and multiplying ( in any order ) gives the desired dot product in the top left of the result matrix .
These use efficient , C implementations under the hood ( including multiplication ) , and transposition is a no-op ( esp . if you call ` transpose ( copy=False )`) , just like with numpy arrays .
As a hack ( though scipy is relatively stable these days ) , you can do the dot product directly on the sparse data : #CODE
The sparsity is 50% , so it's actually faster than ` dot ( x , x )` by a factor of 2 .
For kmeans , you want argmax ( dot ( k x N centres , each Nvec x )); the centres get dense anyway , so may as well keep them dense .
This may not be the elegant solution you are looking for , but if just want results for most cases , you can " clip " large and small values of your plotted data to ` + ?
This gives you the min of the absolute value distance , and somehow we need to return the actual array value .
We could add ` value ` and come close , but the absolute value throws a wrench into things ...
Good answer , I've never used the ` outer ` method of a ufunc before , I think I'll be using it more in the future .
The first function should return ` array [ indices ]` , by the way .
Some of the scipy.linalg routines do accept flags ( like sym_pos=True on linalg.solve ) which get passed on to BLAS routines , although more support for this in numpy would be nice , in particular wrappers for routines like DSYRK ( symmetric rank k update ) , which would allow a Gram matrix to be computed a fair bit quicker than dot ( M.T , M ) .
Golub and Van Loan also provide a way of storing a matrix in diagonal dominant form .
Numpy : Creating a complex array from 2 real ones ?
Surprisingly , the effect of using empty instead of zeros was negligible .
Let's say I have an array of zeros :
To calculate and plot ' similarity ' versus lag , if you don't want to roll your own , i am not aware of a native Numpy / Scipy option ; I also couldn't find one in the ' time series ' scikit ( one of the libraries in the Scipy ' Scikits ' , domain-specific modules not included in the standard Scipy distribution ) but it's worth checking again .
Don't really need specific values just peaks on a histogram or something like that ...
I'd like to roll this up and somehow vectorize this , but I'm not sure how to or if I can .
If the sampling of this random position is uniform over the string , I thought of two approaches to do it : one using multinomial from numpy.random , the other using the simple randint function of Python standard lib .
I changed your code to actually return values ( and used ` randint ` instead of ` rand ` - isn't that what you meant ? ) like this ...
suppose I have a python list or a python 1-d array ( represented in numpy ) . assume that there is a contiguous stretch of elements how can I find the start and end coordinates ( i.e. indices ) of the stretch of non-zeros in this list or array ? for example , #CODE
Assuming there's a single continuous stretch of nonzero elements ...
Also , it's not really correct to call this a fail when the OP doesn't mention how to treat zeros within the chain , so the best one can do is state the assumptions of a given approach , which I clearly do .
The solution I presented is , from my experience anyway , a good solution to this problem ; but your criticism confused this point , so I was just trying to explain why " nonzero " is still a good solution .
If you really want the answer to have the end indices included in the island , you can just change the last line to : ` return zip ( edges [: : 2 ] , edges [ 1 :: 2 ] -1 )`
When using integer array indexing the resultant shape is always " identical to the ( broadcast ) indexing array shapes " .
The general principle is " allocate more than you think you'll need , and if things change , resize the array as few times as possible " .
Doubling the size could be thought of as excessive , but in fact this is the method used by several data structures in several standard libraries in other languages ( ` java.util.Vector ` does this by default for example . I think several implementations of ` std :: vector ` in C++ do this as well ) .
I can append a one-dimensional array to an empty array : #CODE
but I cannot append an empty array to a matrix : #CODE
You can solve the two problems by first loading the arrays from the files files into a list of arrays , and then using concatenate to join all the arrays .
for each append statement .
that's exactly what concatenate expects to get .
You're right , I must have not been using concatenate properly when I first tried your suggestion .
Unfortunately it seems that concatenate is too slow , or possibly not working at all , on these larger data sets ( Maybe it uses a lot of memory ? ) .
If you want to include the change between the last and first element , you can use ` roll ` , as others have suggested .
What I want this to do is display a single red dot in the center of a 512x512 image .
Looking at the function docs , I think you'll need to set the ` converters ` parameter to strip quotes for you ( the default does not ): #CODE
The items in numpy arrays are statically typed , and when you call ` zeros ` you make an array of floats .
The " unelegant " way filters out the zeros with ` dense_array.nonzeros() ` before inserting values in dok_matrix , that's why it doesn't crash .
I am fitting a Gaussian kernel density estimator to a variable that is the difference of two vectors , called " diff " , as follows : gaussian_kde_covfact ( diff , smoothing_param ) -- where gaussian_kde_covfact is defined as : #CODE
This works , but there is an edge case where the diff is a vector of all 0s .
A simple way to detect the case is to compute the max of the diff and compare this to eps ( numpy.finfo ( x.dtype ) .eps for the vector x ) .
My best guess it that you are taking the log of values = 0 .
But if you're trying to do something fancy that involves all the elements in a row ( like eliminating all rows that sum to more than 100 ) , it might be more complicated .
I am trying to plot the following numbers on a log scale as a scatter plot in matplotlib .
I think it might be good to plot both on a log scale .
The x-axes appear log scaled but the points do not appear -- only two points appear .
Also , how can I make this log scale appear on a square axes , so that the correlation between the two variables can be interpreted from the scatter plot ?
I'm not sure I understand understand what " Also , how can I make this log scale appear on a square axes , so that the correlation between the two variables can be interpreted from the scatter plot ?
I want the same for log scale axes .
Sometimes these things prove to be problems , even unexpected ones .
I set up a quick test case with 1000000 random integer points , and benchmarked the ones that I could run ( sorry , can't upgrade numpy at the moment ) .
@USER ' s method , slightly modifed by using ` ravel ` instead of ` flatten ` , yields a nice way to sort ` a ` in-place : #CODE
I'd be interested to know if it's possible to sort() a numpy array by multiple indices without making an intermediate copy .
@USER : I've modified my answer above to show how to sort a numpy array on multiple indices , in-place .
first it refuses to reshape .
Currently just generating 13 numbers then dividing by their sum
@USER - I think stack is not great .
And yes for the second problem , I need the sum of all of the elements in a given set to be equal to 1
Likewise , for your " rows sum to one " part ...
I thought I should get 0-based indices for the bins , and I only wanted 3 bins .
I'm assuming this is due to some trickyness with how bins are represented in linspace vs . digitize .
You're getting the 3rd bin for the maximum value in the array ( I'm assuming you have a typo there , and max_x should be " max ( my_array [: , 0 ])" instead of " max ( my_array [: , 1 ])") .
The key point here is to use the ` weights ` argument : each value ` a [ i ]` will contribute ` weights [ i ]` to the histogram .
The histogram can then be plotted with something like : #CODE
If you only need to do a histogram plot , the similar hist() function can directly plot the histogram : #CODE
Clearly the string join operation creates a large performance bottleneck ; in my actual application it will dominate total runtime .
The one I pointed out in a comment to other answer as to encode the binary representation of the array as a Base64 text block .
It looks like string format is faster than string join if you have a well defined format such as in this particular case .
Dingle - For whatever reason I am not finding this to be faster than my original example of join and str .
:-) my HTTP knowledge is a little bit rusty now , but you can at least encode the raw floats in Base64 to get better bit-density than in decimal .
fortran - After looking at your suggestion a bit more , I'm confused about how in practice you would decode the data at the client side , given that the client will not necessarily be written in Python .
In this case I would need to know how to take a binary representation of a numpy array and translate it into something like a VB Variant .
Also , using the Barnes-Hut simulation will cut down the complexity of the simulation to O ( N log N ) from O ( N^2 ) .
My question now is , ( 1 ) how can I make sure my encoded numpy array will travel well to clients on different operating systems and different hardware , and ( 2 ) how do I decode the binary data on the client side .
What the ` tostring ` method of numpy arrays does is basically give you a dump of the memory used by the array's data ( not the object wrapper for Python , but just the data of the array . ) This is similar to the ` struct ` stdlib module .
say the line ` a = sin ( x )` the data went through a round trip from python to C .
Your sub-question : a = sin ( x ) , how many roundtrips are there .
The trick is to pass a numpy array to sin ( x ) , then there is only one ' roundtrip ' for the whole array , since numpy will return an array of sin-values .
Note , that in the real software I'm not actually using the exp function - that's here for illustration only !
This answer , as it is , is only a code dump and should have at least a few sentences explaining why and / or how it is usefull .
numpy.searchsorted should be preferred to digitize by performance reasons : #URL
I want " diff " to be computed up to two decimal places , that is compute it using ` % .2f ` of v1 and ` % .2f ` of v2 .
If I compute diff without rounding , then I print the results using " % .2f " to a file , which means I can get things like v1 = 0.982769777778 being printed as " 0.98 " and v2 = 0.985980444444 being printed as " 0.99 " , while diff is printed as " 0.00 " which is misleading .
If I use numpys reshape ( eg . matrix.reshape ( 9,260,745 ) or any other combination of 9,260,745 ) it doesn't yield the required structure since the above mentioned ordering is lost ...
Did I misunderstand the reshape method or can it be done this way ?
They are for splitting an array into equal subsections without re-arranging the numbers like reshape does ,
reshape , to quote its docs ,
Edit2 : Really shouldn't join with floats ...
@USER - Here's one way ( using the " join " array in the example ... ):
np.vstack ([ join [ name ] for name in join.dtype.names ]) .T
python numpy roll with padding
I'd like to roll a 2D numpy in python , except that I'd like pad the ends with zeros rather than roll the data as if its periodic .
If you want this to be more direct then maybe you could copy the roll function to a new function and change it to do what you want .
It could be more optimized by avoiding ` zeros_like ` and just computing the shape for ` zeros ` directly .
It looks like the concatenate calls are causing the double execution time .
Then , select the upper triangular part starting at the first diagonal , ( the default option in triu ) .
The row index will correspond to the number of padded zeros you want .
Pad seems to be quite powerful and can do much more than a simple " roll " .
The code below first pads a certain amount of zeros in the up , down , left and right margins and then selects the original matrix inside the padded one .
I want to get the index of the min value of a numpy array that contains NaNs and I want them ignored #CODE
For a sufficiently large array , the indices returned by ` np.argsort ` may themselve take up quite a lot of memory , and on top of that , indexing with an array will also generate a copy of the array that is being sorted .
You can ditch the square root calculation on the distance calculation ... the maximal sum will also have the maximal squared sum , although that only yields a constant speedup .
For smaller r's ( or N ) this might be okay , but if you absolutely need to find the max , as opposed to using an approximation heuristic , you should try branch and bound with different strategies .
because I would think that converting A to a numpy array to use numpy's sum function would be slower than just using the built-in Python sum function , but I could be wrong .
For each trajectory you could find the sum or mean of the distances for every pairing of it with another trajectory and use this as a fitness measure .
You could then do the same thing with trios : find the sum or mean of the accumulated distances for all trios ( among remaining possible trajectories ) that each trajectory is involved with and use that as the fitness measure to decide which ones to drop before moving on to foursomes .
r=5 people in a network with maximim compatibility / max sum of C ( 5 , 2 ) pair weights .
ValueError : shape mismatch : objects cannot be broadcast to a single shape
The last operation yields " shape mismatch : objects cannot be broadcast to a single shape " .
Alright : NumPy's array shape broadcast adds dimensions to the left of the array shape , not to its right .
This is a guess so I'm not going to put it as an answer , but it seems likely that not all zeros are of the same type .
Is there a list of how to translate the differences that exist ?
To prevent zeros from being stripped from the end of floats :
your ` np.set_printoptions ( precision=3 )` suppress the end zeros .. how do you get them to display like this ` [ 0.078 0.480 0.413 0.830 0.776 0.102 0.513 0.462 0.335 0.712 ]` ?
append versus resize for numpy array
I would like to append a value at the end of my ` numpy.array ` .
I am using ` resize ` method and then set the last index available to the new value .
Can you confirm that ` resize ` is the best way to append a value at the end ?
My simple timing experiment of append vs . resizing showed that resizing is about 3x faster and its the fastest way that I can think of to do this .
Use resize
so P_i would be the sum of all P_i_j for all j and Aj would be sum of all P_j for all i
so P_i would be the sum of all
P_i_j for all j and Aj would be sum of
I have a C++ function returning a std :: vector and I want to use it in python , so I'm using the C numpy api : #CODE
Your ` std :: vector ` object appears to be local to that function .
std :: vector already has its destructor .
@USER - Yes , ` std :: vector ` has a destructor , but the Python CObject does not , and that is what is responsible for deleting the vector .
Last question : I put a ` std :: cout ` inside the ` DeleteVector ` to check if it is called when I delete the python object .
You can use numpy's clip function to deal with it if you would like .
Y = clip ( Y , 0 , 1 )
suppose it's the sum of the difference between start coordinates and difference between end coordinates
" Different by at 3 or less " with tuples other than the ones in the same list appears to me a graph / 2d space problem .
@USER ' s intuition is interesting : to reduce the number of comparisons ( where by " comparing " two points we mean checking if their distance is ` = 3 `) , " virtually slice " the 2D plane into suitable " cells " , so that each point need only be compared with ones in adjacent " cells " .
reshape - get mean - reshape #CODE
is supposed to work for any array size , and reshape doesn't make a copy .
As a matrix of values : it's important for me to get a numpy.array so that I can easily transpose and access the columns that are numeric .
` transpose ( floats_and_ints ) # etc ..
I would like to simulate SQL's join by clause ( inner , outer , full ) purely in python .
to the fields used to join the array .
Now , an inner join is ( again , from a logical point of view ) a projection of their cartesian product -- in the general case , taking a predicate argument ` on ` ( which takes two arguments , one " record " [[ dict ]] from each table , and returns a true value if the two records need to be joined ) , a simple approach would be ( using per-table prefixes to disambiguate , against the risk that the two tables might otherwise have homonimous " fields ") : #CODE
Now , of course you don't want to do it this way , because performance is ` O ( M * N )` -- but , for the generality you've specified ( " simulate SQL's join by clause ( inner , outer , full )") there is really no alternative , because the ` ON ` clause of a ` JOIN ` is pretty unrestricted .
For outer and full joins , you need in addition to keep info identifying which records [[ from one or both tables ]] have not been yielded yet , and otherwise yield -- e.g. for a left join you'd add a bool , reset to ` yielded = False ` before the ` for r2 ` inner loop , set to ` True ` if the ` yield ` executes , and after the inner loop , ` if not yielded : ` , produce an artificial joined record ( presumably using ` None ` to stand for NULL in place of the missing ` v2 ` values , since there's no ` r2 ` to actually use for the purpose ) .
To get any substantial efficiency improvements , you need to clarify what constraints you're willing to abide on regarding the ` on ` predicate and the tables -- we already know from your question that you can't live with a ` unique ` constraint on either table's keys , but there are many other constraints that could potentially help , and to have us guessing at what such constraints actually apply in your case would be a pretty unproductive endeavor .
The primary use case and hence core constraint is an ' inner ' join on two datasets .
And a nonconstraint is the post join order of records .
@USER , consider preprocessing each table into a dict mapping the value of the relevant key into the set of rows with that value ( or indices thereof , etc ) -- walking on both those dicts in sorted keys order " in sync " ( a typical case of "" merge sorted lists "") gives you a well-performing way to do any kind of join .
Finding a list of indices from master array using secondary array with non-unique entries
Were I to generate a list of id numbers of length m separately and need the information in the ` data ` array for those ids , what is the best method of getting a list of indices ` idx ` of the original array of ids in order to extract ` data [ idx ]` ?
This works well since there are no repeated elements in ` a ` as it is a list of unique id numbers .
So if I have a matrix ( list of lists ) where each column represents a unique word , each row represents a distinct document , and every entry is a 1 or 0 , indicating whether or not the word for a given column exists in the document for a given row .
How many unique words ?
Would like solution to handle 1000+ docs and could live with 100 words or less , averaging probably 30 unique words / doc .
Can provide min of 1-2GB , no problemo .
Split / strip on everything else .
Its just getting that list of elig . keyword sets & doc count / ids .. your hit22 approach sounds solid and cool / fun .. just don't know what to google :)
Numpy array , how to select indices satisfying multiple conditions ?
If one wants to get the corresponding indices ( rather than the actual values of array ) , the following code will do :
I'd like to set the first three elements of the diagonal to be a certain color , the next three to be a different color , and the last three a different color .
I just got bitten by this trying to call some SSE-code from python , the problem seems to be that GCC wants to assume that the stack is aligned on 16-byte boundaries ( the largest native type on the architecture , i.e. the SSE-types ) , and calculates all offset with that assumption .
The answer seems to be to compile with #CODE which changes the function prologues to always align the stack to 16 bytes .
If you don't need to start with zeros you could use ` np.empty ` for a small speed boost .
One speedup would be to remove the ` sum ` portion , as in this implementation it sums a list of length 2 through ` INTERVAL_LENGTH ` .
Also , instead of starting by doing ` sum ( volume [ i+1 : j+1 ])` , just do ` vol = volume [ i+1 ] + volume [ j+1 ]` , as you know the initial case here will always be just two indices .
This doesn't answer your problem exactly , but I think especially with the sum issue that you should see significant speedups with these changes .
A log file is created that can be displayed graphically .
Jython is robust and I am fine with it being a few dot releases behind .
Rbf solves Aw = z to be exact at the cj ( the w coefs can vary a lot , print rbf.A rbf.nodes ); then for phi ( r ) = r , y ( x ) = sum ( wj |x - cj| ) , strange .
on SO says " Scipy 0.8 will have single precision support for almost all the fft code " ,
Of course , you could also just store the indices where the positive values start after finding them once with binary searches .
There is a std :: nth_element and std :: partial_sort ,
It does the histogramming , then produces a cumulative sum of the counts in each bin .
You'll need to do the cumulative sum of the resulting counts yourself .
lists various rules for the number of bins , e.g. ` num_bins ~ sqrt ( len ( a ) )` .
The final vertical line at ` max ( data )` was added manually .
Numpy transpose multiplication problem
I tried to find the eigenvalues of a matrix multiplied by its transpose but I couldn't do it using numpy .
Instead I got ` ValueError : shape mismatch : objects cannot be broadcast to a single shape ` when multiplying ` testmatrix ` with its transpose .
numpy join entries intersecting at a cell
In numpy , how can I join the entries that intersects at a cell ?
In the example , I want to join rows / columns B and F into one row / column BF , where each element is the average of the ones with the same color .
Pull out row F , transpose it and average it with column B .
I think you're going to have to roll your own unfortunately .
You can turn the vector into a sparse diagonal matrix and then use matrix multiplication ( with * ) to do the same thing as broadcasting , but efficiently .
and I want to find the indices of the element s that meet two conditions i.e. #CODE
If you want the indices of the masked values , you can do : #CODE
The values that are actually the problem appear to be ones I hadn't noticed before , that are actually " None " .
but I am wondering whether there isn't a way to write lines 3 to 6 in one line ( especially since I might want to concatenate 100 arrays ) .
I forgot how to transpose NumPy arrays , but you could do : #CODE
Like I said , I forgot how to transpose a numpy array .
A few versions ago numpy obtained ` assert_allclose ` which is now my favorite since it allows us to specify both absolute and relative error and doesn't require decimal rounding as the closeness criterion .
I know this is wrong ` data [ np.where ( max ( np.sum ( obj_slice ))]` ..
Why not remove the last row before the transpose ?
Note that the max values for ` row_n ` , ` col_n ` are ` m-1 ` and ` n-1 ` respectively , as zero indexing notation is used .
weave.inline ( code , [ ' Wijl ' , ' q ' , ' diff ' , ' mx ' , ' my ' , ' mz ' , ' h_x ' , ' h_y ' , ' h_z '] , type_converters= converters.blitz , compiler= ' gcc ')
The solution therefore is just to roll your own function ` f ` that works the way you desire .
The ` clip ` is relatively negatively impacted since it doesn't get to use ` L ` or ` G ` ( however calculating those on the same machine took only 17ms each , so it is not the major cause of speed difference ) .
When choosing to penalize the in-place methods instead of ` clip ` , the following timings come up : #CODE
Overall the multiplication methods are much faster than ` clip ` .
Instead of penalizing ` clip ` I penalized the in-place methods .
It does show that multiplication is still much faster than clip .
You can write multiple pickles to the same open file , one after the other ( use ` dump ` , not ` dumps `) , and then the " lazy evaluator for iteration " just needs to use ` pickle.load ` each time .
flush ( which writes to disk any changes you make to the array ); and
Also note the ` digitize ` function , shown in the accepted answer to this question : #URL
The correlation matrix of N data vectors is a symmetric N N matrix with unity diagonal .
The function Correlate of numpy works with 2 1D arrays that you want to correlate and returns one correlation value .
I need to take a csv file and import this data into a multi-dimensional array in python , but I am not sure how to strip the ' None ' values out of the array after I have appended my data to the empty array .
Then use ` reshape ` .
If you have non-numerical values in your data and want to handle them as strings , you'll need to use a structured array , specify which fields you want to be strings , and set a max length for the strings in the field .
In such a system , -1 would be represented as all ones .
Of course , so would the sum of a number and its bitwise negative , so we have the situation where #CODE
The ~ is the ones ' complement operator and if you're using with ints it can be used in any python program ( it's not exclusively of numpy )
For fitting y = A + B log x , just fit y against log x .
For fitting y = Ae Bx , take the logarithm of both side gives log y = log A + Bx .
So just fit log y against x .
@USER : Usually the natural log , but any log works .
Changing the base of log just multiplies a constant to log x or log y , which doesn't affect r^2 .
@USER What do you mean by " For fitting y = A + B log x , just fit y against log x .
@USER Yes , if you create ` q = log ( x )` then ` y ( q ) = A + Bq ` is a simple linear equation ( polyfit ) .
` In [ 27 ]: %timeit ylist =[ exp ( i ) for i in x ]
One way is to select multiple columns of the recarray and cast them as floats , then reshape back into a 2D array : #CODE
a single array rather than call append / insert / concatenate .
Preallocation mallocs all the memory you need in one call , while resizing the array ( through calls to append , insert , concatenate or resize ) may require copying the array to a larger block of memory .
Numpy : how to retrieve k max values from each row ?
I want to retrieve k max values from each row in a numpy array .
It uses many Numpy functions ( 3 ` reshape ` s and 2 ` repeat ` s ) to do this .
The difference is that we are no longer creating delta as a zeros matrix .
on 4.6 million rows with about 70 columns and found that the numpy path took 2 min 16s and the csv-list comprehension method took 13s .
For instance , if you just want to join the tables : #CODE
How can you find the indices ` {i1 , i2 , ..., iN} ` either in matlab or python numpy ?
So given a matrix M , one would need to find the indices of a set of ` N ` linearly independent column vectors .
If you want the column indices instead of the resulting square matrix , just replace ` return B ` with ` return colset ` .
Worse , I believe your original response suggested a determinant to determine singularity . det is a terrible thing to do to any matrix .
Besides , the QR method doesn't work in Python , because NumPy / SciPy's ` qr ` method doesn't give the ` E ` matrix from your solution .
By the way , Using det to determine if a matrix is singular is the absolutely , positively , definitely worst way you can do that .
Essentially , you should virtually NEVER use det in MATLAB unless you understand why it is such a bad thing to do , and you choose to use it despite that fact .
Also , i find that when coding ML algorithms , i need data structures that i can operate on element-wise and that also understand linear algebra ( e.g. , matrix multiplication , transpose , etc . ) .
Calculating e ( base of the natural log ) to high precision in Python ?
Using a series sum you could calculate it : #CODE
You realize the last digit of the series sum is wrong ?
then I get an other error : TypeError : list indices must be integers , not tuple
Assume that I have an affinity matrix A and a diagonal matrix D .
What I need is true matrix operation , not dot product or something like that ( array multiplication ) .
Isn't it the same for diagonal matrices ?
Numpy allows you to exponentiate a diagonal matrix with positive elements directly : #CODE
So , as long as your matrix D is diagonal , you should be able to directly use your formula .
D is actually a diagonal matrix .
Then you could do sqrt ( D ) instead of ( D** ( 1 / 2 ))
Based on previous comment this formula should work in case of D being diagonal matrix ( I have not chance to prove it now ) .
In your case , the matrix is diagonal , and so the square root of the matrix is just another diagonal matrix with the square root of the diagonal elements .
Needlessly slow , because you compute the max and then compare it to all of a .
The result will be stored in a new array of size ` max ( i ) +1 ` .
This would be a lot clearer if the values of ` d ` were unique .
np.bincount returns an array for all integers range ( max ( i )) , even if some counts are zero
For a general sum of labeled array , you can use scipy.ndimage.sum .
In the general case when you want to sum submatrices by labels you can use the following code #CODE
How to create the histogram of an array with masked values , in Numpy ?
In Numpy 1.4.1 , what is the simplest or most efficient way of calculating the histogram of a masked array ?
histogram ( m_arr.compressed() )
handling inside histogram will be a
histogram calculations .
masked array definition of a histogram
Both of these approaches attempt to convert each entry in myarray into a record with the given dtype , so the extra zeros are inserted .
So you have to transpose it .
i have a function defined by a combination of basic math functions ( abs , cosh , sinh , exp ... )
Numpy broadcast array
However , in this particular case , since ` l ( i , i )` and ` l ( j , j )` are just the diagonal elements of ` l ` , you could do this instead : #CODE
In other words , an array that has complex entries is first sorted based on the absolute value ( i.e. complex magnitude ) of those entries , and any entries that have the same absolute value are sorted based on their phase angles .
This does not directly apply to masked arrays , but I do not think it will be that hard to transpose there , and it is quite efficient , I've had no problem applying it to large 100MPix images .
If you want to divide the data set once in two halves , you can use ` numpy.random.shuffle ` , or ` numpy.random.permutation ` if you need to keep track of the indices : #CODE
But , doesn't the last method , using randint , have a good chance of giving same indices for both test and training sets ?
Fails with : " TypeError : list indices must be integers , not tuple "
The arrays do not have to be the same size , but they should broadcast against each other .
Unlike Joe Kington's answer , the benefit of this is that you don't need to know the original shape of the data in the ` .mat ` file , i.e. no need to reshape upon reading in .
how to extract frequency associated with fft values in python
I used ` fft ` function in numpy which resulted in a complex array .
the result has nonzero values at indices 0 , 2 and 6 .
and find the corresponding fft frequency , and then convert to Hertz : #CODE
I believe to convert to Hertz , you multiply by the frame rate and take the absolute value .
No sir , my aim is to read a wav file , perform fft on it , and with each frequency value i have to create a new wav file.i have done till performing fft.this is the output i got :
fft on numpy :
` idx= np.argmax ( np.abs ( w ) **2 )` -- why to square an array of absolute values for finding only index ?
` np.abs ( w )` is an array which contains only real numbers as an absolute value of a complex .
By fft , Fast Fourier Transform , we understand a member of a large family of algorithms that enable the fast computation of the DFT , Discrete Fourier Transform , of an equisampled signal .
Here we deal with the ` numpy ` implementation of the fft .
Of course ` numpy ` has a convenience function ` np.fft.fftfreq ` that returns adimensional frequencies rather than dimensional ones but it's as easy as #CODE
Efficient way to create a diagonal sparse matrix
If a sound sample consists mainly of one note , its FFT will have one coefficient which is very big ( in absolute value ) , and the others will be very small .
SciPy has a MATLAB ` .mat ` file translator , so you could dump your MATLAB workspace to a file and bring it in fairly easily with SciPy.io.mio #URL
You could just write that algorithm in matlab to create the same randn numbers as numpy does from the rand function in matlab .
So in order to get the same matrices in both , you have to transpose :
Then you can just dump your multidimensional arrays in their entirety .
I'm not familiar with NumPy , but I'd imagine it's got a function similar to Matlab's ` reshape ` function for remapping vectors to matrices .
Here's the stack trace of the error : #CODE
` min , argmin ` both raise ` ValueError ` on empty array
` std , var ` return ` nan ` on an empty array
Perform y_b = ifft ( fft ( x_b ) * h_scaled )
Drop padding P from either side of y_b and concatenate with y
i love numpy and still wonder why the various stack commands need a tuple . yes , it is faster then python list processing . maybe they are cases where it is fast to concatenate list then arrays , because the arrays are alinged in memory for fast access .
I have a numpy matrix that contains mostly nonzero values , but that occasionally will contain a zero value .
) count the non-zero values in each row , and put that count into a variable that I can use in subsequent operations , perhaps by iterating through row indices and performing the calculations during the iterative process
) count the non-zero values in each column , and put that count into a variable that I can use in subsequent operations , perhaps by iterating through column indices and performing the calculations during the iterative process
For example , one thing I need to do is to sum each row and then divide each row sum by the number of non-zero values in each row , reporting a separate result for each row index .
And then I need to sum each column and then divide the column sum by the number of non-zero values in the column , also reporting a separate result for each column index .
You can see that I am creating an array of zeros and then populating it from a csv file .
Some of the rows will contain values for all the columns , but other rows will still have some zeros remaining in some of the last columns , thus creating the problem described above .
The last 5 lines of code below are from another posting on this forum , and those last 5 lines of code return a printed list of row / column indices for the zeros .
But I do not know how to use that resulting information to create the nonzero row counts and nonzero column counts described above .
The faster way is to clone your matrix with ones instead of real values .
Then just sum up by rows or columns : #CODE
Perhaps you will need to translate NumNonZeroElementsByColumn into 1-dimensional array by #CODE
A fast way to count nonzero elements per row in a scipy sparse matrix ` m ` is : #CODE
The ` indptr ` attribute of a CSR matrix indicates the indices within the data corresponding to the boundaries between rows .
Similarly , for the number of nonzero elements in each column , use : #CODE
A stupid question though ; how do I append items to a numpy array ?
One way to go is add an x_sum and y_sum to your " clusters " object and sum the coordinates as points are added .
If things are moving around , you can also update the sum as points move .
If your points are numpy vectors that can be added , then you don't even need to sum the components , just maintain a sum of all the vectors and multiply be 1 / len at the end .
( the unique values in col1 are 100002 and 10002 , and in col2 are 2006,200 7 and 200 8) .
If you didn't know the unique values in the first column beforehand , you can use either ` numpy.unique1d ` or the builtin function ` set ` to find them .
Edit : I just realized that you wanted to select data where you have unique combinations of two columns ...
I think that's the transpose of what the question asked for .
What I'd like to do is replace the list comprehension with something like numpy.take , however take seems to only deal with single dimension indices .
Is there something like take that will work with multidimensional indices ?
Also I know you could do this with a transpose , slice and then reshape , but that was slow so I'm looking for a better option .
Since ` math.sqrt ` is called in the innermost loop , you should have ` from math import sqrt ` at the top of the module and just call the function as ` sqrt ` .
Instead of sqrt you could use x** 0.5 , which is , if I remember correct , slightly faster .
Third : You can get rid of the sqrt as Philip mentioned by squaring ` analysisdist ` beforehand .
in other words , the duplicates in ` indices ` are ignored
` numpy `' s ` histogram ` function is a scatter operation .
` a += histogram ( indices , bins= a.size , range =( 0 , a.size )) [ 0 ]`
You may need to take some care because if ` indices ` contains integers , small rounding errors could result in values ending up in the wrong bucket .
` a += histogram ( indices , bins= a.size , range =( - 0.5 , a.size-0.5 )) [ 0 ]`
A custom C routine in numpy could easily be twice as fast still , eliminating the superfluous allocation of and multiplication with ones , for starters , but it makes a world of difference in performance versus a loop in python .
If You have matrices with lots of zeros use scipy.sparse.csc_matrix .
I tried " append " ing to a different array , say y1 , and then y = y1.copy() .
i.e. zero_crossings will contain the indices of elements after which a zero crossing occurs .
Another way to count zero crossings and squeeze just a few more milliseconds out of the code is to use ` nonzero ` and compute the signs directly .
On my machine these are a bit faster than the ` where ( diff ( sign ))` method ( timings for an array of 10000 sine samples containing 20 cycles , 40 crossings in all ): #CODE
Since a ) using numpy.signbit() is a little bit quicker than numpy.sign() , since it's implementation is simpler , I guess and b ) it deals correctly with zeros in the input array .
However there is one drawback , maybe : If your input array starts and stops with zeros , it will find a zero crossing at the beginning , but not at the end ...
I have a bit of code that attempts to find the contents of an array at indices specified by another , that may specify indices that are out of range of the former array .
print input [ indices ]
So I thought I could use masked arrays to hide the out of bounds indices : #CODE
So I'm having to fill the masked array first , which is annoying , since I don't know what fill value I could use to not select any indices for those that are out of range :
print input [ np.ma.filled ( indices , 0 )]
So my question is : how can you use numpy efficiently to select indices safely from an array without overstepping the bounds of the input array ?
Without using masked arrays , you could remove the indices greater or equal to 5 like this : #CODE
Edit : note that if you also wanted to discard negative indices , you could write : #CODE
In your test , you're filtering ` indices ` to find the entries matching a condition .
Using ` indices.filled ( 0 )` means that when an item of ` indices ` is masked ( as in , undefined ) , you want to take the first index ( 0 ) as default .
I had assumed that using masked arrays to index would effectively compress them first , which seems like a reasonable default .
It's going to be quite a lot faster to compute the determinant of the first matrix in the sequence , and then compute the difference det ( M_{n+1 } ) - det ( M_n ) , which is the product of the changed coefficient and its minor .
plotting histograms whose bar heights sum to 1 in matplotlib
I'd like to plot a normalized histogram from a vector using matplotlib .
but neither option produces a y-axis from [ 0 , 1 ] such that the bar heights of the histogram sum to 1 .
I'd like to produce such a histogram -- how can I do it ?
this does not normalize the height of your histogram .
@USER : of course , it does not normalize the histogram .
This will indeed produce a bar-chart histogram with a y-axis that goes from ` [ 0 , 1 ]` .
Further , as per the ` hist ` documentation ( i.e. ` ax.hist ? ` from ` ipython `) , I think the sum is fine too : #CODE
Remember that ` normed=True ` doesn't mean that the sum of the value at each bar will be unity , but rather than the integral over the bars is unity .
As already stated in the answer , normed=True means that the total area under the histogram is equal to 1 but the sum of heights is not equal to 1 .
However , I wanted to , for convenience of physical interpretation of a histogram , make one with sum of heights equal to 1 .
This diverted me to : Matplotlib - Stepped histogram with already binned data
This has worked wonderfully for me though in some cases I have noticed that the left most " bar " or the right most " bar " of the histogram does not close down by touching the lowest point of the Y-axis .
If you want the sum of all bars to be equal unity , weight each bin by the total number of values : #CODE
In Numpy , how do I create an array of indices which can be used return the values of the source array in sorted order ?
Take a look at numpy.argsort - it will return the indices that would sort your array .
But the array will have different numbers of rows every time it is populated , and the zeros will be located in different rows each time .
The five indicates that all possible values in row 0 are nonzero , while the four indicates that one of the possible values in row 1 is a zero .
` any ` is much more clear than ` sum ` in this use .
So you could find the indices of the rows which have a 0 in them , put them in a list or a tuple and pass this as the second argument of the function .
The code leaves the client code free to call it with ` v ` equal to anything that can be passed to ` sum ` .
I am new to Python , but the return is calculated incorrectly : it should be max ( min ( v ) , q1-whisker ) for the lower bound and min ( max ( v ) , q3+whisker ) for the upper bound .
Numpy arrays do not have a method ' append ' like that of lists , or so it seems .
Downvoted because they are not the same , * is element-wise multiplication , dot is the true matrix multiplication .
For arrays ( prior to Python 3.5 ) , use ` dot ` instead of ` matrixmultiply ` .
Its unreadable when you have a stack of multiplications , for instance x ' *A ' *A*x .
By the way , why is matrix multiplication called " dot " ?
In what sense is it a dot product ?
@USER - Matrix multiplication is sometimes referred to as a " dot product " in textbooks ( in those books , the dot product you're thinking of is called a " scalar product " or " scalar dot product ") .
The scalar dot product is just matrix multiplication of two vectors , after all , so using " dot " to mean matrix multiplication in general isn't much of a stretch .
@USER - Is the scalar dot product really just matrix multiplication of two vectors ?
If the * operator is matrix multiplication , then ` u.T * v ` would be the scalar dot product , but ` u * v ` would be undefined ( since the dimensions don't match ) .
If matrix multiplication is defined as a sum over the last axis of the left array with the second-to-last axis of the right array , then I would assume that matrix multiplication of 1-D numpy arrays would fail because the right array does not have a second-to-last axis .
The inner product of two 1D vectors is strictly identical to the scalar dot product .
( Incidentally , ` u.T ` and ` u ` are identical if ` u ` is one-dimensional . " Row vectors " and " column vectors " are 2D , and aren't vectors , strictly speaking . In numpy , vectors are vectors , and don't have a 2nd dimension , so you have to reshape them into 2D to have a row vector or column vector . )
Maybe I would be less confused if it were more common for books to refer to " dot " for matrix multiplication as you describe .
@USER the point is that dot [ generalizes to arbitrary dimensionality ] ( #URL ) without ambiguity .
For me dot also seems unnatural name , and it was confusing at first .
An important point to note here is * is element-wise multiplication , dot is the true matrix multiplication .
matrix norm : #CODE
qr factorization : #CODE
An important point to note here is * is element-wise multiplication , dot is the true matrix multiplication .
There is a situation where the dot operator will give different answers when dealing with arrays as with dealing with matrices .
To be specific , * is element-wise multiplication , dot is the true matrix multiplication .
Update : As mentioned in my comment below , I should have stated that I'm trying to do this on 2D arrays , and therefore get a set of 2D indices back .
Looks like I should have mentioned in my question that this is 2D array ... and I'd like to get the 2D array indices out for every cell , randomly without replacement .
For a 2D array I think it will depend on what you want or need to know about the indices .
You can then use the list ` idx ` to iterate over randomly ordered 2D array indices as you wish , and to get the values at that index out of the ` data ` which remains unchanged .
Note : You could also generate the randomly ordered indices via ` itertools.product ` too , in case you are more comfortable with this set of tools .
If you truly want to sample , as you say , all the indices as 2d pairs , the most compact way to do this that i can think of , is : #CODE
I'm assuming that you have a multi-dimensional array from which you want to generate an exhaustive list of indices .
You'd like these indices shuffled so you can then access each of the array elements in a randomly order .
The only downside to this approach is that if ` d ` is large , then ` indices ` may become pretty sizable .
I am creating a 2d array full of zeros with the following line of code : #CODE
If one matrix does not contain data for the last test condition , I want to make sure that its contents are not treated as zeros when the averaging is done .
Plotting mplot3d / axes3D xyz surface plot with log scale ?
There are a loads of posts detailing semilog / loglog plotting of data in 2D e.g. plt.setxscale ( ' log ') however I'm interested in using log scales on a 3d plot ( mplot3d ) .
My ' Y ' data spans several orders of magnitude ( like 9 ! ) , so it would be very useful to plot it on a log scale .
I can work around this by taking the log of the ' Y ' , recreating the numpy array and plotting the log ( Y ) on a linear scale , but in true python style I'm looking for smarter solution which will plot the data on a log scale .
Is it possible to produce a 3D surface plot of my XYZ data using log scales , ideally I'd like X Z on linear scales and Y on a log scale ?
For instance , if you want that x and y axis are on log scale , you should write : #CODE
The log scaling for Axes in 3D is an ongoing issue in matplotlib .
` ax.set_yscale ( ' log ')` will cause an exception in 3D
You can get the transpose of summaryMeansArray with ` summaryMeansArray.T ` .
But what I can't figure out how to do is use a student id number to refer to a particular student ( assume that two of my students have student ids as shown ): #CODE
I know that I could create a dict mapping student ids to indices , but that seems fragile and crufty , and I'm hoping there's a better way than : #CODE
I am going to be needing to sum all the assignment points for over a hundred students once a day , as well as run standard deviations and other stats .
Here's why : Everything you want to do for one student ( like find the sum of all assignment points ) , you'll probably want to do for every student .
Alternatively , @USER ' s suggestion above is another ( more simple and direct ) way of handling it , if you decide to roll your own .
RuntimeError : Optimal parameters not found : Both actual and predicted relative reductions in the sum of squares are at most 0.000000 and the relative error between two consecutive iterates is at most 0.000000
You could also use an fft with zero padding and quadratic fitting around the peak to find the frequency if you want another method .
I have a 2d numpy array of bools , and I'd like to know how many unique rows my data set contains and the frequency of each row .
One way to get the unique rows is to view things as a structured array ...
My only concern is : it possible to perform the same using unique as an array and not as ` dict() ?
` I'm getting in trouble to print unique dictionary without the dictionary format .
Linearize the system , and fit a line to the log of the data .
( i.e. ` y = A * exp ( K * t )` can be linearized by fitting ` y = log ( A * exp ( K * t )) = K * t + log ( A )` , but ` y = A*exp ( K*t ) + C ` can only be linearized by fitting ` y - C = K*t + log ( A )` , and as ` y ` is your independent variable , ` C ` must be known beforehand for this to be a linear system .
Just to give an example , let's solve for y = A * exp ( K * t ) with some noisy data using both linear and nonlinear methods : #CODE
Because it was doing the things I mentioned , resulting in a solver for the ` exp ` function , _not_ a general-purpose solver for _any_ possible function , which is what scipy's curve_fit is .
So , in the outer optimization only the offset needs to be chosen with a non-linear optimizer .
@USER - how do I write an outer optimization , though ?
The idea is to translate the ' noisy ' data into log and then transalte it back and use polyfit and polyval to figure out the parameters : #CODE
You can surely translate the math into python .
where ` C = exp ( -c )`
You could join the effort , since they don't seem to have many OS 10.5 users available for testing .
If you use sum_x = float ( sum ( x )) and sum_y = float ( sum ( y )) , it works .
In ` sum ( x ) / len ( x )` you divide ints , not floats .
So ` sum ([ 1 , 5 , 7 ]) / len ([ 1 , 5 , 7 ]) = 13 / 3 = 4 ` , according to integer division ( whereas you want ` 13 . / 3 . = 4.33 ... `) .
To fix it rewrite this line as ` float ( sum ( x )) / float ( len ( x ))` ( one float suffices , as Python converts it automatically ) .
Note that Python has a builtin ` sum ` function .
If I wanted to use ` col_deriv=0 ` , I think that I would have to basically take the transpose of what I return with dfunc .
You may need to reshape to get the arrays to line up for broadcasting .
This says the minima occur at indices [ 0 , 0 , 3 ] , [ 0 , 4 , 0 ] , [ 1 , 1 , 1 ] and [ 1 , 3 , 3 ]: #CODE
Create a mask and use the compress function of the numpy array .
Because this is a O ( N log M ) algorithm , where N is the number of elements in A and M is the number of elements in M , putting together a bunch of individual masks is O ( N * M ) .
The ` add_x ` function is used to ensure that the array indices loop around ( using clock-face arithmetic ) , as the grid has periodic boundaries ( anything going off one side will re-appear on the other side ) .
It can allow static typing of the indices and efficient , direct access to a numpy array's underlying data buffer at c speed .
In that example , which is doing operations very similar to what you're doing ( incrementing indices , accessing individual elements of numpy arrays ) , adding type information to the index variables cut the time in half compared to the original .
I suspect you'd get the most out of adding type information your indices ` x ` , ` y ` , ` orig_x ` and the numpy arrays .
I'd like to sum one particular row of a large NumPy array .
How does numpy zeros implement the parameter shape ?
Efficient way to count unique elements in array in numpy / scipy in Python
I want to count the number of occurrences of each unique element in the array .
For arrays of ones and zeros you can encode the values : #CODE
Please help - perhaps you have the eye to see the problem where I cannot .
This way they correlate properly .
I prefer ( 2 ) which is declare an numpy.ndarray of 2 axises first then append data later .
From the append documentation : #CODE
That is , why your example ( 1 ) fails is that as you don't specify an axis argument for the first append , x is flattened , and hence the second append fails as the shapes no longer match .
It's much more efficient to allocate the correct size upfront and then use slicing to fill it in rather than using append which implies a reallocation and copy every time .
This NAMBE is the absolute difference between a base vector and another vector , divided by the base vector and multiplied by a hundred , in pseudo-code notation : #CODE
` sum ( a-b ) / sum ( a )` is not the same as ` sum (( a-b ) / a )` .
Then ` sum ( a / b ) == sum ([ -1 , 0 ]) == -1 ` , but ` sum ( a ) /( sum ( b ) == 1 / 0 ` which doesn't make sense .
#URL > sum ( a ) / sum ( b )
( In the example above , n = 4 . ) I would like to know the answer in order to compress my data as much as possible .
Also , there is a very , very small speed-up by using the sum method of the resultant numpy array rather than using numpy.sum .
` sum ( b << i for i , b in enumerate ( x ))`
if ` x ` is no multiple of 8 you have to pad in zeros
Regardless of rank , the ` .T ` attribute and the ` .transpose() ` method are the same they both return the transpose of the array .
Can you please suggest me how to make it ( conserving the correspondence betweens the bins of the three vectors ? I mean a sort of 3D histogram )
I needed to ` np.clip ` only a set of the array , but even using ` np.ma.clip ` on a masked array will still clip the masked values .
for Fortan-like ( column-major ) indexing , the indices just need to be swapped : #CODE
In the above , index_tuple and a_matrix are a tuple containing the indices of interest and the matrix being indexed , respectively .
Converts a tuple of index arrays into an array of flat indices , applying boundary modes to the multi-index .
you can get the desired indices by #CODE
In python ( with matplotlib ) the closest I got was with a simple histogram : #CODE
I also tried the normed=True parameter but couldn't get anything other than trying to fit a gaussian to the histogram .
I want to generate an array with the index of the highest max value of each row .
Both matrix objects and ndarrays have ` .T ` to return the transpose , but matrix
objects also have ` .H ` for the conjugate transpose , and ` .I ` for the inverse .
The only disadvantage of using the array type is that you will have to use dot instead of * to multiply ( reduce ) two tensors ( scalar product , matrix vector multiplication etc . ) .
The main argument for using ` matrix ` would be if your code is heavy in linear algebra and would look less clear with all the calls to the ` dot ` function .
which I must merge ( or stack , if you wish ) with a one-dimensional array : #CODE
It seems there is not other way to transpose the given array , so that hstack is able to digest it .
Matlab for example would return an array of just ones since it evaluates left from right .
However , since I am calling the " log " function in several places , I'm not sure where the error is coming from .
I assume the warning is caused by taking the log of a number that is small enough to be rounded to 0 or smaller ( negative ) .
That will give you a stack trace pointing to the place in your code that called log with the improper value .
Is it possible that you're calling some other log function that isn't in numpy ?
Putting ` np.seterr ( invalid= ' raise ')` in your code ( before the errant ` log ` call )
Perhaps this Matlab toolbox will help ; it's quite easy to translate Matlab into Python , generally .
taking log of very small values using numpy / scipy in Python
I have an Nx1 array that corresponds to a probability distribution , i.e. the sum of the elements sums to 1 .
I find that when I take log ( my_array ) , I get the error " FloatingPointError : invalid value encountered in log " .
I'd like to represent vectors corresponding to a probability distribution and their take log without rounding to 0 , since then I end up taking log ( 0 ) which raises the error .
Python seems happy taking log of 10^-very large : #CODE
The second solution is to handle zeros explicitly , for example replace 0 .
* np.log ( 0 ) with zeros in the resulting array , or only include points that have nonzero probability in the probability array
Depending on what you're doing afterwards , you could use a different transform that doesn't explode on zero values like log does .
If you're just looking to visualize the data , you could always add some tiny value before you take the log .
If you want to alter e.g. a list directly , you need to use indices ( although you shouldn't need ` range ( len ( ... ))` , you can use ` enumerate ( the_list )` and you get indices and the current items ) .
So , in this solution , I wouldn't be counting to total_samples ( which is the total number of samples at the base rate ) but to the total number of samples I expect in the file -- the sum of the lengths of all the channels .
You can now build the array of stream indices for the first N bytes by something similar to #CODE
I think the log ( f ) on the last line should be changed to numpy.log ( f )
When I call the built-in numPy svd function , using the array I built from my image ( where each element is a tuple ) , I get the following error : #CODE
File " C :\ Python24\Lib\ site-packages \numpy\linalg\ linalg.py " , line 720 , in svd
For a task such as face recognition , people often vectorize * entire images* , then concatenate those vectors into a large matrix ` X ` of size ( h*w ) -by- ( num images ) , * then * perform PCA on ` X ` which is equivalent to the SVD of ` X X^T ` .
You have to decide SVD of what matrix ( of the three possible ones : R G and B ) you need .
I think you must have another file named ` parser.py ` somewhere in your files ( the ones that Python can find ) .
I read somewhere that I just need to index my array using arrays / lists of indices for both rows and columns , but that doesn't seem to work : #CODE
Clearly , this mechanism doesn't carry well into the case of an array of indices .
I think I'm gonna use Justin's answer because if my rows / columns are stored in an array , I'd have to reshape it every time .
I tried using fft module from numpy but it seems more dedicated to Fourier transforms than series .
Maybe it a lack of mathematical knowledge , but I can't see how to calculate the Fourier coefficients from fft .
} for n=1 , 2 , 3 , 4 , 5 , 6 ( using Sum ( c_n exp ( i 2 pi n x ) ) as Fourier series ) .
In the end , the most simple thing ( calculating the coefficient with a riemann sum ) was the most portable / efficient / robust way to solve my problem : #CODE
This method is most effective when there are many more elements than there are unique elements .
In this case , NumPy only loops once over the elements of ` ar3 ` , so it can't broadcast because it does not even know how many elements will be assigned .
The hurdle for me has been finding a quicker way to do that operation since I have to divide each entry for a given year only by the sum of entries corresponding to that year .
I thought the ' cimport numpy ' tells Cython to use C functions for numpy , instead of the Python ones .
which I append after each loop calculation .
+1 : ismember does all kinds of additional stuff , like calling ` unique ` that are not needed in your case , you you can definitely streamline the Matlab ( or numpy ) code .
This is a O ( N log N ) solution .
In Python , how do I join two arrays by key column ?
Basically , an join using the first column as key .
The bad news is that it will only work for extensions written in Python-like code that Cython can translate .
Discrete correlation is simply a vector dot product : #CODE
This means re-casting x values as a percentage of the max x value , while re-casting y values as a percentage of the x-values in the original data .
@USER : I added a ` resize ` function which allows you to stretch / shrink and shift the raw data to fit any bounds you wish .
If you have to roll your own , perhaps this article on Logistic regression will give you some ideas .
Edit : A ` resize ` function was added so that the raw data could be rescaled and shifted to fit any desired bounding box .
` leastsq ` attempts to minimize the sum of the squares of the residuals ( differences ) .
It searches the parameter-space ( all possible values of ` p `) looking for the ` p ` which minimizes that sum of squares .
Since transpose doesn't copy the array like I assumed there is no reason to anything as crazy as I was doing .
I'm actually a rookie in C , but I managed to write the function which normalizes every row in a matrix to sum to 1 .
If by normalise you mean divide the whole row by its sum , then you can write fast vectorised code which looks something like this : #CODE
Make explicit copies of ` x ` and ` y ` in ` resize ` : #CODE
The original ` resize ` repeats itself .
The solution is to make ` resize ` work on just one array , and call it twice ( or as needed ): #CODE
) re-size the data based on new max and min coordinates for x and y .
This needs to be reproducible when re-sized across a spectrum of possible max and min values .
the " where " function should return the indices of values less than or equal to 0 , and the assignment should just set those values to -inf . thanks .
I have a list of several hundred 10x10 arrays that I want to stack together into a single Nx10x10 array .
This is a simple way to stack 2D arrays ( images ) into a single 3D array for processing .
I am using the following code to digitize an array into 16 bins : #CODE
The ` np.flatnonzero ` function returns an array of indices specifying the locations at which the given array is non-zero ( or True ) .
Overflow in exp in scipy / numpy in Python ?
I'm computing a ratio in log form , i.e. log ( a ) + log ( b ) and then taking the exponent of the result , using exp , and using a sum with logsumexp , as follows : #CODE
Their log will be -Inf .
Well , just take a look at the result of log ( a ) -log ( b ) , then you will know why it is overflowing .
In your case , it means that ` b ` is very small somewhere in your array , and you're getting a number ( ` a / b ` or ` exp ( log ( a ) - log ( b ))`) that is too large for whatever dtype ( float32 , float64 , etc ) the array you're using to store the output is .
Isn't ` exp ( log ( a ) - log ( b ))` the same as ` exp ( log ( a / b ))` which is the same as ` a / b ` ?
@USER Cournapeau : Meaning that the answer that uses logs and exp is less precise , right ?
Since the numbers are very big / smalls , one generally takes the log to stay in a " reasonable " range , the so-called log domain : #CODE
Problems still arise because exp ( -a ) will still underflows up .
For example , exp ( -1000 ) is already below the smallest number you can represent as a double .
gives -inf ( log ( 0 + 0 )) , even though you can expect something like -1000 by hand ( -1000 + log ( 2 )) .
The function logsumexp does it better , by extracting the max of the number set , and taking it out of the log : #CODE
Then exp ( -10 ) will be not ` overflow ` !
To access it as a two-dimensional Fortran-ordered ` n x m ` matrix , you can reshape it : #CODE
Now , let's say for the sake of argument that ` np.fromfile() ` gives a ` C ` matrix ( does it ? can I change that ? ) Now , ` reshape ` changes the striding .
I assume that the transpose also works by changing the striding ( or am I wrong here ? ) So the end result is still a ` C ` matrix , no ?
So , we reshape it as jxi , and then transpose it to be ixj .
Additionally , many numpy functions take an ` out ` parameter , so you can do things like ` np.abs ( x , x )` to take the absolute value of ` x ` in-place .
To do the actual calculation , we need the square root of the sum of squares of differences ( whew ! ) between pairs of coordinates in the two vectors .
We can use ` zip ` to pair the coordinates , and ` sum ` with a comprehension to sum up the results .
( The transpose assumes that points is a Nx2 array , rather than a 2xN .
I want to sum the items in the first array by group ( the second array ) and obtain n-groups results in group number order ( in this case the result would be [ 3 , 9 , 9 ]) .
There's nothing wrong with looping over the unique groups , though .
With the loop you're only looping ( in python , anyway ) over the number of unique groups .
This is a vectorized method of doing this sum based on the implementation of numpy.unique .
According to my timings it is up to 500 times faster than the loop method and up to 100 times faster than the histogram method .
The i-th element of the output is the sum of all the ` data ` elements corresponding to " id " ` i ` .
Bi_Rico and Sven : perform good , but will only work for Int32 ( if the sum goes over 2^32 / 2 it will fail )
Alex : is the fastest one , good for sum .
This is good because you have many statistics to group ( sum , mean , variance , ... ) .
In ` numpy ` , what's the most efficient way to compute ` x.T * x ` , where ` x ` is a large ( 200,000 x 1000 ) dense ` float32 ` matrix and ` .T ` is the transpose operator ?
@USER , surely , it could truncate it .
" or " why does ` a + b ` denote the sum a ` a ` and ` b ` ?
Numpy solver : max value
Then , replace every occurence of ` mat [ row*ncol + col ]` in your C code by ` mat [ row*rowstride + col*colstride `] .
Try ` eigh ` instead of ` eig ` .
Upper diagonal of a 2D numpy array
This seems simple ( and is trivial to write a three-line loop for ) , but how can I use numpy slicing make a list of the index locations of the upper diagonal of a numpy array ?
` scipy.linalg.triu ` and ` nonzero `
Also one can get the requested indices by simply unziping the answer , ie .
gives you a column vector , so that you can do concatenate or hstack operation .
If ` a ` is a two-dimensional NumPy array , you can use ` numpy.hstack ` to add zeros to left and the right : #CODE
For the sake of example , I added ` 2 ` zeros to the left and ` 1 ` zero to the right .
The ` h5dumpImport ` package imports an ASCII dump of a dataset generated by the ` h5dump ` command line tool .
I have an NxM array in numpy that I would like to take the log of , and ignore entries that were negative prior to taking the log .
When I take the log of negative entries , it returns -Inf , so I will have a matrix with some -Inf values as a result .
I then want to sum over the columns of this matrix , but ignoring the -Inf values -- how can I do this ?
Is there a nicer solution ( i.e. getting rid of ` transpose `) to this problem ?
this will truncate the data array and add the the noise to the original array after the 10th location , you could then add the rest of the data array to the new array if you need it .
I think one of the weaknesses of numpy arrays is that they are not easy to resize :
The idea is to replace data that you want deleted by shifting the remainder of the array , leaving zeros ( or garbage ) at the end of the array .
As long as you can hold a single temporary copy of your data in memory ( for the diff ) , and a boolean array of the same length as your data ( 1-bit-per-element ) , it should be pretty efficient ...
Simply pad each row with 3 zeros so that the spectral multiplication doesn't wrap around , and you'll get what the orginal question asked for .
For zero padding , I just need to add n =( length* 2-1 ) to fft ??
For a 1-D sequence with n variables , this solution would pad with n-1 zeros .
Good call using the ' n= ' option of the fft instead of padding by hand .
You could stack the two together and then sort .
The answer by mishaF is only missing the last step -- making the entries of the last column unique .
How to dump a boolean matrix in numpy ?
I want to dump it to a file so that I can fiddle with it , but for the life of me I can't work out how to make numpy dump it in a recoverable fashion .
Finally , I've tried ` networkx.from_numpy_matrix ` with ` networkx.write_dot ` , but that gave each edge ` [ weight=True ]` in the dot source , which broke ` networkx.read_dot ` .
Firstly I load a lot of data to memory , convert it to ` int8 ` numpy array with only ` 0 ` and ` 1 ` as possible values and then dump it to HDD using two approaches .
( You can always do map ( sum , x ) . )
I remember that for a while , I needed to explicitly append the path to the site-packages directory at the top of my scripts for Python to know where to look .
I know about vstack , concatenate etc .
type , filled with zeros .
type , filled with ones .
No , there is nothing quite like append in Numpy .
There are functions that concatenate arrays or stack them by making new arrays , but they do not do so by appending .
Katrielalex can you please modify your answer to include that there is nothing quite like ` append ` in numpy .
@USER : well , there is an ` append ` in numpy .
It's just that it's less efficient not to preallocate ( in this case , much less efficient , since ` append ` ing copies the entire array each time ) , so it's not a standard technique .
If you want it filled with zeros , do as katrielalex said :
This assumes you want to initialize with zeros , which is pretty typical , but there are many other ways to initialize an array in numpy .
If you don't know the size of big_array in advance , it's generally best to first build a Python list using append , and when you have everything collected in the list , convert this list to a numpy array using ` numpy.array ( mylist )` .
The way I usually do that is by creating a regular list , then append my stuff into it , and finally transform the list to a numpy array as follows : #CODE
For example : ` test_array = numpy.array ([[ 10 , 1 ] , [ 2 , 12 ] , [ 3 , 5 ]])` and I'd like to do something along the lines of ` test_array.where ( min ( test_array [ 0 ] * test_array [ 1 ]) )` and have it return that same structure that ` numpy.where ` returns pointing to ` [ 10 , 1 ]` .
The sparse matrix dimensions are : 100000x500000 , which is quite huge , so I really need the most efficient way to sum all the resulting sparse matrices into a single sparse matrix , using some C-compiled method or something .
A simple and efficient way to add sparse matrices is to convert them to sparse triplet form , concatenate the triplets , and then convert back to sparse column format .
How to get the first-order optimality from that equation when I use that equation to get min v , R using scipy.optimize.leastsq ?
I don't have problem to get min v , R using scipy.optimize.leastsq but I get confused when I want get first-order optimality ...
Find indices of elements equal to zero from numpy array
NumPy has the efficient function / method ` nonzero() ` to identify the indices of non-zero elements in an ` ndarray ` object .
What is the most efficient way to obtain the indices of the elements that do have a value of zero ?
I want to shuffle each of them , such that corresponding elements continue to correspond -- i.e. shuffle them in unison with respect to their leading indices .
If you want to avoid copying arrays , then I would suggest that instead of generating a permutation list , you go through every element in the array , and randomly swap it to another position in the array #CODE
By resetting the state , you ensure that the calls to the random number generator will give the same results in the second call to ` shuffle() ` , so the whole algorithm will generate the same permutation .
The first approach I tried was SQL with indices using SQLite , which didn't do too bad , but as the filters return a lot of row the performance dropped .
So that min ( x ) = min ( x [ i - n ] : x [ i + n ]) .
Update- I wasn't happy with gradient so I found it more reliable to use numpy.diff .
Regarding the issue of noise , the mathematical problem is to locate maxima / minima if we want to look at noise we can use something like convolve which was mentioned earlier .
Do you know how this gradient is calculated ?
If you have noisy data probably the gradient changes a lot , but that doesn't have to mean that there is a max / min .
For that I guess use convolve .
The " +1 " is important , because " diff " reduces the original index number .
But if you run the functions you provided you get maximas at indices 2 , 6 and minimas at indices 1 , 3 , 5 , 7 , which to me doesn't make much sense .
For me at least , most of my local max / min uses are for global max / min within some local area ( e , g , the big peaks and valleys , not every variation in the data )
Note , these are the indices of x that are local max / min .
For mean-preserving kernels ( what any good smoothing filter should be ) the sum of the kernel elements should be precisely equal to 1.00 , and the kernel should be symmetric about its center ( meaning it will have an odd number of elements .
But it is often far easier to first find a sequence of useful kernels ( of varying widths ) and convolve them together than it is to directly find the final kernel in a single step .
minm and maxm contain indices of minima and maxima .
Should work with zeros now .
If they're not , either find a way to normalise them ( try taking the log ) , or use an appropriate non-parametric test .
No this simple inc is just an example , really it is complicated function , that accepts array and some indices and does some computation using them .
np.bincount returns the count for all integers in range ( max ( value )) even for zero counts , which might not be what you want , but it's fast .
@USER When you have a look at the options above , you see that sorting actually takes 5-fold the time then the efficient counting with diff .
You want to find the unique values first using something like np.unique and then do the binary search stuff using only those unique values .
Replacing ` len ( list ( g ))` with ` sum ( 1 for i in g )` gives a 2x speedup
The first one finds out how many unique elements there are so that can my arrays for the unique values and counts of the appropriate size .
Using the values array given in my code , the sorting is taking 4.75 seconds and the actual finding of the unique values and counts takes .67 seconds .
With the pure Numpy code using Paul's code ( but with the same form of the values array ) with the fix I suggested in a comment , finding the unique values and counts takes 1.9 seconds ( sorting still takes the same amount of time of course ) .
It makes sense for most of the time to be taken up by the sorting because it is O ( N log N ) and the counting is O ( N ) .
Additionally , I think it's clearer what's happening here ; the two step ` diff ` approach is a bit opaque at first glance .
This is slower than the winning answer , perhaps because ` scipy ` currently doesn't support unsigned as indices types ...
" [ ... ] because there is not any key in index " - can you explain how the indices in the index array relate to the average values any better ?
I have searched for use numpy histogram to solve the huge array :
scipy provides a correlation function which will work fine for small input and also if you want non-circular correlation meaning that the signal will not wrap around . note that in ` mode= ' full '` , the size of the array returned by signal.correlation is the sum of the input signal sizes - 1 , so the value from ` argmax ` is off by ( signal size -1 = 20 ) from what you seem to expect .
i.e. the following is true : ` Ar = - A.conjugate() = fft ( a [: : -1 ])`
If x ( t ) is real , then X ( -f ) = conj ( X ( f )) .
Therefore , if x ( t ) is real , then x ( -t ) has transform conj ( X ( f )) .
The return value is length ` M = len ( x ) + len ( y ) - 1 ` ( hacked together with ` hstack ` to remove the extra zeros from rounding up to a power of 2 ) .
I think you are almost there use the flatten function #URL
You can treat rank-1 arrays as either row or column vectors . dot ( A , v ) treats v as a column vector , while dot ( v , A ) treats v as a row vector .
You can use convolve to calculate the laplacian by convolving the array with the appropriate stencil : #CODE
Both these methods generate the intermediate arrays vs just generating the the integer indices of the arrays .
And lastly , yes , I need a 4^13 by 13 array , though I reshape it along the way due to the way MPI works .
If you don't care about the order of the indices , you can just omit it ( but it is cheap anyway as long as you don't have any follow-up operations that would transform your array into a contiguous array . )
I'd have to reshape though .
To get all the different combinations , I have to roll the axes .
For every ` n ` in the range 0 to 4^ 4-1 , ` a [ 0 ] .flat [ n ] , a [ 1 ] .flat [ n ] , a [ 2 ] .flat [ n ] , a [ 3 ] .flat [ n ]` will be a valid permutation , with no duplicates .
Basically , I'd like to be able to translate the following cython code to python / ctypes : #CODE
IndexError : too many indices
Is there an easy way to take the dot product of one element of an array with every other ?
a [ 0 ] dot a [ 1 ] , a [ 0 ] dot a [ 2 ] , a [ 1 ] dot a [ 2 ] .
Now append ` ( shared_data , shared_path )` to your list .
perhaps I'm missing something , how dose that comparison return an object on which reshape can be called ?
Should I just manually populate ` ndarray ` with zeroes , or is there any way to reuse the ` zeros ` function ?
To be compatible / similar to Matlab , where functions like ` zeros ` or ` ones ` originally came from .
What should the semantics of a constructor be , e.g. how would you express a simple ` zeros ` or ` empty ` or ` ones ` with one single constructor ?
Thank you for the comments about the ` zeros ` though - it helps me understand where it came from .
because ` [: ]` just copies the whole array , no need to add the indices .
You can use ` zip ` to transpose a matrix represented as a list of lists : #CODE
Plus , if I have 4 dimensions , I thought I should have 4 eigenvalues and not 150 like the eig gives me .
the one person in that thread who implemented it also used the svd function which leads to different eigenvalues
You probably used ` cov ` incorrectly by not transposing the matrix first .
If ` cov_mat ` is 4-by-4 , then ` eig ` will produce four eigenvalues and four eigenvectors .
To see all this , let X have the SVD X = QSV^T , where S is a diagonal matrix of singular values .
Then consider the eigendecomposition D = Q^T X X^T Q , where D is a diagonal matrix of eigenvalues .
The covariance matrix , created from an m x n data matrix , will be an m x m matrix with ones along the main diagonal .
You can indeed use the cov function , but you need further manipulation of your data .
My preference is to calculate them using eig in NumPy's ( or SciPy's )
LA module -- it is a little easier to work with than svd , the return values are the eigenvectors
By contrast , as you know , svd doesn't return these these directly .
Granted the SVD function will decompose any matrix , not just square ones ( to which the eig function is limited ); however when doing PCA , you'll always have a square matrix to decompose ,
by the ones down the main diagonal -- a given data point has perfect covariance with itself ) .
This performs much better since it removes the need to calculate the full covariance matrix , and is also more numerically stable ( I don't know the details but I understand SVD is more stable than eig ) .
It looks like you somehow forgot this * and * read the last paragraph to say that " svd will not work " .
Since there is a lot of zero values around the central part of the array which contains the meaningful data , I would like to " trim " the array , erasing columns that only contain zeros and rows that only contain zeros .
:-) ` reshape ` confuses me , especially since Matlab traverses column-wise by default , while Numpy traverses row-wise by default .
Then reshape it , and check the elements to see if they match the desired output .
which is handy in more general cases , when the latter indices are not continuous .
Advanced indexing in contrast also uses lists or arrays of indices and copies the array .
The first entry of the output array is obtained by using all first indices of the three lists , and the second by using all second indices : #CODE
Thanks Sven , but is there a better way to extract indices which arent contius ( or regularly spaced ) from an array , than using advanced indexing .
( My example was too simple , as all the indices im extracting are continously spaced )
If your indices aren't regularly spaced , you will of course have to use advanced indexing .
A [ indices you want , rows you want , col you want ]
The ` eig ` method gives me a different set of eigenvalues / vectors to work with which I don't mind , except that these eigenvalues , once summed , equal the number of dimensions ( 4 ) and can be used to find how much each component contributes to the total variance .
This other page says that ( " The proportion of the variation explained by a component is just its eigenvalue divided by the sum of the eigenvalues . ")
So , if I use the values returned by ` svd() ` , which are the values used in all tutorials , I can get the correct percentage of variation from each dimension , but I'm wondering why the values returned by ` eig ` can't be used like that .
In other words , can I use the ` eig ` method and still have the proportion of variance for each variable ?
Additionally , could this mapping be done only in the eigenvalues so that I can have both the real eigenvalues and the normalized ones ?
You see that the diagonal matrix d from SVD , squared ,
@USER - The numpy and scipy convolve functions allow some different approaches to how the boundaries are treated .
convolve1d is for 1d arrays , convolve is the multi-dimentional version .
several NumPy builtins will do the job -- in particular , diff , ediff1d , and gradient .
One of the ideas I had was to create another array of ones , of the dimensions that i want to increment ( 2x3 in example below ) and to pad this temporary array with zeros so it was of the same dimensions as the original .
I could then sum them .
I wish to initiate a symmetric matrix in python and populate it with zeros .
Now a polynom is a sum ... computed thanks to the ` sum ` function !
I think the sum variant of ncoghlan's answer is simpler : def f ( cfs , x ): return sum ( c*x**i for i , c in enumerate ( reversed ( cfs )))
Calculate sum of ` sqrt (( X [ i ] -Y [ i ]) ^2 )` or of ` abs ( X [ i ] -Y [ i ])` , normalize to the range of X and Y , that is , from min ( X , Y ) to max ( X , Y ) .
The sqrt version is more sensitive to small differences .
Second , the formula for cosine similarity is dot ( a , b ) /( norm ( a ) x norm ( b )); NumPy has a dot function , however , inner is the NumPy function that implements the dot product .
I don't know how to convert your ` ByteBuffer ` into an array , but here's an implementation of the ` reshape ` function : #CODE
Btw the word append is commonly associated with dynamic data structures , which ` numpy.array ` is not .
I guess you are trying to put ones in a section of an array filled with zeros ?
And I'm not trying to put ones in an array filled with zeros .
I'm taking a small " matrix " and increasing its dimension by padding / filling it with zeros .
I've tried using various permutations of dstack / hstack / vstack / concatenate , but none of them seem to do what I want .
Perhaps elements of your arrays should be numerical ones , so you could harness all the power of linear algebra ;-) .
so you can see that every object in there is unique .
If you want to reshape a sequence into a square grid , you need to take the square root , not divide by 2 !
Clearly , ` x** 0.5 ` is the same as ` sqrt ( x )` , I just didn't see the second ` * ` .
Modified both times , but only have it modified by the sum of the percentages ?
Py2exe then runs , but when I try to launch the resulting exe file , it creates a log file containing the following message : #CODE
Also , some errors refuse to go away , and do not impact the program , but do cause the program to inform the user that an errors log was created after exit .
pickling the standard ones is resulting in very large intermediary memory requirements ( c . 140MB per matrix ) , and that is what I was attempting to optimize by using a sparse matrix ..
Then , if needed , " to persist and exchange these objects " , you can just compress them using gzip , pytables , jpeg , or whatever , but there's no need to limit your data manipulation based storage requirements .
I'm trying to implement a Taylor Series expansion on a Matrix to compute exp ( A ) , where A is just a simple 3x3 matrix .
The formula , BTW for this expansion is sum ( A^n / n ! ) .
After your edit , what's your result for : ` sum ( a [ 0 ]= = 13 )` ?
Obviously , I can create every permutation of the array , but this seems brutish .
If you use NumPy's roll function , then the value ( s ) that fall off the end of the array as you roll it forward just get pushed back onto the front , like a treadmill .
After that's set up , then just diff the two vectors along the correct axis
and sum the 0s .
the max value of those sums , NP.argmax ( tx ) .
Now , if we reshape this into a 2D , 3x3 array , the strides will be ` ( 3 * 8 , 8) ` , as we would have to jump 24 bytes to increment one step along the first axis , and 8 bytes to increment one step along the second axis .
Similarly a transpose is the same as just reversing the strides of an array : #CODE
At this point , I take the sum of each row and divide it by the number of elements in the filter , giving me the average for each pixel , ( shifted by 1 row and 1 col , and with some oddities around edges , but I can take care of that later ) .
What I was hoping for is a better use of stride_tricks to get the 9 values or the sum of the kernel elements directly , for the entire array , or that someone can convince me of another more efficient method ...
However that order may not be the same for ' small ' arrays which kind of ' co-operates ' with your machines cache and those larger ones , which don't ( at least not so straightforward manner ) .
Clearly it's better to reshape to a 3D array first and then instead of summing just do dot product ( this has the advantage all so , that kernel can be arbitrary ) .
@USER : I actually changed my implementation just 1 min ago .
Fourier Transform : ` conv ( A , B )` is equivalent to ` ifft ( fft ( A ) *fft ( B ))` , i.e. a convolution in direct space becomes a multiplication in Fourier space , where ` A ` is your image and ` B ` is your filter .
Since the ( element-wise ) multiplication of the Fourier transforms requires that A and B are the same size , B is an array of ` size ( A )` with your kernel at the very center of the image and zeros everywhere else .
Depending on your implementation of the Fourier transform , this can be a lot faster than the convolution ( and if you apply the same filter multiple times , you can pre-compute ` fft ( B )` , saving another 30% of computation time ) .
If I understand the help correctly , fftconvolve doesn't allow you to precompute ` fft ( B )` , right ?
However , because we're storing temporary arrays that are much bigger than our original array on each step of ` mean ` ( or ` std ` or whatever ) , this is not at all memory efficient !
Mathematically , elements of the rotated tensor , T ' , are given by : T ' ijkl = g ia g jb g kc g ld T abcd with the sum being over the repeated indices on the right hand side .
To use ` tensordot ` , compute the outer product of the ` g ` tensors : #CODE
Just use the np.zeros method to create a array filled with zeros of an arbitrary size
Best of all i think is the reshape option , so i dont have to change all my previous code .
Best of all i think is the reshape option , so i dont have to change all my previous code .
Noticed in the comments that you created the numpy array via append .
Noted that numpy.append doesn't append in-place , so isn't an efficient way of extending long arrays .
The default behavior when appending to numpy arrays is to flatten them .
But once you have a 2-d numpy array , you can append to it without flattening it ; you just have to specify the axis argument : #CODE
Still , it may be easier to simply fill the array and then reshape .
Basically transpose the array , map a general rotate function over it to do the rotation , then transpose it back .
To get " clip " instead of " roll " , use #CODE
numpy roll does this .
Yeah , roll is good stuff ; this is certainly elegant .
You're not clearly doing anything wrong ( The example creates a .bin file with all zeros and a properly formatted .hdr file , as it should , on my machine . ) .
Use ` multiprocessing.Process ( target = somefunc , args = ( sa , )` ( and ` start ` , maybe ` join `) to call ` somefunc ` in a separate process , passing the shared array .
Most numpy operations and a good chunk of the C-backed Scientific Python stack release the GIL and can operate just fine on multiple cores .
numpy and pil reshape arrays
and a large-ish array of unique values in a particular order : #CODE
Which I found to be faster or slower than @USER ' s code depending on the size of the arrays under consideration and the number of unique values .
` index ` holds the indices into the array cutoff associated with each element of ` A ` .
ValueError : too many indices for array
` v [: , 0 ]` is the first column , and ` .argsort() ` returns the indices that would sort the first column .
take ( a , indices , axis=None , out=None ,
The ones after ATLAS and LAPACK in the tutorial .
You'll find that every " append " action requires re-allocation of the array memory and short-term doubling of memory requirements .
Numpy- weight and sum rows of a matrix
@USER : Could you perhaps point us in the right direction as to how to join the individual blocks back into a single array ?
@USER Pavlovic : Please elaborate more what you mean by " how to join the individual blocks back into a single array " !
Please note that the " blocks " are just a view to the original array , so no need to join any blocks , the single array is just the original array ;-)
Surprisingly , concatenate beats view processing in a few instances .
Just pass the original shape back into reshape .
I also agree that you should profile your code and see that using ` nonzero ` or ` where ` and then finding the ` min ` index is actually the bottleneck in your code .
You do posses then a quite unique situation .
The closest thing I could find to what you're asking for is nonzero .
Return the indices of the elements that are non-zero .
Thanks , but the ` y > = 5 ` and ` nonzero ( y )` hit every array element -- I'm looking for a function that returns as soon as it finds the first index .
because you are trying to take the min value of an empty array .
If you call ` L2_Norm.shape ` , you'll see that the dimensions of the array is ( 40 , 20 ) so as the error states , ` L2_Norm [ 19 , 39 ]` is out of range because 39 > the max index in that dimension of 19 .
Executive summary : you can achieve O ( n log n ) with a recursive divide and conquer algorithm ( outlined on the Wiki page , above ) .
that outputs the N* ( N-1 ) / 2 unique pairs ( since r_ij == r_ji ) .
It gives me a stomachache to use this method for finding closest pairs in O ( N^2 ) time , because the divide-and-conquer O ( N log N ) solution was literally the first algorithm I learned in my algorithms class in school .
Perform a series of outer products using tracedot in scipy
To perform an outer product between two vectors in Python ( scipy / numpy ) you can use the outer function , or you can simply use dot like this : #CODE
Now the question is , suppose I have a list of vectors ( or two lists ... ) and I want to calculate all the outer products , creating a list of square matrices .
Do you need to compute the list of outer products , or just their sum ( or some other property ) ?
It's a list of gradient vectors , and I need the outer product of each vector with itself .
The third ( and easiest to generalize ) way to compute outer products is via broadcasting .
Actually the answer provided by pv .
I am trying to take each row , one at a time , sort it in a reversed order to get max 3 values from the row and take their average .
I have an array , but i want to be able to work with a subset of this array ( specified by a list of indices ) in such a way that the changes in this subset is also put into the right places in the large array .
The problem is if i don't want 0:10 , but an arbitrary set of indices .
All compiled numpy C code relies on data being accessible as an inner product of strides and indices .
How to correlate two time series with gaps and different time bases ?
I'm running the accelerometers at their highest gain , so the data has a significant noise floor .
The accelerometers are in similar environments but are only moderately coupled , meaning that I can tell by eye which events match from each accelerometer , but I have been unsuccessful so far doing so in software .
My initial goal is to correlate shock events on the vertical axis , though I would eventually like to a ) automatically discover the axis mapping , b ) correlate activity on the mapped aces , and c ) extract behavior differences between the two accelerometers ( such as twisting or flexing ) .
If the data contains gaps of unknown sizes that are different in each time series , then I would give up on trying to correlate entire sequences , and instead try cross correlating pairs of short windows on each time series , say overlapping windows twice the length of a typical event ( 300 samples long ) .
To remove a column * in-place * , you'd have to move the other columns together and then resize the array .
All the advice I've ever read about numpy says one should not try to resize numpy arrays .
The simplest way is to reshape it and apply the function along an axis .
Since you don't need to change the strides or the size in this case , it is exactly equivalent to using ` reshape ` .
You can reshape it .
I don't think that's possible with ` reshape ` , but with ` stride tricks ` that's easily the case .
You can't reshape with partial rows resulting .
I need to store this " mat " in a file and should access the value of the matrix using the co-ordinates ..
Assuming you have a numpy ` matrix ` or ` ndarray ` , which your question and tags imply , there is a ` dump ` method and ` load ` function you can use : #CODE
For very big sparse matrices on clusters , you might use pytrilinos , it has a HDF5 interface which can dump a sparse matrix to disk , and works also if the matrix is distributed on different nodes .
** I want to write this matrix " mat " into a file and using the indexes of the matrix i should be able to access the values ** using another program !...
Create the sparse matrix and then dump its contents to the file .
` mat.dump ( ' output.mat ')` where ` mat ` is my matrix ..
AttributeError : dump not found `
If you are only interested in plotting the data on a log scale , consider the ` matplotlib ` methods , ` loglog ` and ` semilogx ` and ` semilogy ` :
This would allow you to avoid calculating the log of the various arrays and allow you to customize exactly how the various quantities are displayed .
accumulate numpy array for just one column
I have a NumPy array , i want to accumulate the values of one column , say the 2 nd column .
This solution should be more efficient for large arrays , as it uses a ` set ` to compute the indices for the second array , and pre-allocates memory : #CODE
You can write a.T [ 0 ] [ 1 ] to use indices of the transpose of the array , which are the other way around in 2D .
I have a set of values for frequency and power spectrum and I have to plot Power spectrum Versus frequency on log scale .
Also , the same line , if plotted on log scale using loglog function , does not show up .
No matter how hard I try , I keep reading it was " tens or dot " and not " tensor dot "
I would like to use ` bincount ` to sum arrays , however it supports only doubles .
I want to sum each row of the array into the corresponding index .
What do you mean by ' sum arrays ' in this context ?
Basically I'm going to leverage how scipy sparse matrices handle repeated entries at the same indices ( they sum them ): #CODE
Python C-API and Numpy : core dump on import_array
the nonzero method doesn't work for empty strings
It the nonzero method works with ' None ' .
You can easily get the indices of non-empty strings like so : #CODE
Now you only have to join the slices you are interested in , first you get the starting index of each slice : #CODE
I try to implement Hampel tanh estimators to normalize highly asymmetric data .
Given that , you can determine the difference between the first and last indices in the list that give you the desired range .
The goal then is to find the indices that will minimize a cost function corresponding to the desired symmetric values about the median .
The values from the list at the indices are ` x [ n1 ] = m-b ` and ` x [ n2 ]= m+c ` .
I need to load each column into separate numpy arrays ( and decode the date into a python datetime object ) .
You can append elements to a list , but you can't change the size of a
Numpy arrays have many array related methods ( argmin , min , sort ,
I prefer to use numpy arrays when I need to do some mathematical operations ( sum , average , array multiplication , etc ) and list when I need to iterate in ' items ' ( strings , files , etc ) .
I want to multiply each row by its transpose in order to obtain a 3x3 matrix for each row ,
You could easily do the same thing with the " transposed " version of P , but here I just reshape it and let the broadcasting rules stretch it into the other dimension .
Have you made any benchmarks to compare yours solution with the other ones ?
In python , I would like to convolve the two matrices along the second axis only .
It would change a value of - 1.0e-8 to 0.0 though ( both of these using a clip of something similar to [ 0 , 1 ]) .
You should also take note that in your original formulation of the sum , you are looping over a range of size ` data.shape [ 0 ]` ( the number of rows in data ) , but then in the sum you are
taking the sum for each column .
( BTW , are your matrices close to full rank ? If they happen to be low rank ones , then alternative avenues exists to speed up calculations ) .
Both the hardware as well as the software stack changed from the setup in the original answer .
The software stack is for both nodes the sam .
The new benchmark results are similar to the ones in the original answer .
simple dot product of matrices with different sizes
Could you help me decode this ?
then to calculate the same result we need to replaces ones with zeros and zeros with ones , e.g. consider matrix ` 1-a ` : #CODE
I know I'm overlooking some simple way to get the appropriate indices into the recarray .
To encode the ints and floats as binary you can use ` struct.pack ` .
Instead of using the mask ` repelling_force_prefactor ` for slicing directly , I suggest to precompute the indices where it is non-zero and use them for slicing : #CODE
If you have to update some array of the original shape of ` height_r_t ` with this values , you can use slicing with ` indices ` again , or use ` np.put() ` or a similar function .
Slicing with the list of indices will be more efficient than slicing with a boolean mask in this case , since the list of indices is shorter by a factor thousand .
the advantage of masked arrays is that you do not have to slice and expand your array , the size is always the same , but numpy automatically knows not to compute the exp where the array is masked .
Also , you can sum array with different masks and the resulting array has the intersection of the masks .
>>> unique , index , inverse = np.unique ( a , True , True )
` method= ' min '` assigns the minimum rank of the tied values to all the tied values : #CODE
uhm , ` hessian ` does its computations on something which already has three indices .
You could try to use meshgrid , maybe you have to flatten xn , yn : #CODE
If you want to skip the sum when you have a divide by zero , you could also just do the calculation and then test for the ` NaN ` before returning : #CODE
I have a data set in scipy that is already in the histogram format , so I have the center of the bins and the number of events per bin .
How can I now plot is as a histogram .
You can then associate the words as an additional group in the netcdf4 / hdf5 file and use the common indices to quickly associate the appropriate slice from each group , or just name the group the word and then have the data be the vector .
EDIT : Checking the reshape function and reanalizing my code , I noted a problem with transformations from 2D arrays to 1D arrays and back .
How can I do this without generate a 2D array , reshape it to a 1D array , and re-reshape the array back to 2D ?
Use ` reshape ` to turn a 1D array to 2D ( or any other shapes ) .
Use reshape #CODE
If you cast your indices to unsigned ints , you could use the decorator ` @USER .boundscheck ( False )` before the function .
bounds checking is performed for each array lookup , use the decorator ` @USER .boundscheck ( False )` before the function , and then cast to an unsigned int type before using i and j as the indices .
Change ` np.exp ` to ` exp ` and add ` from libc.math cimport exp ` to your imports at the top .
Get rid of the ` transpose ` function completely .
Then just pass ` t2 ` as the second parameter to ` diss2Timbres ` instead of the one returned by the ` transpose ` function .
I want to draw probabilistic functions ( like the binomial distribution ) , but i don't find a function that returns the probability for given parameters .
To write it myself i need binomial coefficients ( I could write that myself ) , for which I haven't found a function either .
` scipy.stats.binom.pmf ` gives the probability mass function for the binomial distribution .
( a * b ) is the multiplication of rows and columns of each elements in the matrix , you need to sum each row to get the final matrix .
be nonzero .
If anything about ` random.expovariate() ` does not suit your needs , it's also easy to roll your own version : #CODE
looking at ` .flags ` and testing ` .base ` for my solution , it looks like the reshape to ' F ' format creates a hidden copy of the vstacked data , so it's not a simple view as I thought it would be .
Here is a simpler answer than some of the previous ones #CODE
What is the correct way to join them all together to create a unified structured array like the following ?
But imagine I have ~ 100,000 rows with over 1,000 unique IDs ( which are distinct from row indices ) , and I want to select all the rows that match a set of 10 IDs .
Here , ` ids ` is the numpy array of ` id_list ` .
You can roll your simple own with basic functions and checks here is a mockup to start : #CODE
hdf5 and ndarray append / time-efficient approach for large data-sets
What is the most time-efficient approach to append the rows to the data sets ?
whereby date is unique for the particular file and could be ignored .
The issue is that I have to look-up the key , get the complete numpy array , resize the array , add the row and store the array again .
It works with NumPy arrays and you should just be able to append the data you want without reading the rest of the array .
It's basically an on-disk version of a NumPy record array , where each column can be a unique data type .
Tables have an append method that can easily add additional rows .
This is essentially the same as the answer by pv . , except that the extension has been renamed and is now loaded using ` %load_ext ` .
I have an array of floats that I have normalised to one ( i.e. the largest number in the array is 1 ) , and I wanted to use it as colour indices for a graph .
You can flatten it and then reshape it
Numpy's log method gives -inf for log ( 0 ) .
So if the difference is large , obviously it is nonzero , and therefore infinity !
I am trying to utilize Numpy's fft function , however when I give the function a simple gausian function the fft of that gausian function is not a gausian , its close but its halved so that each half is at either end of the x axis .
y = exp ( -x^2 )
If ` A = fft ( a , n )` , then ` A [ 0 ]`
This was slightly fudged in the answers from unutbu and Steve Tjoa above , because they're taking the absolute value of the FFT before plotting it , thus wiping away the phase issues resulting from not using the " standard order " in time .
When I try index [: , None ] , leads to the program spitting out ' ValueError : broadcast dimensions too large ' .
Are x and y column and row indices to h ?
Sometimes it crashes , sometimes it doesn't but I can surely make it crash by increasing the 320x240 zeros to , say , 3200x2400 and / or calling blah.copy() .
I put in some debug statements ... beginning of buffer looks like [ ' \x10\x00\x0b\xe5 '] ... now I did a dump of a ' short ' buffer to ' char ' buffer so endian-ness got reversed .
numpy : efficient execution of a complex reshape of an array
and then I need to reshape it into a 4D array useful4d ( N , I , J , K ) using non-trivial mappings for the indices .
Calculating the indices is fast irrespective of whether I do this one by one in a loop as shown or by numpy.vectorizing the operation and applying it to an arange ( M ) .
Then reshape tempfid to the same shape as newfid and then use the results of ` np.ix_ ` to set the values .
@USER : this looks promising , however I get a ' ValueError : broadcast dimensions too large .
But obviously in this case there is a large overhead incurred by a reshape or something else going on behind the scenes .
This is easily by using .T which takes the transpose .
The max in each dimension don't exceed what they shouldn't :
@USER Ewert My apologies : Earlier when I tried the method of vectorizing and using array-based indices to access arrays I must have made a timing error .
I can guarantee that the C++ data are stored contiguously ( since they live in a std :: vector ) , and it just feels like Numpy should be able to take a pointer to the beginning of that data alongside the number of values it contains , and read it directly .
Find unique elements of floating point array in numpy ( with comparison using a delta value )
I've got a ` ndarray ` of floating point values in numpy and I want to find the unique values of this array .
so I want to be able to set a delta value to use for the comparisons when working out which elements are unique .
This should be decently fast if there are many repeated values within the precision range , but if many of the values are unique , then this is going to be slow .
Also , it may be better to set ` U ` up as a list and append through the while loop , but that falls under ' further optimization ' .
Doesn't ` floor ` and ` round ` both fail the OP's requirement in some cases ?
Of course this method will exclude all but the largest member of a run of values that come within the tolerance of any other value , which means you may not find any unique values in an array if all values are significantly close even though the max-min is larger than the tolerance .
Then when you append , you have have object type arrays , which ` np.interp ` has no idea how to deal with .
I thought it's a little similiar with diag / diagonal / trace functionality .
The , if you sum over just one axis , ` get_pot ` should then return an iterable array .
Note also that the set of eigenvectors may not be unique .
` np.linalg.eig ` tries to return a set of eigenvectors , but does not guarantee a particular , unique set .
@USER : If you want a unique representation of the eigenspaces of your matrix , you could compute the [ Grassmann coordinates ] ( #URL ) of each eigenspace ( but this is probably well beyond the scope of this forum ) .
Can't you just dump the data generated by the Fortran program to a file and then read it from python ?
We have pretty large files , the order of 1-1.5 GB combined ( mostly log files ) with raw data that is easily parseable to a csv , which is subsequently supposed to be graphed to generate a set of graph images .
I have come far enough that I am reading data , sending to a pool of processor threads ( pseudo processing , append thread name to data ) , and aggregating it into an output file , through another collector thread .
collections.deque is supposed supposed to offer fast append and popleft atomic operations that don't require locking ( snip from queue documentation ); you can find deque and mmap docs in python documentation ; I opted for bulk line reads as some sort of quick optimization ; reading and queueing lines one by one seemed a bad idea ( more readline calls , more queueing operations ) so I thought its better and faster to bulk read and bulk process ;
How can I get the indices of the maximum elements ?
Moreover , if there is more than one maximum , it retrieves the indices of only the first maximum : this should be pointed out .
For the indices of all the maxima use instead ` i , j = where ( a == a.max() ` .
Do you mean ` indices = np.where ( a == a.max() )` in line 3 ?
You also mention that this works better than ` unravel ` , but the solution posted by @USER answer the problem of the absolute maximum , not jsut along one axis .
To get the chi-squared value for this fit , evaluate the polynomial at the ` x ` values of your data points , subtract the ` y ` values , square and sum : #CODE
I offered one such method that is a bit more verbose , but is faster than all of the other suggested ones that are benchmarked above , at least on my machine ( this still holds true now with updated versions of scipy and numpy ) .
I've got a list ( used as a stack ) of numpy arrays .
If you are looking for the exact same instance of an array in the stack regardless of whether the data is the same , then you need to this : #CODE
plotting 3d histogram / barplot in python matplotlib
Secondly , as a variant of this , how can I do the same thing , but this time histogram the bars into N bins in each X , Y , Z dimension ?
I.e. instead of a bar for each point , just histogram the data into those bins in every dimension , and plot a bar for each bin .
and you want to get the indices of the rows tha are not equal to #CODE
You can get these indices by #CODE
Last , ` nonzero() ` gives us the indices of those rows .
weird behavior with 3d histogram in matplotlib in Python
Are the operations you're doing simple enough ( eg , dot product etc ) that you could simply implement them yourself ?
If we know that each column is sorted in ascending order , then as soon as we reach a value higher than the max then we know every following element in that column is also higher than the limit , but if we have no such assumption we simply have to check every single one .
Try something like ` numpy.frombuffer ( vtkout )` and reshape after this .
I'm trying to create a histogram with argument normed=1
I expected that the sum of the bins would be 1 .
And how to create a histogram with such normalization that the integral of the histogram would be equal 1 ?
Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen ; it is not a probability mass function .
The sum seems to be correct .
But look at the histogram , the 3rd element is 1.215122 .
@USER Normalized histogram , as I understood it , is a probability density function .
@USER : can you provide some links to backup your definition of normalized histogram ?
` counts * diff ( bins )` gives you what you are looking for .
Probability * densities * can be anything non-negative as long as the * integral * ( not the sum ) over the range is equal to 1 .
The sum of the areas of the bins should be one .
See my other post for how to make the sum of all bins in a histogram equal to one :
Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen ; it is not a probability mass function .
If ` normed=1 ` , then the heights of the bar is such that the sum , multiplied by 0.41 , gives 1 .
And how to create a histogram with such normalization that the integral of the histogram would be equal 1 ?
I think that you want the sum of the histogram , not its integral , to be equal to 1 .
EDIT : Note that , in this case , b are identifiers rather than indices .
What I'm doing is using broadcasting to subtract each element of ` b ` from ` a ` , and then searching for zeros in that array which indicate a match .
For example , b is like identifiers , not indices .
It's also worth noting that both of your solutions only extract unique entries in ` b ` , so if that is important to the OP , than this is also a consideration .
For example if ` a ` is 100x100 , then zip is 5000x slower than taking the transpose .
That is , you need to be able to write a piece of code that takes ` ( x , t )` and gives back ` y ` such that ` exp ( -y ) == y - x + exp ( -t**3 )` to within round-off error .
Writing K for x - exp ( -t^3 ) , we want to solve exp ( -y ) = y - K ; I get y = K + W ( exp ( -K )) where W is Lambert's W function , eg here
1 ) I wasn't aware you could broadcast with ` == ` .
I have a very large matrix ( 10x55678 ) in " numpy " matrix format . the rows of this matrix correspond to some " topics " and the columns correspond to words ( unique words from a text corpus ) .
Each entry i , j in this matrix is a probability , meaning that word j belongs to topic i with probability x . since I am using ids rather than the real words and since the dimension of my matrix is really large I need to visualized it in a way.Which visualization do you suggest ?
@USER , but how can you get an overall picture of the probability distribution across 50000 unique data points ?
For word frequencies , try a log scale -- see [ Zipf's law ] ( #URL ) .
If you do then it's worth trying a colored matrix plot ( e.g. , imshow ) , but if your ten topics are basically independent , you're probably better off doing ten individual line or histogram plots .
OP is referring to the fact that sin ( x ) / x -> 1 as x -> 0
Do you mean you want to tile the array ?
@USER : I typed ` numpy.source ( numpy.tile )` and saw that ` tile ` calls ` reshape ` and ` repeat ` for each element in the ` reps ` tuple .
` reshape ` returns a copy which means re-allocating all that memory each time .
But I can't quite just flatten it out .
Then I could flatten with itertools as in Winston's answer , and then do the transforms in numpy .
If i understand that the data is always a list of lists of lists ( object - polygon - coordinate ) , then this is the approach I'd take : Reduce the data to the lowest dimension that creates a square array ( 2D in this case ) and track the indices of the higher-level branches with a separate array .
They must have the same shape , or shapes that can be broadcast to a single shape .
Scipy interpolation how to resize / resample 3x3 matrix to 5x5 ?
That is , for each band ( ignoring , for the moment , ones at the start and end ) we'll take that one and the two on either side , fit a quadratic curve , and look at its value in the middle .
So for each spatial position , we have a vector of input spectral data , from which we compute the smoothed spectral data by multiplying by a matrix whose rows ( apart from the first and last couple ) look like [ 0 ... 0 -9 / 5 4 / 5 11 / 5 4 / 5 -9 / 5 0 ... 0 ] , with the central 11 / 5 on the main diagonal of the matrix .
So if ` S ` contains the matrix I just described ( er , wait , no , the transpose of the matrix I just described ) and ` A ` is your 3-dimensional data cube , your spectrally-smoothed data cube would be ` numpy.tensordot ( A , S )` .
Why are ` [( 10 , 2 ): ( 10:10 )]` still zeros ?
On Google I found that C++ has ` std :: vector bool `
upvote because it is concise but norm is from ` np.linalg ` instead of directly from ` np ` and it does not have an optional ` axis ` argument , useful to make the rms function more general
However , this version is nearly as slow as the ` norm ` version and only works for flat arrays .
You can simply drop the outer for-loop and will get the same result .
Or maybe a preconditioned gradient solver ?
where D is the diagonal , L is the lower triangular section , and U the upper triangular section .
Why NUMPY correlate and corrcoef return different values and how to " normalize " a correlate in " full " mode ?
The correlate function ( in mode= " full ") returns a 40k elements list that DO look like the kind of result I'm aiming for ( the peak value is as far from the center of the list as the Lag would indicate ) , but the values are all weird - up to 500 , when I was expecting something from -1 to 1 .
I can't just divide it all by the max value ; I know the max correlation isn't 1 .
It shouldn't be hard to either add them into your own distribution of Numpy or just make a copy of the correlate function and add the lines there .
Another , quite possibly better , alternative is to just do the normalization to the input vectors before you send it to correlate .
FWIW , in ` Numpy ` indices like ` [ i , j ]` are interpreted to mean ` ( i+ 1 )` th row and ` ( j+ 1 )` th column .
Just to point out that ` numpy.where ` do have 2 ' operational modes ' , first one returns the ` indices ` , where ` condition is True ` and if optional parameters ` x ` and ` y ` are present ( same shape as ` condition ` , or broadcastable to such shape ! ) , it will return values from ` x ` when ` condition is True ` otherwise from ` y ` .
What's ` append ` ?
The docs for ` append ` say it returns :
Note that ` append ` does
sum over bin range in python
I want a binned sum on the last axis , with every bin containing 20 items .
It's fast , but seems error-prone if I get the arguments to reshape wrong .
I've looked at np.digitize , histogram , bincount , and some others , but those are value based ; I want sum over a set of ranges .
You can speed things up further by doing ` imar.shape = ( x , y , z )` rather than calling the more expensive ` reshape `
I guess I was hoping there was some more elegant way because the reshape method seems ugly to me .
There are several approaches to handling the reshape .
I was getting a related ` MemoryError ` with the string ` join ` of a few million lines .
Basically , it is possible to strip away the plotting and call the underlying functions directly , not super convenient , but possible .
Using arrays in Numpy I want to multiply a 3X1 array by 1X3 array and get a 3X3 array as a results , but because dot function always treats the first element as a column vector and the second as a row vector I can ' seem to get it to work , I have to therefore use matrices .
( ` transpose ` doesn't work because ` A.T ` is exactly the same as A for 1d arrays : #CODE
` _BLOCK_MAX ` is the max for an 8-b yte unsigned int
On the other hand , I have seen Python programs run faster than Fortran ones because the Fortran program unnecessarily preallocated huge arrays , " just in case " there was a lot of data to process .
This generator method , even with the slightly slower Crypto.Random , will max out from disk i / o long before it maxes computation ( although I'm sure the other answers do too ) .
So the numpy version is about 8x faster ( easily fast enough to max my old platter drive ) .
I have two 1D arrays , one that has some values of interest ( a ) and another that provides indices into that array ( b ) .
Although it has potentially many more calls to the roll function , which I would assume could slow things down for very large arrays .
Can you think of anyway to do this without using roll ?
You could append the whole array a to itself , and recalculate the indices for ` a [ #URL but I think that would be more complicated , not less .
@USER : Actually ` dim.count ( None )` seems even more readable than ` sum ( 1 for item in dim if item is None )` :)
The trick here is to iterate over all axes , and for each axis reshape the array to a two-dimensional array the rows of which are the desired one-dimensional subarrays .
J contains the indices along the height dimension that I'd like to select .
I don't see any mention of that in the docs for convolve : #URL Am I missing something ?
It's not supported for numpy's convolve , but it is for ` scipy.ndimage.convolve ` .
#URL Also , most numpy functions ( e.g. ` sqrt ` , ` mul ` , ` add `) take an out parameter .
You can do ` np.sqrt ( x , x )` to take the sqrt in-place .
This is a big improvement over the convolve function I had been using ( I think it was just numpy.convolve ) .
The first one , the inner or dot product , returns a scalar .
Some examples of functions I will write are conjugate gradient for solving linear systems or accelerated first order methods .
I'm writing a library in Cython that currently has about 18K lines of Cython code , which translate to almost 200K lines of C code .
There seems to be a problem with the join_by function in numpy.lib.recfunctions when doing an outer join on multiple keys .
Numpy append : Automatically cast an array of the wrong dimension
I'm reading a set of netcdf files with pupynere and want to build an array with numpy append .
The problem with append is ( from the numpy manual )
Is there a way to append " a " and " b " without the if statement ?
If performance matters , you can first collect all arrays you want to join in a Python list , compute the final size of your array , and than allocate and populate the array .
As pointed out , append needs to reallocate every numpy array .
You can just add all of the arrays to a list , then use ` np.vstack() ` to concatenate them all together at the end .
This avoids constantly reallocating the growing array with every append .
Initialise list and append all read data , e.g. if data appear in order ' aaba ' .
The ` T ` attribute is the transpose of the array , see the documentation .
The transpose of this matrix is #CODE
I don't really care that the zeros function comes from numpy or python , I just want / need to use it .
` sum ` , making it hard ( er ) to access
Python's builtin ` sum ` , which does
No one wants to type ` pylab.zeros ` over and over in a shell when they could just type ` zeros ` .
In addition to what @USER already said about overriding python's builtin ` sum ` , ` float ` ` int ` , etc , and to what everyone has said about not knowing where a function came from , ` numpy ` and ` pylab ` are very large namespaces .
Sure , it's easy enough to guess where ` zeros ` or ` ones ` or ` array ` is from , but what about ` source ` or ` DataSource ` or ` lib.utils ` ?
As another example , how would you distinguish between ` pylab `' s ` fft ` function and ` numpy `' s ` fft ` module ?
` fft ` is a completely different thing with completely different behavior !
( i.e. trying to call ` fft ` in the second case will raise an error . )
I did end up doing something similar to your second example , but I needed the variable ' out ' to have the same number of indices as there are rows in ' a ' .
Not sure if I understand your question , but do you want to find out the 20 indices calculated in the second step ?
Therefore as long as you retain the indices that created ` b ` from ` a ` , you can always view the changed data in the base array .
This assumes ` some_func ` returns the indices in the subarray where some condition is true .
I think when a function returns indices and you only want to feed that function a subarray , you still need to store the indices of that subarray and use them to get the base array indices .
Yes I do understand how I can use the indices like that .
This code goes into a function and the function needs to return the indices into the array where certain conditions are met .
So the function might be something like ` def some_function ( arr )` and it returns the indices in arr that meet a series of conditions .
I don't see any way to get the indices that locate a subarray in its base array .
I think you just need to store the ( base array ) indices that you used to create the subarray and then apply them as an offset to the returned ( subarray ) indices .
Use a secondary array , a_index , which is just the indices of the elements of a , so ` a_index [ 3 , 5 ] = ( 3 , 5 )` .
If you can guarantee that b is a view on a , you can use the memory layout information of the two arrays to find a translation between b's and a's indices .
This is untested , but it might work : mylist = (( mat [ i , j ] for i in range ( 3 )) for j in range ( 5 ))
@USER , @USER , not familiar with generator , but using " mylist = (( mat [ i , j ] for i in range ( 3 )) for j in range ( 5 ))" , " mylist " is a generator object , not a list , so np.array ( mylist ) return a generator object array , which violates my mind .
Try using appending ` [: , :] ` to the matrix ( ie . use ` mat [: , :] ` instead of ` mat `) in your call to ` np.asarray ` - doing this will also allows ` asarray ` to work on images .
as , apparently , repeated indices are ignored .
Is there an equivalent operation to ` += ` which does not ignore repeated indices ?
In principle you can do it with ` numpy `' s ` bincount ` and ` unique ` , but I'd guess it'll only make the code much less readable without any sensible performance improvement .
This code allows me to map from the 3D point to a 1D index as the following ipython log indicates : #CODE
I want to create histogram and calculate it using ` opencv ` method ` cv.CalcHist ` .
Why does the following code produce zero histogram ?
I want to sum the return every 2 days , and if the sum is in the top two , I will set every element in a similar shaped array to True .
For the first set of two days , columns 2 and 3 are tied for the second highest sum .
Please roll back the edit and ask a new question instead .
You can use ` numpy.lexsort() ` to get the indices that sort your arrays using ` prices ` as primary key and ` names ` as secondary key .
Applying advanced indexing using these indices yields the sorted arrays : #CODE
I would eventually like to find ` index2 = abs ( data - n2 ) .argmin ( axis = 1 )` , so I can perform an operation , say sum data at index to data at index2 without looping through the variables .
Running Cumulative sum of 1d NumPy Array
I want to create another numpy array y which is the cumulative sum of x , so that
python+numpy : efficient way to take the min / max n values and indices from a matrix
What's an efficient way , given a numpy matrix ( 2-d array ) , to return the min / max ` n ` values ( along with their indices ) in the array ?
if ` n ` is small then perhaps running ` argmax ` a few times ( removing the max each time ) could be faster .
No expert with NumPy , but do we really need to sort ( O ( n log n )) for something which is trivially done in O ( n ) ?
@USER : The complexity of the OP's algorithm is ` O ( m log n )` , where ` m ` is the number of elements in the array and ` n ` is the number of highest elements to find .
The algorithm in my answer is ` O ( m log m )` .
If you have an 18x4 ndarray then just use ` .T ` to transpose it to an 18x4 ndarray .
You can just read all your values into a vector , then reshape it .
Interestingly , if I comment one of the ` append ` commands ( either will do ! ) it works !
If that results in too many breakpoints prior to the ones related to the mapped files , you might want to run it without breakpoints for a while , break it manually ( Ctrl+C ) and set the breakpoint during ' normal ' operation ; that is , if you have enough time for that :)
Once it breaks , inspect the call stack with #CODE
A little annoying :) which is one reason why I almost never use the ` trace ` feature ; seeing _all_ syscalls is often more instructive , and ` grep -v ` can remove ones I don't want to see after the fact .
This is simple with numpy , because transpose is just ` x.T ` , if x =[ 1 , 7 ]
2 . now I want to sum as in this way ...
" list indices must be integers , not
Finally , sum over all matrices : #CODE
This will seed the state with unique values drawn from your operating system facilities for such things ( ` / dev / urandom ` on UNIX machines and the Windows equivalent there ) .
" list indices must be integers , not
But this isn't correct because I need to sum over n and add those sums together .
( Note that you'll need to prepend fake data onto your row values if you're placing data into a diagonal position with a positive value ( e.g. the 3's in position 4 in the example ))
I am just posting it because I made a few changes to make it more modular so that it would work for different orders of matrices and also changing the values of k1 , k2 , k3 i.e which decide where the diagonal appears , will take care of the overflow automatically .
The error seems to be related to your Riemann sum method ( right / middle / left ) - indicated by regularfry .
This would be an nxn matrix with 40's on the diagonal , but I'm wondering if there is a simpler function to use to scale this matrix .
Or how would I go about making a matrix with the same shape as my other matrix and fill in its diagonal ?
` numpy.fill_diagonal() ` is specifically meant to fill the diagonal with a given element .
If you want a matrix with 40 on the diagonal and zeros everywhere else , you can use NumPy's function ` fill_diagonal() ` on a matrix of zeros .
This approach also has the advantage of showing explicitly that you fill the diagonal with a specific value .
If you want the diagonal matrix ` b ` to be of the same size as another matrix ` a ` , you can use the following shortcut ( no need for an explicit size ` N `) : #CODE
Shift all indices in NumPy array
It will always have zeros on the diagonal .
Point ( 4 ) is intentional , I need the matrix to have a zero diagonal - I just copied the code and forgot to delete it .
Edit : sorry , the previous version had a mistake - decode instead of encode .
Basically , you extract the rows and then transpose the results so that you get a matrix with eigenvectors as columns .
@USER Urbiz - It's returning an empty matrix because you're not putting in a row of zeros , as Bashwork ( and wikipedia ) does above .
I know ` zeros ` and ` ones ` would work for v = 0 , 1 .
I could use ` v * ones ( n )` , but it won't work when ` v ` is ` None ` , and also would be much slower .
Although ` tile ` is meant to ' tile ' an array ( instead of a scalar , as in this case ) , it will do the job , creating pre-filled arrays of any size and dimension .
Apparently , not only the absolute speeds but also the speed order ( as reported by user1579844 ) are machine dependent ; here's what I found :
python & numpy : sum of an array slice
However , it would be much more efficient to use ` array_ [ 1 :] .sum() ` rather than calling python's builtin ` sum ` on a numpy array .
According to my timings ` sum ( x )` is just some 5- 10% slower than ` x.sum() ` ( on ` numpy 1.5.1 `) .
With short ( ~10 element ) arrays , sum is only a few percent slower , but by the time you get up to > 1000 elements , it's a matter of microseconds vs milliseconds .
They're both linear-time algorithms , but iterating through every element of a numpy array in python ( which is what sum does ) is much slower than iterating through every element of the memory buffer in C ( which is what numpy does )
For lists ` sum ( x ) - x [ 0 ]` is faster than ` sum ( x [ 1 :]) ` ( about 40% faster OMM ) .
As far as my numpy ( 1.5.1 ) source tells , ` sum ( . )` is just a wrapper for ` x.sum ( . )` .
Thus with larger inputs execution time is same ( asymptotically ) for ` sum ( . )` and ` x.sum ( . )` .
@USER - Yes , I was purposefully comparing python's ` sum ` to numpy's ` sum ` .
I assumed the OP was referring to python's ` sum ` , as well .
However I personally feel quite comfortable to work with ` IPython ` profile ` scipy ` , even tough it shadows ` pythons sum ( . )` .
@USER Kington : the OP ( myself ) was referring to Python's ` sum ` because the OP just started using numpy yesterday , and wasn't aware that it has its own , much faster , version of ` sum ` !
This requires reading the whole file once and storing the start indices of all strings in memory .
This array can be dropped after extracting the indices , though .
This will search the indices where the values of ` t2 ` would have to be inserted into ` t1 ` to maintain order .
Examples of valid indices include : an ` int ` , a ` slice ` , an ellipsis or a tuple of the above .
If that is what is desired , ` atleast_Nd ` isn't good enough - probably need to convert the ` int ` indices to ` slice ` s .
Here is a slightly more complex version that always returns a view into the original array ( of course provided that you don't do any advanced indexing ; this should be guaranteed by your specification of valid indices ): #CODE
How to get the bar heights of histogram P ( x ) corresponding to a given x ?
I have plotted a histogram ( say P ( r )) of Gaussian random numbers ( r ) using the code given below where I have used the numpy.hist command .
I mean I need the bar height corresponding to a given value on the x-axis of the histogram .
In SVD decomposition $A=UDV^T$ only $D$ is unique ( up to reordering ) .
D is only uniquely determined if you adopt the convention that its diagonal entries are sorted in descending order .
Actually , the U and V are also unique for unique singular values .
Singular value 1 ( the 3.4 ) is unique - and therefore columns 1 of U and V are the same in both answers .
Also , even though columns 2 and 3 are not unique , they should lie in the same linear subspace for both answers .
( where ` sum ` can of course be replaced by anything ) .
I'm sorry , I don't really want the sum , that's just part of the " silly example " bit .
The thing that takes the place of ` sum ` , does it have to return a scalar ?
gives you a as an array of list b in the shape given in reshape .
Before I roll out my own , I am looking for an API similar to ` biglm `' s ` update ` - the latter does the exact job I want , and does it well , but it's in R instead of Python .
ones ?
there is incremental qr and cholesky in cholmod available , but I didn't try it , either license or compilation on windows problems , and I don't think I tried to get incremental_qr to work
I use Python and NumPy and have some problems with " transpose " : #CODE
If ` a ` is for example ` [[ ] , [ ]]` then it transposes correctly , but I need the transpose of ` [ ...,...,... ]` .
also tried " print a.transpose " which is the same but without sucess , not transpose ...
The transpose of a 1D array is still a 1D array !
If you want to turn your 1D vector into a 2D array and then transpose it , just slice it with ` np.newaxis ` ( or ` None ` , they're the same , ` newaxis ` is just more readable ) .
Numpy will automatically broadcast a 1D array when doing various calculations .
PS : There is a simple way to transpose a 2D matrix - #CODE
I may be wrong , but isn't the transpose of [ 5 , 4 ] = [[ 5 ] , [ 4 ]] ?
transpose of x = [[ 0 1 ] , [ 2 3 ]] is xT = [[ 0 2 ] , [ 1 3 ]] , well the code is : #CODE
You can only transpose a 2D array .
Unnecessary complicated solution , why not just ` dot ( b , a )` ?.
With proper ` b ` you can just use ` dot ( . )` , like : #CODE
For numpy this list includes ' linalg ' , ' fft ' , ' random ' , ' ctypeslib ' , ' ma ' , and ' doc ' last I checked .
So , once you've run this command , you can call ` dot ` or ` linalg.eig ` without the numpy prefix .
The same bytecode ( BINARY_ADD ) is generated ; the interpreter will pop two objects off the stack and do the C-extension equivalent of ` do a.__add__ ( b )`
I agree ; on my machine , numpy array sum is 70-140 times faster than the builtin sum over a builtin list ( 70 in the case of ` float ` and 140 in the case of ` np.float64 `) .
In that case , it's somewhat disconcerting that using ` np.float64 ` increases the execution speed by a huge constant factor ( 2 in the case of a simple sum ; 10 in the case of my code ) .
If you're after fast scalar arithmetic , you should be looking at libraries like ` gmpy ` rather than ` numpy ` ( as others have noted , the latter is optimised more for vector operations rather than scalar ones ) .
Even a simple X.a resolution has to resolve the dot operator EVERY time it is called .
From my understanding , when you use python standard operators , these are fairly similar to using the ones from " import operator .
" If you substitute add , mul , and mod in for your + , * , and % , you see a static performance hit of about 0.5 sec versus the standard operators ( to both cases ) .
There is no need for a tuple construction : ` reshape ( x.shape + ( -1 , ))` can be simplified as ` reshape ( x.shape , -1 )` .
However , it is used in many places in the official documentation , so I take it it is quite official ( one can for instance find many instances of ` reshape ( i , j , k )` at #URL ) .
@USER I still consider ` reshape ( i , j , k )` to be an inconsistent misfeature to avoid , but it's been around long enough that I don't militate for its removal .
I've never seen ` reshape ( some_tuple , j )` before in code or in documentation .
@USER Actually ` reshape ( some_tuple , -1 )` simply doesn't work .
In fact , I meant what I wrote in the comment : " reshape ( int , int , )" , which would be here ` x.view ( np.float64 ) .reshape ( len ( x ) , -1 ))` .
` ( end - start ) == sum ( mask )` .
You can keep its size to 800MB using ` zeros (( 1000,100 0 , 10 , 20 ) , dtype=float32 )`
It has the two outer functions .
which is what you expect to rounding error - integral of exp ( i x ) from 0 , pi / 2 is ( 1 / i ) ( e^i pi / 2 - e^0 ) = -i ( i - 1 ) = 1 + i ~ ( 0.99999999999999989 + 0.99999999999999989j ) .
Also only did 7-pt and 15-pt rule , since I didn't feel like calculating the nodes / weights myself and those were the ones listed on wikipedia , but getting reasonable errors for test cases ( ~ 1e-14 ) #CODE
@USER : The new array created by your ` hstack() ` call will occupy ` a.nbytes + DatDifCor [ k ] [ i , j ] .nbytes ` bytes , provided both the objects you stack are NumPy arrays of the same dtype .
I am willing to test that out , but am looking for a less tedious way of handling broadcast and smart indexing .
It won't fix your algorithm , but it will strip out a lot of the python overhead .
But will it handle broadcast , slicing etc ?
You can use ` scipy.weave.blitz ` to transparently translate your expression into ` C++ ` code and run it .
Broadcast operations can be written as outer-products with vector of all ones .
This link shows how they can be used to form outer products #URL ( search for outer products to go to the relevant part of the document ) .
FWIW , in my modest machine dot ( ., . ) with random vector of shape ( 4000 , ) will take some 6us .
Well , single ` dot ` seems then to be reasonable efficient .
Perhaps the culprit is copying of the arrays passed to dot .
As Sven said , the dot product relies on BLAS operations .
If both arrays passed to dot are in C_CONTIGUOUS , you ought to see better performance .
Of course , if your two arrays passed to dot are indeed 1D ( 8 , ) then you should see both the C_CONTIGUOUS AND F_CONTIGUOUS flags set to True ; but if they are ( 1 , 8) , then you can see mixed order .
What's the state of the art with regards to getting ` numpy ` to use mutliple cores ( on Intel hardware ) for things like inner and outer vector products , vector-matrix multiplications etc ?
I doubt you can achive any speedup by the multithreaded computation fo dot products of vectors of size 4000 .
Such a dot product needs only a few microseconds to compute .
@USER : The absolute path to where the atlas library is installed ....
@USER : check any other of the gazillion here that uses ` join ` .
Add newlines and join them .
throws an ` AttributeError : log ` .
I have numpy arrays of start indices and end indices from which I'd like to construct a flattened array of ranges .
Just to let you know you should use ` rand ` for both ( uniform distribution ) or ` randn ` ( normal distribution ) but not mix them .
What would the timings be with ` svd ` ?
I have tried ` svd ` ( 3 seconds * without the loop * ) and ` eigvals ` ( 12 seconds ) , to be compared to 30 seconds with ` eig ` .
Although , with ` svd ` , you only get the singular values of a ' *a , not a .
` dot ( f-order , c-order )` etc .
Possibly what you were seeing may have been related to a blas-optimized dot import error being caught and handled silently ( this code snippet is from numeric.py ) #CODE
` blah.T ` changes shape of the array , so replacing ` blah ` with ` blah.T ` won't work unless the proper gemm ` transpose ` flag is set at the same time .
The function must return zero for x t.min() and the max index ( or -1 ) for x > t.max() .
I think you may be able to improve on your first function by short circuiting once you've tested for x < min ( t ) and x > max ( t ) , but I haven't had a chance to test it yet .
Explanation of python code snippet >> numpy.nonzero ( row == max ( row )) [ 0 ] [ 0 ] << from a script using numpy
Lemme roll back my changes ..
Is there a way to dump a NumPy array into a CSV file ?
I have a 2D NumPy array and need to dump it in human-readable format .
Are you able to have PIL resize it and then you can call numpy.reshape() ?
this is the wrong way to resize .
Ploting a sum in python
I'm trying to plot a function in python that has infinite sum in it .
How can I make the sum big ?
Is the syntax of the sum correct ?
Your sum has ` 1 / n !
That means that the terms decay REALLY REALLY REALLY fast , so there's no need whatsoever to make the sum go up to 10*100 : try instead 100 , which should be a perfectly good upper limit for you .
To calculate an infinite sum you'd wish to calculate until the sum has stopped changing significantly ( e.g. the summands have fallen under certain very small threshold ) .
I updated the answer : You should at least take the absolute value of each element .
Since you want to pair every two items , it makes sense to reshape the 6x6 array into a 18x2 array : #CODE
And finally , we just reshape the array to be 6x3 : #CODE
Is matrix multiplication like " dot product " when pointwise multiplcation is like every point multiply corresponding point ?
Given the return of convolve , it's possible to get the dx and dy ?
How to transpose a 2 Element Vector
Applying transpose does not seems to do the trick .
I tried reshape , but seemed a bit counter intuitive ,
I have initially tried to loop over the columns , masking the array except the column to be recalculated , and the replacing the values with the new ones with numpy.putmask but this does not retain the order , as it attempts to place a value in each element and failing that tries with the next calculated value on the next element , la : #CODE
For this example , if I take f = sum , then
or f = max
For the special case of ` f = sum ` : #CODE
The extra zeros in ` B ` do not affect
i have two lists of tuples , where tuples in the each list are all unique .
You still have to supply indices for the other two dimensions for this to work correctly .
` numpy.indices ` calculates the indices of each axis of the array when " flattened " through the other two axes ( or n - 1 axes where n = total number of axes ) .
These are the identity indices for each axis ; when used to index b , they recreate b .
I'm still getting to know numpy's very rich indexing system ; it's nice to know about an automatic way to generate broadcastable indices .
@USER - the length of the correlation is the sum of the lengths of the inputs .
I don't see a diff -- which is faster ?
looking into the code of ` fft ` and ` log10 ` I can see that they use ` asarray() ` , which strips the subclass and returns an ndarray , explaining the behavior .
I am not sure about ` fft ` , but ` np.log10 ` is a ufunc .
It wouldn't surprise me if ` fft ` always returned an ` ndarray ` though ( I haven't looked at the source code , but the FFT clearly doesn't fit the definition of a ufunc ) .
From for loops to numpy dot implementation
As in my code this matrix filling is recalled a huge number of times , these for loops should be changed with a dot product ... but I dunno how to do it ...
What I try to do is to create a symmetrical matrix , filled with zeros and ones ( ones with a probability of 0.7 ) .
In fact this row wise version is about 5 times faster than the diagonal version , which I guess shouldn't be all that surprising given the memory access patterns it uses compared to diagonal assembly .
Here a triangular matrix is formed directly from a full matrix of weights , then added to its transpose to produce the symmetric matrix : #CODE
The same can be said about ` log ` , ` log2 ` and ` logn ` , but not about ` log1p ` [ 2 ] .
It seems that module overlays the base numpy ufuncs for sqrt , log , log2 , logn , log10 , power , arccos , arcsin , and arctanh .
Usage --- ` from numpy.dual import fft , inv `
Do you want to do image processing for exact matches , or do you want to be able to detect scaled versions , slightly different ones , etc .?
You can use scipy.ndimage.correlate to correlate your template against the image .
Python / matplotlib Show confidence levels in a histogram
In particular , starting with the bin containing the highest count I wanted to find and colour , say red , all the highest bins whose area sum to less than say .6 .
It assumes an already generated pdf ( histogram ) with a bins represented by the varialble " bins " and bin width represented by the variable " binwidth " .
I've looked for , and found , some of the usual suspects ( ` log ( 0 )` and the like ) , but none of the obvious ones seem to be the culprits in this case .
I want to represent a matrix in python , as well as get the norm of the matrix .
The documentation includes a lot of examples on a per function basis , so for example if you wanted to find how to take the norm , take a look at
Then , you can sum the elements of check that are equal to zero , in each row #CODE
If you just want to sum and average data , the sum , mean and std methods on ndarrays allow you to do it " axis-wise " : #CODE
numpy : column-wise dot product
Given a 2D ` numpy ` array , I need to compute the dot product of every column with itself , and store the result in a 1D array .
You can compute the square of all elements and sum up column-wise using #CODE
( I'm not entirely sure about the second parameter of the ` sum ` function , which identifies the axis along which to take the sum , and I have no numpy currently installed . Maybe you'll have to experiment :) ...
From linear algebra , the dot product of row i with row j is the i , j th entry of AA^T .
Similarly , the dot product of column i with column j is the i , jth entry of ( A^T ) A .
So if you want the dot product of each column vector of A with itself , you could use ` ColDot = np.dot ( np.transpose ( A ) , A ) .diagonal() ` .
On the other hand , if you want the dot product of each row with itself , you could use ` RowDot = np.dot ( A , np.transpose ( A )) .diagonal() ` .
I am simply trying to import the data and print the max value .
The reason is that argmin ( as well as argmax ) returns index of the variable -- in case of a matrix , you need to convert your n-dimensional matrix to a 1-dimensional array of indices .
Note that the condition here is flipped , as the mask is True for the excluded values , not the included ones .
[ Edited to use indices instead of mgrid ; I'd actually forgotten about it until it was used in another answer today ! ]
This is from a while ago , but for posterity : this is equivalent to the easier-to-read ` max ( set ( lVals ) , key= lVals.count )` , which does an O ( n ) count for each unique element of ` lVals ` for approximately O ( n^2 ) ( assuming O ( n ) unique elements ) .
Best is ' max ' with ' set '
Create a numpy matrix with elements a function of indices
How can I create a numpy matrix with its elements being a function of its indices ?
Un-numpy and un-pythonic would be to create an array of zeros and then loop through .
To further explain , ` numpy.indices (( 5 , 5 ))` generates two arrays containing the x and y indices of a 5x5 array like so : #CODE
basically you create an ` np.ufunc ` via ` np.frompyfunc ` and then ` outer ` it with the indices .
I have a list of float values ( positive and negative ones ) stored in a variable ` row ` of type ` type ' numpy.ndarray ' ` .
EDIT : seeing as the question has now changed and you need the positions as well as values , use ` numpy.argsort ` to obtain the indices instead of values : #CODE
I've run into a similar issue with a fairly complex mat file at our company .
This is at least allowing us to access the deeply nested elements without having to alter our mat files .
Using ` np.where ( myarray == 1 )` , I can get the indices of the pixels : #CODE
My script yields the indices taking into account the order of the pixels on the curve : #CODE
EDIT , full error log : #CODE
strange , I'm getting the same error , maybe something else is going wrong , I'm editing the question with the full error log
So is sqrt a separate function ?
But the ` sqrt ` function only needs to solve a couple of quadratic equations .
So a good implementation will provide a custom ` sqrt ` function , which is faster and more accurate than ` power ` .
What happens is that the square root of -1 is calculated as exp ( i phase / 2 ) , where the phase ( of -1 ) is approximately ?.
In your example , the square root is calculated by evaluating the the module and the argument of your complex number ( essentially via the log function , which returns log ( module ) + i phase ) .
And in bonus , you showed me the diff function that I wasn't aware of .
find the dot product of sub-arrays in numpy
Here is the full error log : #CODE
strange , I enter this : quotes = [ 733965.0 , r [ ' open '] , r [ ' close '] , r [ ' max '] , r [ ' min ']] , and the error message is " TypeError : ' float ' object is unsubscriptable " , although I think it is supposed to take a float as an argument
We find all indices where the distance of B_Alt to k in A_Alt is less than a specified range .
Let's take the nearest neighbour to be the entry ( or entries in case of a tie ) with the smallest absolute altitude difference while not having nan as a value .
You're writing zeros to a file ...
Is writing zeros not working ?
If you want to write the numpy array to the file , pass it in instead of the zeros array that you're creating .
@USER - zeros were working , but I did want to pass c as float 32 , as I'm creating my own height data .
You want the " Probability / Percentile " values to be a cumulative histogram ?
If we want this to look like one continuous plot , we can just squeeze the subplots together and turn off some of the boundaries .
Edit : If you want percentile values , instead a cumulative histogram ( I really shouldn't have used 100 as the sample size ! ) , it's easy to do .
Do i have to find the percentile from 0-100 one by one and plot it against the range of min & max of the data ?
Including the ones they built themselves .
The space required by a sparse matrix grows with the number of nonzero elements , not the dimensions .
The questions , then , are 1 ) how to translate ' visual detection ' into a reliable algorithm and 2 ) how to determine the optimal value of sigma .
Based on your comments , I found the following page [ link ] ( #URL ) which looks like a good start for using fft in python .
I just found this quote in context of explaining import strategies : * " Lets consider the case where you ( for whatever reason ) want to compare numpy's and scipy's fft functions .
scipy's fft checks if your data type is real , and uses the twice-efficient rfft if so .
numpy's fft does not .
Basically it is not a fair comparison - numpy's time include making the output usable , not just doing the fft .
I found that numpy's 2D fft was significantly faster than scipy's , but FFTW was faster than both ( using the PyFFTW bindings ) .
For signed ones , you can use half the value used to mod ( i.e. 2**15 ) and will have to validate the result before applying modulo
If you don't want to do this because of code practice or the sheer volume of function that you would need to import , you could write a function to concatenate the module name in front of the entry .
If you want to use ` loadtxt ` , you can either first load the raw byte array and then decode : #CODE
I'm attempting to broadcast a module to other python processes with MPI .
from it's buffer , and it streams to std out .
Does that mean you're just _appending_ to the array , rather than inserting a new value at ` 493 ` even though the array is only ` 15 ` items log ?
Then when the room runs out , extend it again ... you could even write a separate append method that would do this , rather than overriding ` __setitem__ ` to have an unexpected side-effect .
But this seems not possible with SciPy sparse matrices ; the indices are taken as numeric ones , so ` False ` select row 0 and ` True ` selects row 1 .
You can use ` np.nonzero ` ( or ` ndarray.nonzero `) on your boolean array to get corresponding numerical indices , then use these to access the sparse matrix .
Since " fancy indexing " on sparse matrices is quite limited compared to dense ` ndarray ` s , you need to unpack the rows tuple returned by ` nonzero ` and specify that you want to retrieve all columns using the ` : ` slice : #CODE
@USER Eweiwi : In ` Python ` indices starts from ` 0 ` , so in this case ` argsort ( . )` will provide what you are looking for , like ` (( F- t ) ** 2 ) .sum ( 1 ) .argsort() ` .
FWIW , with ` numpy ` 1.6 in my modest machine : for ` n ` = 1e5 , timing s are ` cdist ` 3.5 ms and ` dot ` 9.5 ms .
So ` dot ` is only some 3 times slower .
However with much smaller ` n ` ( < 2e3 ) ' dot ' will be faster .
You can also use the development of the norm ( similar to remarkable identities ) .
I've looked at the random module which doesn't seem to have an appropriate function and at numpy.random which although it has a multinomial function doesn't seem to return the results in a nice form for this problem .
To clarify , I'm not looking for explanations of how to write a sampling scheme , but rather to be pointed to an easy way to sample from a multinomial distribution given a set of objects and weights , or to be told that no such function exists in a standard library and so one should write one's own .
This will only matter if the endpoints of the intervals are representable by floating point numbers , and if the extra probability of 1 /( 2^53 ) matters the op should probably roll his / her own functions .
Despite the complication , and assuming one returns a sampler so one doesn't have to recompute the cumulative sum , +1 because is efficient for large arrays due to numpy doing binary search .
Anyway your answer is correct and after some mental wrestling it reveals the beauty of ` multinomial `' s .
I reckon the multinomial function is a still fairly easy way to get samples of a distribution in random order .
Based on the feedback of ` phant0m ` , it turns out that an even more straightforward solution can be implemented based on ` multinomial ` , like : #CODE
IMHO here we have a nice summary of ` empirical cdf ` and ` multinomial ` based sampling yielding similar results .
The " wild " ` nan ` s , like the ones created by ` float ( ' nan ')` or by ` float ( ' inf ') - float ( ' inf ')` , will not work well as dictionary keys .
If there are no zeros in ` iterable ` this also works : #CODE
As ` pv ` already elaborated , it's not really practically feasible to try to produce such huge covariance matrix .
Lets define first a simple helper function in order to make it more straightforward to handle indices and logical indices of NaNs : #CODE
The probability distribution of the sum of two random variables , x and y , is given by the convolution of the individual distributions .
Also , strictly speaking , ` histogram ` wont give you a ` pdf ` so simply way .
This ( pdf_x= histogram ( x ) ** [ 0 ] ** ) ignores the bin " locations " for the histograms , converting the histogram into a pdf , before convolution .
@USER : Please note that based on ` pv ` s answer , there exists a real barrier , which you can't expect to overpass .
The computational complexity for det and LU is essentially the same , and as optimized libraries for LU exist , one probably is hard pressed to beat them .
The majority of the time seems to be spent inside ` det ` .
det ( A ) * inverse ( A ) = adjoint ( A )
It computes ` det ( A ) * inverse ( A ) ^T ` .
Cofactor is the transpose of the adjugate .
theta= sin-1 ( sqrt ( x^2+y^2 ) / sqrt ( x^2+y^2+z^2 ))
I'm not exactly clear on your meaning , but if you are looking for 3d arrays that contain the indices x , y , and z , then the following may suit your needs ; assume your data is held in a 3D array called " abc " : #CODE
For @USER ' s purposes it's much more efficient to generate an open grid using ` np.ogrid ` - numpy's broadcasting rules will mean that the result of his computation will be 3D without having to allocate 3 whole xyz matrices of indices
BUILD_LIST builds a list from directly off the stack as opposed to creating it via a series of appends or whatever .
Then find out the zero elements , and sum over column wise .
Then those are matches where the sum equals the number of columns .
Basically , test if each element of the row is in the corresponding column of the matrix , then multiply along the column ` ( axis = 1 )` , and sum the result .
For the SciPy function ` fmin_ncg ` , is there a way of suppling the hessian and gradient as a variable rather than a function ?
To do this , I've supplied the gradient and hessian .
where ` myFunc ` returns 3 values : the function evaluation , the gradient , and the hessian .
To me this seems inefficient as the code has to go through a large dataset , and there are calculations that are common to the function , the gradient and the hessian .
e.g. Imagine a function ` f ( x ) = a ( x ) *b ( x )` with gradient ` g ( x ) = a ( x ) *c ( x )` , hessian ` h ( x ) = a ( x ) *d ( x )` ... in Matlab I can calculate ` a ( x )` once , where as it appears that i have to calculate this three times in python .
Matplotlib so log axis only has minor tick mark labels at specified points .
I am trying to create a plot but I just want the ticklabels to show as shown where the log scale is shown as above .
I'd like it to be like 8x10^8 or .8x10 ^9 to save space instead of putting all those zeros .
Can I do this and still maintain a log axis ??
Get indices of array where two conditions ( on different arrays ) are true
I want to select all of the indices where ` a == 1 ` and ` b == 0 ` .
I would like to get the following indices back : #CODE
@USER , @USER , I think the problem is that you have to reshape ` S ` for [ broadcasting to work ] ( #URL ) .
There's probably a dozen other valid solutions , including better ones than these ...
It's worth noting that ` reshape ` creates a view , not a copy : ` >>> s = numpy.arange ( 5 ); s.reshape ( 5 , 1 ) [ 3 , 0 ] = 99 ; repr ( s )` -> `' array ([ 0 , 1 , 2 , 99 , 4 ])'` .
I started with the latter , but since I have to do a lot of append
append row ( with an in average constant time , like C++ vector just always k elements )
delete a set of elements ( best : inplace , keep free space at the end for later append )
So assuming you do know this , why do you need to append to the array ?
In any case , numpy arrays have the ` resize ` method , which you can use to extend the size of the array .
( I was not aware of it before . ) I guess I should still rather do the bookkeeping myself and always resize to double the size , rather then resizing for every new row .
Do you know how to resize the image ?
You can manually resize the image with ndimage.zoom .
But also see " reshape " option in the rotate documentation : #URL
This property is used by the ` np.where ` function to retrive indices : #CODE
Can I slice segments off the beginning of the list and append them to an empty array ?
( The ` reshape ` method returns a new " view " on the array ; it doesn't copy the data . )
- Fitting empirical distribution to theoretical ones with Scipy ( Python ) ?
From what I understand , Latent Semantic Analysis lets me find a low rank approximation of a term-document matrix i.e. given a matrix X , it will decompose X as a product of three matrices , out of which one would be a diagonal matrix ?
So you mean that I truncate Vt by k rows and then compare columns or maybe run k-means on the columns to get the final clusters ?
More or less , just substitute for X= U_k * S_k * V_k^T ( where U_k , S_k , V_k , represents the partition of ' k ' -largest singular values of U , S , V= svd ( X ) .
Fitting empirical distribution to theoretical ones with Scipy ( Python ) ?
Thus your likelihood p ( x ) will be the sum of all the values for keys greater than x divided by 30000 .
What if ` normed = True ` in plotting the histogram ?
1 , the one that gives you the highest log likelihood .
2 , the one that gives you the smallest AIC , BIC or BICc values ( see wiki : #URL , basically can be viewed as log likelihood adjusted for number of parameters , as distribution with more parameters are expected to fit better )
` scipy ` does not come with a function to calculate log likelihood ( although MLE method is provided ) , but hard code one is easy : see Is the build-in probability density functions of ` scipy.stat.distributions ` slower than a user provided one ?
To get a 2D image I just sum over one of the axes using ` sum ( array , 2 )` and then use the matplotlib function ` imshow ( array2D )` to obtain the 2D image .
That is to say , what I want to do for each slice z is to scan line by line right to left and left to right ( x axis ) and the first time I have a 1 I want to fill the rest of the line with ones .
Just one detail : If there's a row filled with only zeros , it won't work !
I updated the example ; now it works also for rows filled with only zeros .
are quite straightforward ones .
But still , I'll have the same problem when there will be a row filled with zeros ( which shouldn't change after the transformation ! )
This can be reshaped in the original way you tried , i.e. ` reshape ( -1 )` .
yes , there is a trade-off between minimizing the total within-cluster sum of squares ( called * distortion * here ) and minimizing the number of clusters .
and so on , up to max side lengths around 5000 or so .
Also , if there is then I could just append to the b and c arrays each time instead of overwriting and starting from scratch each loop .
The histogram way is not the fastest , and can't tell the difference between an arbitrarily small separation of points and ` 2 * sqrt ( 2 ) * b ` ( where ` b ` is bin width ) .
You set the " element size " , which acts as a bin size ( the code will call interactions on everything within ` b ` of another point , and sometimes between ` b ` and ` 2 * sqrt ( 2 ) * b `) , give it an array ( native python list ) of objects with an x and y property and my C module will callback to a python function of your choice to run an interaction function for matched pairs of elements .
Allocate the final matrix ( e.g. 256x256 ) with all zeros
Replace each cell in the matrix with the sum of the values of the matrix in an NxN box centered on the cell .
The computation of the box sum can be made very fast and independent on N by using a sum table .
used ` sqrt ( average of squared values )` instead of ` sum ` for the averaging pass
@USER : the problem is that to compute the weighted sum you need an NxN loop for each cell .
Using a single square there is no inner loop and each cell computation is O ( 1 ) using the sum table .
The weighted sum result is approximated by the multiple passes and actually after very few of them the output is already smooth .
I would instead center an n-dimensional Gaussian on each point and evaluate the sum over each point you want to display .
Perhaps in this case you could vectorize over an array of indices into ` ReadAsArray ( h , i , numberColumns , numberRows )` .
It just changes the " strides " by which ` numpy ` indices the data .
When you call the ` reshape ` method , the value returned is a new view into the data ; the data isn't copied or altered at all , nor is the old view with the old stride information .
To handle edges well do I need to reshape ?
It uses the least squares method and probably weighs the sum of the squares by 1 / sigma**2 .
Not sure what I should do about that either , unless I'm supposed to replace cv :: Mats for std :: vectors or raw arrays , both of which would seem like a bad practice to me .
Have you tried comparing a cross product in numpy or a dot product in opencv ?
For example , say you have ` arr = numpy.arange ( 0 , 9 ) / ( 9.0 )` and you want the values at indices ` 3 ` , ` 5 ` , and ` 8 ` .
Since you don't want to have to type indices like that out all the time , you can generate them automatically -- with ` numpy.indices ` : #CODE
Since many ` numpy ` operations are vectorized ( i.e. they are applied to each element in an array ) you just have to find the right indices for the job -- no loops required .
You can use the function ` tile ` .
With this function , you can also reshape your array at once like they do in the other answers with reshape ( by defining the ' repeats ' is more dimensions ): #CODE
Addition : and a little comparison of the difference in speed between the built in function ` tile ` and the multiplication : #CODE
The resulting code is simpler ( if you know the functions of course , see the documentation for repeat and tile ) and faster .
I like tile better than multiplication since you can do it to numpy vectors :)
For a large number of repetitions , tile is also a lot faster than multiplication .
I'm not exactly sure what you are trying to do , but as a guess : if you have a 1D array and you need to make it 2D you can use the array classes reshape method .
if you have a real-valued square symmetric matrices ( equal to its transpose ) then use scipy.linalg.eigsh
Using the SciPy library linalg you can calculate eigenvectors and eigenvalues , with a single call , using any of several methods from this library , eig , eigvalsh , and eigh .
Note that it is not necessary to store information about whether each index was ` True ` or ` False ` ; a structure that simply stored indices where the array was ` True ` would be sufficient to reconstitute the array .
You have to flatten the numpy array explicitly : #CODE
Second , as you can see from the above , all the ` tostring ` shenanigans are unnecessary ; you could also just explicitly flatten the ` numpy ` array .
I still need to sum up all the True's to get a score .
you can also use ` np.sum ( array1 [ ans ])` or ` np.sum ( array2 [ ans ])` if you want sum by itself . everytime you have a ` false ` as an entry it will not take the value into account .
Do you want to calculate the geometric mean of their absolute values instead ?
` mat = numpy.zeros (( 3 , 7 ))` .
Which will actually create unique lists for each row instead of reusing them every 3rd row .
My mathematics contain only probabilities on which i perform matrix arithmetics ( multiplication , dot function in numpy ) .
Unlike ` min ` , ` sum ` doesn't require branching , which on modern hardware tends to be pretty expensive .
This is probably the reason why ` sum ` is faster .
It is interesting to note that ` min ` is slower in the presence of NaNs than in their absence .
On the other hand , ` sum `' s throughput seems constant regardless of whether there are NaNs and where they're located : #CODE
min and max does not need to branch for floating point data on sse capable x86 chips .
So as of numpy 1.8 min will not be slower than sum , on my amd phenom its even 20% faster .
Anyway ` dot ( . )` based seems to be the most stable one .
Thanks , but a dot product is just a tad more likely to overflow when ` x ` contains large values , and I also want to check for inf .
Well , you can always do the dot product with ones and use ` isfinite ( . )` .
I may be , being stupid here as I've not used timeit much , but does concatenate not take 10x as many loops ?
** 6.38 ms per loop** , still in favor for using concatenate directly .
I find it easiest to just know which dimension I want to stack over and provide that as the argument to np.concatenate .
I have a ( 15 , 7 ) matrix and I want to multipy it by its transpose , i.e. AT ( 7 , 15 ) *A ( 15*7 ) and mathemeticaly this should work , but I get an error :
V #URL #URL cannot be broadcast to a single shape
@USER : there's only one way to get the transpose in Numpy and it's hard to get wrong .
Also , in newer ( ` > = 1.5 ` , I think ? ) versions of numpy arrays have a ` dot ` method , so you can just do ` X.T.dot ( X )` instead of ` np.dot ( X.T , X )` .
I am doing this in Python and numpy , but if there is a fast matrix method for this , I can probably translate .
I'm guessing you can modify this only to output products with the right sum .
what is the max element / case of a numpy matrix or what is the maximal size of a numpy matrix ?
Have you profiled to see how much time is being spent in eig vs everything else ?
I had a nonzero condition before that line in my code .
The other thing is the nonzero function .
where ` dYdX ` is the gradient , which seems to be , in your case ,
Ah right , so you are using the Newton-Raphson method to find zeros in a function ?
I'm not sure ` numexpr ` can handle indices , but I bet that the following ( or something similar ) would work : #CODE
@USER But this would still require to solve the same system , since the matrix I must invert is still B , now it's its transpose .
Another point is that unless you are using the matrix class ( I think that the ndarray class is better , this is something of a question of taste ) , you need to use ` dot ` instead of the multiplication operator .
And if you want to efficiently multiply a sparse matrix by a dense matrix , you need to use the ` dot ` method of the sparse matrix .
Unfortunately this only works if the first matrix is sparse , so you need to use the trick which Anycorn suggested of taking the transpose to swap the order of operations .
How can i use the unique ( a , ' rows ') from MATLab at python ?
There's this command unique ( a ) in Numpy .
Try ` unique ` instead of ` unique1d ` -- the example from that thread should work .
Assuming your 2D array is stored in the usual C order ( that is , each row is counted as an array or list within the main array ; in other words , row-major order ) , or that you transpose the array beforehand otherwise , you could do something like ...
Of course , this doesn't really work if you need the unique rows in their original order .
By the way , to emulate something like ` unique ( a , ' columns ')` , you'd just transpose the original array , do the step shown above , and then transpose back .
The results are wrk_arr which is the unique sorted array of your_arr .
And a transpose will do the trick for the column case .
Pretty much , I want all the nonzero values to equal 1 after adding the transpose , but I get the fancy indexing error messages .
Alternatively , if I could show that the matrix G is symmetric , adding the transpose would not be necessary .
The second option will only set the values to one if they have a nonzero value .
I have a list of matrix ( or arrays if it need to be ) and i want to add to every single of of them a new column of ones ( of the same number of lines ) .
Anyway it might be that working with ` svd ` ( singular value decomposition ) will actually be more straightforward one .
Because you have a symmetric matrix ( assuming you take the absolute value ) , you can " tell " numpy to use a more efficient algorithm this way .
Comparing timings , ` eigh ` takes ~ 5.3 seconds while ` eig ` takes ~ 23.4 seconds .
In sum , the solution techniques that power these functions are better suited for computation with much larger matrices ( i.e. , these functions wrap different underlying fortran routines , ARPACK , SEEUPD , among them . )
this sounds a good solution to reduce polynomials but why not simple : ` sum ([ np.poly1d ([ 1 , -x0 ]) **i * c for i , ( c , x0 ) in enumerate ( zip ( ks , offsets ))])`
oh , np.hstack it is ... concatenate make it looks strange .. thanks alot for the help .... accept and upvote .....
I have an N x N sparse matrix in Matlab , that has cell values indexed by ( r , c ) pairs such that r and c are unique id's .
The problem is , that after converting this matrix into Python , all of the indices values are decremented by 1 .
I am looking for a solution that will give me the same indices as the Matlab matrix .
Why not just operate internally 0 based indexing with python and the if needed translate them 1 based for matlab processing ( and vice versa ) .
If the indices represent something like a user id , then a dictionary might be a more appropriate data type - but this depends very much on what you are trying to achieve .
Despite of performance , resize can reshape the array in place
Since ` reshape ` returns a new view when possible , the data itself isn't copied .
By the way , my original answer to this question , which can be viewed in its edit history , is ridiculous , and involved vectorizing over indices into ` a ` .
:( The results you are getting out of this function ( ` [[ 0 20 40 ] [ 60 80 100 ] [ 121 141 161 ]]`) are for some reason - inconsistent with the those of the other tested ones ( ` [[ 0 20 40 ] [ 61 81 101 ] [ 121 141 162 ]]`) . If you can fix this , I'll be happy to include your solution in my answer + upvote yours ! :)
I already am using a minor Formatter per definition showOnlySomeTicks and the minor log axis formatter is the default and fine for where I want the ticks .
2 ) I need to sum a certain fields and get the totals .
I don't know for sure but I suspect the difference may be due to the fact that FFT / IFFT are " circular " , ie they make up for the fact that the data doesn't go on forever by pretending the signal is periodic , while convolve() fills in with zeros .
There is also another difference because of normalization -- FFT normalizes its output by dividing by I think the length of the array ( not all FFT implementations due this , but Numpy's does ) , whereas convolve doesn't due that because it thinks you actually want to add stuff up , not average it .
1 ) numpy.convolve is not circular , which is what the fft code is giving you :
@USER If you expand both the orignal signals to 2^n and then convolve , you should get a real signal out .
But if you want a circular convolution , won't adding zeros change the signal ?
scipy.signal.fftconvolve does convolve by FFT , it's python code .
How to have logarithmic bins in a Python histogram
As far as I know the option Log=True in the histogram function only refers to the y-axis .
And set the scale of xaxis to log scale .
I am using nonzero but I am getting an error .
And is there a way to get the common indices from two nonzero statements .
and these two indices would contain : #CODE
Try ` indices = nonzero (( new_arr 0 ) ( old_arr 0 ))` : #CODE
@USER : No such assumptions were mentioned , but if you're talking about " common indices " , then you'd just end up resizing the larger array to the dimensions of the smaller one anyway .
( Thinking about it , my previous example wasn't all that useful if all it did was return ` nonzero ( condition )` anyway . )
Then , you'd be able to do exp ( eigenvalues ) .
@USER , you're right , the mistake was with the ` eig ( a )` , should have been ` eig ( ex )`
For almost all practical proposes exp ( -1000 . ) is zero .
#URL has 19 different way to compute the matrix exp .
I have no experience with numpy in particular , but adding a decimal point with a set amount of zeros may help .
The numbers are already floats , it's just that exp (-8 00 ) is about 3.7E-348 , which is less then numpy's ( or python's ) precision
I have an array A , and I have a list of slicing indices ( s , t ) , let's called this list L .
Pycuda messing up numpy matrix transpose
It simply manipulates the strides to obtain the transpose .
The basic reason is that numpy transpose only creates a view , which has no effect on the underlying array storage , and it is that storage which PyCUDA directly accesses when a copy is performed to device memory .
The solution is to use the ` copy ` method when doing the transpose , which will create an array with data in the transposed order in host memory , then copy that to the device : #CODE
Multidimension histogram in python
I have a multidimensional histogram #CODE
A histogram stores several points ; in your example line of code , you have described one point : ` ( x , y , z )` .
How to get indices of N maximum values in a numpy array ?
possible duplicate of [ python+numpy : efficient way to take the min / max n values and indices from a matrix ] ( #URL )
For example , what would the indices ( you expect ) to be for ` array ([ 5 , 1 , 5 , 5 , 2 , 3 , 2 , 4 , 1 , 5 ])` , whit ` n= 3 ` ?
To get the indices of the four largest elements , do #CODE
Unlike ` argsort ` , this function runs in linear time in the worst case , but the returned indices are not sorted , as can be seen from the result of evaluating ` a [ ind ]` .
The subsequent sort only handles k elements , so that runs in O ( k log k ) .
I am trying to join both of them to get something like ( user , time , location , purchase )
nonzero() returns the indices of a matrix as a 2-d array .
Then , eliminate the most deeply nested loop altogether by replacing the ` append ` with #CODE
Would all the triples be unique ?
If there are no constraints on the target language , I'd simply translate to ` R ` and use ` RApache ` .
Do you know how you will rotate it ... whether in a 2D fashion or 3D ... whether pitch ( nose up or down ) , yaw ( nose left or right ) , roll ( sides up or down ) or a combination ?
I used it once to transpose an array , but I don't know if it might fit your needs .
I think the OP wants to ** geometrically ** rotate the ellipsoid through a certain angle , not transpose the array .
So I am trying to compress those by as much as possible without loss of data .
I probably won t be able to compress that any further ?
However , you aren't going to be able to compress it that much -- if you have 15GB of data , you're not magically going to fit it in 200MB by compression algorithms .
There is a faster and more space-efficient way : just dump it .
performing sum of outer products on sparse matrices
Basically I'm trying to multiply each col of x by each col of y and sum the resulting nxn matrices together .
Your equation is a sum of inner products , not outer products .
You must transpose the columns of ` y ` , not ` x ` .
Please edit your question to be unambiguous respect to transpose .
Are you aiming to count how many times each nonzero element is summed in outer product ?
Note that a sum of outer products in the manner you describe is simply the same as multiplying two matrices together .
For n=10000 and m=100000 and where each column has one nonzero element in both X and Y , it computes almost instantly on my laptop .
And the last step would be to set nonzero elements to 1 , like Z.data [: ]= 1 .
I employed eat's advice and set all nonzero elements to one after as well .
There are many good , freely available , molecular dynamics packages out there you could use : LAMMPS , Gromacs , NAMD , HALMD all come immediately to mind ( along with less freely available ones like CHARMM , AMBER , etc . ) Modifying any of these to suit your purpose is going to be vastly easier than writing your own , and any of these packages , with thousands of users and dozens of contributors , are going to be better than whatever you'd write yourself .
These days , it seems few scientists write their own MD implementations , but it is useful to be able to modify existing ones .
I can create a class which wrap a NumPy array inside , and resize this array when it's full , such as : #CODE
You do know that numpy has an append function , right ?
@USER : Yes I know the append function , but I need a dynamic array that increase it's capacity by a factor such as 1.25 when it is full .
@USER , for what it's worth , the ` append ` method of an ` array ` doesn't increase its capacity by a factor -- it increases its capacity by exactly one .
@USER , I tested the append method of array by measure the time it cost , since resize the array will use more time .
Numpy is working for everything except the one files which are already compiled ones .
Numpy.mean , amin , amax , std huge returns
My problem occurs when I use numpy to get the mean , min , max , or standard deviation .
Returns a float std as one would expect .
q = rand ( 5600,160 00 ) , giving me a nice large test array .
Of these , np.std is significantly slower : most functions take 0.2 seconds on my computer , whereas std takes 2.3 .
While I don't have the exact memory leak you have , my memory usage stays mostly constant while running everything except std , but doubles when I run std , and then goes back down to the initial amount .
I've written the following modified std , which operates on chunks of a given number of elements ( I'm using 100000 ): #CODE
Numpy does a " naive " reduce operation for std .
You can tile a value in any shape : #CODE
I want to pass the arguments generically and i don't want to use the function reshape afterwords
So using ` empty ` with ` fill ` seems significantly faster than ` tile ` .
@USER , it's not clear to me that these differ significantly in terms of parameters -- tile just infers the dtype from the type of the argument , which you can make explicit by passing ` numpy.int64 ( x )` or whatever .
What is the best way to subtract from each patch it's average , in other words each patch has a unique mean and I want to subtract it .
Then computes the histogram for each file and the results are added to a two dimensional matrix in order to create something like a 2D-Hist .
Scale 2D color plots like ` pcolor ` and ` imshow ` by passing a matplotlib.colors.LogNorm instance using the ` norm ` keyword .
As i tried to find a solution to this log problem , i found as well the link you gave here , but the solution of ` set_yscale ( ' log ')` is unfortunately not working with ` imshow ` .
Ah ok , the ' histogram ' is presented as a colour plot ?
If you only need the indices , you could try ` numpy.ndindex ` : #CODE
` is simply applying your function to your data , could you have the data be precisely an array of indices ?
Then ` func ` would use the indices for windowed access to ` data_a ` , ` data_b ` , and ` data_c ` .
If you want something _really_ barebones and unmanaged ( and insanely cheap ! ) , try the ones at #URL Otherwise , there are plenty of _excellent_ managed VPS services .
Is there something similar in Python's scipy or am I supposed to do it using the ` fft ` module ?
np.correlate does not use fft , and is only faster when the second series / window is small relative to the first . there is also scipy.signal.fftconvolve . see also #URL
We can translate it pretty easily into pure numpy , using ` ix_ ` to make the arrays broadcast against each other nicely .
Note that this doesn't vectorize , unless you broadcast and create a big array , unutbu's answer with searchsorted is the efficient vectorized version .
Technically , @USER answered my question but clip is noticeably faster .
If the similar ones are the majority , I switch ( 0 -> 1 or 1 -> 0 ) that particular element .
y is the sum of the neighbors of every element .
Do the same convolve with all ones , you get the number of neighbors number of every element : #CODE
But it was rather a general question , I didn't know that this could have been done with a method convolve .
But before doing that , try image processing methods first , such as convolve , morphology image process .
Or , even better , is it possible to get a list of indices of all the occurrences of the maximum value in the array ?
1 ) How many unique features are there in total ( aggregated over all users and documents ) ?
Instead of the mean , you can also take the sum , which is similar to concatenating all documents per user , except that the idf weighting is different .
Cool , so the best would be to do something like def grayscale ( x , min , max ): t = int_max* ( x-min ) /( max-min ) return np.uint8 ( t ) *0x1 + np.uint8 ( t ) *0x100 + np.uint8 ( x ) *0x10000 + 0x0f000000 , where int_max would be the maximum value of 8-b it ints .
How to truncate the values of a 2D numpy array
I have a two-dimensional numpy array ( uint16 ) , how can I truncate all values above a certain barrier ( say 255 ) to that barrier ?
actually there is a specific method for this , ' clip ' : #CODE
There isn't a built in way to do it , as it's usually easier to just roll your own , than it is to make a " one-size-fits-all " function meet your needs .
Yes , you can reshape it afterwards .
Still , you'll have to restore the 2D shape , hence the predicate call to the cursor method rowcount . and the subsequent call to reshape in the final line .
I know how to create an histogram in Python , but I would like that it is the probability density distribution .
With the following code I am building a simple histogram telling me how many elements of my array ` d ` are between every bin .
You are probably using ` hist ` from matplotlib , not ` histogram ` from numpy .
If you're using NumPy's ` histogram ` , try setting ` density=True ` .
Matteo is probably using ` hist ` from matplotlib , not ` histogram ` from numpy .
I tried with histogram from numpy with density=True but I got this error : TypeError : histogram() got an unexpected keyword argument ' density '
I expect that there will be a unique point where the infinite solutions cross the unit-ball defined by the one norm in the 1st quadrant which is the solution i'm looking for .
Unless I am missing something , this system of equations has no unique solution .
However the restriction is that of these solutions I want one that is distance one away from the origin using the one norm .
@USER : I don't think that the one norm will help you define your solution space .
|| x || is actually 5 in your example when using that norm .
yes sorry , i think i meant the inf norm . basically I just wanted x to be a vector with 1's and 0's in it that satisfied the matrix
Thanks anyway , this works on the smaller ones so +1
But eventually you may need to twist all of the thresholds and norm specification ( ` p `) :( .
Does the line [ x.replace ( year= x.year +10 ) for x in B ] create new copies of the date objects or does it manipulate the original ones ?
I want to be able to find a couple points on an eye diagram ( generally used to qualify high speed communications systems ) , and as I have had no experience with image processing I am struggling to write efficient methods .
As you can probably see , these diagrams are so called because they resemble the human eye .
The measurements that are normally taken are jitter ( the horizontal thickness of the crossing region ) and eye height ( measured at either some specified percentage of the width or the maximum possible point ) .
@USER I know that I can figure out how to transform my data directly into a bitmap , which would compress it greatly , and the processing would then be a lot more efficient , as I can test by simply importing these images using ` PIL.Image ` and using ` ImageFilter.FIND_EDGES ` .
The jitter needs to be within 1 ps ( I actually am currently interpolating to get more exact , not just the closest discrete point ) , and as these are actually simulated eye diagrams , the amount of data that I have to input is quite large in order to get an accurate measurement out .
Assigning multiple array indices at once in Python / Numpy
Each row is mostly zeros , occasionally interspersed by sequences of non-zero numbers .
Using where and diff a few times , we can get the start and stop indices of each run .
At this point , since the only route I know of is what jterrace suggest , we'll need to go through some contortions to get similar start / stop positions for the zeros , then interleave the zero start / stop with the values start / stops , and interleave the number 0 with the last_values array .
Most efficient way to sum huge 2D NumPy array , grouped by ID column ?
In SQL this would look like SELECT sum ( score ) FROM table GROUP BY id
the output is [ 0 . 50 . 21 . 18 . ] , which means the sum of id == 0 is 0 , the sum of id == 1 is 50 .
This may be a bit more effective than using ` np.any ` , but will clearly have trouble if you have a very large number of unique ids to go along with large overall size of the data table .
While I really like this approach , I should note that in general finding candidates by l2 norm is not better than finding a particular symbol from needle .
But after a small modification by computing dot product with randomized pattern of the same length as needle , this method will be just awesome .
You have to pay per data you move over , so I want to compress all my sim soutions as small as possible .
So my question is , what is the best way to compress numpy arrays ( they are currently stored in .mat files , but I could store them using any python method ) , by using python compression saving , linux compression , or both ?
I have a small NXN array " block " that I want to plug into a specified region ( i.e. , a diagonal region at " start ") of a large array " wall " .
The Ruby code should be easy to translate over to Python ; much easier than it would be to translate it to say , C++ .
Doing this is ` O ( n log n )` I believe .
Instead of ` O ( n^2 )` behavior , it drops down to ` O ( n log n )` as well .
For instance , the matrix of all z-z ' values is simply obtained from the 1D array ` Z ` of coordinates as ` Z-Z [: , numpy.newaxis ]` ( the diagonal terms [ z=z '] do require some special care , when calculating ` 1 / abs ( z-z ') **3 ` : they should be set to zero ) .
Using the above module , I've also tried to cut out a few steps for the collision detection by comparing the returned distance to the sum of the particles ' radii as such : #CODE
More exactly , the diagonal of the matrix is the variance and by square rooting the values , the standard error of each coefficient can be determined !
Saves the covariance matrix's diagonal to a 1D matrix which is just a normal array
do you mean the first ( array.append + numpy conversion ) or ( numpy.append + reshape ) ?
Using concatenate seems to be twice as fast as the first version and more than 10 times faster than the second version .
If you want to create a subarray consisting of specific elements from the original array , you can use another array to select the indices : #CODE
numpy sum along axis
Is there a numpy function to sum an array along ( not over ) a given axis ?
to sum along axis i .
Call sum twice ?
As of numpy 1.7.1 there is an easier answer here - you can pass a tuple to the " axis " argument of the sum method to sum over multiple axes .
So to sum over all except the given one : #CODE
You can just pass a tuple with the axes that you want to sum over , and leave out the one that you want to ' sum along ' : #CODE
If you want the other axis you could transpose everything : #CODE
The moment ` svd ( self.A )` is called , it throws an error .
Yes , the ` full_matrices ` parameter to ` scipy.linalg.svd ` is important : your input is highly rank-deficient ( rank max 3,241 ) , so you don't want to allocate the entire 12,596 x 12,596 matrix for ` V ` !
NumPy : Find sorted indices from a masked 2D array above and below a threshold
But with the sorted indices , I need to divide them into two simple subsets : less than or equal to and greater than or equal to a given datum .
For example , with ` datum = 51.1 ` , how do I filter down ` sorted_ind ` to the 10 indices above ` datum ` and 8 values below ?
And it will create a flatten array every time .
Say I have N arrays of shape ( n_i , m_i ) , and I want to combine them into a single array of size ( sum_{1 , N}n_i , sum_{1 , N}m_i ) such that the component arrays form blocks on the diagonal of the result array .
In other words , X minimizes norm ( A*X - B ) ,
How to find the min max with excluding zeros in a numpy array ( or a tuple ) in python ?
I want to find the minimum and maximum within the array which should not take zeros in accout .
For example if the number are only negetive . zeros will be probelmatic .
A simple way would be to use a list comprehension to exclude zeros .
You could use a generator expression to filter out the zeros : #CODE
This worked perfectly for me - I had a case where I needed to retrieve the index of the min / max nonzero value .
Using the ` reshape ` method is more direct , readable , robust , and efficient than this , and less errorprone .
For instance , the GROUP BY operation implicitly requires ( at least ) one field / column of continuous values ( to aggregate / sum ) and one field / column to partition or group by .
Numpy array , how to select indices satisfying multiple conditions ?
` np.where ( mask )` converts the boolean mask into the indices where ` mask ` is True ;
` [ a [ second_mask ] for a in ... ]` subsets the indices to only select those where ` second_mask ` is True .
generates a flat array of indices where ` mask ` is true , which is then decimated further by applying ` second_mask ` : #CODE
I would decide on a resolution for the analysis ( i.e. the size of your closed interval , call it delta X ) , and then as @USER mentioned , find the max and min of the function within that closed interval , including the end points .
That will give you n intervals ( or delta Xs ) , and you will have found the max and min of each interval ( let's call them the delta Ys ) .
It seems like then you should be able to slide the window over the width of a single delta X , and sum the delta Ys for the window .
Find the smallest sum , and that should correspond to the " smoothest " 15% -- given that smooth means least Y variance .
It seems to work by using ' outer ' to create the values you want , but with twice as many dims as you want ; and then by using calls to ' concatenate ' to eliminate these dims one at a time , with large amounts of copies at each step .
Would be better to allocate the final array using empty() , create a reshaped view of that array with 2 * dims , and just do result_view [: ] = outerprod to get the whole reshape in one step .
Or even just transpose followed by reshape() ?
If you're only multiplying EV_subset by ones , you don't really need to call ` np.kron() ` .
The following may help ( in the general case when one of the arrays is not ' ones ') .
Gets it done in a single ' multiply ' op and a quick reshape .
Specifically , I would like to select certain elements of one indexable container using elements of another container as the indices .
you want to stack them : #CODE
Probably the simplest way to do this is to locate all of the zeros using something like : #CODE
And then just calculate the adjacent indices to the pixels that are zero ( all combinations of +1 , -1 the zero indices ) .
I build an array of the count of adjacent zeros , and use that to get the positions which are adjacent to exactly one zero .
If you change the ordering of the indices , like this : ` points [ ' a '] [ 0 ] [ 0 ] = np.array ([ 1 , 1 , 1 ])` , it works ok for me ( python 2.6.5 , numpy 1.3.0 on Ubuntu 10.04 ) .
@USER : AFAIK , it seems natural to first specify the named column , and only then go for the numeric indices .
I would like to be able to have rows consisting of zeros and a few ( say 5 ) ones , and add them a few at a time ( on the order of the number of cpus being used . )
Sorting numpy array according to the sum
I want to sort an numpy array according to the sum .
It says that I am missing some modules , such as numpy ones .
i would call that the cumulative max - running max suggests a window to me . unfortunately googling for that doesn't turn up anything useful .
i don't have numpy installed , but max.accumulate might work . check out " accumulate " in the docs .
@USER max doesn't have an accumulate attribute in numpy .
Or you ' constructing ' it by doing something like ' append ' for lists ( i.e. , adding a row w each iteration ) ?
mat , data = createmat ( ID ) initializes data to be a 1850x1850 ndarray filled with 0s .
In this situation ` numpy.unique ` can be used to generate an array of unique " key " values : #CODE
The standard way to do this which I wish to pythonize is to perform a moving linear regression to the data and save the gradient ...
the calculation for linear regression is based on the sum of various values .
so you could write a more efficient routine that modifies the sum as the window moves ( adding one point and subtracting an earlier one ) .
for evenly spaced data the denominator is fixed ( since you can shift the x axis to the start of the window without changing the gradient ) . the ( ? X ) in the numerator is also fixed ( for the same reason ) .
I've seen quite a few ' How to flatten list of lists ' questions on SO , but I'm still having trouble with my problem .
I've been trying to flatten my list like this : #CODE
How can I flatten a list with non-iterable data types ?
If you are selecting over trailing axes , you can reshape the trailing axes to -1 , and apply func to axis=-1 : #CODE
I am trying to append a large dataset ( > 30Gb ) to an existing pytables table .
I've considered appending ` numpy.zeros (( len ( new_data ) , N ))` to the table , then using ` Table.modifyColumns() ` to fill in the new data , but I'm hopeful someone knows a nice way to avoid generating a huge array of empty data for each chunk that I need to append .
Why not just append a column of zeros ?
To what are you suggesting I append a column of zeros ?
Keep in mind that depending on how the data set was created in the h5 file , it may not be possible to simply append a column to the data .
h5py supports resize if the underlying dataset supports it .
concatenate needs both elements to be array , however a [ 0 ] is not an array .
You're right about ` append ` ( and have my +1 ) , but I think ` concatenate ` is supposed to take a tuple of arrays , as per [ this ] ( #URL ) and [ this ] ( #URL ) , for example .
The results are : for ' dot ' #CODE
still ' dot ' is the winner , I am still searching for a better method but no success
Get original indices of a sorted Numpy array
I have an array of distances ` a = np.array ([ 20.5 , 5.3 , 60.7 , 3.0 ] , ' double ')` and I need the indices of the sorted array ( for example ` [ 3 , 1 , 0 , 2 ]` , for ` a.sort() `) .
I am looking for the indices of the first occurance of a specific element in each row .
Well , could there be any solution which contains only the the rows with the indices of first element where the element exits without -1 ?
When checking conditions of numpy arrays , I usually end up with using ` numpy.where ` , with only a condition as arguments , it returns the indices of the array : #CODE
Somewhat shorter , just use compress in this case , which combines both : #CODE
I have an integer list which should be used as indices of another list to retrieve a value .
I think its just the problem that I have to convert the b to comma seprated indices butnot very sure .
I dont understend ' convolve ' , get ' con ' fused
The numpy ` append ( arr , item )` doesn't append in-place and therefore would require re-creating arrays for every point in the table ( which is slow and unneccesary ) .
I could also iterate over the list of data-columns and wrap them into an array after I'm done ( which is what I've been doing ) , but sometimes it isn't so clear cut about when I'm done parsing the file and may need to append stuff to the list later down the line anyway .
Python lists are relatively cheap to build and append ; NumPy arrays are more expensive to create and don't offer a ` .append() ` method at all .
don't offer a .append() method at all " NumPy certainly does have an append method .
It works more or less as python append except it is not " in place " .
Then , ` a.append ( 4 )` gives the message ` AttributeError : ' numpy.ndarray ' object has no attribute ' append '` .
So NumPy has an append * function * not an append method -- which makes your statement in your Answer above completely correct !
I'm reading in a list of housing prices , and trying to get the sum of the squares .
Since the sum of the squares of the housing prices is greater than a 32 bit integer , it overflows .
Off topic : checked your site ; you might like to see [ this ] ( #URL ): seems you are not the only ones looking for warmer water around the arctic ...
If you're going to be reading in a lot of files that potentially have that problem , you can just pad the array with zeros or truncate it , like this .
edit : great Idea from Joe Kington to fill image with zeros !
use ` rowp.extend ` instead of ` append ` .
Update : Use ` extend ` instead of ` append ` : #CODE
I'm trying to sum the elements of separate data array by their characteristics efficiently .
Because now it sums all 1000 values of the different rows to one total sum .
Each index in each dimension corresponds to a unique value in your input lists .
You can then use something like this cookbook example to accumulate the datasets variable into the appropriate bins using age , year , and cause .
If I understand , you want to choose a pairing such that ` len ( result ) == min ( len ( a ) , len ( b ))` and ` sum ([ abs ( p [ 1 ] -p [ 0 ]) for p in result ])` is minimised .
concatenate both arrays
use 2 stacks : go through the array putting elements with key 1 on the left stack , and when you cross an element with key 0 , put them in the right stack .
sort should be slowest step and max total space for the stacks is n or m .
where the dimension is understood from the length of the ` indices ` tuple .
So if ` indices ` equals , say , ` ( 6 , )` or ` (8 , 5 )` then I want to have the equivalent of #CODE
The dimension of ` result ` has been set earlier to the correct dimension , so can check it , but it would be nice to only use the length of ` indices ` to determine it .
If you need to know the length of indices : #CODE
And if it is 1 I could do result [: , indices [ 0 ] , iteration ] = ..., or if it is 2 , result [: , indices [ 0 ] , indices [ 1 ] , iteration ] = ...
Try using ` result [( slice ( None ) , ) + indices + ( iteration , )]`
Hi , the combinations ( range ( nsamples ) , 2 ) already returns an iterator to the upper triangular matrix indices , therefore the two methods are equivalent : timing the two version there is a small gain in the two for loops version ...
Numpy modify ndarray diagonal
is there any way in numpy to get a reference to the array diagonal ?
I want my array diagonal to be divided by a certain factor
A quick way to access the diagonal of a square ` ( n , n )` numpy array is with ` arr.flat [: : n+1 ]` : #CODE
@USER - As a side note , with newer versions of numpy ( > = 1.5 , I think ? ) ` dot ` is also an ndarray method .
b ) Offload more of the grouping and aggregation into Python ( haven't figured out a way to join three tables that is actually faster )
Drop the * user_rating.id * column and convert the * user_id * and * tag_id * columns into a PRIMARY KEY for that table , because these 2 fields are a unique combination in the table .
([ 1 ] Remove the unique ID column and add a joint primary key on video_id and tag_id )
Are there any fundamental differences to use numpy.zeros or zeros ?
If you want to refer to ` zeros() ` directly , you can ` from numpy import zeros ` .
The first code sample doesn't work because you didn't import ` zeros ` .
From the stack trace , it would appear that the problem is with the ` tokenize ` module , which is part of Python , not part of ` numpy ` .
I'm wondering what is the most efficient way to sum elements of an array by given characteristics .
For example I have 1000 draws of data , and I what I'm looking for is the sum of each draw ( column ) across sexes for a given year-disease ( ie , the draws are by sex , year , disease , and I want the sum of both sexes for each year and disease ) .
Any thoughts on how to get an array that will be shape ( 20 , 1000 ) that has the sum of the draw across both sexes for a given year-disease ?
This results in an dict associating each ( year , disease ) with the corresponding sum of the draws .
numpy histogram with 3x3D arrays as indices
What is an elegant way in numpy to to create a histogram volume of the input channels ?
You can use numpy's ` histogramdd ` to compute the histogram of an n-dimensional array .
If you don't want a histogram for each 2d slice , be sure to set the ` bins ` for that dimension to 1 .
To get the overall histogram , you could compute them individually for the R , G and B channels and then take the maximum value of the three for each position .
What you have to do is flatten each of your three 3D arrays and then combine them into a 2-d array of dimensions ` ( x_dim*y_dim*z_dim , 3 )` , which you pass to ` histogramdd ` .
The fact that your original data are 3D is a red herring , since the spatial information is irrelevant to calculating the histogram .
However , unless ` x_dim ` , etc are much larger than ` N ` , then I imagine this will give you a rather sparse and noisy histogram .
This took 30 sec of CPU time and the python process seemed to max out at about 1.5GB memory usage .
Thus we can convert the 3 arrays R , G , B into a single array of " color values " and use ` np.bincount ` to produce the desired histogram .
Masked arrays also support many operations besides ` sum ` .
Increment Numpy multi-d array with repeated indices
I'm interested in the multi-dimensional case of Increment Numpy array with repeated indices .
I'm using this with cubic bins and flatten cubes of the same size for i , j and k .
In numpy , negative indices indicate indexing starting from the end of the array and working backwards .
Calculating cumulative returns in Numpy , problem with array indices
Now I am getting an error saying that array indices used must be of integer or boolean type .
What's the stack trace ?
My understanding is that in the generated html file , the highlighted lines are ones where python functions are called - is that correct ?
Replicating the indices result of Matlab's ISMEMBER function in NumPy ?
I have been trying to find a Python code pattern that replicates the indices result .
If your corruption is coming from the network , if you compress the files to a tar.gz and then take the md5 of them , when they arrive on your end , you can verify the md5 to make sure it's correct .
Essentially , the sum operation in this this field ( GF ( 2^8 ) is normal XOR operation , and the product operation a*b is ( a*b ) mod m ( x ) .
Your solution works well with normal operations ( for example , mat ( a ) *mat ( b )) , but when applying inverse operation the result is not what I expected ...
Then to calculate the disability weight attributable to each disease given the comorbidity of multiple diseases , I do the following : Set the denominator as the sum of present disability weights , and use the disability weight of a given disease as the numerator .
However , I often end up subsetting one column of an array , and then having to designate this reshape .
Thus , you need to transpose ` y ` for it to work .
And if it is transposed ( flipped at the diagonal ): #CODE
Numpy sum of values in subarrays between pairs of indices
If this turns out to be too slow , please tell us more about the size of ` A ` , how many pairs of indices you expect to get , whether the ` ( a , b )` ranges tend to overlap , etc .
Assuming your index pairs are stored in a NumPy array ` indices ` of shape ` ( n , 2 )` and ` n ` is fairly large , it is probably best to avoid any Python loop : #CODE
The ` cumsum ` one above is probably more efficient if there are many indices .
Currently , ` printMultiples() ` cannot know how many spaces to put after each number because it doesn't know how many times it will be called in the outer ` while ` loop .
This makes accessing elements work with both strings and numerical indices : #CODE
A general solution for Python iterables ( not specific to numpy or arrays ) that works in linear average time ( O ( n+m ) , n is the number of subarrays and m is the number of unique subarrays ): #CODE
Filtering the non-repeats out is O ( m ) , where m is the number of items in the mapping , which is the number of unique items in the multidimensional array , so the overall operation is O ( m+n ) on average .
Edit : If the columns are of types ` S10 ` and ` S12 ` respectively as indicated in the comments , you can do a minor optimisation of this code since you don't need to explicitly join the columns : #CODE
calculate the cumulative sum of the numbers in that column .
X = zeros (( 2 , 2 ) , dtype=float64 )
Y = zeros (( 2 , 2 ) , dtype=float64 )
Could you post the absolute smallest case , from start to finish , that reproduces it , as well as your python / numpy version numbers ?
When I run this ` print zeros (( 2 , 2 ) , dtype=float64 ) [ 0 , 0 ]` I get ` 0.0 ` .
The zeros function , when passed only the shape , assumes a float64 type ..
There are often creative things you can do with accumulate and ufuncs ..
It performance worsens as the number of unique elements in ` a ` rises .
For the argument sake , yes something similar , regarding the HOG , I have previously implemented it using nested loops to extract its cells histogram , I thought that the stride tricks would save some performance and eliminate the usage of 2 loops that is really expensive in this context using a language like python or matlab .
The small complication in your Question is just that you want to use a string for row_index , so it's necessary to remove the strings ( so you can create a 2D NumPy array of floats ) , replace them with numerical row indices and then create a look-up table to map the the strings with the numerical row indices : #CODE
well , the keys are probably extracted from a python list rather than a NumPy array -- the whole point of using a look-up table is to remove the strings so you can represent the data as a NumPy array , so to get the keys from a nested python list called ' data ' ( assuming keys in 1st col ) use : keys = [ row [ 0 ] for row in data ]; for values , better to use ' 0 ' -based indices -- to easy to get confused otherwise , so vals = range ( ken ( keys ))
I need the pixels to saturate ( max=255 ) or truncate ( min=0 ) instead of wrapping .
` x = np.arange ( 0 , 54 , 0.2 )` will give you 270 items , but in your histogram you set 274 bins .
The bidimensional histogram of samples x and y .
As @USER implies , you need to take into account that ` histogram2d ` returns the edges in addition to the histogram .
Another detail is that you probably want to explicitly pass in a range , otherwise one will be chosen for you based on the actual min and max values in your data .
but personally I wouldn't use contours in this case , since the histogram is likely to be noisy .
It happens during an append process like :
I bypassed the issue by storing the indices of the two kinds of objects ( i.e. , cone_indices and sphere_indices ) .
I'd accumulate the values of ` time_stamp ` in a list and do one ` hstack ` at the end , and would do exactly the same for ` record_t ` etc .
the process hangs . gdb says something's blocking a semaphore / lock acquisition , and the stack trace suggests something floating point-related : #CODE
Numpy , making use only of the functionality of ` dot ` .
Both the hardware as well as the software stack changed from the setup in the original answer .
The software stack is for both nodes the sam .
The new benchmark results are similar to the ones in the original answer .
simple dot product of matrices with different sizes
You can get rid of the sqrt by squaring the radius instead if you really want speed .
It may also help to change your lambda to truncate the hours portion of your date : ` lambda s : np.datetime64 ( s [: 10 ] , ' D ')`
Yes the thread may become useful but I want to solve this problem this way , trying to create a triangular bottom zero matrix ( zeros on the right corner ) .
This basically merges all of the matrices in ` matrices ` into a single array using ` np.hstack ` and then takes the max of that new array .
This will stack all of your matrices along a third axis and then give you the max along that direction , returning a 5x5 matrix for your case .
The OP wants an output matrix where each element is the max of all of the elements located at that same position in the input matrices .
@USER just because there are many such ndarrays created in the current code ( not mine ) , and the orginal code directly dump the vstacked content to output without formatting , so I want to know if there is a straightforward way to format them without much refactoring the original code .
But if I append the last subslice , I get : #CODE
Another suggestion , i would try first to resize the array , then fill it up : #CODE
I hope you wouldn't mind answering these two follow ups : ( 1 ) How can I access a specific value within the structured array [ say the 3rd entry in the 10th row ] ? and ( 2 ) is there a way to reshape the array to turn it into a 373x15 shape ?
This works because the max of array == item is True or 1 , and returns the first index where this occurs .
In your case , numpy has to go through the entire array twice , once to create the boolean condition and a second time to find the indices .
It's O ( log ( n )) instead of O ( n ) .
``` >>> cond = " import numpy as np ; a = np.arange ( 40 )"```
` timeit ( " np.searchsorted ( a , 39 )" , cond )` works for 3.47867107391 seconds .
Just a note that if you are doing a sequence of searches , the performance gain from doing something clever like converting to string , might be lost in the outer loop if the search dimension isn't big enough .
The best I've come up with is to roll my own _to_col function : #CODE
Most specifically , I am seeking an OpenCV command in Python that takes an image and a pixel location as input ( and possibly also some parameters about the size of a detection window ) and then just returns a Python array that contains the HOG feature vector ( i.e. a list or NumPy array , etc ., where the jth element of the list is the jth histogram component from a histogram of oriented gradients ) .
I can probably translate this into Python myself .
It would probably be just as much work to translate the top Matlab routine from Maurits .
The final ` reshape ` does copy the data , but only once unlike using a double ` repeat ` which will copy the data twice .
Doesn't ` arr [ cond ] , arr [ ~cond ]` mean it tests every element of the array for the same condition twice ?
I lost control of the OpenID domain used to log in to that other account , so I can't log into it and change it either .
Consider I have a 3D matrix of N X M X M matrix , where I need to get rid of any matrix MXM that has all its elements the same [ all zeros to say ] .
What I need to do is get rid of the first 5X5 matrix since it contain all zeros .
I agree , this is the best workaround , but I think you need to reorder the size of the reshape ...
It should be ` ... reshape ( im2.size [: : -1 ])`
Although you will always need to hand roll this for the problem at hand , because ( as mentioned in code comment ) you'll need to define which indices you're summing over , and be careful about the array lengths or things won't work !
My suggestion for a C implementation would be to use a simple struct to hold the indices and the values : #CODE
In that case , you can simply loop over the keys in the dictionary , and extract the indices from the tuple .
At some point , you'll have to deal with the data that is there , though , which means looping over your dummy indices ...
I loaded a big-endian raw array from file , but when I applied a log transformation , the resultant array would be little-endian .
I got around the problem by first allocating a second big-endian array , then performing an in-place log : #CODE
I'm using the numpy package for Debian squeeze , which I realise is out of date , but I'm having some problems backporting the 1.6 package from unstable .
Hi Faheem , it looks like the dev version of the docs is identical to the official v1.6 ones : #URL
I'm attempting to use the fft module in numpy : #CODE
It didn't occur to me to exploit the unsigned wrap around combined with the add-assign operator to have some control over the type of the result of the sum .
How to select all locations of unique elements in numpy 2d array with bounding box around them ?
I have a 2D numpy array I want to find the ' every ' location of all the unique elements .
We can find the unique elements using ` numpy.unique ( numpyarray . )` .
Now I have to know all the locations for every unique element .
It is to be noted that all the unique values will be adjacent to each other .
The only gaps between them could only be zeros .
This will work for the example with zeros , as well .
In the particular case of your example , where your unique values are sequential integers , you can use ` find_objects ` directly .
( 0 is ignored , exactly as you want . ) However , in general , you'll need to do a touch of pre-processing to convert arbitrary unique values to sequential integers .
These are ` slice ` objects , so the " max " value will always be one higher than the " max " in the previous example .
If your unique values are not sequential integers , you'll need to do a bit of pre-processing , as I mentioned before .
Ignoring the zeros for the moment , is there a way to do this using ` ndimage.find_objects ` ?
For example , if every element has the same possible values ( eg , [ 0 , 1 , 2 , 3 ]) , then generating an array of every possible length L 1D array with those values can be done with ` lambda x : indices ( repeat ([ 4 ] , L )) .reshape (( L , -1 )) .transpose() ` ; then I can just do ` calc_dG ( result )` , and use result [ results that are in the desired range ] to get the arrays that I want as a final result .
What is indices ?
MySQLdb uses ` re ` and string interpolation to join parameterized SQL with arguments before passing the query to the server as a string .
I have developed a PyQt4 application for analysing delimited log files containing numeric data .
append field to record array ?
How do I append columns to this structured array ?
Rectangular bounding box has already been solved in the Question : How to select all locations of unique elements in numpy 2d array with bounding box around them ?
We want to construct the indices in such a way that zero values within the extremas are considered within the region .
In this sense it is more of a region segment problem with exactly similar value with exception of zeros inside the region .
It is to be noted that the region is only consist of a unique value and zero .
We want to find which sift features lie in one unique area without any comparison of the features .
In our case there can be zeros ( or any other value ) in between .
So this is merely detecting a region constrained by a unique value .
Just change the zeros to each unique value before running connected components .
then it will be a region with zeros withing the region which will not help the cause anyways ...
I have a 2D matrix and I want to take norm of each row .
But when I use numpy.linalg.norm ( X ) directly , it takes the norm of the whole matrix .
I can take norm of each row by using a for loop and then taking norm of each X [ i ] but it takes a huge time since I have 30k rows .
The above computes the L2 norm .
For a different norm , you could use something like : #CODE
If you are computing an L2-norm , you could compute it directly ( using the ` axis=-1 ` argument to sum along rows ): #CODE
Other ` ord ` forms of ` norm ` can be computed directly too ( with similar speedups ): #CODE
How to do the same If I want to apply norm column-wise to a matrix ?
For a 2D array , the 0-axis refers to rows , so ` axis=0 ` causes ` norm ` to sum down the rows for each fixed column .
Dicrete Fourier Transform : How to use fftshift correctly with fft
For testing , I'm using the Gaussian function Y = exp ( -x^2 ) .
The ( symbolic ) Fourier Transform is Y ' = constant * exp ( -k^2 / 4 ) .
The definition for the output of ` fft ` ( and ` ifft `) is here : #URL
In above array all the ones are connected .
pointers in order to reach the data , if I wanted to reach my array in a more familiar ( or tidier ) way to me , with indices like
how to fill the start and the end of a unique entry with the same value in numpy array ?
In the above array the unique entries are considered except the value which is 255 .
We want to fill values between each unique entry .
where ` b ` is a list of tuples consisting of ` ( unique value , start index , end index + 1 )` .
I use lots of numpy dot products and addition of numpy arrays , and if I did not use multiprocessing it would be a no-brainer : Make sure numpy is built against a version of blas that uses multithreading .
If you are already using multiprocessing , and all cores are at max load , then there will be very little , if any , benefit to adding threads that will be waiting around for a processor .
I'm not an expert in multiprocessing / threading ( yet ! ) but to my understanding , if a single processor is at max load , adding more threads will only cause more overhead to swapping between them .
If all of the cores are at max load and if you split the same job into small chunks , they will all still be at max load .
` numpy.dot ` uses either BLAS ` dot ` , ` gemv ` or ` gemm ` , depending on the operation , but of those , only ` gemm ` is usually multithreaded , because there is rarely any performance benefit for the O ( N ) and O ( N^2 ) BLAS calls in doing so .
I think that you need to use cv.CreateMat instead or use cv.CreateMat and copy from the image to the mat using cv.CvtColor or some similar thing .
The ' Image.fromarray ' is a little ugly because I clip incoming data to [ 0,255 ] , convert to bytes , then create a grayscale image .
if r == max ( r , g , b ) and r > 125 and g < 70 and b < 110 :
P diagonal and S symmetric and C will only have one element per row .
P and S will be diagonal and C will only have one element per row .
But if S is diagonal , why not simply invert it ?
Good point , unfortunately my statement was incorrect :( S is merely symmetric , not diagonal .
SQL join or R's merge() function in NumPy ?
Is there an implementation where I can join two arrays based on their keys ?
if you have more rows than columns then , typically , there is no unique solution .
no need to do that reshape thing .
You create a great big array ` array - array [ row , :] ` , then reduce it into a smaller array using ` sum ` .
For every row , I need to find the sum of the contiguous nonzero values that sum to the largest value .
Did you try ones not from Numpy ?
I am trying to relate the python / numpy indices of two arrays with different sizes , but I cannot pass index one from the small array to the large array through a subroutine .
I am finding indices in a1 and relating these indices to ` a2 ` .
I've tried to store the numbers in a list and use random.choice() to pick an element , remove it , and then append the new element .
I'm thinking to store the numbers in a numpy array , sample a list of indices , and for each index perform the calculation .
Python lists are implemented internally as arrays ( like Java ` ArrayList ` s , C++ ` std :: vector ` s , etc . ) , so removing an element from the middle is relatively slow : all subsequent elements have to be reindexed .
Oh , I see what you had in mind : with ` a [ m1 , m2 ]` you get the diagonal of the intended result : ` a [ m1 , m2 ] == diag ( a [ m1 , :] [: , m2 ])` .
Yes , I should have mentioned that a lazy code dump from Python to Cython gives you nothing , unlike PyPy .
( Don't let the " sparse " fool you , it works for dense matrixes too . ) Also , ` eigs ( r , 1 )` is slower than ` eig ( r )` even for 10x10 ` r ` : the algorithm is just slower .
[ Greg Willden ] ( #URL ) [ comments ] ( #URL ): It should be ` evecs.T ` here to add transpose to evecs in the zip statement .
/ export / madison / src / roll / hpc / BUILD / ATLAS / CONFIG / ARCHS / P4E64SSE3 / gcc / gemm
/ export / madison / src / roll / hpc / BUILD / ATLAS / CONFIG / ARCHS / P4E64SSE3 / gcc / misc
_configtest.c : 1 : warning : conflicting types for built-in function ' exp ' / usr / bin / gcc4 _configtest.o -o _configtest
reference to ` exp ' collect2 : ld returned 1 exit status
reference to ` exp ' collect2 : ld returned 1 exit status failure .
_configtest.c : 1 : warning : conflicting types for built-in function ' exp ' / usr / bin / gcc4 _configtest.o -lm -o _configtest success !
Now , when reading your log , I see that you do not have LAPACK installed .
The year 2008 was 366 days long , so don't reshape .
I don't see a real need to reshape the array , since you can embed the year-size information in your sampling process , and leave the array with its original shape .
I don't know if the reshape operation is needed or not , I just thought it would be convenient for me as I can basically select which year I want to extract for each day very easily .
If you want to change the logic you can try with ` ceil ` or ` floor `
I would guess the ones you're seeing on the SciPy docs are the ones available in the scipy namespace , while the other list is those available in the numpy namespace .
return sum ( 100.0 * ( x [ 1 :] -x [: -1 ] ** 2.0 ) ** 2.0 + ( 1-x [: -1 ]) ** 2.0 )
In Numpy , I can concatenate two arrays end-to-end with ` np.append ` or ` np.concatenate ` : #CODE
Is there a way to concatenate two arrays into a view , i.e. without copying ?
@USER : the cost of copying is the problem ; otherwise I could just ` concatenate ` them and replace the original arrays with views into the concatenation .
The problem is that it creates copies on broadcast operations ( sounds like a bug ) .
I have tried to find this problem on stack overflow with no luck , but i'm sure i'm not the first person to have this trouble ?
There isn't guaranteed to be a unique " line " between two arbitrary entries in a array .
The only time such a unique line would exist would be if the two ending entries lay in the same row , same column , same diagonal or anti-diagonal .
May the stack never overflow on thee .
I think a transpose needs to be added to the otherwise great solution provided by Joe .
If you run it without the transpose , you can see the profile doesn't match up ; with the transpose it looks okay .
Here's the version WITHOUT the transpose .
Here's the version WITH the transpose .
Actually , I'm calling start and join in between , so it shouldn't be a problem , I think at least .
You current ` start ` and ` join ` them directly after each other , while reading out the queue after that .
What I'm doing here is depending on a particular implementation detail ( ` unique ` seems to start at the end of the array ) which could change at any time ...
The documentation guarantees with ` return_inverse=True ` that the returned indices are the indices of the unique array that reconstructs the original array .
Because you can rely on this relationship , Joe's method does not depend on a mere implementation detail -- ` unique ` will always behave this way .
However , you might consider using ` numpy.fft.rfft2 ` ( " real fft ") and ` numpy.fft.irfft2 ` instead of ` numpy.fft.fft2 ` and ` numpy.fft.ifft2 ` because you're dealing purely with real values .
However , some information is lost in the recorded image because of the multiplication by zeros in H , by consequence the restored image approximates the input image with components covering a finite bandwidth , lower than that of the display , revealing the Gibbs oscillations .
Given just the histogram without the knowledge that the samples are drawn from an exponential distribution , I'm not sure I would recognize an exponential distribution here ...
I suspect that ` convolve ` would fall into this basket , therefore it would not respect the rules which you define unless you wrote your own pure python version .
I mean instead of an array as the outer container holding small containers holding a couple floating point values as a complex number , turn that around so that your complex number is the outer container .
Kind of like R's SQL functions in the ` RDB ` or ` sqldf ` libraries , if anyone is familiar with those ( they import / export / append whole tables or subsets of tables to or from R data tables ) .
Memory Error when numpy.dot ( A , B ) , where A is 250000x108 float matrix , B is transpose of A
Any idea of do A dot B , without Memroy Error ?
Memory Error when numpy.dot ( A , B ) , where A is 250000x108 float matrix , B is transpose of A .
( i.e. do they contain a lot of zeros ? )
The dot product you are trying to calculate produces a 250000x250000 array , which will require 500Gb of memory if you are using the default double precision floating point .
I do not understand the behavior of this ` numpy.ma.max ` ( min , mean , etc . ) #CODE
About the inverse the function is inv ( A ) but I wont recommend to use it since for huge matrix it is very computionally costly and inestable .
Understanding NumPy's nonzero function
I am trying to understand numpy's ` nonzero ` function .
However , I do not understand why the number of indices in each element of the tuple exceeds the number of rows / columns of the array .
Each element of the tuple contains one of the indices for each nonzero value .
From your example , the indices of the nonzeros are ` [ 0 , 0 ]` , ` [ 1 , 0 ]` , and ` [ 1 , 1 ]` .
The first element of the tuple is the first index for each of the nonzero values : ` ([ 0 , 1 , 1 ])` , and the second element of the tuple is the second index for each of the nonzero values : ` ([ 0 , 0 , 1 ])` .
Your second code block just returns the nonzero values of the array ( I am not clear from the question if the return value is part of the confusion ) .
so ` zip ( arr.nonzero() )` are meant to return these paired indices .
Swig makes it a lot easier to solve with ` std :: vector ` from C++ , even if you ended up wrapping your original function .
Keep in mind that this ( along with your original example ) will sum the time fields , rather than integrating across time .
The x-values need to be incorporated to get the correct area of each trapezoid ( the area of each trapezoid is ( x 2 - x 1 ) * ( y 2 + y 1 ) / 2 and the final integration result is the sum of all such areas ) .
If I convolve the image in the spatial domain , move to frequency domain , then Inverse Filter the convolved image with the kernel's fft , I get a mess .
If I convolve the image in the frequency domain , then Inverse Filter that image , I get a good image .
( assuming that you've already import ` zeros ` from numpy ) .
Perhaps you should create your array with ` empty ` or ` zeros ` .
Are there any method that perform same as using " column stack " in numpy PYTHON
So is the column stack giving me this error ?
The matplotlib idally want you to prepare ( 1 ) X , Y being x and y coordinate of grid point ( or corner point ) , and ( 2 ) C being value of the tile surouded by 4 adjacent grid points .
If this id color band like RGBK and you want to show that color , then probably pcolor is not good , and you have to look for some other function or something that understands those 4 number . pcolor simply read range of number , then apply scale between min and max then apply color scale from blue to red ( or whatever you choose if you do )
In effect , it is storing one MxN array for each pair of indices ( i , j ) .
I am trying to join recarrys in python such that the same value joins to many elements .
Why is matplotlib handling the " join " function ???
why do you have quotation in the second ones ?
It overwrites a number of Python builtin functions , such as ` abs ` , ` all ` , ` any ` , ` min ` , ` max ` , ` sum ` , ` round ` , etc .
I think you might have assumed that ` reshape ` modifies the value of the original array .
` np.asarray ([ lst ]) .T ` ` .T ` means array transpose , which switches from ` ( 1 , 5 )` to ` ( 5 , 1 )` .
You could also reshape your original array to 1d via ` ary.reshape (( -1 , ))` .
You absolutely can use it with other numpy arrays -- just ones of the same shape , and lists are considered to be 1d arrays .
Finding unique points in numpy array
What is a faster way of finding unique x , y points ( removing duplicates ) in a numpy array like : #CODE
I thought of converting points to a complex numbers and then checking for unique , but that seems rather convoluted : #CODE
So the only real cost is calling ` np.unique ` , followed by another virtually costless call to ` view ` and ` reshape ` : #CODE
Numpy vectorization algorithms to sum up numbers with the same time stamps
I want to produce another two arrays Q and U , where Q [ i ] has time stamp U [ i ] , and Q [ i ] is the sum of all elements in P that have time stamp U [ i ];
Retreive indices of matching points in another array
How can I get the indices in " points " of points in a ([ 0 , 2 ]) , preferably without loops ?
The aforementioned Colormath library uses a dot product .
So what we need to do first is to reshape them to a ` ( rows*cols , 3 )` and then back again to ` ( rows , cols , 3 )` .
+1 for helping me think of reshape in new ways !
I'm interested in the best / fastest way to do array ops ( dot , outer , add , etc . ) while ignoring some values in the array .
How do I concatenate A and B so that I get a 3x4 matrix C #CODE
` mask ` is a boolean array that equates to ` True ` are array indices matching the condition and ` False ` elsewhere .
I'd like to generate a histogram of the data and while I've succeeded this can't be the best way to do it .
You can use a boolean array to select the required indices : #CODE
Is there a way to perform a roll on an array , but instead of having a copy of the data having just a different visualisation of it ?
exponential moving sum in numpy / scipy ?
I am looking for a function to calculate exponential moving sum in numpy or scipy .
For a given time t , the exponential moving sum is the sum of all the values A [ i ] that happens before t , with weight exp ( - ( t-T [ i ]) / tau ) for each A [ i ] .
The exponential moving sum is a convolution .
Can I average over an axis and then do an fft and get the same result as doing an fft and then averaging over the axes that was not involved in the fft ?
I would like to show the log of the variance of the 2D Fourier Transform of carbon_flux averaged over longitude .
This results in the masked values being used to calculate the fft ( I can tell by the first value of the fft being to the order of 10e19 ) .
From what I understand , the result of doing the averaging before the fft will differ to doing the averaging after the fft .
Does the fft use the masked values ?
Lastly , I have calculated the log of the 2D Fourier Transform of carbon_flux averaged over latitude .
I fail to understand how to calculate the log of the VARIANCE of the 2D Fourier Transform averaged in latitude .
Does the value of my resultant fft image simply need to be squared to become the variance ?
However , the FT is a linear operation , so as long as you average the data * before * you take the absolute value , it should't really make a difference .
I suggested filling with zeros because that's common in my field ( MRI ) , but probably not the best idea for your case !
UPDATE #1 : After trying Rahul's suggestion of using pip to reinstall nose , I think I've found where the problem is : whenever I do easy_install or pip from bash , the package is installed into Python 2.6 directories instead of 2.7 ones .
So it does not make much sense to me to reshape it to a " 1d-matrix " .
How does it compare to ` reshape ` in generality ?
@USER Since neither ` expand_dims ` nor ` reshape ` copies the array to a new location in memory , both approaches are really equivalent .
@USER : Isn't ` reshape ` more general ?
I for myself use ` reshape ` a lot more often .
As for using ` reshape ` , one can always just use ` .size ` as you do , so I don't see the problem .
I have a large 2D numpy array for which I know a pair of indices which represent one element of the array .
thanks , I tried this and it results in the following error ; TypeError : can only concatenate list ( not " int ") to list
You select multiple elements of an array ` A ` with ` A [ #URL where ` start ` and ` stop ` are zero-based indices .
Note that for the block of zeros to be centered on your x , y coordinates , it must be of odd size .
For instance , the block of zeros in the following is not centered the coordinates x , y = 4 , 6 but on the center coordinates of that cell , that is x , y = 4.5 , 5.5 : #CODE
where the edges are indices into points , so that the start and end coordinates of each edges is given by : #CODE
PPS by JD Long : I tried following these instructions on May 16 , 2012 and , possibly because of versions , or possibly because of idiosyncrasies on my Mac , I had to also run ` sudo port install py27-tornado py27-zmq ` in order to get a fully functioning ipython / notebook stack working .
I would simply loop through the array with a counter , if status changes log the time and counter , then reset the counter .
Now i just have to get the zeros in the Outputs
The functions I am trying to implement are mainly simple comparisons ( e.g. how many elements of a certain column are smaller than number x or how many of them have absolute value larger than y ) .
how many elements in last column of A have absolute value larger than 3 ?
NumPy indices are zero-based , in R , indexing begins with 1
NumPy ( like Python ) allows you to index from right to left using negative indices -- e.g. , #CODE
resize with averaging or rebin a numpy 2d array
I would like to resize it to ( 2 , 3 ) by taking the mean of the relevant samples , the expected output would be : #CODE
I believe I should reshape to a 4 dimensional array and then take the mean on the correct slice , but could not figure out the algorithm .
It returns the indices one would use to sort the array .
You can translate the indices back using ` idx ` : #CODE
Someone in the mrjob community recommended doing this install for a worker instance , then using ssh to log into the worker instance , download the completed install directory .
Would like to build a list of indices into a 2 dimensional bool_ array , where True .
Why do I get rows of zeros in my 2D fft ?
The figures published show " the log of the variance of the 2D-FT " .
Every second row of the ft consists completely of zeros .
I expect that the reason that I get rows of zeros is that I have re-created the timeseries for each pixel .
It may not be clear in the image but it is only every second row that contains completely zeros .
If ` X [ k ] = fft ( x )` , and ` Y [ k ] = fft ([ x x ])` , then ` Y [ 2k ] = 2*X [ k ]` for ` k in { 0 , 1 , ..., N-1 } ` and zero otherwise .
Try to plot the values on a log scale ( not a log ( 1+x ) scale ) .
Are these eigenface images generated by histogram equalization on the eigenvector ?
This is simpler than histogram equalization .
Yes , for visualization purposes , just map ` min ( eigenface )` to 0 and ` max ( eigenface )` to 255 .
Generally , working with the numpy / scipy stack , you want the repetitive task over a large array to happen in native Numpy / Scipy functions -- i.e. in a C loop over homogeneous data -- as opposed to explicitely in Python .
In short , this creates a vectorized version of the python function , that can be broadcast element-wise on an array .
I want to know because I just implemented dynamic arrays on top of ` numpy.ndarray ` , and I'm doing a ` resize ` , which calls ` realloc ` , to get the final size right .
I may be able to skip the final ` resize ` as an optimization ( at the expense of permanent overallocation ) and I want to know if that's even worth trying .
I'm trying to fit a piecewise function with absolute values using Numpy .
Fast way to find locally maximal gradient values in a numpy array ?
I have a 2-d array for which I want to detect all locally maximal array indices .
That is , given an index ( i , j ) , its maximum gradient is the largest absolute change from any of its 8 neighboring values : #CODE
The output of the algorithm should be a 2-d array of tuples , or a 3-d array , where for each index in the original array , the output array contains a value indicating if that index was locally maximal and , if so , the angle of the gradient .
My initial implementation simply passed over the array twice , once to calculate the max gradients ( stored in a temporary array ) and then once over the temp array to determine the locally maximal indices .
You are really computing differences , if you wanted the gradient , the calculation for the " diagonal " neighbours must be different to the " row and column " neighbours
To compute the differences , you can pad the image with a row and a column of zeros and subtract .
So I basically will create 8 arrays ( 4 padded + 4 result arrays ) and then is there some fast way to take the max over each of the four result arrays ?
You can concatenate the arrays in a new dimension .
As Cyborg pointed out , there are only four differences which need to be computed to complete your calculation ( note that there really should be a factor of 1 / sqrt ( 2 ) for the diagonal and antidiagonal calculations if this really is a spatial gradient calculation on a uniform grid ) .
I have found the solution , but stack overflow won't let me post it until 8 more hours ..
However , if we reshape ` y ` so that it only has one dimension , we'll get a copy which will use the full amount of memory .
My trading is not extremely time sensitive - I do have some intraday strategies but they aren't really high frequency i.e. 5 min bars at the highest .
So if we take an array of ones and apply ` updateDistance ` : #CODE
I have seen some ` deque ` and ` itertools ` suggestions regarding optimization of moving window loops , and also ` convolve ` from numpy , but I couldn't figure it out how to accomplish what I want using them .
And I plan to use `' same '` , for display in stack with raw / smoothed / filtered signal .
Just to extend this a bit , it is possible to have ` window ` be a kernel whose sum is ` 1.0 ` , like normalized gaussian kernel , if some more esoteric behaviour is needed .
I'm not sure about the reason for the performance difference , but one reason might be that all operations like ` == ` , ` argmax ` , and ` max ` can preallocate their output array directly from the shape of the input array , which is not possible for ` argwhere ` .
The goal however is to run a Monte Carlo analysis of the two parameters ( beta and gamma ) which have random values .
Also , the append operation is very slow and demands more memory , because it creates copies .
I need to create views on this array , but I don't know how to create a view given a list or range of indices to include in the view .
This is OK , but I hope there is a way to construct a view from multiple arbitrary indices , so a function like this would be great : #CODE
which would return a view that points to those indices given in a list ( or any iterable form ) , and if I change myView , it would change myArray like it should .
But it is possible to use a list of arbitrary indices in various ways .
Actually I use up all the indices of the original array once , and want to create multiple arrays that have shorter length , and reference to the original array .
( It can be done with evenly spaced views in my case , but I wanted to know if it's possible in a more general way . ) I want to copy the data to an OpenCL device memory , and use PyOpenCL InOut for the filtered arrays , to change the correct indices in the original array .
As far as I know , you can't have a view on arbitrary indices ; only on consecutive indices or regularly spaced indices .
Also , as a related matter of API design principles , if the caller passes a 1-D array to some function that returns a new array , and the return value is also a vector , is it common practice to reshape a 2-D vector ` ( r , 1 )` or ` ( 1 , c )` back to a 1-D array or simply document that the function returns a 2-D array regardless ?
If you pass such a function a 1-D array of shape ` ( c , )` it will broadcast to shape ` ( 1 , c )` , since broadcasting adds new axes on the left .
It can also broadcast to shape ` ( r , c )` for an arbitrary ` r ` ( depending on what other array it is being combined with ) .
On the other hand , if you have a 1-D array , ` x ` , of shape ` ( r , )` and you need it to broadcast up to shape ` ( r , c )` , then NumPy expects the user to pass an array of shape ` ( r , 1 )` since broadcasting will not add the new axes on the right for you .
( example : if 2d requires several dot products . )
Given an array of the log of the small numbers ` x ` , compute : #CODE
Now I want to compute something like ` 1-exp ( y )` or even better ` log ( 1-exp ( y ))` , but I'm not sure how to do so without losing all my precision .
Then I try to compute the log sum of exponents : #CODE
Exactly like Raymond Hettinger said , but you would of course need to multiply by -1 afterwards , because you want 1 - exp instead of exp - 1
Python 2.7 ' s math.expm1 ( ` exp ( x ) -1 `) and math.log1p ( ` log ( 1+x )`) do this for you if the platform's C library's precision ( typically double ) is enough .
The returned single value , say p , is just the log of the sum of all probabilities in probs .
Notice that if there is a large scale different between the largest and the smaller value in the input probabilities , the small ones will be disregarded but just if their contribution is very small in comparison with the large numbers which in most applications should be fine .
What you can do is either divide the single sums in several ones where values of roughly the same scale are added together first before being combined . or you can use the some in-between pivot value instead of the max , but you must make sure that the largest value is not too large because then exp ( probs [ i ] - pivot ) would cause an overflow in that case .
Once this is done you still need to do calculate log ( 1-exp ( p ))
Maechler M , Accurately Computing log ( 1 ? exp ( ? |a| )) Assessed by the Rmpfr package , 2012
You should then use the log1p approach when a log ( .5 ) and the expm1 when a > = log ( .5 ) .
I need to use ` dot() ` function , and I know ` c = dot ( a , b )` works .
But ` dot ( a , b , c )` does not .
It isn't a ufunc and doesn't follow the usual broadcasting rules -- it can't because of the semantics of a dot product .
There is only one version of all of the standard numpy ufuncs -- using dot as an example #CODE
Just multiply the pixels together and then take the sum of all pixels :
One of the testers then selected the best masks ( the least distorted ones ) and saved them as 1.bmp for digit 1 , 2.bmb for digit 2 ...
My spreadsheets might have hundreds or thousands of rows , max , and I'd gladly have the system do a bit of data copying to make my code easy .
The reality is that data manipulation programs may increase the length of strings beyond their initial max .
The " just truncate it !
The questions were : ** 1 ) ** Is tabular / numpy really supposed to truncate data like this ?
As the first answerer noted , NumPy has an absolute requirement for data structures to be uniform in their size .
The PDF has the shape f ( x , alpha , mean ( x ))= ( 1 / Gamma ( alpha+1 ) ( x )) (( x* ( alpha+1 ) / mean ( x )) ^ ( alpha+1 )) exp ( - ( alpha+1 ) * ( x / mean ( x )) where x > 0 .
The integral has a gamma function and an incomplete gamma function , so trying to invert it is kind of a mess .
( 1 / Gamma ( alpha+1 ) ( x )) (( x* ( alpha+1 ) / mean ( x )) ^ ( alpha+1 )) exp ( - ( alpha+1 ) * ( x / mean ( x ))
My impression is that after standardizing , the pdf just looks like a standard gamma pdf , which is in numpy.random and in scipy.stats .
If your domain is 0 to positive infinity , your distribution appears to match the gamma distribution which is built into Numpy and Scipy , with ` theta = 1 / alpha ` and ` k = alpha+1 ` .
You don't actually have to do the inversion yourself , because you can just use Numpy's gamma distribution to directly obtain values distributed according to your desired PDF .
I am just not getting passed the incomplete gamma function .
Each view is a unique ndarray object , meaning it has it's own properties , for example shape : #CODE
You will overwrite built in functions such as ` sum ` and ` abs ` for example ...
item_a was just to get the values of the array , but indeed not needed as it is the indices I'm interested in
Create a sparse diagonal matrix from row of a sparse matrix
I need to extract rows from large matrix ( which is loaded to coo_matrix ) and use them as diagonal elements .
Can you suggest a better way to extract a row from a sparse matrix and represent it in a diagonal form ?
However the main question is how to omit the materialization ( ` .todense() `) and create the diagonal matrix from the sparse representation of the row .
But what I am looking for is to omit materializing the row begore creation of the diagonal matrix .
It takes advantage of the underlying dense arrays ( indptr , indices , data ) that define sparse matrices .
This will enable the big dot product .
That was the way I wrote my code first , and I walked away from that once I realized it forced me to slice and transpose all over the place .
Since there is a trick about transposition ( the transpose of a matrix or matrix is NOT the transpose of the big equivalent matrix ) , and since bmat frowns upon the original data structure , I came up with that code : #CODE
I have tried using detrend_common but I have a masked array so I filled the masked values with zeros and it worked great .
About the method with sum ( .. ) , is True always equal to 1 in python ( or at least in numpy ) ?
` python -m timeit -s " import numpy as np ; bools = np.random.uniform ( size=1000 ) > = 0.5 " " sum ( bools )"`
You should " flatten " the array of arrays first . unfortunately , there's no builtin method , see #URL
This is a little hackish , but you can reverse the order of your dimensions then return the transpose .
numpy function to set elements of array to a value given a list of indices
You can just give it a list of indices : #CODE
If changing a row to all zeros is acceptable , you can just use : #CODE
` b ` is a transpose of ` a ` .
For your edited question with an array of ` dtype =o bject ` -- there is no direct way to compute the transpose , because numpy doesn't know how to transpose a generic object .
However , you can use list comprehension and transpose each object separately : #CODE
It can use , if available , a BLAS implementation for a very , very small subset of its functionality ( basically dot , gemv and gemm ) .
baselines fft
term1s fft
But since I do have my baseline data , i used fnumpy.ft to create its fft .
But in terms of actual implementation , dividing out the baseline's fft from my terms fft and then taking the inverse does not seem to be what i want .
so in terms of implementing in python , knowing the fft of my baseline , how can i adjust my terms signal for it
Would I just iterate over the values I supplied to plot and just make a disconnected dot subplot with markers ?
scipy sparse matrix indices
If you print the entire array the indices are correct , but if you slice on the row you get relative indices .
Constructing numpy array using tile
My question is : How can I get b from a using tile ?
You already have the good syntax for tile : ` b = np.tile ( a , 3 ) .reshape (( 3 , 3 , 3 ))`
In addition to the approaches below , see also ovgolovin's answer , which uses a NumPy matrix to avoid the need to reshape ` mean ` altogether .
The usual way to express this kind of reshape in NumPy is to use [ ` np.newaxis `] ( #URL ): ` A - mean [: , np.newaxis ]` .
Then you won't need to reshape : #CODE
I am trying to make a contour plot with the contour levels scaled by the log of the values .
I'm trying to write a function in Python ( still a noob ! ) which returns indices and scores of documents ordered by the inner products of their tfidf scores .
Return the " scores " and indices from the second one to the end ( i.e. not itself )
I believe it's just a matter of rearranging some indices , without the data being actually copied .
Does the transpose apply to the indexed inner array , or to the result after multiplying them ?
compress numpy array ( matrix ) by removing columns using another numpy array as mask
Is there a way to select only those columns from ` A ` that correspond to ones in ` B ` ?
The only solution I can think of requires generating diagonal block matrices [ X ] , [ Y ] and [ A ] , with dimensions mn x m , mn x m , and mn x mn , respectively , and with ( block ) diagonal elements x , i , y , i , and A , respectively .
Of course , computing the products X T AY and X T AX would compute the desired b A ( x , i , y , i ) and q A ( x , i ) ( as the diagonal elements of the resulting m x m matrices ) , along with the O ( m 2 ) irrelevant b A ( x , i , y , j ) and b A ( x , i , x , j ) , ( for i j ) , so this is a non-starter .
I think the correct forumla is ` sum (( x ' *F ) . *y ' , 2 )` .
Or even simpler : ` sum ( x . * ( F*y ))`
The following would make me believe a matrix should be the outcome : " [ X ] , [ Y ] and [ A ] , with dimensions mn x m , mn x m , and mn x mn , respectively , and with ( block ) diagonal elements x , i , y , i , and A , respectively .
I have columnar data : depth , temperature , gradient , gamma , for a set of boreholes .
Thanks very much Denis , so to get ' columns ' returned into 1-D arrays you just transpose the array ?
This probably requires changes in other parts of the code ( or you need to flatten the array again ) .
` Abs ` returns the type it is given , so you have to either select ` E [ i ]` , use an operation such as sum , or just have for i in E .
All of those operations in the abs() work elementwise in numpy arrays , so you're doing some things that end with an array , taking the absolute value of every element in that array , then comparing to 10** ( -4 ) and ending up with an array of booleans .
And I would like to add a column of zeros to get array b : #CODE
While your solution is elegant enough , pay attention not to use it if you need to " append " frequently to an array .
I want to append ( 985 , 1 ) shape np araay to ( 985 , 2 ) np array to make it ( 985 , 3 ) np array , but it's not working .
I am getting " could not broadcast input array from shape ( 985 ) into shape ( 985 , 1 )" error .
Too bad I can't do ` insert ( a , -1 , ... )` to append the column .
A minor performance improvement is to avoid the overhead of initializing with zeros , only to be overwritten .
This has a measurable difference when N is large , empty is used instead of zeros , and the column of zeros is written as a separate step : #CODE
So I need to compare the jacobians calculated by my own with the ones from leastsq .
To check how good it is , you can compare the returned covariance matrix with the outer product of your Jacobian .
Perhaps you need a random permutation ?
Var ( x ) and cov ( x , x ) don't give the same result in numpy
A property of the covariance is , that cov ( x , x ) = var ( x )
You must use z=cov ( x , bias=1 ) in order to normalize by N , because var is also norm by N
The default ddof of ` cov ` ( None ) and ` var ` ( 0 ) are different .
Again you would have to translate the raw bytes into something Incanter resp . numpy can understand , but you could use Python code on the Java side without the need to wrap every single function in numpy / scipy with JNI .
I am trying to insert ` nan ` s at certain indices .
The list of indices will always be ascending , never have duplicates , but may have gaps like the example .
For example , ` a ` is generated from ` a = z [ z ! =0 ]` ; ` a ` then changes through some processing , and now I need to insert ` nan ` s where there were originally zeros .
The indices are easy to grab using ` b = numpy.argwhere ( z == 0 )` .
Is there a way to use numpy slices to do cyclic permutation ?
I wrote this function ( used later to select elite species in the genetic algorithm ) to select k best values out of n , where not all n values are unique .
Did some web search , and this guy has qr decomposition coded : #URL .
The function will search adjoining elements in an array and look for ones with values within 0.05 of each other much like a floodfill algorithm does .
you seem to be causing a stack overflow .
The code will be faster if you drop the call to ` np.where() ` and simply use ` nonzero = x !
If you are going to use the 2D / 1D output for something like computing a norm , then you don't need a 2D output , and can get away with 1D array in every case by broadcasting .
This is faster than using ` tile ` , the timing is bellow : #CODE
numpy : point sum
sum with start do the trick . because i need a return value of Vec3 , and numpy.sum return a numpy array . pandac is a render engine written in c++ and wrapped in python so i think it is ok .
@USER : ah , okay , I've never used freemat so I guess I just assumed it would show any small nonzero imaginary component as numpy does .
Condition numbers close to 1 are fine , ` cond ( Y1 )` gives 45 .
To achieve the two-argument case ( returning all indices where the condition is True ; credit to Rex Kerr for pointing this case out ) , a list comprehension can be used : #CODE
In the two-item case , ` numpy.where ( cond )` , you get a list of indices where the condition-array is true .
which obviously is less compact , but this isn't a fundamental operation that people normally use in Scala ( the building blocks are different , de-emphasizing indices , for example ) .
In the three-item case , ` numpy.where ( cond , x , y )` , you get either ` x ` or ` y ` depending on whether ` cond ` is true ( ` x `) or false ( ` y `) .
Note that in Scala you can more easily have ` cond ` be a method that tests ` x ` and ` y ` and produces true or false , and then you would #CODE
I'm allocating a ( possibly large ) matrix of zeros with Python and numpy .
The following function finds the unique elements of of a 1d array : #CODE
outer loop .
Result of fft transformation doesn't map to HOURS , but to frequencies contained in your dataset .
This reshape the ` noise_big ` to be of shape ` ( X , Y , 1 )` instead of ( X , Y ) , which allows direct multiplication with the ` screen_array ` object , having shape ` ( X , Y , 3 )` .
So far everything is very good , but the strange thing is that the sum of this new array is not zero : #CODE
The standard deviation is also not at all near what I was expecting but I guess the root of that problem is the same as this : Why doesn't the sum result in zero ?
When I apply this method to a small array ; for example [ -5 , -3 , -2 ] the sum becomes zero .
Note how ` sum ([ 1e308 , 1 , -1e308 ]) == 0.0 ` and ` math.fsum ([ 1e308 , 1 , -1e308 ]) == 1.0 `
It helps to sort the data by its absolute value before doing a summation ( especially in a case like this where you expect positive and negative contributions to cancel each other ) .
@USER Kern : By this do you mean that I break up the array into smaller ones and knowing the mean ( 0.0 ) I find the separate necessary elements for standard deviation and find it manually ?
The problem is because the partial sums get to be so large that individual elements are less than floating point precision compared to the partial sum .
The recommendations I made are simply about implementing the sum accurately .
My recommendation for implementing ` std() ` is to implement it manually , and use one of the accurate techniques for doing the sum that is part of the ` std() ` formula .
The sum became exactly zero now , so did the mean which was also not zero . but the very strange thing is that the standard deviation did not change from what it was before .
Unless ` numpy.std() ` uses another method to calculate the sum ( for example in the ` math.fsum() ` module ) this result is not acceptable because while ` numpy.sum() ` and ` numpy.mean() ` changed , ` numpy.std() ` didn't !!!!
I want to cluster 10000 indexed points based on their feature vectors and get their ids after clustering i.e. cluster1 :[ p1 , p3 , p100 , ... ] , cluster2 :[ ... ] ...
The only requirement for the point tuple is that the x-y coordinates be in indices 0 and 1 , respectively .
Is there any fast way to obtain unique elements in numpy ?
This is just an example and in my situation ` indices1 , indices2 ,..., indices4 ` contains different set of indices and have various size .
Besides , ordering is not important and element in indices array are of ` int32 ` type .
The following way of obtaining the unique elements in all sub-arrays is very fast : #CODE
PPS : here is an ( unsuccessful ) exploration of mtrw's suggestion ; finding the unique indices beforehand might be a good idea , but I can't find a way to implement it which is faster than the above approach : #CODE
Thus , finding the set of all distinct indices is itself quite slow .
PPPS : ` numpy.unique ( concatenated array of indices )` is actually 2-3 times faster than ` set ( concatenated array of indices )` .
This is key to the speed up obtained in Bago's answer ( ` unique ( concatenate (( )))`) .
Couldn't you get another speedup by finding the unique ` indeces ` , then using them to lookup the ` tab ` ?
( Also np.concatenate gives a ( 4*n , ) array and np.array gives a ( 4 , n ) array , where n is the length if indices [ 1-4 ] . The latter will only work if the indices1-4 are all the same length . )
Doing it in this order is faster because you reduce the number of indices you need to look up in tab , but it'll only work if you know that the elements of tab are unique ( otherwise you could get repeats in result even if the indices are unique ) .
+1 for ` concatenate ` .
@USER , ya that that's an option if indices has a lot of repeats , in that case the overhead of duplicating the repeat might be worth it .
EDIT : to be more precise : the enumeration should not only skip over the masked entries , but also show the indices of the non-masked ones in the original array .
The enumeration over only valid entries with indices from the original array : #CODE
Group by max or min in a numpy array
I would like to aggregate ` data ` by grouping on ` id ` and taking either the max or the min .
For the outer list comprehension , from right to left , ` set ( id )` groups the ` id ` s , ` sorted() ` sorts them , ` for k ...
` iterates over them , and ` max ` takes the max of , in this case , another list comprehension .
I've been seeing some very similar questions on stack overflow the last few days .
Thus if I want to find , say , the max of each window excluding NaN's , I can flip the sign of the data , apply the min formula , and then flip the sign again on the way out , with only a small performance penalty .
Abiel , see np.nanmax -- max ignoring NaNs
Annoyingly it's O ( n log n ) time and O ( n ) memory , when we know it can be solved in O ( n ) time and O ( k ) memory for k bins .
It is easy to modify this solution to return ` arg max ` or ` arg min ` .
It was one year ago since @USER pointed that this solution is O ( n log n ) .
It relies on the fact that if ` o ` is an array of indices into ` r ` then ` r [ o ] = x ` will fill ` r ` with the latest value ` x ` for each value of ` o ` , such that ` r [[ 0 , 0 ]] = [ 1 , 2 ]` will return ` r [ 0 ] = 2 ` .
The accepted answer may still be faster though , considering the max / min isn't explicitly computed .
Of course this only makes sense if your ` data_id ` values are suitable for use as indices ( i.e. non-negative integers and not huge ... presumably if they are large / sparse you could initialize ` ans ` using ` np.unique ( data_id )` or something ) .
Then you wouldn't have to translate your code at all .
NumPy : vectorize sum of distances to a set of points
As part of this algo , I have to compute the sum of distances from objects to their " medoids " ( cluster representatives ) .
I can compute the required sum using #CODE
Am I missing some kind of vectorized idiom for computing this sum ?
Not sure whether you can do the sum in one single steps .
Did you try making a mask or zeroes and ones and then using per-element array multiplication ?
Returns the pixel sum within radius , or return ( array [ mask ] = 2 ) for whatever need .
For example , consider the case of a matrix with many rows , and you wish to compute the sum of each row : #CODE
` np.sum ` take an ` axis ` parameter , so you could compute the sum simply using #CODE
Is there a way to return the indices of k-minimum values along an axis of a numpy array without using loops ?
EDIT : Thanks to @USER ' s answer , I now understand that the goal is to minimize the sum of squared residuals , not the actual function .
Also you need the approx_grad=True flag unless you want to compute the gradient analytically and provide it .
Since they only appear in the term A*l_exp**E , they can vary simultaneously without affecting the model value , so there will not be a unique solution .
they use fft over there .
That transpose is just there for some unrelated reason , right ?
Matlab applies the fft over the columns of the matrix , numpy applies the fft over the last axis ( the rows ) by default .
If the image sizes vary , create a column with an unique identifier for each image ( the functional equivalent of a filename ) , then create a new group and create one array / carray per image , named identically to the list in the aforementioned table .
So the only alternative is to cast the integer array to floating point , then stack them .
The types don't matter , you should reshape dates to be ( 1251 , 1 ) before using hstack .
I would recommend benchmarking for your particular case to see which is better , also noting that ` np.setdiff1d ` has an option if both arrays contain only unique values , which can speed things up .
Currently , I am using a dictionary to store these tuples and use the max operator to retrieve the maximum key value in the dictionary .
Other possible ones if you are also interested in stuff like the lowest value or popping off the list the value you found could pass through sorting ( so you examine the original list only once ! ): #CODE
Find the index of max value using ` argmax ` and get the word from a separate list .
This is very fast , but constructing the ndarray only to find the max is not .
@USER No , don't use a Numpy array if taking the max is the only use for it .
would get out of bounds for ` i = 47 ` and ` i = 48 ` since you use ` i+2 ` and ` i+3 ` as indices into ` viewVY ` .
` histogram ` is really constructed for floating point histograms .
Then , the starting indices of each subarray are needed : #CODE
Now that we have both the starting and ending values , we can use the indices function from this question to get an array of selector indices : #CODE
That way you avoid the conversion , which is most of the time in both my method and the histogram method ?
The fastest solution so far uses Bago's set-up but replaces the ` sum_by_group ` function with the built-in ` histogram ` function .
Method4 ( histogram ) : 2.82
Passing a list of ndarrays to concatenate seems significantly faster than passing a list of lists .
The histogram approach is a good idea .
( An nonzero exit status usually indicates an error on Unix style systems . A couple programs are different , e.g. , ` diff ` . ) Try examining the ` stderr ` produced by the subprocess to see what error messages are printed there .
Numpy : Reduce memory footprint of dot product with random data
It seems to me that I don't actually need all of ` random_array ` around at once ; in theory , I ought to be able to generate it lazily during the dot product calculation ... but I can't figure out how .
Having a play around with this , I'm not sure it's possible - ` np.dot ` needs to know the sizes of all its inputs ( as 2D dot product == matrix multiplication ) .
You could generate the random numbers on the fly and accumulate the result as you go .
numpy : Computing mean and std over multiple non-consecutive axes ( 2nd attempt )
For each triplet of indices i , k , j ( where each index ranges over the set { 0 , 1 , 2 , 3} ) , the element ` outbox [ i , j , k , 0 ]` is the mean of the 6 elements specified by the numpy expression ` box [ i , 0:2 , j , 0:3 , k ]` .
This means that the first n - k == 3 dimensions of the result range over the same indices as do the n - k non-axes dimensions of the input ndarray ` box ` , which in this case are dimensions 0 , 2 and 4 .
permute the dimensions ( using the ` transpose ` method ) so that the axes specified in the function's second argument are all put at the end ; the remaining ( non-axes ) dimensions are left at the beginning ( in their original ordering );
collapse the axes dimensions into one ( by using the ` reshape ` method ); the new " collapsed " dimension is now the last dimension of the reshaped ndarray ;
This approach seems like the right one to me . mean is so much faster than std I wouldn't worry about calculating the mean twice .
If you are looking for the max of rows 2 through 10 ,
then use ` mx.max ( axis=1 )` to find the max of all rows , and then slice it down to just rows 2 through 10 : #CODE
There isn't a straightforward way I know of to get the complement of a set of integer indices .
Let's say I have an numpy array A of size n x m x k and another array B of size n x m that has indices from 1 to k .
making histogram from a csv file
I am trying to read a column of data from a csv file and create a histogram for it .
I could read the data into an array but was not able to make the histogram .
( In the case of a one-dimensional array , ` dShape ` will be a one-element tuple , and ` zeros ` knows what to do with that . )
It looks like numpy uses the axis param , and if the input is multi-dimension and there is no axis param given they flatten the array and return cumsum on the whole thing .
It's pretty common in numpy , if no axis is given most functions that take an axis flatten the array .
Matlab's ` eig ` detects if the input matrix is real symmetric or Hermitian and uses Cholesky factorization when it is .
See the description of the ` chol ` argument in the ` eig ` documentation .
Or , even better , is it possible to get a list of indices of all the occurrences of the maximum value in the array ?
( Assuming that ` x ` and ` y ` are numpy arrays . ) Similarly , there's no need to make a new array of ones just to add 1 to each item in a numpy array .
The most efficient is probably ` tofile ` ( doc ) which is intended for quick dumps of file to disk when you know all of the attributes of the data ahead of time .
I am finding the eigenvalues and eigenvectors of a real symmetric matrix , in which rows sum to 0 .
More specifically , once I find an eigenvector , I use $argsort$ to find the permutation that sorts one of the eigenvalues and apply the permutation to the original matrix .
The code itself is recursive , and if it finds a set of values in the eigenvector that are equal , it extracts the symmetric submatrix corresponding to the indices for which we have equal values , and applies the algorithm all over again on this matrix .
While this is all very well , and mostly grunt work , I was caught by surprise when a bunch of indices that should have corresponded to equal entries in the eigenvector were not recognized as having equal values .
If they do differ but you are happy with the 7d.p of similarity , then you can truncate the results using something like ` numpy.around ( differences , 7 )` .
Note : I'm not sure what should happen with ties , so used zeros in the above ` Neighbourhoods `
" In your case , it's as simple as creating two vectors in NumPy that are of the same length , putting zeros ( or any value , really ) in the smaller of the two vectors that correspond to the missing trials , and creating a mask matrix that indicates where your missing values exist in your data matrix .
So it calculates the ones it can .
If that doesn't fix the problem , you can exploit a ` scipy.sparse.coo_matrix ` to make the 2D histogram .
With older versions of numpy , ` digitize ` ( which all of the various ` histogram* ` functions use internally ) could use excessive memory under some circumstances .
The histogram now pops up almost immediately even if I use 1 million data points .
This seems to assume that every value is unique , while clearly that isn't always going to be the case .
Since unique will already be required and I am using 1D data , I am probably going to go with ` unique ( ..., return_counts=True ) .
Given an index array ` I ` , how to I set the values of a data array ` D ` whose indices don't exist in ` I ` ?
Other than manually grepping through the poles and zeros of the filter , I'm not sure how you would detect instability of the filter .
An IIR filter is stable if the absolute values of the roots of the denominator of the discrete transfer function a ( z ) are all less than one .
i.e. I want to make a 2D array by iterating over just the first two indices .
don't forget to add the line " zeta= np.zeros ( 5 )" as an equivalent to " zeta = zeros ( 1 , 5 )" in the question .
@USER : It works faster , because otherwise you have to resize the " zeta " array each time you assign a new value to it ( i.e. writing zeros to " zeta " array , you use the size of the array ) .
@USER : I measured the execution time on the larger matrices and I have to admit that filling the " zeta " array with zeros in advance , doesn't boost up the performance ( in Matlab , it does ) .
Just for clarification , what @USER Li is referring to is that matlab will resize an array on demand if you try to index it beyond its size .
` a = 1:5 ; a ( 100 ) = 1 ; ` will resize ` a ` to be a 1x100 array .
Plot line graph from histogram data in matplotlib
I have a numpy array of ints representing time periods , which I'm currently plotting in a histogram to get a nice distribution graph , using the following code : #CODE
However that's not giving me anything close , as I can't accurately simulate the bins option of the histogram in my plot .
Give your C functions a pure C interface without any reference to Python or NumPy , and write trivial wrappers in Python that accept NumPy arrays and translate them to the appropriate C parameters .
So let's say I have a Matrix ' mat ' ( i.e. an ndarray ) where A.shape = ( 10 , 2 ) , and would like certain formatting and rounding , I can get a decent , adjusted output by using the following : #CODE
I also had to change the append " a " file setting to write as I almost missed the correct results at the end of the incorrect results .
Thanks for the feedback about the if statement and the append , great catch .
The broadcasting in the middle ( ` points [ np.newaxis ] - centroids [: , np.newaxis ]`) is " making " two 3D arrays from the original ones .
to translate a code in Python : #CODE
` poly = [( 1 , 1 ) , ( 4 , 2 ) , ( 5 , 5 ) , ( 3 , 8) , ( 0 , 4 )]`
I can add an epsilon to my code when assessing if the matrix sums to 1 ( e.g. -epsilon sum ( m ) +epsilon ) but I want to first understand what the cause of the difference is within Python , and if there's a better way to determine the sum correctly .
My understanding is that the sum ( s ) are processing the machine representation of the numbers ( floats ) differently than how they're displayed , and when sum'ing , the internal repesentation is used .
Howeve , r looking at the 3 methods I used to calculate the sum it's not clear why they're all different , or the same between the platforms .
What's the best way to correctly calculate the sum of a matrix ?
This will set the matrix machine numbers to represent 32-bit floats , not 64-bit on my Windows test , and will sum correctly .
( Maybe I do have a problem with float points )) I asked in case there's some knowledge out there on NumPy , matrix and sum that I didn't grasp yet .
Now , the 64-bit answer of numpy's sum() can not sum up to exactly 1 for the reasons how floating point numbers are being handled in computers ( murgatroid99 provided you with a link , there are hundred's more out there ) .
Also , if we slice with a different range ( e.g. ` xx , yy = np.mgrid [ 10:15 , 3:8 ]` it would tile indicies from 10 to 14 ( inclusive ) and 3 to 7 ( inclusive ) .
The error is saying that matplotlib can't reshape the 1D array so that it is a N by N 2D array .
So , ` mat = np.random.random (( 10 , 10 ))` just generates a 10x10 array of random numbers between 0 and 1 .
What I have now : ` sqrt (( a1-b1 ) ^2 + ( a2-b2 ) ^2 + ... + ( a5-b5 ) ^2 )`
What I want : ` sqrt ( w1 ( a1-b1 ) ^2 + w2 ( a2-b2 ) ^2 + ... + w5 ( a5-b5 ) ^2 )` using scipy or numpy or any other efficient way to do this .
That isn't the norm contained in the question - you have squared the weights .
it * is * the sum .
The result of ` q * q.T ` would be a 1x1 matrix , which would be an unexpected return type for a norm function , the sum will turn it into a scalar .
The suggestion of writing your own weighted L2 norm is a good one , but the calculation provided in this answer is incorrect .
Building on Sven Marnach's answer , if you want to perform a calculation on ` mat [: , 0:8 ]` , then on ` mat [: , 1:9 ]` , etc , you might be able to perform all the calculations " at once " on the 3-dimensional matrix #CODE
alpha is an array with the diffusivities per layer , l is an array with the heights per layer , wz is an array which aggregates the cumulative sum of points ( discrete points ) and A is the " state matrix " .
" My matrix " has a diagonal ( with 3 values ) with 100 , - 200,100 .
Subscript indices must either be real positive integers or logicals .
A = zeros ( nz ) is the same as A = zeros ( nz , nz )
The only difference is I changed the way the index in the 4th loop are calculated ( using the min function ) .
Normally using ` set_scale ( " log ")` works great , but it limits me to log10 .
This holds irrespective of the base , i.e. it holds for natural logs , log base 10 , log base 2 etc .
Hence you just can't plot something which has zeros in a logscale , unless you do something to these zero values .
For whatever it's worth , data isn't filtered out , it's just a linear plot near 0 and a log plot everywhere else .
The limits you're referring to are just the way that limits behave for a log plot .
If you want the limits to strictly end at the min and max of the data , then specify ` ax.axis ( ' tight ')` .
I am using log plots as follows in matplotlib , roughly as follows .
the manual says it's supposed to use the min and max values in the data , but this is obviously not the case here .
To contribute something back , here is how one would take these log 2 plots and make the axes appear with their non-exponent notation : #CODE
They should not be called ` xlim ( log10 ( min ) , log10 ( max ))` but just ` xlim ( min , max )` .
The weird display seems to me to be some bug you trigger since you request a negative minimum on a logarithmic scale which it cannot show ( ` log ( x ) 0 ` for all ` x `) .
says it's supposed to use the min and max values in the data , but this
In the case of a log plot , that's the nearest power of the base .
If you want it to strictly snap to the min and max of the data , specify : #CODE
With " %i " I sometimes get all zeros for values less than 1 , but % .2f will show these as floats .
Using a regular NumPy array will certainly require the whole matrix , including zeros , to fit in memory .
However it does require that the whole matrix , including zeros , is stored on disk ( and therefore read / written to disk when necessary too ) .
The solution we have is to serialize rows of the matrix to disk ( actually mongodb ) independently in sparse format ( column indices and data ) .
However this is somewhat inflexible to changes in the columns , which require a complete flush ( or some headache-making code ... ) , but that's a rare occurrence in our application .
Only chunks with nonzero values actually get stored to disk .
I have two numpy arrays with three dimensions ( 3 x 4 x 5 ) and I want to concatenate them so the result has four dimensions ( 3 x 4 x 5 x 2 ) .
If you have a sequence of arrays that you want to stack this way you can use : ` c = np.concatenate ([ aux [ ..., np.newaxis ] for aux in sequence_of_arrays ] , axis=3 )`
i.e. taking a transpose reverses the order of the indices of a numpy array .
Numpy seems to be able to multiply a 1,000,000 x23 matrix by its transpose in under a second , while the equivalent clojure code takes over six minutes .
In order to match this performance ( assuming its possible in Java ) you would have to strip most of Clojure's abstractions away : Don't use map with anonymous functions when iterating over large matrices , add type hints to enable usage of raw Java arrays , etc .
But if you're insistant upon doing it in Clojure , consider looking up better algorithms , using direct loops as opposed to higher-order functions like ` reduce ` , or find a proper matrix algebra library for Java ( I doubt there are good ones in Clojure , but I don't really know ) written by a competent mathematician .
Use type hints , run a profiler on your code ( surprise ! you dot product function is using up the most time ) , and drop the high-level features inside your tight loops .
If you want to do numerics in Clojure , I'd strongly recommend using Incanter rather than trying to roll your own matrix functions and suchlike .
EDIT : Here are absolute numbers ( in msec . ) I got on my ( rather slow ) PC : #CODE
Plot 3D histogram with uneven length array
Now , I'd like to plot a 3D histogram on data whilst making the #CODE
Compute the volume of each one and sum them .
Perhaps to clearify : By normalizing I mean , the sum of the entrys per row must be one .
Careful , " normalize " usually means the * square * sum of components is one .
When you do ` a / b ` , ` a ` and ` b ` are broadcast against each other .
This can be simplified even further using ` a.sum ( axis=1 , keepdims=True )` to keep the singleton column dimension , which you can then broadcast along without having to use ` np.newaxis ` .
@USER ... well in that case normalizing by the row sum doesn't really make much sense !
The " make it sum to 1 " is the L1 norm , and to take that do : #CODE
Now your rows will sum to 1 .
In case you are trying to normalize each row such that its magnitude is one ( i.e. a row's unit length is one or the sum of the square of each element in a row is one ): #CODE
notably this corresponds to the l2 norm ( where as rows summing to 1 corresponds to the l1 norm )
Is there a function , that does the opposite : Check whether all array elements along a given axis ( I need the diagonal ) evaluates to False .
What I need in particular is to check if the diagonal of a 2-dimensional matrix is zero every where .
First , to extract the diagonal , you can use ` mymatrix.diagonal() ` .
Since it's a numeric matrix though , you can just add up the absolute value of the elements on the diagonal and if they're all 0 , each element must be zero : ` numpy.sum ( numpy.abs ( mymatrix.diagonal() ))= =0 ` .
I have a matrix ` A ` and I want 2 matrices ` U ` and ` L ` such that ` U ` contains the upper triangular elements of A ( all elements above and not including diagonal ) and similarly for ` L ` ( all elements below and not including diagonal ) .
To the OP : It's often useful to know that they take a ` k ` argument , too , for which diagonal to extract above or below ( which can be _really_ useful when you need it ! ) .
Additionally , there are the functions ` np.triu_indices ` , ` np.tril_indices ` , ` np.triu_indices_from ` , and ` np.tril_indices_from ` to generate indices to index the upper or lower triangle with .
How to calculate diagonal degree matrix from a huge ( scipy.sparse ) matrix ?
Given a quadratic matrix of dimension 1 million I want to calculate the diagonal degree matrix .
The diagonal degree matrix is defined as a diagonal matrix , which has the count of non zero values per row as entrys .
If all you need to do is count the non-zero elements , [ ` nonzero `] ( #URL ) method looks promising .
It should really be @USER ' s , in my opinion , as he was the one to point out ` nonzero ` .
If all you need is to count the non-zero elements , there is ` nonzero ` method that could be useful .
Just to add to the great solution , if one wants to generates the diagonal matrix from ` diag_deg ` , he could write ` diag_mat = np.zeros (( x.shape [ 0 ] **2 , ))` then ` diag_mat [ np.arange ( 0 , x.shape [ 0 ] **2 , x.shape [ 0 ] +1 )] = diag_deg ` and finally ` diag_mat.reshape (( x.shape [ 0 ] , x.shape [ 0 ]))` .
I.e. , it will work for ` sin ` and ` sqrt ` but not for ` cos ` .
@USER : I got so sick and tired of operating on ` .data ` that I submitted a [ patch ] ( #URL ) to Scipy implementing ` sqrt ` , ` sin ` and ` tan ` on CSR and CSC matrices .
A ring-buffer is fundamentally different from the contiguous memory block interface that numpy arrays demand ; including the fft you mention you want to do .
Note though , that you would be well advised to append in chunks , rather than per element .
Either you incur an O ( N ) performance hit every time you need to access your ringbuffer contiguously after an append , or you need to allocate some extra memory , so you can delay and amortize such operations as required to keep your memory contiguous .
i open the file in the dump argument list , so shouldn't it close when it is finished with the dump operation ?
I am thinking of doing it the old way like how I used to do it with Matlab , which is using the array A where I want to get the index from to minus the target value and take the absolute of it , then select the min .
You'll have the index of nearest value in diff [ 0 ] [ 1 ]
also , how can I then reindex things so that the first indices are the column labels of each file and the second is the filename ?
If you're using pandas > = 0.7.0 ( currently only available in the GitHub repository , though I'll be making a release imminently ! ) , you can concatenate your dict of DataFrames :
The dataframe has several columns and it is indexed by one of the columns ( which is unique , in that each row has a unique value for that column used for indexing . )
There are at least a few approaches to shortening the syntax for this in Pandas , until it gets a full query API down the road ( perhaps I'll try to join the github project and do this is time permits and if no one else already has started ) .
If the list is ` a = [ ' Alice ' , ' Bob ' , ' Carl ']` and the overall data frame is called ` df ` , then you can do this : ` df [ df.A.isin ( a )]` and it will sub-select the row indices where the set membership condition is true for elements of column ` A ` .
Given that , how do I take data with a shape ` ( N , 3 )` and translate it to the center of mass ?
The ideal would be to keep installed only the ones you want .
is there a 4gb dump when the crash occurs ?
Storing each ( arbitrarily-shaped ) multidimensional array in a ` CArray ` with a unique name and then storing those names in a master index table ?
Note it is totally possible to resize and extend a dataset in hdf5 ( pytables makes heavy use of this ) but not the units of data within that array .
Interpolation of sin ( x ) using Python
I am working on a homework problem for which I am supposed to make a function that interpolates sin ( x ) for n+1 interpolation points and compares the interpolation to the actual values of sin at those points .
This is my code right now ( we are executing in a GUI our professor made , so I think some of the commands are unique to that such as messages.write() ): #CODE
Generate y_n a corresponding list of points for sin ( x_n )
Calculate max error
Calculate error interpolation Lagrange - sin ( x )
plot sin ( x ); plot Lagrange ; plot error
If you want the eigenvalues of ` a ` , have a look at ` numpy.linalg.eigvals ` ( or ` eig ` or ` eigh `) .
This is because the trace is the sum of the eigenvalues , so if we divide by the size of the matrix ( not its dimensionality , which should be two always ) we get the mean .
Compute the cumulative sum : #CODE
However , the multinomial distribution is a generalization of the categorical distribution and can be used for that purpose .
:) I made an update to my answer , but if the problem stated in your question about the ` global name ' coord ' not defined ` is solved you may consider accepting the answer that helped you the most ( and up-vote the useful ones ) .
I've tried copying various DLLs from the IronPython DLLs directory ( basically all the numpy ones ) but I still get the error .
I'm having trouble relating your desired output to your question -- how does " calculating the mean over AA2 " translate to all the GLN and GLU / LYS rows disappearing ?
First , let's find out the unique elements for a certain attribute : #CODE
numpy.random.multivariate_normal ( mean , cov [ , size ])
It sounds like you want the 2d histogram of the intensity values .
Do you want to fit this histogram with a normal distribution ( ie calculate the mean and covariance matrix ) ?
If you want a different distribution , replace ` norm ` above with one of these : #URL
This is the error : Warning : divide by zero encountered in log
I think I will calculate the needed strides based on the ones which passed array already has .
transpose will return a view whenever possible , that way you don't have to worry about knowing the data type .
Is there a python method to re-order a list based on the provided new indices ?
The problem is that I have thousands of lists with the same length , and I want to re-order these thousands of list based on exactly the same new indices list .
Tip : use a generator expression and list indices .
Yes , I can write my own method to deep copy all the thousands of lists , repeatedly re-order all of them based on the exactly same indices , and put them back to the original list .
An alternative buy ugly way is to convert all my lists ( int , float , string ... ) to numpy array , map them based based on the indices list , and convert it back to the original list type .
In the code below T ( variables ) is a 2-dimensional matrix with det ( T ) = 1 , i just indicates the region number and the rest is irrelevant .
All Floating point operations have limited precision and errors do accumulate .
My first try was along the lines of a [ b [: , 0 ] , 1 ]= b [: , 1 ] which puts them into the indices of b , not the values ( ie , in my example above instead of filling in rows 3 , 4 , and 5 , I filled in 2 , 3 , and 4 ) .
I just copied the last cupple of rows , but you can see , that the first index ( row ) is not unique .
I mean a matrix multiplyed by a vector should result in a vector , and a vector can't have the same indices twice by definition .
[ Matrix transpose . ] ( #URL ) You may also find this page helpful : [ NumPy for Matlab Users ] ( #URL )
The source of your confusion is the use of the numpy ` dot ` operator with scipy sparse matrices .
For numpy and scipy matrices ( note matrices , not arrays ) , the ` * ` operator computes dot products , like this : #CODE
Alternatively , the sparse matrix classes own ` dot ` method will also work : #CODE
The repeated entries you are seeing are the individual products which get summed down into final dot products , I think .
Of course you could just set the axis parameter in cumsum or sum to do the above , but the point here is that it can be used for any 1-D function you write .
Do a stack trace to see how you got there .
Note very carefully if the files mentioned in the trace are the ones you * think * should be there .
lut.take ( xa , axis=0 , mode= ' clip ' , out=rgba )
Can someone provide an efficient way to append elements to ' x '
So , can someone suggest an efficient way to append elements to x or some other efficient manner to plot
This is because ` x ` is a list of lists , so ` count ` will always be a list like ` [ 0 , 0 , 0 ]` , so in the first step of the loop you would attempt ` x [[ 0 , 0 , 0 ]]` and get a ` TypeError : list indices must be integers , not list ` .
correct and efficient way to flatten array in numpy in python ?
and I'd like to flatten it , joining the two inner lists into one flat array entry .
You can use the ` reshape ` method .
+1 for flatten() -- it can also do Fortran / column-major flattening . reshape ( -1 ) will also flatten .
I wonder reshape should take far lesser time but its almost similar
I expected this to be a 3dimensional array , with three indices .
Also , given this , how can I reshape " myarray " so that the two columns are merged , i.e. so that the result is : #CODE
Edit 2 : Getting closer , but still cannot quite get the ` ravel ` / ` flatten ` followed by reshape .
` fft (
` fft (
How to parallelize a sum calculation in python numpy ?
I have a sum that I'm trying to compute , and I'm having difficulty parallelizing the code .
It spits out a numpy array , and I want to sum the output arrays from about 1000 calculations .
Ideally , I would keep a running sum over all the iterations .
These functions return a list , which I convert to a numpy array and then sum over .
Is there a way to simply parallelize a running sum of arrays ?
I'm rotating an image about 1000 times , and I need to sum the result from each rotation .
" since numpy knows you want to do a matrix dot product it can use an optimized implementation obtained as part of " BLAS " ( the Basic Linear Algebra Subroutines ) . ... many architectures now have a BLAS that also takes advantage of a multicore machine .
Are you just trying to partition a list onto a pool of workers , have them keep a running sum of their computations , and sum the result ?
Thanks for your response , but I wanted to keep a running sum over multiple iterations , without summing over any lists at the end .
I'm rotating an image about 1000 times , and I need to sum the result of a calculation from each rotation .
I figured out how to do parallelize a sum of arrays with multiprocessing , apply_async , and callbacks , so I'm posting this here for other people .
I'm currently using a loop in python to assign each chunk to a temporary array and then pulling the unique values from the tmpArray .
I've seen examples taking the min , max , and mean from my square chunks but I don't know how to convert them to a majority .
resize with averaging or rebin a numpy 2d array
The problem I'm running into is that I can't seem to figure out how to translate the where , counts , and uniq_a steps from a one dimension to two .
Even though reshape and transpose return views whenever possible , I believe reshape-transpose-reshape will be forced to copy .
I'm still hoping to implement this algorithm across an entire dataset at once . like grid.reshape (( 5 , grid.shape [ 0 ] // 55 , -1 )) .max ( axis=3 ) .max ( 1 ) would provide the max .
When you index an numpy array with another array the arrays need to be the same size ( or they need to broadcast against each other , more on this in a sec ) .
If you want the " outer product " of index with index you can do the following .
That's because the two arrays get broadcast against one another , you can read more about broadcasting here .
It really prints the indices of the fields in the order you would need to place them in order to sort the array .
` argsort() ` returns the indices that would sort the array .
Of , course , ` k ` can be equal to the number of total elements in the population , and then ` choice ` would contain a random ordering of the indices for your rows .
The only problem here is that I think it will append directly to the column , when I would prefer it to append to a new column .
One fairly straightforward way of doing this would be to track the timestamps in a separate list , then use ` np.concatenate ` ( the documentation on this function is quite helpful ) with the ` axis=1 ` keyword argument to join the timestamps array to your averages array .
On each shift you accumulate your values in a base array .
Here I've simply set the return mask to False , but you could also eliminate the edges by expanding the input data by one pixel in each direction and filling with the nearest pixel , zeros , gray , etc .
You can use the arrays sum method to avoid looping over the color dimension , ie ` (( data [ 1 : -1 , 1 : -1 ] - data [ 1+ #URL 1+ #URL ) **2 ) .sum ( -1 ) ** .5 `
hello unutbu , I'm afraid I don't really get your solution , meaning the output . at the moment I managed to kick out all maxima that have absolute value less than let's say 1500 .
They have the same shape and size and i want to convolve them .
i forgot to mention , that i want to convolve 2 2-dimensional arrays : S
My Python code is just doing some math , not using any library other than Numpy and the common ones .
is there any way to have an image where we have static color values instead of dynamically changing ones as above ?
Eumiro helped me a lot on this , but I don't manage to flatten the output list as it should be :( #CODE
@USER , I think the confusion here is that there are _two_ lists , an outer and inner and we are mixing them up .
To be clear , if the first two inner elements of each outer list element match with any other - then they are to be removed ?
if the first two inner elements of every outer element appear in the whole list more than once in this combination , remove all outer elements concerning . many thanks in advance .
I tried for myself just to count the appearance and only take those outer elements being in the list once ( concerning their first two inner elements ) .. but it is unacceptable slow :(
` count_unique ` counts the number of the number of occurrences for each unique key in keys .
Iterate through your main list and for each ` i ` check if ` i ` is in your other array and if not append it .
` offset = sum ( np.array ( i ) * a.strides )`
This will give you the indices of the neighbors : #CODE
A bin is range that represents the width of a single bar of the histogram along the X-axis .
The Numpy ` histogram ` function doesn't draw the histogram , but it computes the occurrences of input data that fall within each bin , which in turns determines the area ( not necessarily the height if the bins aren't of equal width ) of each bar .
I think this would be more accurate : ` plt.bar ( bin_edges [: -1 ] , hist , width=1 )` and ` plt.xlim ( min ( bin_edges ) , max ( bin_edges ))` , to make the bars fit their expected width ( otherwise , there may just be a smaller bin with no values in between ) .
Write Code that does the same like Numpy's reshape
Then using different optimization algorithms ( e.g. gradient descent , etc . ) you can find ` a ` and ` b ` which minimizes the ` h ` output .
For instance , ` curve_fit ( deriv , time , zn , p0 =( 12 , 0.23 ))` if you want ` a=12 ` and ` b= 0.23 ` be the initial guess .
But slightly simpler than your lambda thing , with no imports needed , is just ` sum ( x*x ) ** 0.5 `
You can also feed in an optional ` order ` for which norm you want .
use the function norm in scipy.linalg ( or numpy.linalg ) #CODE
EDIT : The real speed improvement comes when you have to take the norm of many vectors .
" C-number " is an integer n > 1 such that ( b^n ) mod n = b for all integers 1
Which ones ?
Then go through the lists to check which ones are in c numbers and not in prime numbers .
That is , read images where only the first pixel is absolute ( R1 , G1 , B1 ) and all the others are in the form ( R [ n ] -R [ n-1 ] , G [ n ] -G [ n-1 ] , B [ n ] -B [ n-1 ]) , and convert them to standard RGB .
Well , I think it has to overflow , or to put in another way , it's operating mod 255 .
For some reason I did not understand yet , the final reshape in your code copies the data .
Could you show under what circumstance the final reshape copies the data ?
No reshape needed .
After some other experiments , I found out that with this layout it is no more necessary to flatten it .
To calculate average , you can call np.histogram2d with weights = pings , this will sum all the pings in the square area . and then divide the result with count .
If you make a binary-archive you only have to compile ones pr machine arcithecture / osx-version .
I would suggest doing the calculation on the indices : #CODE
The bitwise logical operation in the third line ensures that no indices are used twice .
implemented via cmap and norm ; in contrast , pcolor() simply does not
concatenate numpy arrays that are class instance attributes in python
I want to be able to concatenate attributes of the instances that are contained in the numpy array .
For the first you have the simplest form I can think of , although I usually use normal arrays instead of numpy ones for objects .
you can use numpy.hstack() to concatenate arrays : #CODE
concatenate : Join a sequence of arrays together .
You can transpose a list of lists using ` zip() ` : #CODE
And this is the transpose of the original matrix !
You can turn off the clip flag of the line object created by plt.plot .
The catch is that I need to keep the colors exactly the way they are ( background : I'm resizing a map where provinces are color-coded ) , and so I cannot just perform a resize with bicubic interpolation , because that will also interpolate the pixel colors while smoothing .
The dot product of this vector should simply be ` 1*1+2*2=5 ` .
The second one is correct ; it gives you right correct dot product as well .
In that case the dot product cannot be taken because and m-by-n matrix can be dotted only with an n-by-k matrix .
In the second case , for convenience numpy is generating a one-dimensional array instead of a matrix , so the dot product has a simple definition .
To dot a 2-d matrix with itself , when it isn't square , you must transpose : #CODE
A shortcut for ` transpose ` is ` W.T `
The key here is that numpy / scipy honours the shape of arrays when computing dot products .
it is , therefore , necessary to use the transpose operator to compute the dot ( inner ) product of W with itself : #CODE
You should use ` vdot ` : " Return the dot product of two vectors .
Why don't you just compress the files with the built-in ` gzip ` module ?
I should be able to concatenate them using numpy.concatenate .
The first parameter to ` concatenate ` should itself be a sequence of arrays to concatenate : #CODE
The arrays you want to concatenate need to passed in as a sequence , not as separate arguments .
An alternative ist to use the short form of " concatenate " which is either " r_ [ ... ]" or " c_ [ ... ]" as shown in the example code beneath ( see #URL for additional information ): #CODE
Then I need to sum over columns in each matrix , and multiply each row with a scalar .
I tried dictionaries , but they fail when I need to sum , and multiply .
and then sum over columns ( ` sum ( 1 for x in column [ 2 ])`) or over rows and multiply the result with whatever you want .
Why bother with dicts when you are using indices as keys ?
@USER I finally used your hints , and stored data in dictionaries with pretty time effective calculations for " columns sum " , " row sum " .
Since the dictionary keys are tuples , it is trivial to add the indices by row or column .
Since the entire matrix would need to be processed after construction to do this , we can just construct a dict with whatever sum or product is desired once and then refer to that dict of processed data .
Now you can refer to the col key in ` cols ` for the sum of the rows by col .
Wouldn't that require to iterate over the entire dict every time you want to sum a single column's or row's values ?
@USER I didn't use class as you mentioned , as simple function to sum over columns was easier to fit into rest of the code .
Then to multiply by a scalar and take sum : #CODE
Create as 100x100xN array ( or stack together if that's not possible ) and use ` np.median ` with the correct axis to do it in one go : #CODE
However , if you use the transpose , the length along the last axis is 3 , so it is valid .
Python : Resize an existing array and fill with zeros
diagonal matrix , ` sigma ` , with ` np.diag ( S )` - so far , so good .
resize this new diagonal array so that I can multiply it by another array that
I need to use sigma with its current data , but I also need to resize it so that I can multiply it with another matrix .
Therefore , I just need to get sigma to the appropriate size , filling the remaining values with zeros is exactly what I want .
` np.resize ( sigma , shape )` , on the other hand , returns the result but instead of padding with zeros , it pads with repeats of the array .
However , this will first flatten out your original array , and then reconstruct it into the given shape , destroying the original ordering .
If you just want to " pad " with zeros , instead of using ` resize() ` you can just directly index into a generated zero-matrix .
Thanks , this is very close to what I need- but now my new matrix loses its diagonality , i.e. the elements that were originally along the diagonal don't stay there .
I see the edit ... you do have to create the zeros first and then move some numbers into it .
Like the other answers , you can construct the diagonal matrix with ` np.diag ` before the padding .
You can encode and decode the keys before you pass it to the ` savez ` function .
By definition , a set of linear equations with a singular matrix has no unique answer .
You have 18 equations , but only 9 unique equations .
There is absolutely no unique solution to your problem .
Can you just reshape it first with `` test_arr1.reshape ( len ( test_arr1 ) , 2 )`` ?
The elements are ids that I will use as a key to look up values in a dictionary .
In particular , some of my functions return ` std :: vector ` s , which get translated to tuples in Python .
I am not up to the challenge of typemapping ` std :: vector ` s into numpy arrays .
Before , what you were doing was having SWIG convert the C++ ` std :: vector ` objects into Python tuples , and then passing those tuples back down to ` numpy ` - where they were converted again .
Here's some code which will turn all ` std :: vector int ` objects into NumPy integer arrays : #CODE
Maps any returns of ` std :: vector int ` into NumPy arrays with a ` typemap `
This code should be placed before you ` %import ` the headers which contain the functions returning ` std :: vector int ` .
It would seem sensible to simply store the output as a ` ndarray ` , but I'd really like to be able to access the outputs based on the parameter values not just indices .
One option is to use a numpy ndarray for the data ( as you do now ) , and write a parser function to convert the query values into row / column indices .
After install ffdshow , I can use ` cv.FOURCC ( * " ffds ")` to encode the video by MPEG-4 .
Just append zeros as needed .
fast way to invert or dot kxnxn matrix
This is necessary , because ` tensorsolve ` would multiply with ` b ` over these last three axes and also sum over them so that the result ( the second argument to the function ) is again of shape ( M , N , N ) .
Then secondly , about dot products ( your title suggests that's also part of your question ) .
For pre-2.7 versions of python , you must roll your own ( see manual ): #CODE
Convert numpy array from values to indices
And I like to have a numpy array that has the indices of the True and False values , i.e. [ 0 , 2 , ... ] and [ 1 , 3 , 4 , ... ]
To get the indices of the ` True ` values in ` a ` , you can use #CODE
For the indices of the ` False ` values , use #CODE
Matplotlib yaxis range display using absolute values rather than offset values ?
On Unix , it doesn t hurt to append a ' b ' to the mode , so you can use it platform-independently for all binary files .
Is there a numpy max min function ?
Reason : performance increase since max and min would cause twice the iteration of the array ( which is in my case 7.5 million or more numbers ) .
Internally the ` ptp ` function computes the minimum and maximum separately , so this solution still iterates over the array twice .
See [ the ptp source code ] ( #URL ) for details .
then I want to plot the graph of time , log ( zlist )
" plot ( time , log ( zlist ))
Any ideas how to either make zlist not an array so I can take the log , or just what is going wrong in my program ?
EDIT X2 : It seems to be working now , i'm not sure why but it could either be a ) importing log from math as well , or b , the negative value problem , but regardless , it's working well .
You should include the entire relevant part of the program , including the line ` plot ( time , log ( zlist ))` and your imports
That will apply the log function to each number in zlist .
The NumPy version of ` log ` automatically applies to all items of the array -- no ` map() ` necessary .
I suspect that you did ` from math import * ` after ` from numpy import * ` which means that ` log ` is really ` math.log ` which won't work on an array .
( The loop is order the individual cubes , and the " cube " creates a boolean array of indices . ) I then store points [ cube ] somewhere , but this isn't what's slowing me down ; it's the creating of " cube= " .
Do query , k is the the number of nearest neighbors to return , p= np.inf means maximum-coordinate-difference distance , distance_upper_bound is the max distance .
I was originally going to append the new data to an existing numpy array in the loop until I read that the whole array has to me moved each time in memory .
This was tested with the backport of a gcc 4.7 snapshot from Debian unstable to squeeze .
is it possible to use numpy.take to pick a list of indices ?
I want to take some indices from the elements of a numpy.array .
Get numpy array with only certain indices
Note that I do not want to silence all types of " overflow " warnings , only the ones here .
The try + finally suggested by pv . works fine .
Ultimately , ` n.x ` will be returned as a multi-dimensional array where the ` X ` class will reshape is on return , but is stored in a ` n.P ` which is a vector .
Give each input set to a different process , and join the output .
" code was actually located outside of the outer control loop ( while not all calculations finished ) .
Obviously , the labelled matrix class code is not here , and the fingerprint reader / scorer code is not included ( though it is pretty simple to roll your own ) .
The first example uses structured arrays , while the second is just unpacking the columns ( thus the transpose ( ` .T `)) into three variables .
Numpy : How to get rid of the minima along axis=1 , given the indices - in an efficient way ?
So , if we know the column index of the item we want removed , we can vectorize the operation for an array of column indices , which is what ` pop_col ` does .
This produces a big band of zeros in the internal covariance matrix calculated by ` gaussian_kde ` , making it singular and causing the routine to fail .
Matlab's cross-correlation function ` xcorr ( x , y , maxlags )` has an option ` maxlag ` , which returns the cross-correlation sequence over the lag range ` [ - #URL Numpy's ` numpy.correlate ( N , M , mode )` has three modes , but none of them allow me to set a specific lag , which is different from full ` ( N+ M-1 )` , same ` ( max ( M , N ))` or valid ` ( max ( M , N ) - min ( M , N ) + 1 )` .
So you're asking for a function like correlate that takes a variable lag parameter ?
If you don't need to preserve the order of the columns , ( i.e. if you can use ` roll `) then Mr .
I was just trying to draw histogram using new OpenCV Python interface ( cv2 ) .
Below is the resulting histogram i obtained .
you can call ` cv2.calcHist ([ img ] , [ CH ] , None , [ 256 ] , [ 0,255 ])` to calculate the histogram of channel CH of img , that is img [: , : , CH ] .
How do I clip a floating-point number to just below a limit ?
If your processing is returning values outside of the valid range , and it is deterministic , perhaps you should filter the input rather than clip the output .
Now I would like to consider each array to be a tile of a bigger image and should be placed according to the offset_i_x / y values , to finally write a single figure instead of 4 ( in my example ) .
c ) Draw bounding box for it , then resize to 10x10 , and store its pixel values in an array as done earlier .
The procedure is same as above but , the contour finding uses only first hierarchy level contour , so that the algorithm uses only outer contour for each digit .
In the result the dot in the first line is detected as 8 and we haven t trained for dot .
But when i run the test-training file , it runs with an error ` *** stack smashing detected *** : ` and hence i am not getting a final proper image as you are getting above ( digits in green color )
i change ` char name [ 4 ]; ` in your code to ` char name [ 7 ]; ` and i didn't get the stack related error but still i am not getting the correct results .
The scipy documentation also gives an exact same usage example which translate as following to your code : #CODE
It would also allow you to skip the zeros array in ` coords ` .
The function only takes one input - a matrix mat : #CODE
If I perform this in the main body of my program , the changes to my rot_mat view would propagate to the original matrix mat .
I should also note that it isn't changing mat within the function itself .
At the end , I just try to return mat but no changes have been made .
This makes it easy to switch to the latest libraries and not depend on your Linux distribution ( Linux distributions , especially enterprise ones , can't possibly keep up with Python library updates ) .
In the example above , the column to sum is hard coded and accessed through dot notation .
Is it possible to change the function so the column to sum is passed in as a parameter ?
Normally numpy forces the left and right side of an assignment to match , so for example if I do ` a [: ] = b ` , ` b ` must be the same shape or broadcast to the same shape as ` a ` .
axes permutation b / w the image and data array might be an issue too
You're looking for scipy.ndimage.label more info here . label returns an array the same shape as the input where each " unique feature has a unique value " , so if you want the indices of the features you can do something like : #CODE
Why is numpy much slower than matlab on a digitize example ?
I am comparing performance of numpy vs matlab , in several cases I observed that numpy is significantly slower ( indexing , simple operations on arrays such as absolute value , multiplication , sum , etc . ) .
Let's look at the following example , which is somehow striking , involving the function digitize ( which I plan to use for synchronizing timestamps ): #CODE
As I'm learning to extend Python with C++ , I implemented my own version of digitize ( using boost libraries for the extension ): #CODE
There is a bit of cheating as my version of digitize assumes inputs are all monotonic , this might explain why it is even faster than Matlab .
Why is my own version of digitize much faster than numpy.digitize , but still slower than Matlab ( I am quite confident I use the fastest algorithm possible , given that I assume inputs are already sorted ) ?
Based on the answers so far , I would like to stress that the Matlab function histc is not equivalent to numpy.histogram if someone ( like me in this case ) does not care about the histogram .
Such an output is provided by the numpy functions digitize and searchsorted .
As one of the answers says , searchsorted is much faster than digitize .
Can you edit your question to include your C++ version of digitize ?
This is what I need , np.digitize provides this mapping , help ( np.digitize ): " Return the indices of the bins to which each value in input array belongs .
The beauty of histc is that is returns not only the histogram ( which I do not need at all ) but also the mapping from inputs to edges ( which I need for synchronization purposes ) .
I think Matlab is much slower than my C++ code because I am cheating , not only my function assumes non decreasing bins but also non decreasing input values ( x in the code above ) .I believe that if I would generalize my code , e.g. by having a python wrapper which just sorts the inputs , then applies my function , then re-orders according to the initial ordering , ** my C++ code would be slower than matlab ** .I think it would also be useful to tell digitize ( or searchsorted ) if the input is already sorted , to speed up even more ( gaining a potential factor of 5 , using my code as benchmark ) .
If you have an older version , you could roll your own formatting function and supply it to numpy with ` numpy.set_string_function ` .
Creating a Three Dimensional Matrix structure and writing in mat file in Python
` R = zeros ([8 , 8,530 ] , float )`
And , then I try to save it in mat file as scipy claims to do so .
What does ` lower_a.T ` ` dot T ` actually do ?
dot is a numpy function which does a matrix product then the inputs are 2d-arryas
` lower_a.T ` is simply the transpose of the ` lower_a ` array .
You can do it like this , notice that result is the transpose of your result ( Also in this case result is a 2d array and in your case it is a list of 1darrays ) .
To answer the second part of your question - yes you can append to lists within Cython functions .
I guess their method does not exhaustively map all possible numpy types , but at least the most common ones where it makes sense .
I'm looking for how to turn the frequency axis in a fft ( taken via scipy.fftpack.fftfreq ) into a frequency in Hertz , rather than bins or fractional bins .
To all the DSP experts ; I'm aware that it's actually BW that's relevant , not max frequency .
If you set ` d=1 / 33.34 ` , this will tell you the frequency in Hz for each point of the fft .
We could reshape it by doing either : #CODE
i mean the reshape parts , thanks .
A method of the class needs to update my approximating function in each iteration , by multiplying each basis function with a scalar ( different scalars for different basis functions ) , and forming the sum .
In other words , I need to update my approximating function by forming a linear combination of the basis functions and setting my approximating function to this unevaluated sum of functions .
TypeError is when the array is either null or it's not array , or it's empty , or it's NaN , etc , so maybe try to print x and x2 and compare with what is inside the sum .
But if you just want to make a on / off blurring effect then the function to convolve with ( convolution kernel ) is just an if statement on the distance between the source point and any possible neighbor pixel .
Also , not shown in my example , ` data_str ` can have some numbers with 13 digits after the dot .
Just view it as a 1 character string array and reshape it : #CODE
Is there an " enhanced " numpy / scipy dot method ?
where ` A ` is a ` m x n ` matrix , ` A**T ` is the transpose of ` A ` and ` Q ` is an ` m x m ` diagonal matrix .
Since ` Q ` is a diagonal matrix I store only its diagonal elements as a vector .
( w / r / t the last sentence of the OP : i am not aware of such a numpy / scipy method but w / r / t the Question in the OP Title ( i.e. , improving NumPy dot performance ) what's below should be of some help . In other words , my answer is directed to improving performance of most of the steps comprising your function for Y ) .
First , this should give you a noticeable boost over the vanilla NumPy dot method : #CODE
But why are the arrays copied before being passed to dot ?
The dot product relies on BLAS operations .
On the other hand , the transpose does not effect a copy , though unfortunately returns the result in Fortran order :
Therefore , to remove the performance bottleneck , you need to eliminate the predicate array-copying step ; to do that just requires passing both arrays to dot in C-contiguous order* .
So to calculate dot ( A.T. , A ) without making an extra copy : #CODE
In sum , the expression just above ( along with the predicate import statement ) can substitute for dot , to supply the same functionality but better performance
In the meantime , I used the example above to write a function that can replace numpy dot , whatever the order of your arrays are , and make the right call to fblas.dgemm .
Argmax of numpy array returning non-flat indices
I try to get the indices of the maximum element in a numpy array .
the indices of that .
Next , ` numpy.where() ` extracts the indices of the ` True ` entries in this Boolean array , but the result is not in the format described above , so we need ` zip() ` to restructure it .
The Multiple Precision Toolbox doesn't seem to have an equivalent of ` eig ` , but it does have ` svd ` .
Other basic linear algebra functions missing in Symbolic Math Toolbox are : ` norm , cond , max , min , sort , lu , qr , chol , schur ` .
Besides being extremely slow ( it performs number-to-string conversion of operands on every arithmetic operation : ` + , - , ... `) and lacking essential functionality ( ` eig , det , cond , \ , ... `) , it gives wrong results in functions it has .
E.g. incorrect results delivered by ` svd ` function made my research senseless at some point and error was painful to find .
The concept of " masked values " ( that is of an array of value coupled to a list of indices to be masked ) does not directly exist in R .
I frequently use the numpy.where function to gather a tuple of indices of a matrix having some property .
ix is now a tuple of ndarray objects that contain the row and column indices , whereas the sub-expression X > 0.5 contains a single boolean matrix indicating which cells had the > 0.5 property .
I have written some code that for a range of years ( eg . 15 years ) , ` ndimage.filters.convolve ` is used to convolve an array ( eg . array1 ) , then where the resulting array ( eg . array2 ) is above a randomly generated number , another array ( eg . array3 ) is given a value of 1 .
There isn't a unique useful way of converting an array into a bool , and there is currently no way of overriding the behaviour of the boolean operators ( though I believe this is being discussed for future versions ) , so in numpy , ` ndarray.__nonzero__ ` is defined to raise an exception .
For example , starting from your line ` # Iterate through array2 ` we can remove the explicit loop and simply broadcast over the variables we want to change .
In addition , this greatly improves code clarity once you get used to this style as you aren't explicitly dealing with the indices ( your ` a ` and ` b ` here ) .
If you do a ` numpy.argmax ( array )` it will look for max in the flattened array , and then you can do the ` unravel_index ` with the array shape to find the actual index .
However , because you can only have one stride per dimension , there's no way to reshape this into a ` 96x96x96x125 ` array without making a copy .
However , as you've already noted , we can't reshape this to ` 96x96x96x125 ` without creating a copy .
( The relevant documentation is in ` numpy.reshape ` , if this seems confusing . Basically ` reshape ` will avoid copying when possible , and return a copy if its not , whereas setting the ` shape ` attribute will raise an error when copying isn't possible . )
I have a numpy program where I need to find the index of a value in array B from a sum from array A - and sadly the precission problems of numpy arrays gives me a problem with that :(
B == sum ( A [: 3 ])
B == sum ( A [: 2 ])
sum ( A [: 2 ])
sum ( A [: 2 ])
How can I be sure to find the value in array B that is the precise sum from array A ??
Do a binary search in B with the sum of the values in B and then compare the absolute value of the difference of the value found in the B with the sum from A .
You will need to two check two values in B because the sum could be slightly over or under the desired value in B .
As someone pointed out you could also do ` min ( a.tolist() )` which uses the same type of comparisons as sort , and would be faster for large arrays ( linear vs n log n asymptotic run time ) .
If you can convert a numpy array to list , you can get away with calling min ( l ) .
But to extend this , I would add that sorting takes O ( nlogn ) time , whereas you could do ` min ( a.tolist() )` and get exactly what is required in O ( n ) time .
The best way to get started it to either join us in the #pypy channel on freenode , or to send a message to the pypy-dev mailing list !
In python I have ` numpy.ndarray ` called ` a ` and a list of indices called ` b ` .
10 places around the indices of ` b ` .
Could you find a certain number that you're sure will not be in ` a ` , and then set all indices around the ` b ` indices to that number , so that you can remove it afterwards ?
This code will not allocate new memory for ` a ` at all , needs only one range object created , and does the job in exactly 22 c-optimized numpy operations ( well , 43 if you count the ` b + i ` operations ) , plus the cost of turning the ` unique ` return array into a ` set ` .
Beware , if ` b ` includes indices which are less than 10 , the ` number_not_in_a ` " zone " around these indices will wrap around to the other end of the array .
If ` b ` includes indices larger than ` len ( a ) - 11 ` , the operation will fail with an ` IndexError ` at some point .
Note , if your ranges overlap , you'll get a difference between this and your algorithm : where yours will remove more items than those originally 10 indices away .
It doesn't have the GUI tools of Enthought but otherwise contains a full scientific python stack .
Debug build of Python ( python-dbg ) in tandem with gdb allow you right away debug your extensions while inspecting Python stack etc .
Deal with overflow in exp using numpy
DaveP , asymptotic behaviour of exp is exp ...
In optimization it's not uncommon to minimize log ( f ) .
( approximate log likelihood etc etc ) .
Are you sure you want to optimize on that exp value and not log ( exp ( f )) == f .
You need to see how your function behaves , I thing you should check e.g. exp ( -x ) +exp ( -y ) +x*y
You could use ` histogram ` .
To use gen-expr : ` sum ( 1 for i in a.ravel() if 25 < i < 100 )`
This is why the ` sum ( 1 )` approach is better : it has a fixed and much smaller memory footprint , since you don't have to create an intermediate list .
I'm not sure if there is a built-in way , but it should not be hard to roll your own : #CODE
To make sure I understand what is going on : In the first def statement you are creating an empty array of zeros which is the correct size .
Then you iterate through ( xrange ) using numpy's slicing ( ? ) adding one with each iteration and appending the array of zeros ?
pretty much .. you can think of the loop as working like this : the kernel is made up of a sum of stack of " squares " of ones , and the squares are decreasing in area - each time in the loop is adding another square onto the middle of the stack .
Verbose explanation of the magic : the first thing we're going to need are the indices of each entry in the matrix , and for that we can use mgrid : #CODE
The centre will be at N // 2 , N // 2 ( where // is truncating division ) , so we can subtract that to get the distances , and take the absolute value because we don't care about the sign : #CODE
I'm working on Polynomial Transform for a homework assignment .
Polynomial Transform
An alternative answer is to reshape the array so that it has dimensions ` ( 1 , N )` like so : #CODE
I am looking for a numpy function to find the indices at which certain values are found within a vector ( xs ) .
The returned indices must follow the order of ys .
If it is important to keep the original order of ` xs ` , you can use this approach , too , but you need to remember the original indices : #CODE
if you don't need to keep track of what elements where found and which ones where not you can filter the output to get rid of all the indexes beyond limits : ndx = [ e for e in np.searchsorted ( xs , ys ) if e < len ( xs ) ]
I want to enumerate ` a ` over its first n dimensions ; this means that , at each iteration , the enumerator / iterator produces a pair whose first element is a tuple ` ii ` of n indices , and second element is the k dimensional sub- ` ndarray ` at ` a [ ii ]` .
Instead of consisting of single item indices only , it would consist of a mix of single item indices and sequences of indices .
It would therefore cease to be a bijection between indices and sub-arrays ; one index could refer to many sub-arrays .
As you can see , it's the same , but ` numpy.ix_ ` allows you to use non-consecutive indices .
For example numpy.nonzero ( a ) [ 0 ] [ -1 ] is the index of the last nonzero element of a ( dimension=0 ) , and similarly numpy.nonzero ( a ) [ 0 ] [ 0 ] is the index of the first nonzero element .
If we know that we only care about the first or last element we can use less memory and have better common-case runtime than running " nonzero " like above .
You can use ` argmin ` to find the False values , and this will be faster and take less memory than using nonzero , but this is linear in the length of ` a ` .
The docs clearly state that " In case of multiple occurrences of the maximum values , the indices corresponding to the first occurrence are returned .
Each counter is basically a histogram .
Will look into the sparse matrix although if im reading rightly they are limited to 2d so i will need to flatten 1 dimension .
Second , the call to empty is not strictly necessary -- i.e. , an array having 0 size could ( i believe ) be initialized using other array-creation methods in NumPy , e.g. , NP.zeros , Np . ones , etc .
To whom it may concern : Although Bloscpack and PyTables are different projects , the former focusing only on disk dump and not stored arrays slicing , I tested both and for pure " file dump projects " Bloscpack is almost 6x faster than PyTables .
To solve your can use joblib you can dump any object you want using ` joblib.dump ` even two or more ` numpy arrays ` , see the example #CODE
AttributeError : ' bool ' object has no attribute ' sum '`
Basically you're iterating through each item in ` Xa ` and omitting the ones that don't fall with the range .
when i add two images in opencv , sum is limited to 255 .
ie limit sum to 255 ?
The first column contains unique elements here .
In the latter , use a dictionary with the unique elements as key .
@USER : dicts only allow unique keys , though , so if he wants to store two ` None ` records he's out of luck .
As a bonus , if you use a real database , it will automatically enforce things like the uniqueness of the unique keys ( which is useful for catching programming errors and maintaining consistency of your data . )
The classic approach is to have a supplementary file ( index ) which maps the keys of the data tuples in the table to absolute positions of the tuples in the file .
If you want to keep the storage in a file , the way you do it , then the simple solution to prevent duplicate entries from appearing the next execution would be to simply truncate the file first .
The fit-function I use is a sum of exponentials decaying with different timescales a ( 4:6 ) and different weights ( a ( 0:4 )) .
I want for example that sum ( a ( 0:4 ))= 1.0
To have several functions in the least square fit you may just append the functions as I indicated using np.concatenate .
E.g if you need that \sum ( sin ( p [ i ])= =1 , you can do the following : #CODE
I am trying to sum the values of a nD array along a particular axis to effectively collapse it into a 1D array .
The function ` label ` places a unique tag on each block of pixels that are within a threshold .
This identifies the unique clusters ( shapes ) .
In this case specifically the function is the absolute difference of two vectors : ` S [ i , j ] = abs ( A [ i ] - B [ j ])` .
@USER .F .Sebastian - For whatever it's worth , you can also avoid it by taking the absolute value in-place if you don't have ` numexpr ` installed .
In addition to what @USER has suggested , you can also use the ` outer ` method of any numpy ` ufunc ` to do the broadcasting in the case of two arrays .
In this case , you just want ` np.subtract.outer ( A , B )` ( Or , rather , the absolute value of it ) .
Basically , you can use ` outer ` , ` accumulate ` , ` reduce ` , and ` reduceat ` with any numpy ` ufunc ` such as ` subtract ` , ` multiply ` , ` divide ` , or even things like ` logical_and ` , etc .
Thanks @USER , do you know offhand if there is any internal difference between the broadcasting method and the outer method ?
@USER On my machine for arrays with 10,100,100 0 and 10000 elements , the broadcasting and outer methods have pretty much the same timings , with the broadcast method winning by a small amount .
This is why I use logm() instead of log .
( The ` [ 1 ]` skips over the value in the diagonal , which will be returned first . )
I'm wonder if it is possible translate this kind of functions in matlab to python : #CODE
Manipulating indices to 2d numpy array
On the other hand using numpy arrays for the indices , the arithmetic works , but the indexing does not work .
Is there a way to do vector arithmetic on the indices but also be able to index a numpy array with them ( without deriving a class from tuple and overloading all the operators ) ?
... and working on arrays of indices then works as well using map #CODE
( confusing to me why I need the transpose , though .
Overriding histogram to add elements instead of counting
Proceeding from this post on histogramming , is there a way to override the histogram function to add the numbers instead of counting them ?
A histogram is a count of how many of the sample values are in each given numeric range .
You appear to want to " chunk " the sample values into sub-ranges of a specific size , and get the sum of each chunk .
You can use the ` weights ` argument to ` histogram ` ( official documentation for numpy.histogram ) .
The first line initializes a bunch of zeros and ones and packs them into a ` 2x108 ` array .
That number is the number of both ones and zeros , And the number used in the ` .reshape (( 2 , n ))` function call .
The reason it is so fast is that initializing vectors of zeros or ones is really quick , faster than copying an arbitrary array .
I had originally said that the way ` os.path.join ` works bothers me , because I have to say ` os.path.join ( *args )` if I already have a list of pathnames to join .
I think for that reason ` array ` does not allow ` array ( 1 , 2 )` but ` reshape ( 1 , 2 )` is ok because ` reshape ` does not take any keywords .
That will make the tb_test array ones for where the first condition applies , twos for the second and threes for the third .
Are you able to instantiate the above matrix of zeros ?
From the audioop page , it says that the rms calculation is just what you'd expect , namely ` sqrt ( sum ( S_i^2 ) / n )` , where , ` S_i ` is the ` i ` -th sample of the sound .
To use numpy , I first convert the sound to a numpy array , and always see identical min / max , and the same length of the data ( so the conversion seems fine ) .
And then I want to append it into another NumPy array ( just like we create a list of lists ) .
@USER ` concatenate ` can also take multiple arrays
Sven said it all , just be very cautious because of automatic type adjustments when append is called .
Just pass the list of lists to ` numpy.array ` , keep in mind that numpy arrays are ` ndarrays ` , so the concept to a list of lists doesn't translate to arrays of arrays it translates to a 2d array .
The simplest change is to make the code working is to flatten the initial_theta before passing to fmin and reshape theta inside cost_function to ( X.shape [ 1 ] , 1 ) if you like .
Right now I am initializing a zeros matrix and then populating it with elements I get from a for loop , but that seems inefficient .
Hmm ... the hashes are the same the ids for me in Python 2.6.6 and numpy 1.3.0 , but not in Python 2.7.2 and numpy 1.5.1 .
I have an array of floats ` w ` and an array of indices ` idx ` of the same length as ` w ` and I want to sum up all ` w ` with the same ` idx ` value and collect them in an array ` v ` .
Note that your code might have been a bit more clear without re-using the name ` w ` and without introducing another set of indices , like #CODE
Without indices this should work , right ?
It's also possible to generate an array of indices without using ` enumerate ` .
Numpy provides ` ndenumerate ` , which is an iterator , and probably slower , but it also provides ` indices ` , which is a very quick way to generate the indices corresponding to the values in an array .
I need both index and value . your ` numpy.indices() ` solution will only give me the indices .
Sure , it provides the indices , and ` a ` provides the values .
@USER , then you don't need the indices at all .
I was thinking something like this ( which I know is wrong , because I replace the original y with the indices of y ): #CODE
is there a way to do modulo on the indices but replace the values ?
Conditional sum over matrices in python / numpy
Subdivide the range of ` X ` into equal intervals ` [ min ( X ) , min ( X ) +delta , min ( X ) +2*delta ,..., max ( X )]` .
I need this sum for all starting intervals ( ie . the entire range of ` X `) and I need to do this for many different matrices ` X ` and ` W ` .
Note that the ` __getitem__ ( i )` must be able to accept a single integer index in ` range ( len ( self ))` , not just a list of indices as you seem to indicate .
I am performing a series of operations ( resize , copy , etc . ) to some captured images in OpenCV , but I want to set some specific metadata ( the attribute names should also be defined by me ) to these images , which would survive those operations and can be extracted later on .
I've also learned that if there is a scipy / numpy implementation , it is usually much faster than anything I can roll myself in python .
but after that I am sorting this return with other integers and i get this error return max ( set ( input_list ) , key = input_list.count )
True that I iterate 50 million times and the number of elements in the array might be 25 to 30 max !..
I managed to get this working on the cedar stack by building numpy and scipy offline as bdists and then modifying the heroku python buildpack to unzip these onto the dyno's vendor / venv areas directly .
Heroku haven't officially published buildpacks yet - search for ' heroku buildpacks ' for more thirdparty / heroku ones and information .
Thanks , I'm enjoying playing with the cedar stack and build packs .
For those who wish to use Python 3.4 in production , I've built numpy 1.8.1 , scipy 0.14.0 , and scikit-learn 0.15-git ( 0.14 doesn't work with the others for some reason ) as binaries on Ubuntu 10.04 LTS 64-bit , which works on the Heroku cedar stack .
This reads the data as a one-dimensional array first , completely ignoring all new-line characters , and then we reshape it to the desired shape .
I'm not sure I understand what you want to strip .
Generally the ` join ` string method is pretty fast .
` log ( x )` could be very different from ` log ( x+1 )` !
( example : ` log ( 0.000001 ) = -6 ` , ` log ( 0.0000001 + 1 = 0 and a bit ` .
Adding say ` 1e-16 ` will have the same effect of never taking ` log ( 0 )` but with much less error introduced : using the [ appropriate identity ] ( #URL ) , it's ` log ( x + eps ) = log ( x ) + log ( 1 + eps / a )` , where the error introduced is near 0 if ` eps / a ` is almost 0 .
@USER I'm not sure exactly what you're doing with the logarithms here , but if you're doing any kind of NLP algorithm that uses the log , it's going to be doing the wrong thing if you're not actually giving it the log .
As I said before , there won't be a problem when you add ` 3 ` to every nonzero value .
I want to eliminate this huge gaps and I pick log as a basic solution .
If a take log without adding , then I lost my occurences .
@USER Oh , okay -- if you're just using the log completely arbitrarily anyway , then sure , it really doesn't matter if you just add three .
I'm not sure how you're using these log ( TF-IDF ) matrices for inference , but you could consider adding in a Bayesian prior that the name of an article is going to be used more often in the article , for example .
Note that this doesn't work properly if the sparse format has repeated elements ( which is valid in the COO format ); it'll take the logs individually , and ` log ( a ) + log ( b ) !
= log ( a + b )` .
It's unclear to me how the output of ` convert ` relates to ` mat ` .
I'll state it more strongly : I have no idea how the output of ` convert ` relates to ` mat ` , or perhaps more importantly , to the input ` row ` / ` col ` / ` data ` .
You should probably look at nonzero , anyway .
[ list ( mat [ i ] .nonzero() [ 1 ]) for i in range ( mat.shape [ 0 ])] " for line in mat " doesn't work
@USER ' s ` nonzero ` -based answer will work for all ` scipy.sparse ` types .
LiberMate : translate from Matlab to Python and SciPy .
Your ticks variable appears to be all zeros : #CODE
As already mentioned , you need to have the xticks not be zeros .
You first have to transpose it #CODE
Using a transpose to get a column is a bit opaque .
So append flattens the multidimensional array into 1 big list
My guess is that you did not want to flatten the lists .
we can't actually take the limit of the gradient , but its kinda fun .
In 8 we apply this derivative function to a vector of all ones and get the vector of all twos .
The most straight-forward way I can think of is using numpy's gradient function : #CODE
Example output for one of the segfaulty ones : #CODE
Update : when I do a core dump , the backtrace is inside ` dispatch_group_async_f ` , the Grand Central Dispatch interface .
When you're using boolean indices , you're telling which rows / columns to include and which ones not to : #CODE
I am converting from a scipy sparse matrix to a dense matrix and adding that to an ndarray using a += operator and I am getting a broadcast error .
I also tried using reshape ( M , ) after the conversion and the shape becomes ( M , M ) .
Could you post a short , self-contained example we could try out that gives the broadcast error ?
@USER In the example above , I get the following error : Non-broadcastable operand with shape ( 100 ) doesn't match the broadcast shape ( 100,100 )
So you may want to transpose your matrix using transpose() .
Filter an array in Python3 / Numpy and return indices
Is there any built-in function in Python3 / Numpy which filters an array and returns indices of the elements which are left ?
The filter I have is setting both min and max thresholds - all values below / above min / max have to be filtered out .
I've seen Python's function filter , but I don't see a way to extract indices using it .
You can get the indices of the elements in the one-dimensional array ` a ` that are greater than ` min_value ` and les than ` max_value ` with #CODE
Usually you don't need those indices , though , but you can work more efficiently with the mask #CODE
The command ` numpy.where ` will return the indices of an array after you've applied a mask over them .
I'll agree with @USER , usually you don't need the indices .
For example the sum of a list of numbers could be expressed as ` reduce ( operator.add , numbers )` .
So when using cython , doing dot products and such is not a god idea ?
I most likely will flatten the object down the line because it is easier to get the data from a database , but this is a question that comes up a fair amount when I work with Numpy , so I thought I would ask it .
The function has the following docstring : ` imshow ( winname , mat ) - None ` .
Yes - the difficulty is getting a ' mat ' when all the cv2 functions now create numpy arrays
The whole try catch construct kind of defeats the purpose of me using iterators : Which would be clean code , instead of loads of loops and indices .
If I only use the list , the code sooner or later becomes a horrible mess of indices like list [ site_index ] [ first_physicalIndex ] [ secondPhysicalIndex ] [ rowIdx , colIdx ] ...
I therefore had to transpose your matrix ( by using ` .T `) .
By the way , the log polar transform can now be implemented on top of scikit-image in about 5 lines of code , using `` skimage.transform.warp `` .
min , max and mean over large NumPy arrays in Python
From this array I want to get the min , max and average which can be easily done with ` np.min ( a )` , ` np.max ( a )` and ` np.mean ( a )` .
However , I want also to have the min , max and average of a portion ( begin part or end part ) of this array .
So , if the values in each indices are linked ( 250 - 1 , 270 - 2 , 120 - 5 , etc ) I want to eliminate any value in the B array that has an even number in the A array .
I applied the concept to my code , but I get the error " ValueError : too many boolean indices " .
Alternatively , transpose ` a ` so that broadcasting works : #CODE
( You'd probably want to transpose the result . )
Could they not get things like ` min < X < max ` to work , or did they decide deliberately not to do that for some reason ?
Alternatively , since ` min ` and ` max ` are the same for all three dimensions , you could use ` np.minimum() ` and ` np.maximum() ` like so : #CODE
I recommend calling ` min ` and ` max ` something else so that they don't shadow the builtins .
Hi Sven , when I ` & ` them together it seems as if the ` sum() ` function is counting the number of time that there is a common value in x , y , and z . how do I get it to count the values ? for example say I set the min to 1 and the max to 2 , and then have ` [ 0 0 0 ] , [ 1 1 2 ] , and [ 2 1 2 ]` sum will give me 2 , but what I am looking for is 6 .
Are you looking for the sum of ` Sx ` , ` Sy ` and ` Sz ` from your question ?
It seems to me that what the sum function does is count the sets where X , Y and Z are in that range which in this case is 2 .
The frombuffer idea is interesting , however it does require a " reshape ( A.shape )" command since it only returns 1D arrays .
If I'm reading that right , you are profiling the performance of the random number generator in addition to the eig function .
2 . change eig ( x ) to [ V , D ] = eig ( x ) in matlab , leave python / numpy code as it is ( this might create more memory being consumed by matlab script )
The ` add ` operation does not do the same thing as ` join ` .
changing the values of the diagonal of a matrix in numpy
how can I change the values of the diagonal of a matrix in numpy ?
I checked Numpy modify ndarray diagonal , but the function there is not implemented in numpy v 1.3.0 .
lets say we have a np.array X and I want to set all values of the diagonal to 0 .
If you're using a version of numpy that doesn't have ` fill_diagonal ` ( the right way to set the diagonal to a constant ) or ` diag_indices_from ` , you can do this pretty easily with array slicing : #CODE
One nice thing about this is that you can also fill a diagonal with a list of elements , rather than a constant value ( like ` diagflat ` , but for modifying an existing matrix rather than making a new one ) .
For example , this will set the diagonal of your matrix to 0 , 1 , 2 , ...
To make it clear for future readers , ` mat [: n , : n ] = 0 ` sets _the whole array / matrix_ to 0 , not just the diagonal elements .
If you want a one-dimensional view of the array's main diagonal use : #CODE
Or in general , for the i'th diagonal where the main diagonal is 0 , the subdiagonals are negative and the superdiagonals are positive , use : #CODE
These are views and not copies , so they will run faster for extracting a diagonal , but any changes made to the new array object will apply to the original array .
On my machine these run faster than the fill_diagonal function when setting the main diagonal to a constant , but that may not always be the case .
They can also be used to assign an array of values to a diagonal instead of just a constant .
Also , in NumPy 1.10 and later the ' diagonal ' method of arrays will return a view instead of a copy , so this trick to get a view will no longer be necessary .
Now if you break the problem into smaller ones then they might fit into the cache , so no time consuming coping has to be done , thus giving the program an extra ' boost ' .
Why does append overwrite the list with the element being added ?
Python / NumPy : implementing a running sum ( but not quite )
I'd like to compute the sum of all possible subsets of three adjacent elements in a .
If the sum is 0 or 1 , the three corresponding elements in b are left unchanged ; only if the sum exceeds 1 are the three corresponding elements in b set to 1 , so that after the computation b becomes #CODE
We want to basically make a new array in which the last index contains the sub-arrays that we want to sum ( i.e. the three elements that you want to sum ) .
This way , we can easily sum in the end with the last command .
The last element of this new shape therefore has to be ` 3 ` , and the first element will be the length of the old ` a ` minus 2 ( because we can only sum up to the ` -2 ` nd element ) .
It looks like you want to say ` X [: , : , X.sum ( 2 ) > 12 ] -= 1 ` , because the indices you want to mess with are only those that have a third index along which the sum exceeds 12 , and the first and second indices could be anything .
This has the problem that it doesn't generalize to an ` MxN ` tiling where the tile dimensions should still be 2x2 but there the larger pattern of tiles is more than 2x2 .
If it " works " by changing the numbers in your calls to reshape and transpose , then my comment above still applies -- I think your approach is only valuable if you demonstrate how to feed tile parameters to the reshape and transpose code and get this to work on a generically shaped tiled array where the tiles are also generically shaped .
encode the values of z by using different colours
import the png as a numpy array and decode the colours .
I have a data structure which serves as a wrapper to a 2D numpy array in order to use labeled indices and perform statements such as #CODE
resize using fractional step in numpy
yea its a pain working with arrays tho . numpy makes it a million times easier for arrays and I can use the built in min max average and sort for some of my methods
However , for floating point steps , the rounding errors are accumulate , and occasionally the last value will actually include the endpoint .
If you have only the [ ... ] part without the outer tuple , you will get a list of tuples .
@USER - FWIW , ` numpy.correlate ` ( deliberately ) doesn't use fft , as it explains in the docstring for the function , but ` scipy.signal.fft.fftconvolve ` does , if you want a one-linear to do it .
How can I down-sample this dataframe to a resolution of one hour , computing the hourly mean for the temperature and the hourly sum for radiation ?
So in essence I am splitting on the hour value and then calculating the mean of ` tamb ` and the sum of ` radiation ` and returning back the ` DataFrame ` ( similar approach to R's ` ddply `) .
Thanks , but what I want to have would be something like ` frame.convert ( ' 1h ' , how={ ' radiation ' : ' sum , ' tamb ' : ' mean ' } )` .
Put the 2 teams on the same floor .
I assume that the Python part is a stack of back-end libraries / modules of scientific functions while the Ruby-on-Rail part is mostly as front-end .
How can I sum an interval of elements on an array in python / numpy ?
How can I sum an interval of elements on an array in python / numpy ?
But sum function from numpy doesn't suport " 1:3 "
prints ` 16 ` , the sum of the middle three elements .
@USER : Then something about the order of the indices in Matlab and NumPy is different .
I added a simpler example to make clear how to sum over an array slice .
Note that in Python , indices are zero-based and end indices are non-inclusive .
Edit : More specifically , I'm looking for an practical way to plot the shape of the zeros of a scalar function with 2 variables .
e.g. f ( x , y ) = sqrt ( x^2 + y^2 ) - 4 should give me a circle .
def a ( x ): return [ sin ( x [ 0 ]) + cos ( x [ 1 ]) , 0 ]
but it only outputs 1 solution ( array ([ - 1.37079633 , 0.2 ]) instead of all the possible zeros ) .
For any smooth fn , the zeros will form lines / loop ( infinite ) , and my intention is to numerically find and sample what the lines / loops are .
You have to make some sort of mesh of the space you're interested in , try lots of initial guesses in that mesh and save the coordinates of the zeros .
But in general , nothing can find multiple zeros for you .
I'm guessing that under the hood ` fsolve ` uses non-linear methods like semi-Newton methods , and to get multiple zeros you literally have to use multiple guesses .
And even then , there's never a guarantee it can converge to all the relevant ones for your purposes .
If you want to diagnose the behaviour of your 2D function and its zeros , you are much better of generating a 2D grid of values , and plotting with something like matplotlib's ` pcolor ` .
Then if you really need to precisely find where the zeros are , you know where to start ` fsolve ` looking .
In principal it might be possible to automate this procedure , if you know something about your function , e.g. how many zeros there are for each value of y , then you will know how many times you need to apply fsolve around each minimum .
But there is no general solution to finding all the zeros of an arbitrary nonlinear function , particularly not for multiple dimensions .
Beware , if you do a lot of append / remove operations , numpy might be rather ineffective in terms of speed .
How to flatten axes of a multidimensional array without making copies in NumPy ?
I am wondering if there is a way to flatten a multidimensional array ( i.e. , of type ` ndarray `) along given axes without making copies in NumPy .
For example , I have an array of 2D images and I wish to flatten each to a vector .
Or ` reshape ` it : #CODE
` reshape ` did the trick for me .
` -1 ` tells reshape to work out the remaining dimensions .
want to flatten the first axis of images .
The only numbers that are showing up as ` 0.0050 ` are the ones that are ` e-03 ` .
( Naturally , I can not just append zeros as this would mess up the mean )
OP , I know you were looking for a non-iterative built-in solution , but the following really only takes 3 lines ( 2 if you combine ` transpose ` and ` means ` but then it just gets messy ): #CODE
FYI , I was on CodeGolf and found another nifty way to get the transpose of an array .
You could probably substitute it for the one I have up there : ` transpose = zip ( *arrays )`
If you don't need to join or relate tables , you just need fast simple queries , and you want the data in memory as a numpy array , PyTables is an excellent option .
I am probably going to be reading in subsets of the temporary data , for instance if a user selects more than one geography and runs the fit on the multiple geographies and then selects a couple other different geographies minus one of the first ones .
` argsort() ` returns the permutation of indices needed to sort an array , so if you want to sort by eigenvalue magnitude ( the standard sort for NumPy arrays seems to be smallest-to-largest ) , you can do : #CODE
On my 32-bit Windows Vista machine I notice a significant ( 5x ) slowdown when taking the absolute values of a fairly large ` numpy.complex64 ` array when compared to a ` numpy.complex128 ` array .
If any one can help out or point to some documentation that will help with finding the off diagonal elements it would be appreciated .
What do you mean by " 25 smaller squares " and " off diagonal elements " ?
Also what are " off diagonal " elements and " x ( 0 ) and y ( 0 ) coordinates " in the context of the parent square ?
Try to explain it in terms of operations on elements of the array : you want to start with four floats ( exactly which ones ? ) , do something with them , and get a single other thing .
Do you want the ` diagonal ` method ?
I have the diagonal elements what I need are the off diagonal ones
Here ` x ` and ` y ` contain what I think you are referring to as the " diagonal entries " , ie . each unique x coordinate and y coordinate , calculated assuming that the " upper left " of the square is at ` ( 0 , 0 )` .
The complete array of x and y coordinates , which I think might be the union of the " off diagonal entries " with the " diagonal entries " you mention , can then be conveniently calculated using the Kronecker product .
File " C :\ Python32\lib\ site-packages \numpy\core\ numeric.py " , line 1511 , in indices
It does indeed reshape the data into correct number of channels .
( e.g. ` from math import cos ; y = [ cos ( item ) for item in x ]`) I'm confused ...
You could also try cython , which allows you to translate a dialect of Python to C , and compile the C to a Python C extension module ; this allows one to continue using CPython for most of your code , while still getting a bit of a speedup .
Any really good implementation of DBSCAN ( it is spelled all uppercase , btw , as it is an abbreviation , not a scan ) however should have support for index structures and then run in ` O ( n log n )` runtime .
possible duplicate of [ python list of lists transpose without zip ( *m ) thing ] ( #URL )
` unique ` should be much faster than Counter for numpy arrays .
I am trying to all rows that only contain zeros from a NumPy array .
whether there are only zeros in the row
If you want to delete any row that only contains zeros , the fastest way I can think of is : #CODE
Here is a generalization of the answer , if you ever need to remove a rows that have a specific value ( instead of removing only rows that only contain zeros ): #CODE
That takes linear time per append .
This approach copies the array every append , which is O ( sum ( range ( n ))) .
That takes amortized O ( 1 ) time per append + O ( n ) for the conversion to array , for a total of O ( n ) .
You can switch to absolute imports by default on Python versions newer than 2.7 : #CODE
That's cumulative sum not ... nevermind .
I can weight them how I want to as long as sum of their weights adds to 1 .
Your constraint ( lambda term ) would be the 1 - sum ( weights ) .
@USER Cornett , portfolio variance is variance of ( sum of returns of all the assets weighted by their weights ) .
I was confused because variance is usually the sum of squared errors ( or variation ) of a collection of data .
I was reading the documentation and I'm pretty sure ` numpy.var ` calculates ` sum (( weight*return ) **2 ) /( n - 1 )` while what you want is " portfolio variance " , which is actually ` sum (( weight*return ) **2 ) + 2*weight1*return1*weight2*return2*covariance ( 1 , 2 ) ...
How to find max values from multiple lists in Python
I have multiple lists ( or numpy arrays ) of the same size and I want to return an array of the same size with the max value at each point .
I don't think it's necessary to unpack ` elem ` in ` max ` .
Indeed , it is not - max and min works with either multile arguments or a sequence -
dstack will turn your 2D arrays into a 3D stack of 2D arrays
The 2 inside of the parenthesis of ` max ` performs the maximum operation along the 3rd axis which is the new axis which ` dstack ` has created for us
Concise way to get NumPy to return array of proper shape after logical indexing , without doing a reshape ?
I could easily just reshape the output , but if I need to use logical indexing often in my code , I don't want to have to always account for extra dimensions ( i.e. dimensions that are obviously not relevant to the logical slice I am doing ) that may have changed .
Because array.nonzero() returns two sets of indices for a 2darray , you are in fact indexing on both the first and the second dimension : #CODE
In fact , I don't actually care that it has rank ` r ` , just that it's close to a matrix of rank ` r ` ( measured by the Frobenius norm ) .
( e.g. , you'd have to do arithmetic mod 5 on the elements ` {0 , 1 , 2 , 3 , 4} ` to get a valid group of matrices over that set . You can't use real-valued arithmetic and expect your random draws , linear combinations , etc ., to be closed . )
I'm trying a simple implementation scoring with ` max ( old_rank - new_rank + 1e-4 , 0 )` , but the objective function is too flat and e.g. 10x10 binary matrices always seem to get stuck around rank 6 or 7 for tens / hundreds of thousands of iterations ( evaluating 20 options per iteration ) .
Probably you want something like ` exp ( - 1.0 *abs ( current_rank - desired_rank ) )` but it might require some testing .
So , the size of A B is ` m ` -by- ` n ` and rank ( A B ) is min ( m , rank ( B )) .
If you want to search for a certain rank on B randomly , you need to start off with a valid B with max rank , and rotate a random column j of a random B i by a random amount .
The rank cannot be less than ` k ` ( assuming ` n ` > ` k `) , because ` B_0 ` columns have exactly 1 nonzero element .
The first row of the last submatrix is the sum of all rows of the first submatrix , and the remaining rows of it are all zeros .
As the python documentation for ` cos ` and ` sin ` point out , the arguments should be in radians , not degrees .
you can then rotate this vector around origin using matrix multiplication method ' dot ' : #CODE
SymPy might be able to provide you with the derivative symbolically , find its zeros , and so on .
That's O ( n log n ) though compared to O ( n log k ) ( with k being the number items to return ) for the obvious implementation ( no idea how to get that in numpy though ) .
Yeah I just wanted to point out that theoretically this is less efficient by about a factor of log n than the optimal solution , which I think should be mentioned there .
where I is the scaled input value , Imin and Imax are the desired min and max range of the scaled values , D is the original data value , and Dmin and Dmax are the min and max range of the original data values .
One thing is that amin and amax should just be min and max .
What's the difference between amin , amax and min , max ?
And I don't like the potential confusion of ` min ` , ` argmin ` and ` amin ` .
I have an NxN matrix filled with zeros .
Now I want to add to the matrix , say , n ones and m twos to random places .
In Matlab I would do this by making a random permutation of the matrix indices with randperm() and then filling the n first indices given by randperm of the matrix with ones and m next with twos .
To generate the indices of the elements for where to add ones and twos , what about this ?
Corrected as commented by Petr Viktorin in order not to have overlapping indexes in ` ones ` and ` twos ` .
An alternate way to generate the indices : #CODE
Then `` ones `` and `` twos `` can overlap .
Better to do `` random_indices = random.sample ( indices , n + m ); ones = random_indices [: n ]; twos = random_indices [ n :] `` .
Here we create a random permutation , then set the first 10 indices taken from the permutation in ` a ` to 1 , then the next 10 indices to 2 .
Usually constraining the coefficients involves some kind of regularization parameter ( C or alpha ) --- some of the models ( the ones ending in CV ) can use cross validation to automatically set these parameters .
` abs() ` will truncate the result , while ` fabs() ` will work for floating-point arithmetic .
e.g. the equivalent of ` solve ( x^2 - 2 == 0 )` will give the float ` 1.414213 ` in scipy , and an object representing ` sqrt ( 2 )` exactly in sympy .
F ( z ) = sum ( ` e [ n ]` ^2 , n=1 ... 13 )
Just create an array of zeros and set the area you want to one .
The outer one is NSIS , while the inner is python's distutils installer .
` [ max ( ydata ) , -1 , - 0.5 ]` .
The standard way to use linear least squares to obtain an exponential fit is to do what fraxel suggests in his / her answer : fit a straight line to log ( y_i ) .
Python , numpy : 3 dimensions broadcast
a+b is a ( 2 , 1 ) vs ( 1 , 3 ) sum so it is supposed to be broadcastable ( 2vs1 in dim 1 , 1vs3 in dim 2 , broadcast rule is fulfiled ) .
a+c is a ( 2 , 1 ) vs ( 1 , 1 , 10 ) sum so it is supposed to be broadcastable ( 2vs1 in dim 1 , 1vs1 in dim 2 and 1vs10 in dim 3 , broadcast rule is fulfiled ) .
b+c is a ( 1 , 3 ) vs ( 1 , 1 , 10 ) sum so it is supposed to be broadcastable ( 1vs1 in dim 1 , 3vs1 in dim 2 , 1vs10 in dim 3 .
a+c is a ( 2 , 1 ) vs ( 1 , 1 , 10 ) sum so it is supposed to be broadcastable
( 2vs1 in dim 1 , 1vs1 in dim 2 and 1vs10 in dim 3 , broadcast rule is
I want to calculate the dot product between A and B , which is #CODE
The Fastest norm for a vector would be #CODE
is there a way to get the norm also ( faster ) :)
@USER Isn't the second timeit cheating a bit since you are calling the python sum rather than the numpy one ( and the abs ) ?
I'm guessing that you want the sqrt of the sum of the squares , which is the default for ` norm ` .
This metric is called the Frobenius norm or L2 norm .
If you want a different metric , say the Manhattan or L1 norm , it is simply a parameter to pass in .
See the docs for more more info , but if you're using an older version , I think you have to reshape , or do all ( 1 ) .all ( 1 ) .
Efficiently sum a small numpy array , broadcast across a ginormous numpy array ?
I want to calculate an indexed weight sum across a large ( 1,000,000 x
sum of the weights array entries corresponding to that row's True
This uses ` sum() ` to immediately sum the ` row * weights ` values , so you don't need the memory to store all the intermediate values .
The dot product ( or inner product ) is what you want .
It allows you to take a matrix of size ` m n ` and a vector of length ` n ` and multiply them together yielding a vector of length ` m ` , where each entry is the weighted sum of a row of the matrix with the entries of the vector of as weights .
This is awesome , I didn't understand the dot function at all from my wandering through the docs .
Now we can just pass it to sum : #CODE
You probably want to take the ` ifft ` of the ` fft ` ( post-filtering ) , not of the input waveform .
You are using a real fft , which throws away information , and thus makes the fft non-invertible .
you can fft multi-channel data , you just need to use a 2d array and make sure the axis keyword is set correctly ( -1 by default ) , and ` irfft ( rfft ( n ))` should return n ( within machine precision ) .
you need to sort the eigenvalues by it's absolute value , the following code will give the same result : #CODE
This allows you get largest or smallest eigenvalues sorted by absolute value or algebraic value
While sorting by absolute values ( as suggested by @USER ) gives the same result for both tests , it still is unsatisfying since the my underlying intent was to get the largest eigenvalues from my matrix .
I have checked that I could use the StarCluster package to deploy my stack after setting up AMIs that are derived from StarCluster AMIs .
The versions in Ubuntu 12.10 and Debian 7.0 meet the current Scipy stack specification .
Memory leak resize an image numpy array on linux with OpenCV in Python
Do you really need to copy OpenCV array's in order to resize them ?
I would an easy way ( preferably one line ) to reshape a list #CODE
Using dtype while declaring the numpy matrix of zeros doesnt help .
Adding this to ` z ` gives an array without zero elements that is equal to ` z ` for the indices that ` np.select ` will use .
numpy.polyfit has no keyword ' cov '
TypeError : polyfit() got an unexpected keyword argument ' cov '
And sure enough help ( polyfit ) shows no keyword argument ' cov ' .
Here's the reference to ` polyfit ` in 1.6 : #URL Note that there is no ` cov ` parameter .
You could add some simple code at the beginning of ` ols_linreg ` to reshape 1-D arrays into 2-D arrays if needed .
I would like to implement a diagonal matrix apply function that is created by providing the diagonal ` d ` first , and then doing a bunch of matrix-vector multiplications with ` x ` .
Now , some users are going to provide a diagonal ` d ` of shape ` ( k , )` , some of shape ` ( k , 1 )` .
Now that I understand your question , my suggestion would be simply to reshape .
Calling ` reshape ` returns a view , so it doesn't incur any big copying costs or anything like that .
Simply reshape the arrays , multiply , and reshape again : #CODE
This will work on arrays of shape ` ( 5 , )` too , because the transpose operation on a 1-d array causes no change .
statistics for histogram of periodic data
For a series of angle values in ( -pi , pi ) range , I make a histogram .
Now , let's have a histogram : #CODE
One obvious solution to all my problems ( at least those described above ) is to convert histogram data to a data series and then use it in calculations .
The second column gives completely wrong results ( non-periodic mean / std from numpy obviously does not work here ) .
The 3rd column gives sth I wanted to obtain from the histogram data ( @USER : my raw data won't fit memory of my computer .., @USER : thanks for your input : of course , bin size will influence result but in my application I don't have much choice , i.e. I have to reduce data somehow ) .
If I understood correctly , you want to calculate data's mean , mode , std , etc from the histogram data ?
If so , it doesn't seem possible to me , because you loose a lot of information by taking the histogram of the data .
The author , however , mentioned non-periodic data and seem to be perfectly fine with his ` sum ( counts * bins [: -1 ])` , so I assumed the question is more about estimating moments from the histogram .
@USER : What I messed up in my original question was the way the mean for non-periodic data was calculated ( my original histogram is normalized and this is why I neglected dividing by the sum of counts ) .
Or do you only have the histogram counts , and not the " raw " data ?
If so , you could fit a Von Mises distribution to your histogram counts and approximate the mean and stddev in that way .
@USER : you are right , the normal distribution is not a general solution , in my case I fitted p [ 0 ] *sin ( a ) exp ( - 0.5 ( a / p [ 0 ]) **2 ) with a good result .
Thus fitting * a function * to the histogram data might be a solution in some cases .
Now ` from sympy import Symbol ` produces a similar stack trace .
Does numpy or scipy contain a function which is an inverse of the n-dimensional " gradient " fn ?
A real image , which is absolutely not a scalar field that produces a conservative gradient ( just think of edge discontinuities ) probably doesn't support this inversion process .
I don't understand why a real image is not a scalar field -- couldn't you take the scalar field to be the image surrounded by an infinite grid of zeros ?
The gradient of this should have zero curl .
It's just not a continuous scalar field , hence its gradient is not conservative .
hmm ... in my application i am starting with an image , taking the gradient , modifying the gradient slightly , and then i want to get back a modified image .
Inside of each of the circles , the gradient is zero , as it is on the background .
The relative intensity of the difference circles would just be destroyed by the gradient filter .
The gradient is not a transform like Fourier or Laplace ...
so if you mess with the computed gradient , you've destroyed the correspondence to the original image in a whole region around those edited spots .
The next try would be to detect them using your gradient checking , but then to modify the image .
One way to understand this is via the convolution theorem , and to think of the gradient as a particular kernel for a convolution , in this case something like ( -1 , 0 , 1 ) in 1D .
Specifically for the gradient , there is 0 power in the f=0 band , and this is what people are referring to in the comments , but other information is lost as well .
G*H should return a matrix of all zeros .
( On a side note , I used ` array `' s in my answer , but you could just as easily use a ` matrix ` , as well . However , if you want ` G * H ` to be a 3x6 array of zeros , then you want an array , not a matrix . )
I'm trying to compute a vector , whose sum is 1 and whose elements are defined as such : #CODE
I'm using numpy arrays as containers and exp is numpy.exp .
Because of it , I cannot perform some operations on the result , e.g. if I try to find the max value for each line using result.max ( 0 ) , I'll receive an error : TypeError : cannot perform reduce with flexible type .
How to create a diagonal sparse matrix in SciPy
I am trying to create a sparse matrix which has a 2D pattern run down the diagonal .
Indeed , I do need values other than ones and twos .
For a single output neural net , you want to create a threshold for the output values to translate them to yes or no responses that get the best degree of specificity / sensitivity for your task
I've provided the above to show it's fairly simple to roll your own
Your neural network is going to have an output that you will have to translate in to a classification ( likely yes / no ) .
From this threshold , you translate the output of your neural net into classifications .
( remember that the diagonal just means that your classifier is random and that you're probably doing something wrong )
Now to get the class / probability needed to plot your ROC curve from your neural network , you just need to look at the activation of your neural network : ` activateOnDataset ` in pybrain will give you the probability for both classes ( in my example above we just take the max of probabilities to determine which class to consider ) .
For instance , sums along rows and columns , vector products , max , min , slicing etc ?
Numpy Array : Efficiently find matching indices
The issue being that tensordot takes _pairs_ of axes to sum over .
I am trying to multiply a vector ( 3 by 1 ) by its transpose ( 1 by 3 ) .
By definition , by multiplying a 1D vector by its transpose , you've created a singular matrix .
append values to a single list or array from a loop in python
You can clip only the bottom half with ` clip ( 0 )` .
You can clip only the top half with ` clip ( max=n )` .
I prefer ` clip ` for simple operations because it's self-documenting , but his answer is preferable for more complex operations .
@USER - I just tried it and clip will take one .
First is assumed min .
Did you try the clip _method_ of ` a ` ?
had gone down this route but thought there must be an easier , more matlab less python way to do it with numpy ( as i was using arrays rather than lists anyway ) . clip is perfect
If you want to find the indices of the ` n ` largest elements using ` bottleneck ` you could use
I'm going to roll back the edit .
returns indices not values .
The fix is to use the indices to find the values : #CODE
` numpy 1.8 ` implements ` partition ` and ` argpartition ` that perform partial sort ( in O ( n ) time as opposed to full sort that is O ( n ) * log ( n )) .
My system is best described by a diagonal sparse matrix ( Poisson ) .
I have my diagonal sparse matrix , however , I want to change the boundary conditions ( ie the " edges " of my matrix ) to zero .
It must be a common situation where a modeler wants to describe a system in a sparse diagonal matrix with distinct boundary conditions , is there a best practice for doing this ?
PS : FYI , your matrix is called tridiagonal , not diagonal .
In total there are 140 TV shows and approximately 530000 unique users .
unique_user : All the unique users
It assumes your users are in stored order , but they can be numeric or string ids : #CODE
I guess the question becomes is the python for-loop overhead more than log ( 530,000 ) .
Question - the data / np.max ( np.abs ( data )) - am I right that this is normalising to 1 / -1 before scaling , such that if the max is 0.8 , it would be scaled up ?
You could take advantage of a NumPy array's ability to sum element-wise : #CODE
np.r_ is one way to concatenate sequences together to form a new numpy array .
But instead complicating whole system for nothing , I'll go with ATLAS on Ubuntu 12.04 numpy version , and keep an eye in future not to follow blindly everything I read
By the way , to get faster responses to MKL related questions please join the Intel MKL forum : #URL
I have tested dot products of two 4096x4096 matrix .
I have also found that the numpy dot is the best even for complex matrix multiplications .
I have tested for cython using blas , blas in python , einsum in python but dot is the best .
Use the ` reshape ` function .
` ravel ` or ` reshape ` are the best options , in this case .
and I want to add a third column where each row is the sum ( or some arbitrary calculation ) of the first two columns in that row : #CODE
You can then reshape the result as a column matrix and finally append to the original array #CODE
k_x , max = 1 /( 2*x_max / dim_x )
k_x , min = 1 /( 2*x_max )
and let the grid in Fourier-space run from k_x , min to k_x , max ( same for the y-direction )
The k-space values will range from ` -N / 2*omega_0 ` to ` ( N-1 ) / 2*omega_0 ` , where ` omega_0 ` is the inverse of the sample length , given by ` 2*pi /( max ( x ) -min ( x ))` and N is the number of samples .
One easy way to get this is to just read the data in as a flat collection of z values , skipping the x , y values , ` reshape ` to set the shape you'd like .
1 ) Direct reshape :
Then the best is to make a 1D numpy array from your list of values and reshape it : #CODE
Sum the values in each column and row , then find where the max value is based on those numbers ... but is there a quick and efficient way to do this ?
Its a log graph , so it really illustrates the advantage of combining the two approaches .
I want to obtain the unique pairs of arr1 and arr2 , e.g. #CODE
The same holds for the values in data corresponding to the indices 6 and 8 .
To get the unique keys , just iterate over the keys and split the tuples
( This will probably be slower if there are only a few zeros , though . )
Python list comprehension list - a good way to pick up a max
How about ` assert min ( X.shape ) == 1 ; ndim = max ( X.shape )`
line , = plot ( x , sin ( x )) what does comma stand for ?
Just outsped me , +1 , but I disagree that a more readable way of doing this is using list-like syntax , I would use this instead : ` line = plot ( x , sin ( x )) [ 0 ]` .
It does stop the error from being thrown , but another function that sums the data in the arrays returns zeros on the python 2.x machine and gives this error on the 3.2 machine #CODE
For example if max ( a-b ) > 22 ?
I have also tried to simply concatenate the arrays , but I don't think this is the most efficient way to accomplish this problem .
It doesn't look good but it's better than a reshape or huge matrix changes
This is the same as saying ` y [ 1:3 , [ 0 , 2 : -1 ]]` without having to reshape the array or iterate through excess elements , you specify the indexes you care about by making a list of ` [ 0 ] + ` the remaining columns in that dimension .
The following illustration shows the original cloudpoint at the left ( what is displayed as horizontal lines is actually a dense line-shaped cloud of points ) , the result that ` griddata ` gives me in the middle , and the result I would like to get at the right -- kind of the " shadow " of the cloudpoint on the x , y plane , where non-existing points in the original surface would be zeros or Nans .
I'm trying to compute the sum of each column , then weight the members of that column by that sum .
Scipy sparse matrices have their own ` sum ` method you can use for this .
The returned sum is a dense ` numpy.matrix ` which you can convert into scaling factors .
When you run this script , the ` H ( x , y , gamma )` , ` HenonIterate ( x0 , y0 , n , gamma )` and ` g() ` get defined .
You can create a graph in Networkx by importing a data file ( Networkx has quite a modules to translate among formats ) or by using one of Networkx ' gtraph generators .
To generate the graph shown below , i create a particular type of binomial random graph , erdos-renyi .
If you put a column of ones next to your data , you do get something , but I'm not very familiar with cosine distance so I don't know what to expect .
return a / np.tile ( norm , a.shape [ 1 ])
You could ` transpose ` the array before writing it out .
Or transpose the array after reading it in .
The results using one of the partial derivatives at a time , say ` dx ` are great , clearly representing the gradient as a " shading " effect , as seen in this image from a human back ( code first ): #CODE
With splines , if you go to too high orders , you should start getting zeros or discontinuities .
In the above code , we sample a new ( single ) z with the multinomial scipy function .
Sampling from a joint distribution over topics and sentiment labels just means that the entire T x S matrix should sum to 1 .
` 65283 ` is the max value , corresponding to ` bin ( 65283 )= 0b1111111100000011 ` as the direct python output , or ` 00001111111111 ` if it were little-endian .
What I've found is that the ` bin() ` function outputs in big-endian format ( in the case of this numpy array ) , but with padded zeros omitted .
For example , 65283 has all 16-bits represented , but ` bin ( 652 8) =0b1100110000000 ` ( missing 3 zeros in the middle ) .
To fix this ( and return a 10-bit little-endian number ) , I can get the binary form of 6528 , add 3 zeros in the middle , and do a byte-swap .
But 1 ) this is super tedious ( done per element ) and 2 ) the zero-padding is inconsistent ( e.g. 652 requires 6 more zeros ) .
Interpolation on a log scale
I want to interpolate some data , and to plot the result on a log scale ( ` pyplot.loglog `) .
The problem is that the resulting interpolation looks very strange and shows discontinuities when plotted on a log scale .
What is the best way to interpolate log scaled data ?
Actually , I succeed doing it by interpolating the log of the data , but I was wondering if there were a simpler way of achieving the same result ?
Do you mean unique elements ?
Return the sorted , unique values in ar1 that are not in ar2 .
@USER better to check ` if diff ` instead
If you for some reason don't want the arrays inside rec_history you could change the append line to : #CODE
I would like to replace the zeros with a generic 2x2 matrix and then plot everything with ` imshow() ` .
imshow visually represents a 2D array as a cloud of points positioned on the canvas according to their x , y indices .
What is the simplest way to compare two numpy arrays for equality ( where equality is defined as : A = B iff for all indices i : ` A [ i ] == B [ i ]`) ?
Replace nonzero sparse matrix elements with ones .
structure as S , but with ones in the nonzero positions .
How can i find the indices of elements in a Numpy array which satisfy multiple criteria ?
Example : The function ` numpy.nonzero ` lets me find the indices according to some criterion : #CODE
Often this is not what you want -- i.e. , you still want the sum of that column with the NA treated as if it were 0 :
In sum , to speed up your code , eliminate the loop and use the new mask : #CODE
I apologise for such a seemingly trivial question , but my understanding of the difference between ` log ` and ` ln ` is that ` ln ` is logspace e ?
Correct , ` np.log ( x )` is the Natural Log ( base ` e ` log ) of ` x ` .
For other bases , remember this law of logs : ` log-b ( x ) = log-k ( x ) / log-k ( b )` where ` log-b ` is the log in some arbitrary base ` b ` , and ` log-k ` is the log in base ` k ` , e.g .
` np.log ` is ` ln ` , whereas ` np.log10 ` is your standard base 10 log .
Trying to find the std of an array result in fault
I then convert the list to array and apply std to it .
You can compute std for each of the column vectors individually .
Now you can get the std of e.g. the column local like this : #CODE
Is there a simple way to add zeros at the ends ?
i think that adding zeros will result in a wrong computation of the std
I don't really care about the tail so the std doesn't really matter
I have added a loop that add zeros at the end of each list till I get the same length .
Schuh , noted that adding zero at the end might result in getting wrong std .
@USER I thought about that , but I also access the data by rows so it would move the problem from the middle of my script to the beginning .... however , as there are so many more columns that rows , maybe it would speed things up a little to transpose the data .
Am I right , that for a given similarity matrix A sum of the form A + ( 1 / 2 ! ) *A^2+ ( 1 / 3 ! ) *A^3 + ... which is equal to exp ( A ) will give me subgraphs ?
In a sense that if i-author and j-author are in the same subgraph then exp ( A ) ( i , j ) will be nonzero ?
@USER : no no .. actually I want to later to perform svd of this matrix ..
Basically I'm attempting to append a value from each key in ` genres ` to a list .
I catch the error , and append a 0 to the list instead .
My current function looks like ( computes the minor of mat wrt . to row ii , col jj ): #CODE
If you know the size of ` mat ` from the start , you may be able to hard code the determinant and building ` rM ` .
It looks like you're copying the memory from ` ( mat [ rows ]) [: , col ]` , allocation and copying is a slow process .
Is it not possible to simply make the function call ` np.linalg.deg ` on the chunks of ` mat ` in place , instead of copying it and calculating the determinant on the copy ?
I'm not really sure what your code is doing , but if you have ` hist ` and ` bin_edges ` arrays returned by ` numpy.histogram ` you can use ` numpy.cumsum ` to generate a cumulative sum of the histogram contents .
Using a histogram is one solution but it involves bining the data .
If you have arrays with 200 rows and 300 colums , and you stack 20 of them , you get a three-dimensional array , with shape = ( 200 , 300 , 20 ) .
First I used clip #CODE
To pack it into an array , you could create an array and reshape it , remembering that the value triplet is its own dimension ( hence the extra ` 3 ` at the end ) .
faster way to append value
There is no ` append ` method .
However if you can restructure your code to use iterators , you can avoid ever having to generate a full list and thus avoid using ` append ` at all .
This avoids the need to append values to an existing array , but gives you a copy of the selected values as an array .
I hit the same issue , and interestingly enough also trying to concatenate a multidimensional array from a file .
You can derive the Dirichlet distribution from the gamma distribution .
This question has now been cross-posted to ( and answered at ) Computational Science stack exchange site as Which package should I use to wrap Modern Fortran Code with Python ?
using the column indices
How to efficiently remove columns from a sparse matrix that only contain zeros ?
What is the best way to efficiently remove columns from a sparse matrix that only contain zeros .
What you want is the ` nonzero ` method from numpy .
If you want only the full columns where there are non-zero entries , then just take the 1st from indices .
Except you need to account for the repeated indices ( if there are more than one entries in a column ): #CODE
I used the following to extract just the columns with nonzero entries : ` matrix [: , np.unique ( alldata.nonzero() [ 1 ])]`
Pretty much the same , except you use numpy's unique instead of ` sorted ( set ( columns ))` .
I was wondering if I could use a vectorial operation to get an array with the norm of each of my vector .
I tried with ` norm ( A )` but it didn't work .
FYI : In the next release of numpy ( 1.8.0 ) , you will be able to use the ` axis ` argument : ` norm ( A , axis=1 )` .
Initially I thought you couldn't find the ` linalg.norm ` method as you said norm ( A ) " didn't work " , but after reading Eric's answer I guess it's more likely that you did try ` linalg.norm ` but it didn't give you what you wanted .
` map ( norm , A )` might be faster in this situation .
Update : Eric's suggestion of using ` math.sqrt ` won't work -- it doesn't handle numpy arrays -- but the idea of using sqrt instead of ` ** 0.5 ` is a good one , so let's test it .
I'm trying to sum each row but according to a certain weight vector of [ 16 , 4 , 1 ] .
I'm thinking I have to do some sort of dot product followed by a sum , but I'm not 100% confident where to do the dot .
The dot product inclination is correct , and that includes the sum you need .
So , to get the sum of the products of the elements of a target array and a set of weights : #CODE
Using alternative LAPACK driver in numpy's svd method ?
For some special cases the svd won't converge and raise a Linalg.Error .
Basically , if svd ( M ) fails , try svd ( M ') , and swap the resulting U , V appropriately .
If ` x ` or ` y ` is multidimensional , you will have to flatten the arrays first .
You could then get the unique bin assignments , and then iterate over those in conjunction with ` np.where ` to split the the assigments up into groups .
It is ** very ** simple to use the indices to extract the values .
As I read somewhere here in another answer to the same topic , it's currently a good bit faster than digitize , and does the same job .
@USER , looking at the result of ` in1d ` when used on a 2-d array , I see that you must ` reshape ` it before indexing a 2-d array .
now I want the indices where x is equal to ` [ 4 , 5 ]` ( ` - [ 1 , 4 ]`) .
It's pretty low-level , and mostly focused on how to address the more difficult problem of how to pass C++ data to and from NumPy without copying , but here's how you'd do a copied std :: vector return with that : #CODE
Surely there must be a way to populate a boost :: python :: numeric :: array with data from a simple std :: vector without having to get some 3rd party library .
I can't make an edit because it's too minor , but it should be ` bn :: zeros ` , not ` bp :: zeros ` .
about ` flat ` and ` reshape ` functions : #CODE
you can use ` reshape ` function to reshape your array to its original shape ....
` are ` N-1 ` zeros .
In that case ` h = 0 ` ( and ` y ` stays all zeros ) , because it is an integer division ( ` 1 / 2 == 0 `) .
sum ( triple ) = 1
The solution that I would use for a smaller array doesn't translate because it requires shuffling an in memory array .
What I'd like to create is an iterator using some combination of itertools , numpy and / or random that yields the next random cell ( x and y indices ) .
So based on the current responses and my attempt to translate perl which I know nothing about , I'm understanding that the best I can do is the following : #CODE
You could create a random-ish permutation from ` range ( x_count * y_count )` to ` range ( x_count * y_count )` and then just evaluate that for as many integers as you need .
( for the record : Otherwise , memory isn't such a bad issue because you only need to " remember " where you were previously . Thus you can keep a list of indices that you've already visited . This is bad if you ever do plan to visit each element , because rejection sampling can take very long if implementing with a growing blacklist .
I create first the x*y array and reshape it to 2-D .
( Unless you need the indices too , in which case , randomly sampling ints and translating them into coordinates , a la hexparrot ' s solution , is a reasonable way to go . ) #CODE
@USER Do you want to group by a 24 hr day , split up into the mean value over 15 min increments ?
I also stick with Python 2.7 since the complete scientific programming stack hasn't finished its migration to Python 3 yet .
On the other hand , ` a=a+b ` creates a brand new array for the sum , and rebinds ` a ` to point to this new array ; this increases the amount of storage used for the operation .
For example , suppose ` a = ones (( 3 , 3 ))` .
I just wanted to point out this is not unique to numpy .
You could use ` np.where ` instead of ` nonzero ` which I think is more readable .
numpy : most efficient frequency counts for unique values in an array
In ` numpy ` / ` scipy ` , is there an efficient way to get frequency counts for unique values in an array ?
Also , this previous post on Efficiently counting unique elements seems pretty similar to your question , unless I'm missing something .
or however you want to combine the counts and the unique values .
To count unique non-integers - similar to Eelco Hoogendoorn's answer but considerably faster ( factor of 5 on my machine ) , I used ` weave.inline ` to combine ` numpy.unique ` with a bit of c-code ; #CODE
There's redundancy here ( ` unique ` performs a sort also ) , meaning that the code could probably be further optimized by putting the ` unique ` functionality inside the c-code loop .
If you get the error : TypeError : unique() got an unexpected keyword argument ' return_counts ' , just do : unique , counts = np.unique ( x , True )
In older numpy versions the typical idiom to get the same thing was ` unique , idx = np.unique ( x , return_inverse=True ); counts = np.bincount ( idx )` .
Example : ` arr = zeros (( 3 , 5 ) , dtype =[( ' x ' , int ) , ( ' y ' , float )])` , with structure access like ` a [ ' x ']` , which returns a 2D array of integers .
cython numpy accumulate function
` ext_modules = [ Extension ( " accumulate " , [ " accumulate.pyx "] , extra_compile_args =[ " -O3 " , ])]`
You also might compare your code to numpy when ` section_lengths ` are unequal , since it will probably require a bit more than just a simple ` sum ` .
It might be possible to compare where the indices live in memory using the ` ctypes ` property of the arrays .
Numpy histogram , how to take the maximum value in each bin
In the simplest case , you'd just flatten your array , then do the above .
Thanks it's useful but I would like to return an array the same way the histogram function does .
However this creates a list , not an array , and hence I cannot reshape it to a vector array .
I believe my main obstacle is that I can't figure out how to translate this simple operation in MATLAB : #CODE
Although , I would probably go for the answer using concatenate / hstack .
Yes it's two more characters but i think it's worth it so NumPy can have one function for each of two axes to concatenate along ( r_ & c_ ) .
I need translate that code to Python , and I have done the next : #CODE
I have found , for example , that numpy.linalg.eig returned incorrect eigenvalues for a complex matrix , whereas scipy.linalg.eig returned correct ones .
Is there a function in numpy to replace lower and upper diagonal values of a numpy array ?
To replace the main diagonal I have used ` np.fill_diagonal ` : #CODE
but I also need to replace the upper and lower diagonals that are next to the main diagonal : #CODE
Polynomial models ( beyond linear ) extrapolate terribly poorly in general .
The HTML file generated by Cython indicates that the bottleneck is the dot products ( which is expected of course ) .
Does this mean that I have to define a C function for the dot products ?
You could try writing out the code for a dot product with all of the static typing of the counter , input numpy arrays , etc , with wraparound and boundscheck set to False , import the clib version of the ` sqrt ` function and then try to leverage the parallel for loop ( ` prange `) to make use of openmp .
Just a couple of note ( 1 ) I'm not sure if your ` dot ` implementation is a good candidate for inlining .
In general though , numpy's ` dot ` is going to be highly optimized if compiled against BLAS or MKL
And I'm using the following code to find the sum of elements on each nth diagonal from -7 to 8 of the board ( and the mirrored version of it ) .
Just found the ` .trace ` method that replaces the ` diagonal ( i ) .sum() ` thing and ...
This makes summing the diagonal quickly difficult .
If you need to take the trace multiple times of your matrix , I might recommend extracting the diagonal and saving it separately to prevent cache misses while summing .
But even with them , this method ( and a dot product ` dot ( ones (8 ) , diagonals ( board ) .T )`) , I can make the ` sum ` 10 to 15% faster .
BTW , the ` reshape ` operation could be replaced by : ` numpy.zeros ((( cols - 1 ) , cols ) , ...
You can use the multinomial distribution ( from numpy ) to do what you want .
If you're having a hard time wrapping your head around the multinomial distribution , I found the documentation really helpful .
Note you can reduce your building of results to ` itertools.chain.from_iterable ([ elements [ i ]] *count , for i , count in enumerate ( indices [ 0 ]))` , which will be faster .
I can not figure out how to use those starts and stops indices to slice the initial array .
Along the main diagonal we are not concerned with what enties are in there , so I have randomized those as well .
The approach I have taken is to first generate a nxn all zero matrix and simply loop over the indices of the matrices .
Also if you want to add some number of randomly placed zeros , you can always generate a random set of indices and replace the values with zero .
However , is there some way I can get it to place zeros in places randomly ?
If you don't mind having zeros on the diagonal you could use the following snippet : #CODE
The computational complexity of that is on the order of N log N , where N is the number of data points .
Range queries and nearest neighbour searches can then be done with log N complexity .
Is there a way that's easier on the eye and makes better use of memory ?
Try the ndenumerate function of numpy , which returns the value as well as the indices : #CODE
Since you're doing multiplication among your two arrays , you can use the ` outer ` function , after using ` arange ` to get arrays of your sin / cos .
I kept them in to distinguish them from the ` math ` ones , which won't work for this approach .
@USER - For whatever it's worth , you can use ` outer ` for subtraction , division , power , etc as well .
You can get a grid of the index values with indices : #CODE
Doing the timings correctly , I got that the solution with ` np.outer ` takes approximately 0.009s , the solution with indices takes ~ 0.12s the nested loops take 1.4s and ` np.ndindex ` takes a whopping 3.28s .
Your method is probably failing because you're trying to sum a list of five numpy arrays , rather than five numbers or a single numpy array .
Extracting diagonal blocks from a numpy array
I am searching for a neat way to extract the diagonal blocks of size 2x2 that lie along the main diagonal of a ( 2N ) x ( 2N ) numpy array ( that is , there will be N such blocks ) .
This generalises numpy.diag , which returns elements along the main diagonal , that one might think of as 1x1 blocks ( though of course numpy doesn't represent them this way ) .
To continue the analogy with numpy.diag , one might also wish to extract the off-diagonal blocks : N - k of them on the kth block diagonal .
( And in passing , an extension of block_diag allowing block to be placed off the principal diagonal would certainly be useful , but that is not the scope of this question . ) In the case of the array above , this could yield : #CODE
By way of motivation , this arises from the situation where I wish to extract the diagonal elements of a covariance matrix ( that is , the variances ) , where the elements themselves are not scalar but 2x2 matrices .
use extract_blocks ( array ) to spit all diagonal blocks of any size .
I would like to reshape the following numpy array in iPython : #CODE
I thought the easiest way to do this is to reshape the array and afterwards calculating the mean of it .
Use the ` axis ` keyword on ` mean ` ; no need to ` reshape ` : #CODE
If you do want the array ` B ` out , then you need to transpose the array , not reshape it : #CODE
But I get an error , " can only concatenate tuple ( not " int ") to tuple .
How do I translate the " frame " into python ?
I know the following : The file header is 512 bytes , each frame is 49408 bytes in size with 256 of them being frame header , and the guy who wrote the matlab code set an initial array of zeros with the dimensions [ 512 , 96 ] ( it's a sonar with 96 beams ) .
Usually , matrices are split at the main diagonal into an upper and lower triangle matrix .
Does the choice of diagonal matter ?
I basically want to slice the matrix by its diagonal , and then keeping either the upper or the lower triangle ( including the diagonal ) .
The function np.diagonal() only returns the elements of the diagonal of the matrix .
The point is that your examples use the diagonal from the lower left corner to the upper right corner , which is pretty strange .
All related NumPy functions ( including ` numpy.diagonal() ` use the main diagonal , i.e. the diagonal from the upper left corner to the lower right corner .
So my question is : Do you really need to use the " wrong " diagonal ?
Normally the upper and lower triangles of a matrix are taken about the other diagonal ( top left to bottom right ) .
But I don't know any way to get the " reading by smaller diagonals " order that you are using , short of reading the matrix one diagonal at a time .
To get the diagonal elements you can get their indices with ` np.triu_indices ` ( or , for the lower triangle , ` np.tril_indices `) and then index by them .
The following code extracts the lower-triangle including the diagonal : #CODE
and this extracts the lower-triangle without the diagonal : #CODE
that I then vertically stack #CODE
I've tried initializing the array size ( zeros ) to a size I know that won't be filled by the data I segment and then tried to add the actual data values into that ( initialized array ) .
and just replace the ` 2 ` in the ` reshape ` call with the number of items you want to average over .
I defined a new array ` ones ( 645 )` , lets say ` centVector ` to produce the mean for every row in matrix ` X ` .
If you try to subtract this from ` X ` , ` r_means ` gets broadcast to a row vector , instead of a column vector : #CODE
So , you'll have to reshape the 1D array into an ` N x 1 ` column vector : #CODE
The ` -1 ` passed to ` reshape ` tells numpy to figure out this dimension based on the original array shape and the rest of the dimensions of the new array .
Q's size is N , but with indices starting at 0 , Q [ N-1 ] is really the last item .
You are adding one to it , so you access ` Q ` at indices 1 through N .
Because of this space , I cannot join each line into a single array .
As mentioned in my comment above , use strip ( ) to clean your data .
Once I find this area , how can I tell whether I need to add or subtract it from the base poly ( red vs green areas in the first diagram ) ?
If you sum up the areas for all segments , you get the area of the whole curve .
The sum is 48 8/ 720 = 61 / 90 .
You integrate along the outer curve , make an imaginary straight line that connects the outer to the inner curve , integrate along the inner curve , then pass back to the outer along the straight line that got you there .
OTOH if you really only need to test whether the result of your min call is " masked " , you can do that directly : #CODE
I tried using reshape , but you have to preserve the total number of elements ( which does not seem to be the case for the R function ) .
For a 2D numpy array ` m ` it s straightforward to do this by creating a ` max ( m.shape )` x ` max ( m.shape )` array of ones ` p ` and multiplying this by the desired padding value , before setting the slice of ` p ` corresponding to ` m ` ( i.e. ` p [ 0 : m.shape [ 0 ] , 0 : m.shape [ 1 ]]`) to be equal to ` m ` .
Some numpy functions for how to control the rounding : rint , floor , trunc , ceil . depending how u wish to round the floats , up , down , or to the nearest int .
I want to plot a histogram of this sequence in the list .
When I use standard hist funcion of matplotlib library , I get only two bars.It counts all zeros and all ones and shows me the histogram with two bars .
yes like histogram
The behavior you describe is the way a histogram works ; it shows you the distribution of values .
How would I translate the following into Python from Matlab ?
would translate in " English " to : all rows of outframe , but only every 4th column starting from 4 to nout-1 ( i.e.4 , 8 .. ) .
It will for the most part replace matlab matrices when you translate code .
Actually , I haven't taken a look at the internals , so it's possible that it doesn't actually create a new instance of ` dtype ` for ones that ` numpy ` already has ( like how using ` numpy.array ` doesn't always create a new array ) , which would be nicely efficient .
The first is most complicated as can be of shape that can be broadcast into the same shape as ` selected_indices ` .
it " doesn't work " because numpy doesn't understand how to broadcast the operation to give you the output your want .
using transpose twice you can get a simpler solution than what you proposed : #CODE
Just to be clear ( because min ( x-y , 1 ) isn't valid numpy ) , do you want to cap x-y to an upper bound of 1 before dividing by ( x+y ) ?
I have a numpy histogram that I would like to output as a tab-delimited text file .
It seems that the histogram array has been created , but I have done something wrong in the np.savetxt() line .
As it turns out , I want to truncate the first integer in the longer array , so this is perfect .
So if I understand correctly , you want to reorder each " row " of ` a1 ` using the indices in each row of ` index ` ? in other words a1.take ( index ) if you were 1D , but doing that for each row ?
You can even leave out repeated indices and numpy will figure out what you want .
The fact that your index array is multidimensional , like I said earlier , doesn't tell numpy anything about where you want to pull these indices from ; it just specifies the shape of the output array .
So , these values are all for indices along the last axis .
We need to tell numpy what indices along the first and second axes these numbers are to be taken from ; i.e. we need to tell numpy that the indices for the first axis are : #CODE
and the indices for the second axis are : #CODE
Python's sum vs .
What are the differences in performance and behavior between using Python's native ` sum ` function and NumPy's ` numpy.sum ` ?
` sum ` works on NumPy's arrays and ` numpy.sum ` works on Python lists and they both return the same effective result ( haven't tested edge cases such as overflow ) but different types .
Edit : I think my practical question here is would using ` numpy.sum ` on a list of Python integers be any faster than using Python's own ` sum ` ?
When numpy sum iterates over this , it isn't doing type checking and it is very fast .
In comparison , using python's sum it has to first convert the numpy array to a python array , and then iterate over that array .
The exact amount that python sum is slower than numpy sum is not well defined as the python sum is going to be a somewhat optimized function as compared to writing your own sum function in python .
Or , if you're trying to get the mean of all the elements ( over all elements of B in A ) , use the ` sum ` method : #CODE
Is the ordering by absolute magnitude ?
Also , I've assumed that there might be duplicate eigenvalues and that the Fiedler vector is the eigenvector associated with the second smallest unique eigenvalue .
I think this can't be right in any case , the fiedler vector is the eigenvalue associated with the second smallest * unique * eigenvalue .
NumPy k-th diagonal indices
I'd like to do arithmetics with k-th diagonal of a numpy.array .
I need those indices .
Unfortunately , diag_indices only returns the indices comprising the main diagonal , so at the moment I am doing : #CODE
Is there a way in numpy to get indices for other than the main diagonal ?
The indices of the k ' th diagonal of ` a ` can be computed with #CODE
Where k sets the diagonal location from center .
yes , I know how to _build a new_ diagonal matrix .
i have never used ` np.identity ` , always used eye ..
You can specify the column size and shift the diagonal over .
But if you look at the log : #URL you can see it is installing them in the right order , or atleast attempts to ...
numpy.max or max ?
If the data are in a ** ` list ` ** , I'd use vanilla ` max ` .
But if you have built-in list then most of the time takes converting it into np.ndarray => that's why ` max ` is better in your timings .
In essense : if ` np.ndarray ` then ` a.max ` , if ` list ` and no need for all the machinery of ` np.ndarray ` then standard ` max ` .
the logic is not sound ; the reshape operation and the length of the vectors ; n**2 has no relation with the length of the vectors
It may work in this special case , but it makes your program hard to follow since the ` det ` method only exists in your program and is not documented anywhere .
I would advise against this ; it makes your program harder to read and maintain and there's really no reason not to write ` from numpy.linalg import det ` , then ` det ( A )` instead of ` A.det() ` .
First ensure your numpy array , ` myarray ` , is normalised with the max value at ` 1.0 ` .
This new data set is the length along the path ( norm ) .
one can assign a probability to each element of an array by simply deviding the value of each element by the sum of all array elements .
ValueError : non-broadcastable output operand with shape ( ) doesn't match the broadcast shape ( 10 )
` sum ` is ` __builtin__.sum ` , which doesn't know how to sum a NumPy array so just returns the array unchanged .
+1 as ipython in interactive mode ( -- pylab ) imports np.sum as sum , so it works in ipython but doesn't works in a file as the OP mentioned .
I know I can use join() to concatenate the chars into a string , but I can only find a way to do this one string at a time : #CODE
it will only work if there is no overlap between blocks above and below the diagonal .
I realize I could define a slice , but I want to be able to add an offset to the indices ( e.g. Yinds+offset ) , and that can't be done with slices .
Even more weirdly , passing ` numpy.delete() ` a list performs worse than looping through the list and giving it single indices .
I should get the indices of the following values : ` [ 1 , 2 , 3 , 4 , 6 , 7 , 8 , 9 ]` .
Because you could just format the numpy arrays as Matlab-style ones in strings , write them to a file , and then read the arrays into Matlab .
So , we use ` reshape ` ( to format correctly ) and ` transpose ` ( because of the different way MATLAB and numpy store data ) .
As a breakdown , ` ravel_multi_index ` converts the index pairs specified by ` i ` and ` j ` to integer indices into a C-flattened array ; ` bincount ` counts the number of times each value ` 0 ..
4000 ` appears in that list of indices ; and ` reshape ` converts the C-flattened array back to a 2d array .
I make this second method a little slower ( 2x ) than " a " , which isn't too surprising as the ` unique ` stage is going to be slow .
However , your ` ravel_multi_index ` in combination with ` unique ( , return_inverse=True )` might do the trick !
Then ` reduce() ` will sum everything .
Hmm , it might be worth trying just letting ` sum() ` ( or ` reduce() `) compute a complex sum , and then throw away the imaginary part when the sum is done .
What should I worry about if I compress float64 array to float32 in numpy ?
Those are absolute errors .
When you convert a double that is 1 / 2 ULP ( of a float ; doubles have finer ULP ) more than that or greater to float , infinity is returned , which is , of course , an infinite absolute error and an infinite relative error .
The absolute error will be at most 1 / 2 ULP , 2 -150 .
This explains tradeoffs related to scaling ( along with answer above that explained absolute and relative errors as a function of magnitude ) .
Do you want to resize an image ?
Also , as a quick example , consider a function that takes the ` sin ` of each item and if the result is greater than 0 , multiplies it by 2 , otherwise just returning ` sin ( x )` .
It is giving me a segmentation fault when I try do a dot product with a vector of ones .
Specifically , you can use convolve and deconvolve
For an even faster implementation of convolve , also check out fftconvolve
For N dimensions it is a sum product over the last axis of a and the second-to-last of b :
Ordinary inner product of vectors for 1-D arrays ( without complex conjugation ) , in higher dimensions a sum product over the last axes .
Let ` c = np.dot ( a , b )` and ` d = np.inner ( a , b )` then ` c [ i , j ] == sum ( a [ i , :] * b [: , j ])` and ` d [ i , j ] == sum ( a [ i , :] * b [ j , :]) ` .
and its transpose #CODE
For 1 and 2 dimensional arrays numpy.inner works as transpose the second matrix then multiply .
My thought was to produce a similarly shaped array of distances , and then using argmin to determine the indices of the values to be removed .
You can then calculate the distance using ` np.hypot ` , or if you'd prefer , square it , sum it , and take the square root .
Finally , some boolean mask trickery , along with a couple of ` reshape ` calls : #CODE
On a side note , ` np.r_ ` is quite nice for what you're doing with ` concatenate ` .
Your long concatenate line reduces to ` ind = np.r_ [: 5 , 10:15 , 20:30 : 2 , 8] `
It works by constructing a list of the desired indices , which is valid method of indexing a numpy array .
Unfortunately it seems that PyTables currently only supports a single set of list indices .
I defined the min and max values in the sliders as you indicate ( 0.1 to 30 and 0.1 to 10 ) .
The solution is to transpose the array ` l ` into the correct shape : #CODE
NumPy by itself is a fairly low-level tool , and will be very much similar to using MATLAB . pandas on the other hand provides rich time series functionality , data alignment , NA-friendly statistics , groupby , merge and join methods , and lots of other conveniences .
Data alignment , join , etc all become * possible * due to this , but for people who don't grok that underlying difference it's not even clear what those mean ( e.g. , what is " data alignment " of two numpy arrays ? ) .
Other thing that is great in pandas is the Panel class that you can join series of layers with different properties and combine it using groupby function .
I looked briefly at Scipy's correlate function .
You can use correlate .
You will likely find that if your letter size multiplied by your big array size is bigger than roughly Nlog ( N ) , where N is corresponding size of the big array in which you're searching ( for each dimension ) , then you will probably get a speed up by using an fft based algorithm like ` scipy.signal.fftconvolve ` ( bearing in mind that you'll need to flip each axis of one of the datasets if you're using a convolution rather than a correlation - ` flipud ` and ` fliplr `) .
How can I use the return value of correlate / fftconvolve to index the 2d shape without using loops ?
It's also difficult to set the max_peak if you're setting zeros in the main search array .
So far it's seeming to work by using zeros , but am missing a few instances of shapes .
The way I would do it would be to check that the indices of the small submatrices don't lie inside the big ones , which could be done very fast as a simple comparison ( since you know the offsets and the sizes ) .
Switching the ' mask ' to 1 instead of 0 , and using correlate instead of fftconvolve got them all !
There is the ` diff ` method : #CODE
@USER - good point . anyway , ` diff ` works on python lists too .
` numpy.savetxt ( ' bool ' , mat , fmt= ' %f ' , delimiter= ' , ')`
The column vectors are independent of each other , so I believe I should calculate the distance between each column vector in ` arr1 ` and a collection of column vectors ranging from ` i ` to ` i + A ` from ` arr2 ` and take the sum of these distances ( not sure though ) .
You can subtract ` arr1 ` from ` arr2 ` by ensuring that they broadcast against each other correctly .
The best way I could think of involves taking a transpose and doing some reshaping .
The output should consist of only 3 values for the example arrays you give , as I'm looking for a " best match " given arr1 and each combination of the same size in arr2 , i.e. which index ( indices ) in ` arr2 ` makes it so that ` dist ( arr2 [ i : i+2 ] , arr1 )` is the smallest ?
Once you have the distance matrix , you can just sum across columns and normalize to get the average distance , if that's what you're looking for .
I guess I could just write an excel macro to replace the zeros with strings ...
If you want the mean rather than the sum , remove the weights and use the bin tallys : #CODE
Find highest values in similarity matrix not on diagonal
Note that the values on the diagonal are all equal to 100.0 and that the upper triangular is equal to the lower triangular .
I want to find the indexes of the five different highest values not on the diagonal .
But this is inefficient because I go through the whole matrix while I only need to traverse half the matrix : for the highest value ` 95.8333333333 ` I only care about the indices ` ( 3 , 6 )` and ` ( 3 , 7 )` .
There were 100.0 elements that were not on diagonal too !
I checked the log values and this error occurs for say , 508.038057662 .
Using Stirling Approximation of log terms will cause some range of errors to occur .
I'll have to determine what I gain from storing the timestamp as an integer vs what it cost to convert it back to a string append the ' Z ' and then instantiate a datetime64 object .
A CSR matrix can be fully reconstructed from its ` data ` , ` indices ` and ` indptr ` attributes .
NumPy ` toarray() ` can't handle converting gigantic ones to dense .
This will give you the row indices of the rows where all values are lower or higher than 5 : #CODE
To fetch the data , rather than the indices , use the boolean result directly : #CODE
I believe the calculation of the gradient of the objective function spent me most of the time and would like to find out the reason why it spent so much .
I want to generate a grid of plots , of several arrays , with positive and negative values , with log scale , sharing the same colorbar .
I've achieved the sharing part of the colorbar ( using ImageGrid and common max and min values ) , and I know that I could get a logarithmic scale using LogNorm() on the imshow call in the case of only positive values .
Usually one uses ` ix_ ` to generate indices _into_ an array ; but it just so happens that the shape required for those indices is the same shape that allows for broadcasted assignment .
Numpy-friendly ( doesn't need a fancy class or another module dependency to decode );
I'm keeping an eye on hdf5 , since it is being recurringly mentioned , but the fact is : I am indeed avoiding any extra dependency , specially considering that my GeoTIFF file , and my use of it , are rather simple .
Am I supposed to use a different keyword in Extension to link to the libraries , or link to the mod files instead ?
You can use ` nonzero ` .
It returns the indices grouped by axis -- in other words , a tuple of the form ` ( array ([ x1 , x2 , x3 ,... ]) , array ([ y1 , y2 , y3 ,... ]) , array ([ z1 , z2 , z3 ,... ]) ,... )` : #CODE
You can use the result as an index to get the nonzero values : #CODE
This documentation / tutorial explains that : In order to broadcast , the size of the trailing axes for both arrays in an operation must either be the same size or one of them must be one .
xAxBxC ` so we have a lot of leading ` 1 ` s , which can be broadcasted as other ones .
I run a ` qr factorization ` in ` numpy ` which returns a list of ` ndarrays ` , namely ` Q ` and ` R ` : #CODE
Because the data are not equal zero exactly , we need set a threshold value for zero such as 1e-6 , use numpy.all with axis=1 to check the rows are zeros or not .
maybe ` np.vstack (( v , zeros ( nplots , N )))`
` data = [ v ; zeros ( nplots , N )]` this is concatenating two matrices and stacken them up , note the ` ; ` in numpy you can use ` numpy.concatenate (( v , zeros (( nplots , N ))) , axis = 0 )` where axis is by which axis you want to concatenate by ...
basically when you call ` np.array ` the fist argument must be iterable object , list , tuple and on the second argument must be the type ie ' int ' , ' float32 ' , ' float32 ' and so on ... but you set the type to ` zeros ( nplots , N )` ` numpy ` is complaining that it isn't a type ...
If for example it is the `` flush `` call this might point to a different solution compared to the `` createTable `` call .
It strikes me as strange that I would have to roll back to Python 2.6 to use Numpy on Windows , which makes me think I'm missing something .
I've only figured out the ones immediately below : #CODE
Getting linearized indices in numpy
I need to emulate the MATLAB function ` find ` , which returns the linear indices for the nonzero elements of an array .
numpy has the similar function ` nonzero ` , but it returns a tuple of index arrays .
Is there a function that gives me the linear indices without calculating them myself ?
The easiest solution is to flatten the array before calling ` nonzero() ` : #CODE
I would like each processor to compute the " series of properties " for a limited subsample of my data and then join all the properties together in one array .
Let's assume I have 4 processor and I would like each of them to compute the sqrt of 10000 / 4=2500 points .
I'm not interested in the ` sqrt ` function whatsoever .
Thanks for the answer , but my real task is way more complicated than performing a sqrt .
I hadn't considered that sqrt is in fact a quite trivial function , and I've tried to edit my post to reflect that , as well as to add the results of ` numpy.sqrt ( x )` .
It is * not * in between the two other ones !
PPS : Here is the reason for the discrepancy : the difference in factors comes from two different normalization conventions : the observed factor is simply sqrt ( 14 / 15 ) = 0.9660917 ( because the data has 15 elements ) .
so a simple reshape solved it : #CODE
Here I want to create a array of zeros .
The ` argsort() ` function returns a matrix of indices that can be used to index the original array so that the result would match the ` sort() ` result .
Is there a way to apply those indices ?
The arrangement of non-zero elements within the matrices is such that the resulting sum certainly isn't sparse ( virtually no zero elements left in fact ) .
which works but is a bit slow : of course the sheer amount of pointless processing of zeros which is going on there is absolutely horrific .
I've just been applying this same pattern in a few other places where I have sparse-dense interactions - typically dot product type things - and getting substantial speed-ups ( x2-x3 ) every time .
This has a 32-bit version for mac with all of the key scientific stack packages including scipy , numpy and matplotlib .
the sum of a triple-product ( element-wise ) .
Then multiply them together , letting ` numpy ` take care of the broadcasting ; and then sum along the last ( rightmost ) axis .
You can make this a bit more generalized by using slice notation , the ` newaxis ` value ( which is equal to ` None ` , so the below would work with ` None ` as well ) , and the fact that ` sum ` accepts negative axis values ( with ` -1 ` denoting the last , ` -2 ` denoting the next-to-last , and so on ) .
This way , you don't have to know the original shape of the arrays ; as long as their last axes are compatible , this will broadcast the first three together : #CODE
Instead of actually sorting the array it's given , it returns an array of integer indices that tell you how to reorder the array into sorted order .
Now use ` argsort ` to get the indices .
With those indices , we can recover the sorted matrix : #CODE
Ah , ` axis=None ` is better than my ` flatten ` call .
now the hypotenuse command probably only take 2 parameters , so you will have to find another way to efficiently calculate mag = sqrt ( dx*dx + dy*dy + dz*dz ) .
binding it to a buffer and using a ` glVertexAttribArray ` to broadcast the data to an array of tiles ( point sprites ) ( in this case a 10x10 array ) and this works fine for a static image .
` share_memory() ` requires memory on the order of sum of the sizes of each array , but it's pretty quick .
I just started to translate a Matlab code to numpy , how can I write the following code in python #CODE
The only potentially surprising thing about this is that indices into numpy arrays start at 0 , per Python convention , instead of 1 as in Matlab .
do you want to test for a zero-length array , an array containing all zeros , or both ?
You can do this using a combination of ` np.diff ` , and ` concatenate ` options , like so : #CODE
Matplotlib - Stepped histogram with already binned data
I am trying to get a histogram with already binned data .
I have been trying to use ` bar() ` for this , but I can't seem to figure out how to make it a stepped histogram like this one from the examples , instead of a filled histogram .
Once you have generated the un-binned dataset then you can use the normal matplotlib histogram plotting ( i.e. Axes.hist ) .
You can now have full access to all of the Axes.Histogram plotting options , including ` histtype= " step "` to create the stepped histogram that you wanted .
There's probably a way to accomplish this purely with matrix operations ( multiply the original by a custom kernel and concatenate the result as a new column on the original matrix ) but it won't be as efficient .
Instead of slicing the rows , it may be OK to use ` roll ` : #CODE
Is there an equivalent Matlab dot function in numpy ?
Is there an equivalent Matlab ` dot ` function in numpy ?
The ` dot ` function in Matlab :
For multidimensional arrays A and B , dot returns the scalar product along the first non-singleton dimension of A and B .
In MATLAB , ` dot ( A , B )` of two matrices ` A ` and ` B ` of same size is simply : #CODE
I want to concatenate them , into one array where each column has the original datatype .
Summing values of numpy array based on indices in other array
I want to sum the values of ` c ` according to the indices provided in ` b ` , and store them in ` a ` .
You could use ` ravel_multi_index ` to convert to flat indices : #CODE
If you want to pass from MxNx3 to Nx3 ( and possibly back ) , you can use one of many numpy functions ( ` ravel ` , ` reshape ` , ` rollaxis ` , etc . ) with care .
I want to go through the depth values associated with X , and find ones in array Y that have a depth within 50cm of depth for X .
I'm trying to use SciPy / NumPy to perform fft on voltage vs . time data from an oscilloscope .
I am completely lost when it comes to passing the data to scipy for fft processing .
To sum up : If a person has x and y data stored in two lists or stored in a tuple , how do they perform FFT on these data ?
Assuming that all samples are equally spaced ( i.e. there is the same difference between each time samples ) , you just need to pass ` y ` vector to fft function .
Oddly , the results of the method I used before to compute sem , ` std / sqrt ( # replicates )` , varies _slightly_ from ` sem() ` results , but we're only talking ~ 4e-11 difference max .
NumPy : get min / max from record array of numeric values
How can I determine min / max from this record array ?
I'm not sure how to flatten the values out into a simpler NumPy array .
to flatten the recarray , then you can perform your normal ndarray operations , like #CODE
basically ` ar.dtype.names ` gives you the list of recarray names , then you retrieve the array one by one from the names and stack to ` arnew `
@USER yep , I thought the op wanted a flattened ndarray so I suggested him use ` hstack() ` , but otherwise if the dtypes are uniform and only min / max are needed , sure , ` view ` is a lot lot better .
will broadcast ` v ` over the rows of ` u ` and yields #CODE
This will put them into dictionary form , including the earlier defined class and subject variables , and append them to an outputList .
This should dump the whole dataframe whether it's nicely-printable or not , and you can use the to_csv parameters to configure column separators , whether the index is printed , etc .
Is there a simple way in NumPy to flatten type object array ?
What exactly do you mean by flatten ?
Try different interpolation , and transpose the matrix to get it in the same axis : #CODE
For imshow you have to reverse the first dimension , because imshow uses zero-based row , column indices to the x , y .
Here is a short , relatively simple function that returns weighted values , it uses numpys ` digitize ` , ` accumulate ` , and ` random_sample ` .
First using ` accumulate ` we create bins .
We use ` digitize ` to see which bins these numbers fall into .
The simplest DIY way would be to sum up the probabilities into a cumulative distribution .
it took a few seconds to stack it on my machine .
Takes a sequence of arrays and stack them along the third axis to make a single array .
If you're suggesting that occasions when you're indexing a small array with unpredictable sets of indices and care about every last microsecond are rare , then of course I agree !
The efficient way to do this with numpy is to reshape your index array to match the axes they are indexing i.e. #CODE
Easy ; use ` accumulate ` : #CODE
Do do cosine similarity I was thinking to use a padding technique to add zeros and make these two vectors N X N .
Why cant you just run a nested loop over both jagged lists ( presumably ) , summating each row using Euclidian / vector dot product and using the result as a similarity measure .
Although I'm not quite sure how you are getting a jagged array from a bitmap image ( I would of assumed it would be a proper dense matrix of MxN form ) or how the jagged array of arrays above is meant to represent an MxN matrix / image data , and therefore , how padding the data with zeros would make sense ?
If both arrays have the same dimension , I would flatten them using NumPy .
That's why later I used ` hstack() ` in order to flatten the array ( instead of using the more common ` flatten() ` function ) .
Lstsq expects ( N , M ) and ( N ) , did you try using the transpose of arrayB ?
As in this case dot ( x , y ) and dot ( x_np , y_np ) gave same result , so using list instead of numy array doesn't make difference .... but is this the case for all numpy / scipy functions or I have to do ` my_np_array= array ( my_list )` before calling any / all numpy / scipy functions ?
Just try whichever ones you need and see if it works .
Just multiply everything by the x raised to the absolute value of the largest negative exponent and use the normal polynomial class .
I asked a previous question on how to posterize an image , and from the answer I learned about indexing with an Array of indices , for ex :
I understand how this works , since image > 128 will return an array of multi-dimensional array of indices that satisfy the condition , and then I apply this array to the image and set those to 255 .
Even if it doesn't have the divergence hand-packaged for you , divergence is pretty simple and the derivative tools they give you in scipy ( the ones linked above ) give you about 90% of the code prepackaged in a nice , efficient way .
Therefore , the best method for calculating divergence is to sum the components of the gradient vector i.e. calculate the divergence .
` sum ` implicitely construct a 3d array from the list of gradient fields which are returned by ` np.gradient ` .
So I considered it means to multiply each gradient field by ` A ` .
about applying divergence to the ( scaled by ` A `) gradient field is unclear .
Thus , you can get this weighted gradient simply with : ` A*divergence ( F )`
the functions above do not compute the divergence of a vector field . they sum the derivatives of a scalar field A :
result = sum dAi / dxi = dAx / dx + dAy / dy + dAz / dz
If your only doing reads then it maybe safe , the subprocess only need to keep track of their corresponding index or subset of indices ...
Storing complete log in / Users / nasernikandish / Library / Logs / pip.log Am I doing something wrong ?
Storing complete log in / Users / MYUSERNAME / Library / Logs / pip.log
As for the error at the end , I can't really help without seeing the complete log .
Maybe create a new question about building numpy for Lion Python 2.6 with gfortran support , posting the exact steps you took and the failing log , instead of adding to the comments here .
It follows that the matrix U*V is diagonal , and so , as you say in your own answer , you can construct V^-1 by doing #CODE
This means that the product of U and V must be a diagonal matrix , and if they are appropriately scaled they should be the inverse of each other .
I am wondering what is the best to do if I want to sum the elements ( elements by elements ) of two lists named l1 and l2 : #CODE
or converting the lists as arrays and do a simple sum #CODE
This uses the potentially optimised ` operator.add ` and avoids using ` zip ` / ` izip ` which will truncate input to the shortest sequence allowing errors to fail silently - while ` map ` ( well ` operator.add `) will raise an exception .
Numerous efforts have given me errors such as ` arrays used as indices must be of integer or boolean type `
I had been trying to use the dot function and it wasn't working .
Thanks @USER ; this returns an error : ValueError : ' shape mismatch : objects cannot be broadcast to a single shape ' ; I should mention that I don't know shape of the array z ahead .
The array must be defined on the fly , based on the indices that I'm providing .
Your ` append ` - in-loop algorithm takes quadratic time , so it'll become very slow for large arrays .
Searching for the minimum std dev in the 4 quadrants around the current coordinate , and using the corresponding index to assign the previously computed mean : #CODE
not an expert , but if the variables are independent , can't you just run simple regression against each and sum the result ?
Python : intersection indices numpy array
How can I get the indices of intersection points between two numpy arrays ?
But how can I get the indices into ` a ` of the values in ` inter ` ?
Reversing ` a ` so that the indices are different from the values : #CODE
To simplify the above , though , you could use ` nonzero ` -- this is probably the most correct approach , because it returns a tuple of uniform lists of ` X ` , ` Y ` ... coordinates : #CODE
But note that under many circumstances , it makes sense just to use the boolean array itself , rather than converting it into a set of non-boolean indices .
` scipy.io.loadmat ` will load the mat file if it's pre-v7.3 ; you can then access it like ` matfile [ ' myCell '] [ 0 ] [ ' myStructField '] [ 0 ] [ ' myStructField ']` .
Clearly you have never even tested your ` ODE ` function , because if you did , you would have quickly realised that ` ndarry.shape ` isn't a callable function , it is a tuple , and ` ndarray ` doesn't have a ` dot ` method , and more .
However , you are wrong on the dot method .
For a decade , ndarray had no ` dot ` method ( matrix did ) .
IIRC in the past I've had to use wrappers to flatten and then unflatten the data so that it's happy .
Maybe write some wrappers to flatten the arrays before using them ...
Numpy array operation using another indices array
Why did you transpose density ?
Imagined that , a 2D histogram .
The transpose issue is unfortunate ( rather than transposing , I have switched the order of the lats / lons ) and was a strange design decision ( although I am sure there must be good reason for it ) .
The lists are not the same sizes as they are generated log file timestamp data
Make them ones , zeros , nan's . cleared for me often means using a maskedarray ( check out numpy.ma ) .
But if you insist on clearing out the array , try making a new one of the same size using zeros or empty .
using arange and reshape serves no purpose . use ` np.empty (( n , 6 ))` .
Faster than ` ones ` or ` zeros ` by a hair .
For a minimal working example , let's digitize a 2D array .
@USER , that's because ` e ` is in fact a view on ` e.base ` which itself is the actual copy of the array produced by the ` reshape ` operation .
In the documentation for reshape there is some information about how to ensure an exception if a view cannot be made :
where min and max are index of the arrays .
Yes I am trying to convolve this kernel to the points .
I am trying to convolve but couldn't get it to work so ended up manually looping over and adding the pixels manually .
This one is cute but probably illegal ( as ` a ` isn't actually unique ): #CODE
Here's another approach using set operations that I think is a bit more straightforward than the ones you offer : #CODE
This is very similar your " histogram " method , which is the one I would use if ` a ` was not made up of small integers .
This method finds both the indices of duplicates and values for distinct sets of duplicates .
Yeah , I'll be back w some sample data in a min .
If you take the first column , and search for itself you get the minimum indices for those particular values : #CODE
Do you have a list of zeros and strings such as ' x0 ' or are they 1s ?
I draw figure from the array which contains zeros and values as shown in original post .
We run into issues that join columns are converted into either ints or floats , based on the existence of a NA value in the original list .
I would need to strip off the header , then process the array , then reapply the header .
I see , I am new to this stack overflow business .
More precisely , I think that the function uses the Householder Bidiagonalization to compute the SVD and so , if your matrix is ` m ` by ` n ` , the complexity will be ` O ( max ( m , n ) * min ( m , n ) ^2 )` .
I'm trying to make a histogram of some data that is being stored in an ndarray .
The histogram is part of a set of analysis which I've made into a class in a python program .
how to translate a numpy array to polyphonic music
I think I can t just simple sum the " seq * " array , because instead of a chord I will get noise .
Building the sum is physically correct here because acoustics is a linear theory .
This means that the sum of two solutions of the wave equation , say a " C-note wave " and a " E-note wave " , are again a solution to the wave equation .
I'm primarily consulting stack to see if they are other ways I have not thought of ( or something in numpy ) .
Note that ` numpy.percentile ` and ` scipy.stats.mstats.mquantiles ` do not accept weights and cannot perform weighted quantiles , just regular equal-weighted ones .
@USER : Use ` random.sample ( xrange ( len ( a )) , int ( len ( a ) * 0.07 ))` as indices , or so , modulo rounding .
The first expression determines how many elements you want to sample ( n is an integer between 0 and len ( a ) , but on average 0.07 ) , the second generates exactly the number of indices you want to retrieve .
Be aware that this computes the floor and throws away any remainder .
Lossy Polynomial Regression using numpy
Polynomial regression isn't necessarily linear .
In the common case of a diagonal covariance matrix , the multivariate PDF can be obtained by simply multiplying the univariate PDF values returned by a ` scipy.stats.norm ` instance .
You may also want to use the likelihood function ( log probability ) , which is less likely to underflow for large dimensions and is a little more straightforward to compute .
The result is ` data.gz ` , which , when decompressed , has the same contents as the ones listed above .
Why is there a difference between ` squeeze ` and ` ravel ` in this case ?
It is possible ( likely ) that numpy may have code internally to deal with strange striding ( to optimize either broadcast or access speed ) and ` ravel ` will undo all of those to flatten them , where as ` squeeze ` probably just knocks off dimensions with dimension=1 .
However , a reshape will not cause that : #CODE
` np.expand_dims ` is actually implemented using ` reshape ` which is why it works ( This is a slight error in documentation I suppose ) .
@USER -- Under the hood , ` np.expand_dims ` uses ` reshape ` .
More to the point , code that uses global variables can be absolute hell to debug , since it's not easy to identify where they the value comes from .
Compute outer product of arrays with arbitrary dimensions
I have two arrays ` A , B ` and want to take the outer product on their last dimension ,
Sometimes this may be called with integer indices ` A=Z [: , 1 , :] , B=Z [: , 2 , :] ` and other times
I want to be able to calculate the covariance matrix ( outer product ) of these vectors , over regions defined by slices in the first two axes .
I realized that the Kronecker product is simply an outer product which preserves dimensionality .
sum function in python
What you can do to avoid namespace collisions is to only import what you need : ` from numpy import dot ` for example .
` for all numpy functions and then ` sum ` for inbuilt .
( Note : ` np.sum ( myarrlist , axis=0 )` will do as you request - see ` help ( np.sum )` : numpy sees ` myarrlist ` as a 3D matrix so you have to tell it you only want to sum along axis ` 0 `)
One way you can avoid namespace collisions is to only import what you need : ` from numpy import dot ` for example .
Another way is to do ` import numpy ` or ` import numpy as np ` , and refer to ` dot ` as ` np.dot() ` or ` numpy.dot() `
you are indeed overriding the builtin sum function .
You can then access the numpy dot function as np.dot .
Another option , if you just want the dot function is to do this : #CODE
Then the dot function is then the only function from numpy that would be available .
Is there a better way to broadcast arrays ?
I want to broadcast an array ` b ` to the shape it would take if it were in an arithmetic operation with another array ` a ` .
If you just need to broadcast a scalar to some arbitrary shape , you can do something like this : #CODE
Rather than using ` broadcast ` you can use ` broadcast_arrays ` to get the result that ( I think ) you want .
Finally , further testing shows that the fastest approach -- in at least some cases -- is to multiply by ` ones ` : #CODE
I'm surprised I didn't see it ; it's right next to ` broadcast ` in the docs .
In case you're curious , the reason I'm interested in this is that the function ` scipy.ndimage.map_coordinates ` does not automatically broadcast the input coordinates , so I have to do it manually .
The way around this is to flatten ` np.dot ( a , b )` by using ` np.squeeze ` or something similar so that when they are broadcast together they produce a 2 element array .
Is there any straightforward way to do this while still using numpy's ` histogram ` function ?
I have to translate this data structure to python , but I am new to numpy and python lists .
The simplest thing would be to just dump everything in to a sequence of nested lists , but I have a sneaking suspicion that wouldn't support the type of functionality you are looking for .
@USER : BCD stands for [ Binary-coded decimal ] ( #URL ) and you'll have to decode those manually .
How to invert a permutation array in numpy
This represents this permutation ( ` = ` is an arrow ): #CODE
The inverse of a permutation ` p ` of ` np.arange ( n )` is the array of indices ` s ` that sort ` p ` , i.e. #CODE
The reason why ` argsort ` is correct can be found when you use the representation of a permutation by a matrix .
The mathematical advantage to a permutation matrix ` P ` is that the matrix " operates on vectors " , i.e. a permutation matrix times a vector permutes the vector .
Your permutation looks like : #CODE
Given a permutation matrix , we can " undo " multipication by multiplying by it's inverse ` P^-1 ` .
The beauty of permutation matrices is that they are orthogonal , hence ` P*P^ ( -1 )= I ` , or in other words ` P ( -1 )= P^T ` , the inverse is the transpose .
This means we can take the indices of the transpose matrix to find your inverted permutation vector : #CODE
Which if you think about it , is exactly the same as finding the indices that sort the columns of ` P ` !
Eventually the asymptotic complexity kicks in ( ` O ( n log n )` for ` argsort ` vs . ` O ( n )` for the single-pass algorithm ) and the single-pass algorithm will be consistently faster after a sufficiently large ` n = p.size ` ( threshold is around 700k on my machine ) .
;-) About the same time you were writing this answer to a two year old question , I was sending a PR to use a technique very similar to this in numpy's ` unique ` function , see [ here ] ( #URL ) .
It is like transforming a continuous distribution curve into a histogram again .
I don't know much about ` joblib ` , but I would be surprised if ` RADMat ` is anything other than an array of zeros at the end of your parallel loop ( not only is it slower , but it's possibly incorrect too ;) .
I am starting with an ` Nx2 ` array of unique points , then finding the Delaunay ` edges ` for those points , an ` Mx2 ` array composed of indices into ` points ` .
Numpy : multiplying by a vector of ones
` normalisers = sum ( exp ( outputs ) , axis=1 ) *ones (( 1 , shape ( outputs ) [ 0 ]))`
So , I'll remove the ` exp ` for the sake of simplification ( it's not relevant to the issue here ) , which gives us :
` sum ( outputs , axis=1 ) *ones (( 1 , shape ( outputs ) [ 0 ]))`
As far as I can tell , this is just summing all the rows in the ` outputs ` matrix , and then multiplying the resulting vector element-wise by a vector of all ones .
what's the point of multiplying by all ones here ?
Is this an error in the textbook , or am I just not seeing how multiplying by all ones could possibly have any effect on the values here ?
As mutzmatron writes in the comment , when ` outputs ` is an array , this multiplication is a highly contrived way of changing the shape of the result of ` sum ` from ` ( n , )` to ` ( 1 , n )` .
In contrast to the way presented in your textbook , this is both readable and scalable , because ` reshape ` takes constant rather than linear time and memory .
You are trying to take the dot product between two arrays which have incompatible shapes .
I'm not sure what to recommend , since I don't know what you were expecting by taking the dot product between these arrays .
@USER - Have a look at ` os.path.splitext ` if you're trying to strip the extension off of filenames .
List of indices into ` b ` ( or ` x `) in sorted order by the keys in ` b ` .
Use the indices to get the right elements from ` x ` .
In the special case where the aggregation function ` func ` can be expressed as a sum , then ` bincount ` seems faster than ` pandas ` .
For example when ` func ` is the product , it can be expressed as a sum of logarithms and we can do : #CODE
In this , ` trace ` could be implemented ` sum ( a [ i ] [ i ] for i in range ( len ( a )))`
If I try to apply ` formatting_function ` to an array of ones , I get `' 1.000E +0 '` .
I think the OP wants the sum of the z values associated with the same ( x , y ) pair , so that the row would be ` [ 2 , 2 , 2 ]` and not ` [ 2 , 2 , 0 ]` .
Here I have replace the mean with the more robust median and the standard deviation with the absolute distance to the median .
Matlab VS Python - eig ( A , B ) VS sc.linalg.eig ( A , B )
Any ( nonzero ) scalar multiple of an eigenvector will also be an eigenvector ; only the direction is meaningful , not the overall normalization .
I'm currently exploring the currently available solutions , here are a few interesting ones :
Anywhere there is a " 1 " in ` B ` I want to sum the same row and column locations in ` A ` .
n ( all unique values in ` B `)
I found the unique values by using : #CODE
I realize this make take multiple steps but I can't really wrap my head around using the index matrix to sum those locations in another matrix .
For each unique value ` x ` , you can do #CODE
` [( val , np.sum ( A [ B == val ])) for val in np.unique ( B )]` gives you a list of tuples where the first element is one of the unique values in B , and the second element is the sum of elements in A where the corresponding value in B is that value .
Here I get a stack trace stating : #CODE
I want to be able to access not only the values , but also their indices .
=> makes the last ` n ` columns the first ` n ` ones ,
The solution with ` roll ` : #CODE
` scipy.sparse ` doesn't have ` roll ` , but you can simulate it with ` hstack ` : #CODE
Try to join on outer .
It is possible to read the data with pandas and to concatenate it .
Than concatenate it : #CODE
You will output 2 integers and one float with 3 digits before the dot this way .
sum over values in python dict except one
Is there a way to sum over all values in a python dict except one by using a selector in #CODE
You could loop over the dict to create a generator for the ` sum ` method : #CODE
You need to supply an accumulator or initial value for the ` sum ` method : #CODE
Note that this is using the builtin ` sum ` method .
The basic idea is to move the dimension ( in your case , dimension 0 , the number of rows ) that's irrelevant to the calculation " out of the way " into a higher dimension and then automatically broadcast over it .
From there I would like to simply append each array to a master array so that the final output is something similar to : #CODE
There must be someway to concat / stack / join ( whatever term is supposed to be used ) to do this in numpy
Can you explain what ii is ? and I get the error " operands could not be broadcast together with shapes ( 3 ) ( 4 )"
Function to slice indices in Numpy
I have two index arrays and I want to return all the indices in between , like a slice function , manually it would look like this : #CODE
I am considering submatrices of a matrix , and I want to have access to them from the indices in two corners .
So for each member of ` ind1 ` and ` ind2 ` you want a row in ` final ` containing the interpolated indices ?
As I thought about ` ind1 ` and ` ind2 ` , they can be two indices in any ND-array , and ` final ` should fill up the matrix between them ... like in pelson's answer .
Technically , the only values in the array are zeros and ones , and it is " sorted " .
= 0 , then all rows from p downward were made solely of zeros .
If numpy is installed there , you will need to append this path to your Python's path : #CODE
you can append it permentanly using ~ / .profile
Turn 2D NumPy array into 1D array for plotting a histogram
I'm trying to plot a histogram with matplotlib .
` reshape ` will do the trick .
There's also a more specific function , ` flatten ` , that appears to do exactly what you want .
Or you can use squeeze : #CODE
It is equivalent to reshape ( -1 , order =o rder ) .
If you now use ` np.reshape() ` to reshape the array to one length of the total number of element you will have a solution that always works .
I am treating it as a vector : Long story short its a forecolor of an image histogram :
I don't know why they join the NumPy function ` floor ` and the ` // ` operator .
There is no reason to do that , ` // ` will return either an integer or a float corresponding to an integer , so ` floor ` will do nothing .
Or do you just want to print the y identifiers on top and print the transpose of the array below that ( since the array / matrix is symmetric , that would show the same in this case anyway ) ?
@USER : ya - while I agree that some simple questions could do with more research on the OPs part , in this case people unfamiliar with matrix manipulation ( matlab , numpy style ) could find it difficult to get the min that isn't 0 .
You could sort each record into grid areas and only check the relevant grid and the surrounding ones - It depends if you have a maximum distance ?
When you install a module with Synaptic ( or apt-get or aptitude etc ) it will be installed only for the system-provided Python , ie the ones in / usr / bin .
You can write , if ^t denotes the transpose : #CODE
Python / numpy append arrays , transpose , and save as csv
append / concatentate / vstack / ?
Sure , you could use ` numpy.vstack ` and a transpose -- But in the end , it will be the same number of lines of code and you'll either be growing the same array at every iteration of the loop ( which is likely to be inefficient ) or waiting until the end to stack them all together ( which is what you're doing now ) .
This puts the data vectors directly into rows of ` rarr ` , saving you the conversion to array and transpose .
Python eig for generalized eigenvalue does not return correct eigenvectors
Running eig I get : #CODE
I am surprised as eig is supposed to return 3 arrays !
It looks to me like they * are * normalized : the maximum absolute value of each vector is 1 .
I want to concatenate arrays A , B and C .
If , for example , B is empty , I want to concatenate A with C .
If they aren't 1D then you should also specify exactly what you mean when you say " concatenate " .
Basically , the issue here is that ` resize ` is returning a ` view ` , not a new array .
Is the total amount of memory used by a numpy array the sum of the memory used by the view , plus that of the array itself ?
the reason it's convenient to cast list of lists to array is for fast indexing of certain elements . for example if you have a list of indices ` x ` into the array ` a ` , you can do ` a [ x ]` to retrieve them .
20 chars seems more than enough , though it may consume more memory , so you can simply set it to the max value ...
See also this How to pip install a package with min and max version range ?
I append the numpy based array to an existing binary .dat file .
It's hard to tell what you are doing without seeing code , but you could try using the ` sync ` command to flush the data in memory to disk after some amount of data has been written to the file :
I'm trying to use and accelerate fancy indexing to " join " two arrays and sum over one of results ' axis .
Vectorizing the inner loop was wthat I woss looking for . also thanks for the hint to use convolve for creating the intervals for the mask .
Numpy arrays with elements as indices to other objects
Elements of the array ' cabbage ' are indices to the list ' cucumber ' .
The following script creates an array ' cauliflower ' with the same shape as ' cabbage ' but with the indices replaced with the corresponding tuples from ' cucumber ' .
There are several problems with this approach , such as what if an array already contains zeros ?
Here is what happens : ( I couldn't post the full log because I am a limited user .
Please let me know if there is a way for me ( limited user ) to post the full log .
The SO format likes a specific point , not a huge dump of data ...
Those are actually arrays of indices to other arrays , and I'll be ok with operating on the original arrays as long as the operation is vectorized ( but of course working on the index array as a vector operation will be awesome ) .
But that solution requires sorting , which is O ( n log n ); and indeed , after some testing , I see that the pure python solution below is faster for all inputs !
How can I join them using numpy methods that #CODE
You can transpose and flatten the arrays : #CODE
Unfortunately , your new implementation is not any faster than the first vstack + flatten / ravel .
Is there a faster way to effectively " erase " the array and change the last column to ones ?
I've optimized everything else , but in the profile , it says that 3% of the time is spent in the zeros function .
I have two numpy arrays with the dimensions ( 120 , 360 ) , one of the arrays consists of integers and zeros the second consists of floats .
Also I'd like to replace the integers in the first array with nans and change zeros to ones .
As far as I know dot in numpy accepts only two arguments .
And you can also define your own readers based on the existing ones : #URL
In addition to previous answers , to modify numbers with odd indices you should use A [ 1 :: 2 ] instead of A [: : 2 ]
This is just what I was looking for and allows me the chance to explore the reshape and rollaxis methods .
The trick is that this convolve function can be used in-place so the double for loop : #CODE
( Note that the numpy convolve function does not allow for this inplace usage . )
Which returns the following stack trace : #CODE
Numpy dot product very slow using ints
So why is it over 50 times slower to calculate the dot product using ints than using floats ?
The stack trace for float looks like this : #CODE
.. while the stack trace for int looks like this : #CODE
It's really too bad that BLAS libraries don't work for integer types , and that numpy's built-in ` dot ` is so much slower .
Numpy ` ValueError : operands could not be broadcast together with shape ...
---------------------------------------- Command python setup.py egg_info failed with error code 1 Storing complete log in
That norm is called upon an array X1 in the form [[ x1 , y1 ] , [ x2 , y2 ] , [ x3 , y3 ] , ..., [ xn , yn ]] , and X2 , which is X1 transposed by the following method inside ` Rbf ` class , already hacked by me with debugging purposes : #CODE
How to get the maximum in each bin of a 2D histogram in python
I am binning a 3D spatial distribution of points ( arrays ` x ` , ` y ` and ` z `) with a 2d histogram .
The histogram ` H ` represent the sum of the density along the line-of-sight ( in this case in the z-axis ) .
Now I want to go further and instead of plotting the sum of the density filed along the line-of-sight I would like to get the maximum of the density in each 2D bin .
Take a look at the implementation of ` numpy.unique ` , you want to do something like that to get unique bin values , but you want to do it in such a way so that it also gives you the min / max of ` d ` at the same time .
Getting the max in every bin is very similar to the problem of getting a unique set of bins .
Take a look at the implementation of numpy.unique and see if you can write something similar that gives you a unique set of bins and the max in each bin at the same time .
` coll = plt.scatter ( rand ( 5 ) , rand ( 5 ) , c= " k ")`
I don't necessarily know at the time of plotting all the points which ones will be colored
Here's an interesting way to do it ( though it's not the most efficient way , as I believe it's O ( n ) rather than O ( log ( n )) as ecatmur's answer would be ; it is , however , more compact ): #CODE
What I would basically like to do is the have for example statics over 1 month period ( let's say mean and std in the period from 1 of July 2012 and 31 of July 2012 ) .
Right , ` linspace ` won't generate any of the values in ` x ` except the ones you pass to it ( ` x.min() ` and ` x.max() `) .
Numpy transpose not giving expected result
I am trying a very basic example in Python scipy module for transpose method but is not giving expected result .
Expectation is : ( which will be result in Matlab om transpose ) #CODE
Add new axis and transpose : #CODE
Or reshape : #CODE
A more concise way to reshape a 1D array into a 2D array is : #CODE
When converting MATLAB code it might be necessary to first reshape a
reshape back .
As reshape ( usually ) produces views onto the same
Note that the scan order used by reshape in Numpy defaults to the ' C '
I would point out that reshape ( mafs , ( 4 , 4 , 2 )) will be a syntax error , while reshape ( mafs , [ 4 , 4 , 2 ]) is not .
To add to the other answers , you need to take the absolute value of arguments to ` .set_aspect ` : #CODE
OP is looking for the set diff of the ** rows** .
sqlite3 writes only floating-point numpy arrays not integer ones
` plt.pcolor ( rand ( 5 , 5 ))`
My questions are : first , why does ` mat ` and ` 1-mat ` give identical clusterings here ? and second , how can I annotate the distance along each branch of the tree using ` dendrogram ` so that the distances between pairs of nodes can be compared ?
First of all , the computation m -> m - 1 didn't really change your result since the distance matrix , which basically describes the relative distances between all unique pairs , didn't change in your specific case .
For your second question , you probably need to roll out your own annotation routine to do what you want , since I don't think dendromgram natively supports it ...
In your example , ` mat ` is 3 x 3 , so you are clustering
Why does mat and 1-mat give identical clusterings here ?
The arrays ` mat ` and ` 1-mat ` produce the same clustering because the clustering
nor a translation ( ` mat + offset `) of the entire data set change the relative
For your ` mat ` array , the augmented dendrogram is
Numpy's npv calculation
The npv function had a bug .
The mirr function called the npv function ,
Note that using lambdas costs you a lot of the benefits of using numpy in the first place , and ` lambda x : sum ( x )` is simply a more verbose and slower way of writing ` sum ` here anyway .
Perhaps you want the absolute value , ` np.abs ( b )` ?
@USER In that case ` log ( p ( x )) = log ( 0.01 ) + log ( 0.02 ) + log ( 0.03 )` .
When I cast this as a masked array ( np.ma.masked_array ( t , np.isnan ( t )) where t is the array above ) and run np.ma.corrcoef ( with rowvar=False ) on it the correlation between the variables is given as - 86.52 ( in absolute value , not percentage ! ) .
Whereas running np.corrcoef on the first two points alone produces a correlation of 1 ( again absolute value ) .
To tell the difference , it helps to understand how indices are passed to python objects .
These two items are broadcast together , and the result has the same shape as the third item .
The odd behavior resulting from slicing + broadcast indexing is still unexpected enough that it difficult to code around .
The point is , this condition has a shape ` ( 1000 , )` , so when I compress the original matrix , I get a ` ( 117 , )` result .
If you are looking for one eigenvector corresponding to one eigenvalue , it could be much more efficient to use the scipy.sparse.linalg implementation of the eig function .
I think you meant eigs not eig .
Fixed eig -> eigs .
How to write in python numpy : b = sum ( v ) - a as an implicitly elementwise ( vector ) computation ?
Now we can call ` sum ( axis=1 )` : #CODE
fmin_l_bfgs_b expects that your function returns the function value and the gradient .
If you only return the function value and don't provide a gradient , then you need to set approx_grad=True so that fmin_l_bfgs_b uses a numerical approximation to it .
` x ` and ` id ` are numpy arrays of the same dimensions and ` ids ` is an array of smaller dimension .
Using ` x [( id == dummy )] .max() ` instead of the built-in ` max ` should give some speed-up .
This is about 3x faster for a x being 1000x1000 and id / ids being in 0 ..
100 , but as its rather complex , its only worth it for larger problems with many ids : #CODE
If id / ids has some structure it should be possible to simplify the code , and maybe use a different approach to achieve a much larger speedup .
Otherwise the speedup of this code should be large , as long as there are many ( unique ) ids ( and x / id arrays are not very small ) .
Note that the code enforces np.unique ( ids ) , which is probably a good assumption though .
If you want to compute the max for all ids , do ` ids = ID `
Note that if a particular class in ` ids ` is not found in ` ID ` ( i.e. no x is labeled by that class ) then the maximum return for that class is ` 0 ` .
I like @USER ' s solution if you don't care about the placement of the items in the matrix , but their solution will remove all zeros and shift elements I believe .
It would be nice to be able to virtually tile a vector , like a more flexible version of broadcasting .
The one you've given would give the same answer just with ` mat + vec ` , so I'm not sure exactly what you're after .
EDIT : as Sebastian mentions , for row addition , ` mat + vec ` already handles the broadcasting correctly .
Here its not even necessary , but if mat would be shaped ( 3 , 5 ) then using np.ones ( 3 ) [: , np.newaxis ] does the trick .
Could someone explain to me why this isn't just a slower version of ` mat + vec ` ?
It looks like mat + vec works when vec is the same size as the number of columns in mat , but you need newaxis to add columns .
Inverting a 3x3 matrix using ` inv ` takes about 51.8 us for me .
` for i in range ( 100 ): pass ` takes 2.89 us , so the loop overhead for each inv is totally negligible .
Anyone every tried forming a block diagonal matrix of the 3x3s and using an efficient sparse inversion algorithm for something like this ?
If you look at numpy.linalg.inv , you can see it's just a call to numpy.linalg.solve ( A , inv ( A.shape [ 0 ]) .
If you strip all these out and just call lapack in your for loop ( since you already know the dimensions of your matrix and maybe know that it's real , not complex ) , things run MUCH faster ( Note that I've made my array larger ) : #CODE
Could you do the same with eig :-)
I looked at eye , and skipped over it because the documentation says : " Return a 2-D array with ones on the diagonal and zeros elsewhere .
Yeah , I thought something like this would be possible , but I'm a little surprised numpy uses ` eye ` for this ( as the name implies it is the identity matrix and this is definitely not an identity matrix ) .
@USER : Thank you very much for being so thorough with this answer ( and many of your other good ones ) .
This means that an even progression of 0 , 0.1 , 0.2 ... rounded to full numbers will give you 6 zeros , 9 Ones , 11 Twos , 9 Threes ... not nice .
efficient way to resize numpy or dataset ?
Or should I resize after every 100 or 1000 rows ?
If you want to append data into the array continuously , you can create a large array first , and use index to copy data into it .
Or you can use array object from array module , it's a dynamic array that behaves like list . after append all the data into array object , you can convert it to ndarray .
This is the same idea of ecatmur , but works in terms of indices employing argsort() instead of sort .
@USER , you can simulate larger arrays from smaller ones .
Inverted order of numpy array gradient and matplotlib quiver
Then I get the gradient : #CODE
The gradient should be in the x direction , increasing with increasing x .
` a ` is built as ` a [ yidx , xidx ]` but when you take the gradient , you do : ` velx , vely = np.gradient ( a )` when you should be doing ` vely , velx = np.gradient ( a )` .
Since the gradient along the 0th axis should give you ` vely ` ( presumably ` d / dy ( a ) = vely `) ?
I expect that a combination of A . buying lots of memory and B . setting up your experiment wisely will allow the entire log to fit in the RAM .
However , with my current dict-of-lists solution keeping every variable's log in a continuous section of memory would require a lot of copying and overhead .
Update : In the end , I changed my approach a little and added the log ( as a dict mapping variable names to sequence types ) to the function parameters .
This allows you to pass in objects such as lists or array.arrays or anything that has an append method .
Also , do you actually need to have gigabytes of doubles in memory and find a way to dump them to disk , or can you use an on-disk structure in the first place ?
Because a ` std :: vector ` does the auto-expanding automatically , and you can access its contents as a C array to feed into numpy when it's done being expanded .
Or , alternatively , use a std :: deque or SGI-STL rope or other stride-based container for less copying along the way .
If you know a better way to describe what I'm doing here I'd love to hear about it , google searches involving the stem " log " all lead to page after page on text logs , log rotation and logarithms .
This is going to be more a huge dump of ideas rather than a consistent answer , because it sounds like that's what you're looking for .
These are continuous functions of angle ( cos and sin ) .
indeed , as Colonel Panic mentioned , using the norm and the phase of the complex target a 2D float encoding instead of the real an imaginary parts might make more sense for your problem .
I'm looking to translate the list above ( which is organized as rows ) [ ' Apple Blue 1 6.5 ' , ' Banana Red 4 7.733 '] into numpy arrays with members that would have the same types [ ' Apple ' , ' Banana '] and [ ' Blue ' , ' Red '] and [ 1 , 4 ]
It sort of seems natural that you could tell NumPy that the indices for ` lut ` are stored along axis 0 of ` rgb ` , without having to actually write it out .
Here you want to create indices into lut that broadcast to ( 4 , 1000 , 1000 ) .
I'd suggest using ` tuple ` to force indexing rowwise , and ` np.rollaxis ` or ` transpose ` instead of ` swapaxes ` : #CODE
To roll the axis first , use : #CODE
and append items to it .
So for example if the new items to append are : #CODE
It seems numpy has a " python-like " append method to add items to a numpy n-dimensional array :
mat ( " 1 2 3 ; 4 5 6 ")
Still unclear how to append tab delimited data .
If you want the dot product , it's best to use ` numpy.dot ` instead of relying on a matrix overriding the multiplication operator .
Less worry about the return type and it can be annoying to use ` squeeze ` on matrix to reduce dimensions ( e.g. [[ 1 ]]) .
is so much more readable than any dot ( a , b ) equivalent .
np.einsum ( ' ij , jk , kl -> il ' , a , b , c ) , where the repeated j and k indices are ' contracted ' ( summed over ) .
From numpy docs I only saw specific ones such as ` LinAlgError `
If you require a single large result array , you should probably slice these arrays individually and then concatenate them .
The key here is ` x [[ 1 , 3 , 0 , 2 , 4 ]]` , where you're indexing with a list of the indices you want in the order you need .
You can now reorder your array with the ` indices ` list : #CODE
Now the strangeness is that I'm taking ` np.diff ( np.array ( arr ))` ( where ` arr ` is a subclass of ` np.ndarray `) and the error ( and nan values ) occurs in the diff , but not in the input array .
Ok , I'm going to ignore all that about the fft , ( which is related to signal processing and not prime numbers at all ) .
But what you get depends on the possible second argument to norm !
But then , have you ever tried to use ` min ` or ` max ` as field name in a ` recarray ` ?
This will not give you the indices of the values though .
I didn't think of using histogram for this .
I think you need to use append function to append new array with previous array , asarray function converts input to array .
Mostafar is right . numpy.asarray does not append lists #CODE
You can use the append function as he has defined .
as you can see it can be used to concatenate various lists , arrays and single elements at one go .
That'll work provided the arrays you append to ` base ` have all the same size ( ` np.savext ` will transform the list of arrays into an array itself ) .
I used cPickle and protocol version 2 to dump some computation results .
However , it is strange to dump successfully ( with large size variable ) but failed to load .
Sort a numpy matrix based on its diagonal
I have a matrix that should have ones on the diagonal but the columns are mixed up .
since they're floats , can you guarantee the ones are exactly 1.0 and unique in their columns ?
But , each entry along the diagonal is guaranteed to have the highest value in that row .
You can use numpy's ` argmax ` to determine the goal column ordering and reorder your matrix using the argmax results as column indices : #CODE
But the last step takes a really long time ( ~5 min on my system ) .
An obvious path would be to transpose the array so that the indices that I am selecting would come up first .
You can transpose netCDF variables too large to fit in memory by using the nccopy utility , which is documented here :
Rather than completely transpose a variable ,
you probably want to " partially transpose " it , by specifying chunks that have a lot of data along
If I do a chunking of 1 on the third dimension , is this analogous to doing a partial transpose where that axis is the fastest-one ( ie , contiguous ) ?
I thought that chunking with 1 in the last two dimensions would be the best thing ( other than to transpose the whole thing ) .
" max is %s , min is %s " % ( max ( lengths ) , min ( lengths ))
max is 6753 , min is 6694
From there , you can use ` test ` to filter the elements you want , or get the indices of the items satisfying the condition with ` np.zeros ( test )` .
@USER If I understand you correctly this time , you want a color scale that is more stretched for high depths and less so for low ones ?
@USER still , you can do a transformation for your depth variable , sometthing like , z = log ( 15.0 - depth ) to highlight the higher depths .
Calculating gradient in 3D
I have the following set of points in 3d-space and D'd like to calculate the gradient everywhere , i.e. have a vector field returned .
You are mixing together the indices and the values in ' points ' , so the gradient is giving you wrong results .
Here is a better way to construct the points with numpy and calculate the gradient : #CODE
The resulting gradient is a tuple with two arrays , one for the gradient on the first direction , another for the gradient on the second direction .
Note that this gradient doesn't take into account the separation between points ( ie , delta x and delta y ) , so to get the derivative you need to divide by it : #CODE
You may also be interested in numpy's diff function , that gives the discrete difference along a given axis .
Firstly , how can a gradient be calculated from a 1-dimensional array ?
By definition , a gradient is a vector field , that is six coordinates times the amount of data points .
This uses Python's built-in ` min ` function , rather than the one from numpy - ` np.min ` is mainly only useful if you're using them with numpy arrays ; there is no need for it when using lists .
When trying to concatenate record arrays which has a field of dtype string but has different length , concatenation fails .
As you can see in the following example , concatenate works if ' f1 ' is of same length but fails , if not .
But , again if we just concatenate the arrays ( not the records ) , it succeeds , though strings are of different size .
Is this a bug in concatenate when concatenating records or is this the expected behavior .
You can't concatenate ` np.concatenate ( ( a , b ) )` because numpy sees the dtypes of ` a ` and ` b ` are different and doesn't change the dtype of the smaller string to match the larger string .
This will not be as space-efficient as a dtype of `' |Sn '` ( for some integer ` n `) , but at least it will allow you to perform the ` concatenate ` operation .
I just have to concatenate all these arrays .
I need a running sum over 11000 cases .
@USER , ` sum ` just calls ` __add__ ` , however that's defined .
Then , do essentially what @USER suggested , as in make copies and replace the ` NaN ` s with 0.0 , add the two arrays together , and then replace the flagged indices above with ` np.NaN ` .
# Then sum these slices accounting for nan
Anyway , I'm not familiar with reading slices from a netcdf file , but , you might try the following : ` sum ( nan_to_zero ( np.array ( netcdfvar [ i ])) for i in cases )` , or as BrenBarn points out : ` sum ( np.nan_to_num ( netcdfvar [ i ]) for i in cases )`
What I'm doing so far is appending zeros to the original array : #CODE
You could use a dictionary to map row / column names into indices .
In mathematical terms , sum [( s-u ) ] /( N-1 ) is an unbiased estimator of the variance V even though sqrt{sum [( x-u ) ] /( N-1 ) } is not an unbiased estimator of sqrt ( V ) .
ValueError taking dot product of two sparse matrices in SciPy
I'm trying to take the dot product of two lil_matrix sparse matrices that are approx .
Any ideas on what might be happening - running this on a machine with about 64GB of ram , and using about 13GB when executing the dot .
This is a bad error message , but the " problem " quite simply is that your resulting matrix would be too big ( has too many nonzero elements , not its dimension ) .
Scipy uses ` int32 ` to store ` indptr ` and ` indices ` for the sparse formats .
This means that your sparsematrix cannot have more then ( approximatly ) 2^31 nonzero elements .
That is , for a given permutation of the indexes of the matrix in a list ` perm ` , this function calculates the result of applying this permutation to the indexes of a matrix .
I had passed the 5 min edit time and just wanted to comment more
If you want high poly models and all kinds of fancy lighting and particle effects , Python's probably the wrong way to go .
Find indices of a list of values in a numpy array
Given another array of search values , with repeating elements , I want to produce the indices of these search values in the master array .
` indices ` should now be a 2-tuple of arrays , the first of which contains the indices in the first dimensions and the second of which contains the indices in the second dimension corresponding to pixel values of ` c ` .
` indices ` will now contain an extra entry ( at the beginning ) which are the image indices .
The last 2 elements of the sum [ 9.99995000008e +17 , 9.99998000001e +17 ]
The last 2 elements of the sum [ 9.99993999e +17 9.99996999e +17 ]
print " The last 2 elements of the sum " , c [ -2 :]
print " The last 2 elements of the sum " , c [ -2 :]
Python netCDF4 append an array array
After that I calculate for each point the cell it belongs to and append it to a Array of 4 dimensions : ` x ` , ` y ` , ` z ` and ` v ` : #CODE
Is there a solution to append an array to an array element so that I can access it later something like : #CODE
( and the same for ` YGridPoints ` , ` ZGridPoints ` , ` VGridPoints ` . ) And then you can set ` XGridPoints [ z_ind , y_ind , x_ind ]` to a numpy array and append to that array as you need .
I'd like to compute the dot product of ` R.x ` for each point ` x ` in ` A ` in-place .
You can multiply A with the transpose of the rotation matrix : #CODE
You can now use ` indices ` to randomize ` y ` with fancy indexing ` y_new = y [ indices ]` .
You could use the same ` indices ` to reorder your matrix , but be careful , CSR matrices don't support fancy indexing .
@USER Would it work for you to take the log of everything so that it becomes linear and then just use a normal change of basis ?
Or , if you don't know the initial size of your array ( you're using a buffer whose size may not be known straight away ) , define a temporary list ( say ` array_list `) and append your ` parameterArray ` line to it .
Finding of value from each column of numpy array is based on comparing of sum from another numpy array
-to count unique values along some axis in first array ` A ` ( for example , ` value ( 0 , 0 , 0 )` , ` value ( 1 , 0 , 0 )` , ` value ( 2 , 0 , 0 ))` : #CODE
-calculate appropriate sum from array ` B ` : #CODE
-find value in array A with maximum sum in array ` B ` : #CODE
Since you have to read each value to sum or compare , the algorithm is expected to have O ( n ) complexity ( where n is the total number of items in the matrix ) .
First , define a weighted sum array that holds sum values for each value in A on each Y-axis : ` W [ x ] [ z ] [ value ]`
Then use this weighted sum array to compute dominant values in A : #CODE
I can't just stack the arrays because the data in each array is spaced differently and because they have different shapes .
My idea was to " supersample " all grids to the spacing of the finest grid , stack and plot but I am not sure that is such a good idea as these grid files can become quite large .
you can write smth like ` loop ` which will be append each element to the matrix
As I described above , I can see via ` plt.imshow ` the results of all the manipulations I make but once each " patch " has been processed I want to join them together into one image .
Im currently computing an SVD on a large matrix ( an image , to be exact ) using numpy.linalg ' s svd function .
eig elapsed time : 870.060089
That is subtract the column mean from each element and divide by the column standard deviation ( std ) i .
` scipy.sparse.csc_matrix ` has a ` .mean() ` but is there an efficient way to compute the variance or std ?
Some searching suggested that it make be a form of stack overflow , so I tried to formulate a different solution using bicg , but ` new_vals , check = scipy.sparse.linalg.bicg ( A , b )` doesn't always seem to work .
numpy array - unravel_index of argmax v . argmax of sum
It's probably because the sum function adds up all the values in the array , giving you a single value for m [ not an array ] .
Right now I only want to tune up two parameters but the number of parameters might eventually grow so I would like to use a technique that can do high-dimensional gradient searches .
Is there some way I can roll my own gradient function ?
But never fear ; it sounds like you have a convex problem , and so you should be able to find a unique optimum , even if it won't be mathematically pretty .
Two options that I've implemented for different problems are creating a custom gradient descent algorithm , and using bisection on a series of univariate problems .
To implement gradient descent numerically ( without having an analytical method for evaluating the gradient ) , choose a test point and a second point that is ` delta ` away from your test point in all dimensions .
So do you dump the data out and use hadoop streaming to do this ?
I am trying to optimize ( memorywise the multiplication of X and its transpose X '
Does anyone know if numpys matrix multiplication takes into consideration that X ' is just the transpose of X .
In numpy convention , the transpose of ` X ` is represented by ` X.T ` and you're in luck , ` X.T ` is just a view of the original array ` X ` , meaning that no copy is done .
In fact , if I create aa from scratch instead of using transpose of a , #CODE
If you want to transform it into a structured array , you can either keep the original layout ( each block becomes a unique field ( ` a.view ( dtype =[( ' f0 ' , int )]`) , or transform your 2-block rows into rows of 1 larger block consisting of 2 sub-blocks , each sub-block having a ` int ` size .
I see , transpose only changes the display , but not the initial data structure .
numpy.amax() will find the max value in an array , and numpy.amin() does the same for the min value .
If I want to find both max and min , I have to call both functions , which requires passing over the ( very big ) array twice , which seems slow .
Is there a function in the numpy API that finds both max and min with only a single pass through the data ?
Even comparing a simple ` min ` implementation in fortran beats numpy's by a factor of approximately 2 ...
It'd be interesting to code up such a function by grafting together the numpy min / max functions and running some tests ....
I would guess that you could write Cython code to translate almost perfectly to what I wrote in fortran ( if you were familiar enough with Cython ) .
I tried Cython ( on a double array ) , it was maybe 1.3x slower then numpy itself but max 1.4 slower when doing minmax at the same time .
how much faster this is than doing min followed by max .
as of numpy 1.8 min and max are vectorized on amd64 platforms , on my core2duo numpy performs as well as this fortran code .
turning the else if into an if might be enough , min and max can be done without branching in hardware but as written the compiler will not do that
but I don't think there's a way to find both min and max with one traversal .
EDIT : ptp just calls min and max under the hood
It's annoying because presumably the way ptp is implemented it has to keep track of max and min !
Or it might just call max and min , not sure
@USER turns out ptp just calls max and min
When looking for the min and max simultaneously , it is possible to reduce the number of comparisons .
have you benchmarked this ? on modern x86 hardware you have machine instructions for min and max as used in the first variant , these avoid the need for branches while your code puts in a control dependency which probably does not map as well to the hardware .
Is there a function in the numpy API that finds both max and min with only a single pass through the data ?
I'm really surprised that the eig algorithm cannot diagonalize such a simple matrix !
You answer is correct in principle but if you do the dot product between those eigenvectors you find that they are not orthogonal !
When you have multiple eigenvalues you just have to provide linear independent vectors , not orthogonal ones .
>>> print max ([ np.dot ( evecs [: , i ] , evecs [: , j ]) for i , j in zip ( np.arange ( 64 ) / 8 , np.arange ( 64 ) %8 ) if ( np.abs ( evals [ i ] -evals [ j ]) > 1e-10 )])
If you want to find the indices of the entries satisfying your condition , you should indeed use #CODE
This will return a 3-element tuple giving the indices of the solution along the 1st , 2nd and third axis .
And if you want a series of indices like ` ( x , y )` , you just have to use something like #CODE
( The ` squeeze ` is introduced to collapse the ` ( N , M , 1 )` array into a ` ( N , M )` array )
This time , ` indices ` will only be a 2-tuple , and you can get the ` ( x , y )` couples with the ` zip ( *indices )` presented earlier .
Well , with ` a= np.arange ( 16*16*4 ) .reshape ( 16 , 16 , 4 )` , ` np.where ( a == [ 956,957,958,959 ])` gives you the tuple ` indices =( array ([ 14 , 14 , 14 , 14 ]) , array ([ 15 , 15 , 15 , 15 ]) , array ([ 0 , 1 , 2 , 3 ]))` , as expected ...
But I'd also often recommend unrolling loops like ` max ` into inline Cython functions as the overhead is often way lower .
Use ` np.where ` to generate two arrays of indices for the positive and negative ` y ` values , then use those to index into your sparse matrix .
Just a single pass through the values array with col indices and row pointers suitably updated .
The second is a 1d array , which can be thought of as a list of indices into the 2d array .
I want to select elements of the 2d array using the indices of the 1d array .
Then , fill the first column with the indices you want , and the last columns with your initial array : #CODE
is there a way I can make only the first one i% and then all consecutive ones % 10.6e ?
Y'all , we should keep in mind it's a bad idea to append to a ` np.array ` .
You should just create a basic list ` result =[ ]` , append to this list , ** then ** create an array and take its mean .
I used the numpy std and the warning did not show at all
There were some comments on the numpy / scipy lists about inconstencies between packages such as different default arguments for the ` var ` , ` std ` ... functions / methods .
In this particular case , it stems from inconstencies between NumPy and SciPy on the default arguments for the ` var ` , ` std ` ... functions / methods .
@USER updated to show how to retrieve indices ...
Numpy recarray sort of columns and stack
Even better would be a function that does this for you - and accounts for missing columns - when you append a recarray to another .
I am trying to stack arrays horizontally , using numpy hstack , but can't get it to work .
Is there someway that I can identify the indices concerned and just call them ?
You can easily shuffle the indices to get whatever you're after , though : #CODE
How can you sum the squares of each element in each row , and save them into a list ?
How can you turn that into a list of sum of squares of each row : #CODE
First of all , the csr matrix has a ` .sum ` method ( relying on the dot product ) which works well , so what you need is the squaring .
The simplest solution is to create a copy of the sparse matrix , square its data and then sum it : #CODE
Elementwise operations are thus easily done by accessing ` csr.data ` which stores the values for all nonzero elements .
Like the other one it only makes Y as large as the nonzero elements of Z , it does not densify Z .
I have profiled my script with ` cProfile ` and what takes 80% of the processor time is the NumPy ` dot ` function , the rest is matrix inversion with the function ` numpy.linalg.solve ` .
These computations involve a matrix multiplication ( dot product ) in a 55000 iterations loop , with a 500*500 and 1000*1000 matrix .
So I installed the Ubuntu OpenBLAS packages and linked it so that it is now used by numpy ( is it how to install it ? because I didn't have to compile anything ... ) and it is now more than twice faster without the scipy.sparse dot function .
It shouldn't replace the data , just join it into one data type .
taking the log of every element in a sparse matrix
How can you take the log base 10 of every element in a sparse matrix ( COO ) ?
` im.tostring() ` will dump the image as a byte string .
Maybe the error is just the variance of ` sum ( f ( x )) / n ` ?
Since ` B ` is repeated automatically with the ` putmask ` function , you should not even have to broadcast it .
If you have two numpy matrices , how can you join them together into one ?
z [ k ] = sum_n a [ n ] * conj ( v [ n+k ])
z [ k ] = sum_n a [ n ] * conj ( v [ n-k ])
The only reason I can think of : in a tf-idf sparse matrix of mainly 0s , there is strong overlap between texts because so many terms are 0s - the regression then thinks the texts correlate highly .
if it does bad on the brand new ones it is overfit ..
In all files for your extension , define ` PY_ARRAY_UNIQUE_SYMBOL ` to a unique variable that is unlikely to conflict with other extensions .
If you select a list of actual fitness entries , ` indices ` , the corresponding points are given by ` A [ ' point '] [ indices ]` , which is a simple ` ( n , 3 )` array .
What I would like the last part of the list comprehension to do ( or any other method that works ) is to provide an array that matches the eqn_out list with it's corresponding unique original value ; i.e #CODE
Use these indices for selection purposes : #CODE
Be warned that the dictionary approach assumes that the numbers in ` m_out [: , 1 ]` are unique , so they make sense as dictionary keys .
pay attention when adding and save min at that point ?
numpy / pandas : How to convert a series of strings of zeros and ones into a matrix
What's the most straightforward way to turn the string of ones and zeros into something like a np.array , so that I can run it through the classifier ?
Since you asked primarily for a way to convert a string of ones and zeros into a numpy array , I'll offer my solution as follows : #CODE
I tried ` numpy.cumprod() ` , but that is cumulative product , not the sum of the cumulative squares of the values .
If it were , the factorization would not be unique , e.g. 10 == 2*5 == 1*2*5 == 1*1*2*5 == ...
Numpy array.resize() - zeros ' first '
I can use ` array.resize ( shape )` to resize my array and have zeros added to those indices without any value .
How can I append / pad the zeros to the front , yielding ` [ 0 , 1 , 2 , 3 , 4 ]` ?
That is to reverse the array , resize , and reverse again .
@USER The first reverse needs to make a new in-mem copy though for resize to work .
You cannot avoid a copy " at all costs " , since the ` resize ` method might have to make a copy .
I see no reason why ` numpy ` would need to make a copy for an operation like this , as long as it does the necessary checks for overlaps ( though of course as others have noted , ` resize ` may itself have to allocate a new block of memory ) .
You could try working on an array with negative strides ( though you can never be sure that resize may not have to make a copy ): #CODE
The autocorrelation of a vector ` x ` has to be 1 at lag 0 since that is just the squared L2 norm divided by itself , i.e. , ` dot ( x , x ) / dot ( x , x ) == 1 ` .
= j ` the unit-scaled autocorrelation is ` dot ( shift ( x , i ) , shift ( x , j )) / dot ( x , x )` where ` shift ( y , n )` is a function that shifts the vector ` y ` by ` n ` time points and ` Z ` is the set of integers since we're talking about the implementation ( in theory the lags can be in the set of real numbers ) .
I get 1.0 as the max with the following code ( start on the command line as ` $ ipython -- pylab `) , as expected : #CODE
The only time when the lag 0 autocorrelation is not equal to 1 is when ` x ` is the zero signal ( all zeros ) .
It didn't even work , since bson module doesn't have a ` dumps ` function .
Now I want to construct two slices ` B ` and ` C ` of ` A ` by selecting the indices ` 0 , .., n-1 ` and ` 1 , ..., n ` respectively along the axis ` axis ` .
For example , I'm not sure if you want to find the six columns which give the maximum total if you sum over those columns for each row ( which is very easy ) or something else .
Finally , use ` max ` to pick off the sextuple with the best score : #CODE
` flatten ` your array , then build a ` collections.Counter ` from it .
To find the most frequent value of a flat array , use ` unique ` , ` bincount ` and ` argmax ` : #CODE
To work with a multidimensional array , we don't need to worry about ` unique ` , but we do need to use ` apply_along_axis ` on ` bincount ` : #CODE
You can approximate ` np.unique ( return_inverse=True )` reasonably efficiently using ` np.searchsorted ` ( it's an additional O ( n log n ) , so shouldn't change the performance significantly ): #CODE
That means that you cannot transform your 2-int records into 4-int ones as you want .
I'd like to write a function that normalizes the rows of a large sparse matrix ( such that they sum to one ) .
transpose A
calculate sum of each col
format diagonal matrix B with reciprocal of sum
transpose C #CODE
What's the best way of retrieving the rows in Z using the ORIGINAL indices from X .
That is , how can you retrieve rows from Z , using indices that refer to the original rows position in the original matrix X ?
If you have the original indices in an array ` i ` , and the values in ` i ` are in increasing order ( as in your example ) , you can use numpy.searchsorted ( i , [ 0 , 3 ]) to find the indices in Z that correspond to indices [ 0 , 3 ] in the original X .
This basically means deleting one text , as only 52 row indices in the ` train_range ` .
You're unlikely to get much faster than using an fft based correlation method .
As for the speed of correlation , you can try using a fast fft implementation ( FFTW has a python wrapper : pyfftw ) .
The other point is that , although the non-diagonal elements are many orders of magnitude smaller than the diagonal ones , this seems to be causing some problem in my algorithm .
If you think you need additional precison , you can use ` np.longdouble ` ( Same as ` np.float128 ` on a 64-bit system and ` np.float96 ` on 32-bit ) , but this may not be supported on all platforms and many linear algebra functions will truncate things back to the native precision .
The off diagonal terms in row and column 2 are different , but it has the same 0s elsewhere .
With the double dot , ` P.dot ( A.T )` creates rounding errors when it adds the products .
Those propagate into the next ` dot ` .
What is the most efficient way to produce an array of 100 numbers that form the shape of the triangle wave below , with a max / min amplitude of 0.5 ?
However if I use matlab R2011a , and did ` M=rand ( 1000 ); tic ; svd ( M ); toc ` , it only takes typically 0.68 seconds .
Numpy's ` la.svd ( M )` is calculating U , S , and V , while Matlab's ` svd ( M )` is calculating only S .
Try comparing to ` la.svd ( M , compute_uv=False )` or ` [ U , S , V ]= svd ( M )` .
When I compared la.svd ( M ) and [ U S V ]= svd ( M ) python takes ~ 1.1 seconds and matlab takes ~ 1.07 , quite close to each other .
You need to expand path to absolute path like this : #CODE
In both cases , you can access individual elements by indices , like ` R [ 0 ]` ( which would give you a specific object , a ` np.void ` , that still gives you the possibility to access the fields separately ) , or by slices ` R [ 1 : -1 ]` ...
You could also use record array , which are basic structured array with the ` __getattr__ ` and ` __setattr__ ` methods overloaded in such way that you can access the fields as attributes ( like in ` R.val `) as well as indices ( like the standard ` R [ ' val ']`) .
use " " . join ... but you need to map your floats to string inside the join #CODE
now type ` echo %path% ` try and log in as a normal user instead of an admin ... since afaik it requires elevated permissions to change your path
Any idea why using numpy.savetxt it saves the -- values as zeros ?
I have an array which contain integer numbers , and I have a list with the unique values that are in the array sorted in special order .
How can you read the whole file into an array of arrays so that you can plot all data sets afterwards to the same picture with a ` for ` loop looping over the outer array ?
another possibility : use correlate or convolve #CODE
Let's sum along the rows #CODE
That gives us an array of size ` ( x.size-N +1 )` of values between ` -N ` and ` +N ` : we just have to find where the absolute values are ` N ` : #CODE
` indices ` is the array of the indices ` i ` of your array ` x ` for which the values ` x [ i : i+N ]` are on the same side of the mean ...
Perhaps you shouldn't use a dense numpy array to encode your function ?
I am working with the python scientific computing stack ( scipy , numpy , matplotlib ) and I have a set of 2 dimensional points , for which I compute the Delaunay traingulation ( wiki ) using ` scipy.spatial.Delaunay ` .
The three entries in each row are the indices of the vertices of that simplex in the points array we just saw .
The following code demonstrates how to get the indices of some vertex ( number 17 , in this example ): #CODE
The ` np.eye ` function directly gives you a 2D array with 1 on the diagonal and 0 elsewhere .
Now I really don't know why you want a boolean mask , these indices can be applied to z to give back x already and are more compact .
If they are unique , sort them yourself first , if you care about speed .
Surprised no one mentioned the ` outer ` method of ` numpy.equal ` : #CODE
Use nonzero .
As suggested in your previous question , you can really easily find the indices for which ` y ` is different from 6 #CODE
You could also get the indices for which ` y !
You don't really need the indices , you can just directly index with a boolean array .
If you really want the indices , you can use ` np.nonzero ( y ! =6 )` , but you'll still be using fancy indexing that your ` X ` matrix won't support ...
The goal will be to extract a list of unique dates from column 1 , and for each date extract the rows with that date in column 1 ( column one is ordered ) .
So in matlab I can get the list of dates by ` unique ( AllData (: , 1 ))` and then I can get the records ( rows ) corresponding to that date ( i.e. with that date in columns one ) like this : #CODE
to take your ` dates ` array and filter them to find the unique elements .
to find the indices of these unique elements , as an array of integers we'll call , say , ` matching ` ;
Python Pandas : How to broadcast an operation using apply without writing a secondary function
And if NumPy happens to broadcast that , then it will work .
I then flatten the arrays so that the covariance can be properly calculated across the set , so the input array then looks like this : #CODE
Are you saying eigenvalues should be converted to absolute values before sorting ?
Eigenvalues can be positive , negative or complex and , of course , we are interested in their absolute value ( in case of complex eigenvalues it's non-trivial thing ) .
If N = 1000 , lambda1 = 1 , lambda2 = 0.99 , lambda1^1000 is ~100000 greater than lambda2^1000 , thus coefficient for the eigenvector1 is much much greater ( in absolute value ) than coefficient for eigenvector2 .
Still , eigenvectors are arranged in the order of decreasing respective absolute eigenvalues .
numpy.dstack stack the array along the third axis , so , if you stack 3 arrays ( ` a ` , ` b ` , ` c `) of shape ` ( N , M )` , you'll end up with an array of shape ` ( N , M , 3 )` .
You more likely to want the vector sum .
I first flatten the 3-D arrays into 2-D so that I can calculate covariance ( and then eigenvalues and eigenvectors ) .
But we can use the fact that the arrays have the same number of lines : let's reshape the array !
If ` data ` has a shape of ` ( N*m , d )` , where ` d ` is the number of columns of each file , you could reshape with : #CODE
When you think about it , meshgrid is a bit superfluous for numpy arrays , as they broadcast .
I'm afraid that I can't describe the problem so I draw a sketch of it.Anyway , what I need is to find the max values along the 0th axis in a numpy ndarray , i.e.array.shape ( 5 , 5 , 3 ) , and their corresponding " layer numbers " , and use the " layer numbers " to create a new 2d array with shape of ( 1 , 5 , 3 ) .Hope I'm giving a clear description here .. thanks a lot .
However , there's a function ` argmax ` that gives you the indices of the maxima along a given axis : #CODE
And how can I get the " layer numbers " of the max values ?
also the norm of difference from the true result is 3.13014997999 .
Here is a solution that takes O ( n log n ) time .
The rough reason I think it's optimal is because the information of all of the percentiles is essentially equivalent to the information of the sorted list , and you can't get better than O ( n log n ) for sorting .
As Kevin said , optimal solution works in O ( n log ( n )) time .
If you have a ` dset ` like that , and you want to just get the ` 1 ` values , you could use ` nonzero ` , which " returns a tuple of arrays , one for each dimension of ` a ` , containing the indices of the non-zero elements in that dimension .
and find where the nonzero elements are located : #CODE
If we wanted a more complicated cut , we could have done something like ` ( d 3.4 ) .nonzero() ` or something , as True has an integer value of 1 and counts as nonzero .
I want a function to give me integer values for the indices .
In general you will only have exactly logarithmically spaced indices when ` ( num+1 )` is a power of 2 .
Technically , exact integer logarithmic indices are possible if ` array_size ** ( 1 /( num-1 ))` is an integer ( assuming indices start at ` 1 ` and end at ` array_size `) .
? log ( 1 ? 10 ? K ) / K log 10 ?
You could however use NumPy / SciPy which does provide vector classes that have special functions that implement norm and scalar products etc ., possibly an angle function , too .
The formula OP is looking for is ` from numpy.linalg iport norm ; from numpy import dot ; np.arccos ( dot ( a1 , a2 ) / ( norm ( a1 ) * norm ( a2 ))`
The first argument to ` ones ` , the shape , should be a tuple .
Change ` ones ( N , 1 )` to ` ones (( N , 1 ))` .
I really doubt that you'll do any better without writing a compiled extension to combine ` concatenate ` ` unique ` and ` sort ` .
You can at least drop the ` sort ` as the output from ` unique ` is guaranteed to already be sorted .
Traditional ` list ` insertion complexity is ` O ( n )` , while ` blist `' s complexity on insertion is ` O ( log ( n ))` .
In ` blist ` they use another data structure , so that complexity is ` O ( log ( n ))` .
IE . first sort ( in-place ) then do the ` np.unique ` method by hand ( check also its python code ) , with ` flag = np.concatenate (([ True ] , ar [ 1 :] ! = ar [: -1 ]))` with which ` unique = ar [ flag ]` ( with ar being sorted ) .
This would make the full thing close to what you got ( without doing a unique beforehand ): #CODE
If you roll these suggestions into a function , I'd be happy to add it to the timeit framework I posted in my answer .
But for the ` sin ( 1.2 + x )` part , as ` x ` is already a ` ndarray ` , you should use ` np.sin ` .
You now have to remember that ` sin ` is the NumPy function , not its ` math ` counterpart .
This should open a plot with a diagonal straight line .
If you use a list of booleans instead of an ` ndarray ` , numpy will convert the ` False / True ` as ` 0 / 1 ` , and interpret that as indices of the rows / cols you want .
I also wouldn't use the name ` xT ` ; it implies to me that it's the transpose of ` x ` , which is false .
at E = exp ( - .5 * dot ( c.T , dot ( InvSig , c )))
Get the number of nonzero elements in a numpy array ?
Is it possible to get the length of the nonzero elements in a numpy array without iterating over the array or masking the array .
If it changes the answer , each row will begin with zeros .
The array is filled on the diagonal with zeros .
` np.nonzero ` returns a tuple of indices , whose length is equal to the number of dimensions in the initial array
we get just the indices along the first dimension with ` [ 0 ]`
does not work for me : ` AttributeError : ' tuple ' object has no attribute ' sum '`
Assuming you mean total number of nonzero elements ( and not total number of nonzero rows ): #CODE
This last one , ` count_nonzero ` , seems to behave well when the array is small , too , whereas the ` sum ` trick not so much : #CODE
for each row r of sqrt ( a ) ( or is it each column ? ) and r / sqrt ( a ) can usually be translated to numpy as #CODE
The matrices sqrt ( a ) and a are just examples .
The matlab code is exactly equivalent to ` sqrt ( a ) .
/ a ` , i.e. it divide each element of ` sqrt ( a )` by the corresponding element of ` a ` ( it is also equivalent to ` 1 . / sqrt ( a )`) .
The ` / ` operator in Matlab is right * matrix * division , i.e. ` A / B ` is equivalent to ` A * inv ( B )` .
When I take the FFT of my template , I need the result to be padded to the same size as my search image so that I can convolve them .
So it looks like the buffer is initializing to all zeros , which I can then write to .
So in ` generate_points ` you could ` for val in pair.ravel() : yield val ` , pump that into ` np.fromiter ` , and use ` reshape ( size , -1 )` on the other side to obtain your desired 2D array .
You won't be able to create a 2D array that way , and @USER method of returning a 1D array that you reshape afterwards is a sure go .
It might be nicer to just construct a list and use ` concatenate ` .
But yes , in that case , it'd be easier to use ` concatenate ` on a list .
working with flatten simply allocates every tuple into a single array .
Philip's answer is good , but will only create a couple of processes ( one reading , one computing ) which will hardly max out a modern > 2 core system .
I need to sum
euklidean distance ) for each possible distance from 0 to the edge of the image , i.e. result is an 1D array where the 0th element gives the sum of pixels in distance 0 from center ( i.e. just the center ) , 1st element is the sum of all pixels at distance 1 pixel and so on .
@USER you can use this approach , then SORT the values of ` r ` , then sum the values inside each " bin " ( bins found using ` numpy.digitize ` , for example ? )
Get the indices : #CODE
And ` result ` is an array with ` result [ distance ]` giving sum over all values with smaller or equal distances .
Take the sum of every point INSIDE a given circle , for circles increasing linearly ;
Calculate the value for the " shell " between each circle by subtracting the inner circle from the outer circle .
I'd simply use strip ( ' 0 ') to remove trailing zeros .
To do this I am retrieving column names and row count information from the data table and using this to create numpy zero record arrays which are used to create / append to the quality control pytable table .
The problem occurs when I append the numpy zero record arrays to the new ' mask ' pytable .
Even though the numpy zero record arrays that are used to create / append to the pytable are the correct size - the resulting pytable is considerably larger than expected - with more rows in the pytable then had been appended from the numpy recarray .
Glad you were able to solve your problem , but ( 1 ) your comments will last : ' newest one ' will have a different meaning when the next version of numpy will be released , so use absolute version number instead ; ( 2 ) the " i did that but it didn't work " doesn't tell us * why * it didn't work in your case , or if it * actually * worked but not as * you * expected it .
Instead of creating a ` ( #URL ( waveforms ) , len ( waveforms ))` array of zeros , you may want to make ` savez ` a basic list ` [ ]` .
Then , when you loop on your ` waveforms ` , just append the new ` z ` : #CODE
I actually tried something like that , but I get an error message : ` AttributeError : ' numpy.ndarray ' object has no attribute ' append '`
Following this trick to grab unique entries for a NumPy array , I now have a two-column array , basically of pairs with first element in the range [ 0.9 : 0.02 : 1.1 ] and the second element in the range [ 1.5 : 0.1 : 2.0 ] .
You need to transpose ` A ` before passing it to lexsort because when passed a 2d array it expects to sort by rows ( last row , second last row , etc ) .
So I would only add as a suggestion that the unique part can be done most efficiently after sorting .
@USER -- but the ` lexsort ` solution also creates an intermediate array of indices which I think would be comparable in size .
Just replace the whole thing ( including the unique part with ) for ` A ` being 2 D: #CODE
Edit : A different view ( with much the same result ) , but a bit more elegent maybe as no reshape is needed : #CODE
OK , I liked this method , but appearently at least at the moment ` np.lexsort ` is simply faster then ` np.sort ` on a recarray , so if efficiency is of extreme importance , I would say lexsort + custom unique ( unique is very simple after sorting , check its python code ) should beat everything by a lot .
For example , if you want the indices of the entries of ` names ` for `" Unicorn Lane "` : #CODE
( the ` np.nonzero ` should return a tuple with one element , an array of indices ) .
You can use those indices as identifiers , keeping in mind that they start at 0 when your identifiers start at 1 .
So , to select the ` numbers ` for which the ` Ident ` matches the indices you had found : #CODE
If you use an in0memory version it will still be a few orders of magnitudes slower than ` boost.icl ` ( from experience coding other data structures vs sqlite3 ) but should be more effective than using a c++ ` std :: vector ` style approach on top of python containers .
You may of course vectorize even much more complicated ones .
The builtin ` int ` function on the other hand does truncate toward zero .
The binary mask created can be used nicely to get back the original slices with ` a [ ~mask ]` however this is only the same if the original indices were sorted .
At the moment however its slower then the above , and will create unexpected results with negative indices ( or steps when given a slice ) .
( Note that behind the scenes , ` numpy.delete ` just uses [ ` setdiff1d `] ( #URL ) which in turn uses [ ` in1d `] ( #URL ) . So it's also n log n . ) Would +1 but you already have mine !
Assuming that ` a ` is a 1D array , you could just pop the items you don't want from the list of indices : #CODE
The idea is to use fancy indexing on the complementary of the set of indices you don't want .
Which has the benefit of working in N-dimensions , with continuous or not indices .
then the column wise range max ( col ) - min ( col ) . this is easy again : #CODE
I'm trying to return a ( square ) section from an array , where the indices wrap around the edges .
If you have for every dimension an integer array these are broadcasted together and the output is the same output as the broadcast shape ( you will see what I mean ) ...
Note that this is a 3x3x2 array , while you had a 9x2 array , but simple reshape will fix that and the 3x3x2 array is actually closer to what you want probably .
Actually the surprise is still hidden in a way , because in your examples ` a [ indices ]` is the same as ` a [ indices [ 0 ] , indicies [ 1 ]]` but ` a [ indicies , :] ` is ` a [( indicies [ 0 ] , indicies [ 1 ]) , :] ` which is not a big surprise that it is different .
This basically uses the filesystem to do a matrix transpose .
And to make things even weirder , if one is only indexing the matrix ` C ` ( as opposed to assigning to it ) , it seems using slice indices or a list just return the same : #CODE
Take your matrix of observations ( right hand side variables , where column 1 is for the first variable , column 2 for the second , etc . ) and prepend a column of all ones , such as ` numpy.ones ( N )` where ` N ` is the number of observations in your regression .
I already extracted the feature size by calling " sum ( array [ array > 0 ])"
I want to get the size of the features ( already solved , quite easy via ndi.sum() ) as well as the number of nonlabeled cells in direct vicinity of the feature ( ergo counting the number of zeros around the feature ) .
How to flatten a numpy ndarray along axis ?
The problem is that the " frequency " is not what I want.I want the data array to be flattened along axis-zero , so that the " frequency " would be the 30 values at one location.Is there such a function in numpy to flatten ndarray along a particular axis ?
I guess what you actually wanted was just ` transpose ` to change the axis order .
Depending on what you do with it , it might be useful to do a ` .copy() ` after the ` transposed ` to optimize the memory layout , since transpose will not create a copy itself .
You can also use ` reshape ` which is more powerful then the simple ` ravel ` ( return shape can be any dimension ) .
If your y-values aren't monotonic , there is no guarantee that your mapping of y -> x is unique .
So to me , I see Z as a diagonal matrix where the diagonal contains the data on the diagonal is the values found in DATA corresponding to the index of LATS and LONS .
Probably you need to reshape your x , y and z arrays from 1D arrays to N-D arrays ( where N is > 1 ) .
@USER , I don't understand why I need to reshape the data .
From the documentation , ` numpy.diag ( v , k=0 )` extracts a diagonal or construct a diagonal array .
That should be why you only get a " diagonal area " of values ...
Is it possible to effectively obtain the norm of a sparse vector in python ?
The norm function only works with arrays so probably that's why the csr_matrix is not working , but then I didn't find another way of computing the norm effectively .
And as the last approach I could iterate through each element of the vector and compute manually the norm , but since efficiency is really important I was searching for something faster and easier to implement .
For norm to work , you need to have a square array .
The problem is that I need to compute the norm of vectors , not matrices .
numpy.linalg.norm does a matrix norm , not a vector norm .
Is there a reason to nor just do ` code sqrt ( sum ( i**2 for i in data ))` ?
Calculating a norm by hand is very simple , by just using the underlying data ` vector1.data ` directly .
There doesn't seem to be a way to directly do elementwise exponentiation on sparse matrices , but ` ( vector1.data ** 2 ) .sum() ` is probably the most direct way to do the norm by hand .
True , but using the dot product is just as good or multiply+sum .
The only reason I'd think that dotting or multiplying might be a little slower is that it then has to consider the indices , because it doesn't magically know that the elements are " lined up " perfectly .
The method using sqrt ( vector.multiply ( vector ) .sum() ) used 874us and my function 205us .
You can also use sqrt (( vector.data **2 ) .sum() )
Can't save a dict to mat file
I'm trying to save a dictionary whose values are numpy arrays of different length to a mat file so that collaborators can load the dictionary as a cell array in MATLAB .
How can I save this dictionary to a mat file ?
There are several functions that let you concatenate arrays in different dimensions :
Here , we created an empty array with an extra column , then used some tricks to set the first columns to ` a ` ( using the transpose operator ` T ` , so that ` new.T ` has an extra row compared to ` a.T ` ... )
You want to ` reshape ` the array .
As it says in the ` TypeError ` , ` min ` ( and related functions ) will only work on instances of ` np.ndarray ` ; thus , the new subclass must inherit from the class you are trying to wrap .
You may have to sum the array before returning and this could introduce bad behaviour e.g. an array containing equal +ve and -ve values .
If you have exactly one slice with it , you could just do ` reductions = reductions [: -1 ]` ( if its the last one ) , but otherwise you will simply need to append a value to ` a ` to trick ` reduceat ` : #CODE
The mask is ` False ` ( i.e. values not masked ) for those indices that are ` = ` to the start value and ` ` the end value .
Sebastian , I'm sure you can use a ` np.diff ( a [: , 0 ])` to find out where the indices changed ...
The idea is to sort the array once and then you can easily get the indices of the medians just by counting the indices in the first column of a : #CODE
I know the problem is i am overriding the y1 ( which i initially created with zeros ) with value of x1 causing problem .
I would write a little function that pads an array with zeros to the size you want , so that you can always pass arrays of equal length to ` SigAdd() ` .
@USER I think the problem is that this is a 2-dimensional array : ` y1 = zeros ([ 1 , len ( n )] , int )`
i think it's a fairly reasonable question . there are plenty of similar ones e.g. #URL #URL
If the sum is performed over the other axis , the computation on the C ordered array is faster : #CODE
I am calculating the absolute difference between two ` numpy.uint8 ` s in python with
I want to count the number of adjacent cells ( which means the number of array fields with other values eg . zeroes in the vicinity of array values ) as sum for each valid value !
I somehow believe to must take use of the binary_dilation function of SciPy , which is able to enlarge the value structure , but simple counting of overlaps can't lead me to the correct sum or does it ?
I think you already got it . after dilation , the number of 1 is 19 , minus 5 of the starting shape , you have 14 . which is the number of zeros surrounding your shape .
Consider a 1 surrounded by 8 zeros .
You were counting surrounding zeros * without * overlaps .
` b = 1-a ` allows us to count each zero while ignoring the ones .
We convolve with a 3x3 all-ones kernel , which sets each element to the sum of it and its 8 neighbouring values ( other kernels are possible , such as the ` + ` kernel for only orthogonally adjacent values ) .
With these summed values , we mask off the zeros in the original input ( since we don't care about their neighbours ) , and sum over the whole array .
In order to find the duplicates in the dataset I put the indices into a numpy structured array , sort the array , create another array from the unique values and then compare the lengths of the two arrays : #CODE
Now , what I would really like to do is determine the indices that contain the duplicates .
I've looked into using unique and difference functions , but those don't seem to do the job .
With numpy version 1.9.0 or later , we can use ` np.unique ` to get the unique elements , with the argument ` return_counts=True ` so that the number of occurrences of each unique element is also returned #CODE
For older versions of numpy , one can use ` np.unique ` with the argument ` return_inverse=True ` to also get the array that shows how to recreate ` x ` from the array of unique elements : #CODE
@USER Yes , as of numpy version 1.9.0 , the ` unique ` function has the argument ` return_counts ` .
I am using numpy library for doing simple IRR calculations using the irr function .
If you premultiply both sides by the transpose of A , you'll have an equation with a 3x3 matrix that you can solve for the coefficients you want .
project each ( x , y ) pair into its corresponding pixel , identified via two ( integer ) indices : ` x_i , y_i = tuple ([ int ( c // spacing ) for c in ( x , y )])`
add the value ` v ` to the other array at place ` ( x_i , y_i )` ( holding the sum of values )
The problem is that I want to loop through the list and group all instances where var > =0 and var .1 then append values into a new list .
So i need to take these occasional points and resize them to the standard ( 1 , 2 ) ( and then generate a point based of a distribution but that isn't the trouble ) .
The resize function returns an array so I'm confused as why setting the element of list equal to that array does not work .
When you call ` append ` on the element then you are actually modifying the object .
Given a 2D numpy array ` data ` , and an array ` poly ` of polygon coordinates ( with shape ( n , 2 )) , this will draw a polygon filled with the value 0 in the array : #CODE
` means dot product of two matrix .
If you use a list of ` True / False ` , NumPy will interpret that as a list of ` 1 / 0 ` as integers , that is , indices , meaning that you ' either get the second or first element of your array .
Grouping DF by ' C ' and aggregating with np.mean ( also sum , min , max ) produces column-wise aggregation within groups : #CODE
However , when you give in ` np.median ` numpy arrays do not have the ` .median ` attribute , so it does the normal numpy machinery , which is to flatten the array ( ie , typically ` axis=None `) .
For things like sum , mean , median , max , min , first , last , std , you can call the method directly and not have to worry about the apply-to-DataFrame-but-failover-to-each-column mechanism in the GroupBy engine .
In the Python's standard ` max ` function ( I also can pass in a key parameter ): #CODE
With a larger ( multi-dimensional ) array , I can not longer use ` max ` , so I tried to use ` numpy.amax ` , however I can't seem to be able to use ` amax ` with strings ...
` a ` is a numpy array and ` a.T ` is it's transpose .
In the Python's standard ` max ` function I can pass in a ` key ` parameter : #CODE
With a larger ( multi-dimensional ) array , we can not longer use ` max ` , but we can use ` numpy.amax ` ... which unfortunately offers no ` key ` parameter .
Imagine that instead of the sum is " the sum after clipping the 10% lower and higher of x "
Don't really know my python though :p And swapping all zeroes between the first and last ones in a row / column wouldn't give correct results for all matrices ( I'm thinking of cases where there are an uneven number of ones ) .
` transpose ` does not change the number of dimensions of the array .
It's for including only the data outside the mask in calculations such as sum , mean , etc .. scientific statistical applications .
Then you'll need to modify the shape so numpy broadcasts in the correct dimension , then broadcast it into the 3rd dimension : #CODE
And I want each row to contains more elements by filling zeros : #CODE
I know there must be some brute-force ways to do so ( say construct a bigger array with zeros then copy elements from old smaller arrays ) , just wondering are there pythonic ways to do so .
You should use ` np.column_stack ` or ` append ` #CODE
And a comparison with ` np.c_ ` and ` np.hstack ` [ append still seems to be the fastest ]: #CODE
and ` np.concatenate ` [ that is a even a bit faster than ` append `] : #CODE
@ wim -- edited with hstack , append still the fastest .
strange . you might find ` np.concatenate (( a , z ) , axis=1 )` to be faster than append
yep , this is because append does some extra logic ( which is not actually needed here ) before simply calling concatenate .
There are also ` np.resize ` and ` np.ndarray.resize ` , but they have some limitations ( due to the way numpy lays out data in memory ) so read the docstring on those ones .
By the way , when I've needed to do this I usually just do it the basic way you've already mentioned ( create an array of zeros and assign the smaller array inside it ) , I don't see anything wrong with that !
Note that there's nothing at all with your initial suggestion of creating a large array ' by hand ' ( possibly filled with zeros ) and filling it yourself with your initial array .
It's far more efficient to apply the same function at once on a larger array than three times on smaller ones ...
Is there a fast , best or built-in way to hash the labels and end up with an n-dimensional array of values and n lists telling you how to map the labels values to indices ?
but I couldn't get vectorize to properly translate the coordinates into a linear fashion .
It complains that I have too many indices .
You have to transpose your array ( amended )
So a sort of diagonal slice .
However I want a semi diagonal slice , which might be [ 6 , 5 , 3 ] ( the 3 coming from the 2nd row ) .
Added ... the reason the x and y coords are separated is because I use an outer product multiply elsewhere to generate the x's and the y's .
You really don't need to play with indices so much .
According to the documentation of StratifiedShuffleSplit , for it to work with matrices , I should pass True to the parameter indices , but it doesn't help .
Python Running cumulative sum with a given window
What I want to do is generate a numpy array that is the cumulative sum of another numpy array given a certain window .
For example , given an array ` [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]` let's say I want a cumulative sum with a window of 3 .
I have a relatively large numpy array and would like to do a cumulative sum with a window of 400 .
windowedcumOFI = [ sum ( OFIs [ i- ( w-1 ): i+1 ]) if i > ( w-1 ) else sum ( OFIs [: i+1 ] for i in range ( len ( OFIs ))]
You could probably do something like this with a combination of convolve and cumsum also .
I think ` [ sum ( lis [ max ( 0 , i-w +1 ): i+1 ]) for i in range ( len ( lis ))]` is a bit neater than using that ternary construct .
So you could use convolve or ` stride_tricks ` based approaches ( these are not obvious , but solve these things nicely ) .
The other approach of course would be to use convolve : #CODE
I don't know if you want the non full windows too , this would be the same as using ` mode= ' full '` ( default ) for the convolve .
You're fundamentally doing many fewer calculations that explicitly re-calculating sum for each window .
I'm trying to translate some Matlab code to python , and I get different answers from scipy's qr and Matlab qr ( mainly with the signs of the answer ) , so I figured I would implement my own version of qr
@USER QR decomposition is not unique .
For an invertible , square matrix uniqueness follows if the diagonal elements of R are positive .
For a rectangular matrix , a similar result holds ( well , some part of the Q matrix is not unique in general ) .
You should * really * use ` np.linalg.qr ` , which is just a Lapack wrapper like the matlab qr function .
If anyone knows how to translate the above code correctly into cython I'm willing to give it another shot , otherwise I might need to think about other tricks to improve the speed .
You need to convolve your data with the template .
The fast fourier transform convolve does give me some answer in 0.5 seconds , but I don't exactly understand how to intrepret it .
Scipy / Numpy : summation over multiple indices
Suppose I have an expression of which I need to find the sum :
What is the fastest or most efficient way to go about calculating such a sum in scipy / numpy .
` np.einsum ` may be an option too for these kind of sum .
Which should in this case however should be slower then nevsan's ` np.dot ` based approach , since ` dot ` should normally be better optimized ( ie . ` np.einsum ( ii- , a , b )` is slower then ` np.dot ( a , b )`) .
However if you have more arrays to sum over , it may be a nice option .
One solution is to sort both arrays ( adding an index column so that the sorted arrays still contains the original indices ) .
You might improve things by reversing the order of iteration ( iterate on ` coordMED.T ` , the transpose of ` coordMED ` ... )
You can just transpose it ...
In a more general case you could use a ` join ` : #CODE
Why ndimage.label ( image ) labels extra shapes among the considered ones ?
( 3 ) the pattern of nonzero values in the shape drawn with the smooth brush looks like : #CODE
When I run ` label ( img )` on the file ` twozg.png ` using the default structure ( the + structure ) , I get 6 features , because the brush shape has one pixel that is only connected to the rest via a single diagonal .
What does the pattern of nonzero values look like in the shape drawn with the smooth brush ?
If there are many zeros in there , ` label ` will find many disconnected features .
Sure , for my example the full 3x3 structure results in one feature , but in general , it depends how sparse the zeros are in the shape drawn by the smooth brush .
Even the full 3x3 binary structure could result in multiple features if there are enough zeros in there .
That should help you determine if the nonzero pixels are dense enough to guarantee that the smooth brush shape is really a single feature .
To answer your question , the reason it labels an additional area is because you are using the default structure with a connectivity of one , and it doesn't know to look for a diagonal connection .
In addition to what Warren pointed out , where robust labeling is needed ( that will treat diagonal pixels as connected ) , one can change this structure .
I am unable to find a way to access each file loaded and remove the ones I want .
Perhaps use ` diff ` or some ` diff-like ` tool to check that our definitions are the same ?
scipy.ndimage.label performs connected component labeling ( it assigns a number to a cluster of 1 pixels surounded by zeros ) .
There is another stack overflow question that comes close :
numpy.polyfit has no keyword ' cov ' that has a nice explanation ( with code ) of how to use scikits.statsmodels to do what you want .
But that's probably an overkill ; it would probably be easier to read / decode your .dat file in Python itself .
This means that we can simply make a tuple of ` ( value , index )` and sort , giving us the indices of the values ( the values are also given , but we can easily throw these away ) .
Your idea to use a histogram would probably be a good start towards a method for automatically determining a proper threshold .
Yes it works but it is still not general.I thought you would use the histogram to determine that difference you talk about in an automated way.I ' m gonna accept your answer anyway.Thanks
You'd have to have a fairly strange image for it to be worth going to the trouble of a histogram when partitioning into two sets .
For example I would like to find the indices of the entries within a certain distance range , so I write : #CODE
If the arrays are the exact same shape , then ` numpy ` won't broadcast them : #CODE
No , because the lines in the original text CSV , have different length , and as such I would get a bias that would favour the bigger lines to get picked instead of the smaller ones .
If you're concerned about disk space and want to compress the data down using ` np.float16 ` ( 2 byte floats ) , you can do that too using the basic skeleton above , just substitute ` np.fromstring ` for ` struct.unpack ` and ` ndarray.tostring ` in place of ` struct.pack ` ( with the appropriate data-type ndarray of course -- and ` line_size ` would drop to 6 ... ) .
i just get as a result an array of 10 zeros .
and x resulting from linalg.solve ( A , b ) will specify a linear combination of columns from A to sum up to the random b vector .
You should not add any row of zeros , since your matrix is already a square .
Note : if you have random matrix , the probability that the matrix is singular is small and zeros will likely be only solution .
I need to find the gradient field of the image in order to divide the pixels into bins .
The only thing you need to correct is write 1.0 /( 1+z ) , otherwise there will be mostly zeros
I'm calling a function in a python script which I want to return a set of indices .
Basically I want to strip the ` ( array ([ index1 ]) , array ([ index2 ]))` return from numpy.where() so I just have ` ([ index1 ] , [ index2 ])` .
I want the function to return a value and then the two indices as so ` [ val , index1 , index2 ]`
Then I calculate the distance between those points and a given point and append it on to a list .
data = transpose ( transpose ( data ) / normalisers )`
The code is trying to create an array of zeros with shape x*y by x*y .
Now we set each element to the sum of all the elements that precede it :
O ( n ) initial setup , O ( log n ) selection .
Thank you both Warren and Nneoneo ... could you give any tips on how to no sum the results but not over the whole array , rather only summing the results row by row ?
is there a way I could sum up each entry individually ?
Python ValueError : too many indices for array - k-means cluster analysis
I'm writing a moving average function that uses the convolve function in numpy , which should be equivalent to a ( weighted moving average ) .
I would really like to use convolve for the WMA as well as MA as it makes the code cleaner ( same code , different weights ) and otherwise I think I'll have to loop through all the data and take slices .
If I use the ' same ' option in convolve , I get the central part , but what I want is the first part .
You could also roll your own with Cython if you are really concerned about the speed , but it likely won't be much if any faster than np.convolve() .
But , Numpy's advanced indexing lets you do a lot more than just get indices ; it lets you build entirely new arrays from sets of coordinate arrays .
The error is just saying that recarray does not have any method called append .
I want to ' concatenate ' the rows of the matrix to get a new matrix #CODE
Looks to me more like you want to concatenate the columns .
I meant , concatenate all rows in a given column .
You can stack , transpose , and reshape : #CODE
So my error isn't just sqrt ( n ) .
I would like to use those errors for a fit , so using sum ( ( experimental - function ) ^2 / ( error of the pixel ) ^2 ) .
Up to now , to handle 2D I just flatten my errorfunction as input for leastsq .
Since I am a beginner in Python , would you please assist me on this : for example , I have a file ( ` stats1.txt `) in a folder ` c :\ \programming ` ; this .txt file has one column with 1000 values ; now I would like to get descriptive statistics of these values ( mean , median , max , min , std ) .
Does really huge mean you have many patches of ones or large patches ?
You can use ` label ` from ` ndimage ` to segment each patch of ones .
I also tried to transpose the points so that the subtraction part is faster in C order , but it seems the dot product is still the most expensive part .
Just glancing at your code , it looks like something ( A subtraction of arrays and dot product . ) that ` numpy ` is already very optimized for .
You might consider using an optimization algorithm that doesn't need to calculate the gradient at each step ( e.g. Powell's method ) , as this will ( usually ) reduce the number of times the objective function is called .
To clarify , I'm asking how to copy arbitrarily ( but similarly ) shaped 2D slices from arbitrary 2D indexes of an 2D array into another , vertically stacked - not particularly along the diagonal , or 2x2 sized .
The idea is to create a " helper array " v , which is simply [ 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 ,... ] , and then use the observation that the indices you need are always simple slices of v .
is there a way to do this on arbitrary slice sizes ( not necessarily 2x2 ) ? and for arbitrary slice positions , not necessarily along the diagonal .
But you seem to want only the diagonal , you could do that with stride tricks as well to make sure you do not copy data if you want ( next versions will create a view with ` diagonal ` , old ones always a copy ): #CODE
Now ` diagonal ` is the array you created in your for loop ( on newer versions add a ` .copy() ` if you do not want a view ) .
Edit : Since the ` slices ` array is 2D and not 3D because you append , a reshape was missing here : #CODE
This might not be faster if you have such small arrays , but its constant ( expect for the copying of the data in ` diagonal ` call ) with the array size .
This is only a reshape away though .
is there a way to generalize this to arbitrary indexes ( not necessarily in the diagonal ) ?
I'm trying to stack images side by side .
Is there a way to stack along the first axis for a and b ?
It seems likely that ` std ` is returning zero because the standard deviation of your data is zero .
If they're outside , ` std ( stats )` will only evaluate the value of stats from the final iteration of the ` for ` loop .
Maybe you meant to append them all into lists and then work with those ?
Use a debugger ( or even print statements ) to look at the stats object that you are calling average and std on .... as DSM mentioned , it is probably std 0 ( i.e. a single number or something like that )
I have contstructed a 3d numpy matrice of zeros then normalized the rows .
I want to loop trough the nested list so that the last index of each ` tupleindex [ 1 ]` gets ` +1 ` when following from another tuple with ` index [ 1 ]` , for instance if ` index [ 1 ]` in ` tuple ( 2 , 2 ) , 2 ` , is following from ` index [ 1 ]` in ` ( 1 , 1 ) , 1 ` , the matrice column gets ` +1 ` matrice - the matrice 3*3 of zeros and then normalized ` row / = row.sum() `
Hi Warren , data is a tupled list [( 0 , 1 ) ( 0 , 2 ) ( 0 , 3 )] and the matrice is 3*3 of zeros .
Try ` reshape ` ing the image into a one-dimensional array and then shuffling it .
IIRC ` shape ` will be ` ( height , width , channels )` , so therefore his ` reshape ` should already flatten it .
numpy unique without sort
How can I use numpy unique without sorting the result but just in the order they appear in the sequence ?
Is the problem with the second ` unique ` using ` np.unique `' s ` return_index ` argument that it might produce incorrect results ?
That this ` unique ` might return a sequence with some elements not respecting the order imposed by the original sequence , e.g. , ( purely for demonstration ) ` unique ([ 1 , 0 , 1 ]) --> [ 0 , 1 ]` ?
` np.unique `' s documentation ( #URL ) states that the indexes returned with ` return_index=True ` will indicate the * first * occurrences , so your second ` unique ` should be safe and correct , right ?
Was going to suggest ` concatenate ` , but really your solution seems already best to me .
This is the median of the data range : ( min + max ) / 2 = ( 1.5 + 3.5 ) / 2 = 2.5 .
( min + max ) / 2 is not actually the median , but rather the [ mid-range ] ( #URL )
which you can then flatten or reshape as you like .
` r_ ` is basically a convenience function to generate the right indices , e.g. #CODE
Be careful : the ` mean ` and ` sigma ` arguments correspond to the distribution of the log of the ` lognormal ` distribution ; the actual arithmetic mean of the distribution is ` exp ( mean + sigma**2 / 2 )` , which evaluates to ` inf ` in standard double precision floating point when ` mean=2000 ` and ` sigma=800 ` .
It wants to minimize the sum of the squares of this vector .
You might also play with using the l2 norm instead of the sum for your cost function , i.e. , use ` np.linalg.norm ` in place of ` np.sum ` , and see how that affects your results .
` leastsq ` wants to minimize the sum of the squares of the vector returned by the objective function , so it's * almost * like using the l2 norm with ` minimize ` .
In fact , I get answers that are almost identical using ` leastsq ` and the l2 norm with ` minimize ` : ~ ` [ .78 ,. 86 ]`
The sum of their radii of the circles is smaller than the distances between their centres , so the circles don't overlap .
As you know , R is [[ cos ( t ) -sin ( t )] [ sin ( t ) cos ( t )]] and T should be [[ cos ( t ) sin ( t )] [ -sin ( t ) cos ( t )]]
` T ` appears to just be the transpose of ` R ` , which for a rotation matrix is the same as the inverse .
I don't know - also looking back at your post I guess ` T ` is not just the transpose of ` R ` .
T is the transpose of R and as expected , T^{-} is R .
` T ` appears to just be the transpose of ` R ` , which for a rotation matrix is the same as the inverse .
So the offset in function ndimage.affine_transform() is inv ( T ) [ -ty , -tx ] , not [ -ty -tx ] .
Supporting this project would be a staggering reward / efford project for Microsoft to boost its standing in the scientific computing sphere ; going from being almost entirely of the radar , to the very top of the stack .
I have 3-D masked array of dtype=uint8 , and I want to do something like ` arr.max ( axis=-1 )` , but instead of always picking the max , i want to either ( 1 ) find first ( or last ) unmased element , ( 2 ) pick random arbitrary unmasked element or ( 3 ) pick median or mode along axis , like ` numpy.ma.median ` or ` scipy.stats.mstats.mode `
i then tried finding max , which runs quick . but i dont want always use max value .
I am wondering how I can exploit the array broadcasting rules of NumPy and functions such as ` tile ` to simplify this .
However , this method is not as fast as mgilson ' s answer , because ` concatenate ` is annoyingly slow .
And it's actually a bit faster for larger ones ( although the speed gain is probably due to my use of ` empty ` instead of ` zeros `) : #CODE
The shape of the outer n-d array is dictated by the window's degrees of freedom within the larger array ; in every dimension , the number of positions the window can take is equal to the length of the larger array minus the length of the window plus one .
In this case , we have a 3x3 window into a 5x5 array , so the outer 2-d array is a 3x3 array .
We have to define the strides for the outer n-d array , and for the inner n-d array .
I moved the ` windows_strides ` definition outside of the def and changed ` std ( 1 )` to ` var ( 1 )` and I think as you pointed out that it's not much more to do here python-wise .
The second difference calculated with ` diff ` replaces the inner loop over ` Z [ j-1 , i-1 ] -2*Z [ j , i-1 ] +Z [ j+1 , i-1 ])` .
You won't be able to remove both for loops as calculating column i depends on column i-1 which ( in your second bit of code ) is just zeros except for the first column .
[ coo_matrix ] ( #URL ) uses integer indices to only represent parts of matrix data
` idtopick ` is an array of ids #CODE
` idtolook ` is another array containing the ids I'm interested in #CODE
The centre diagonal the +1 -1 diagonals and the +4 -4 diagonals ( usually >> 4 , but the principle is the same ) , i.e. I have a typical PDE system matrix of the form : #CODE
How to find unique vectors of a 2d array over a particular axis in a vectorized manner ?
I'd like to know the unique ` n-vector ` values that exist along the ` t-dimension ` as well as the associated ` t-indices ` for each unique vector .
In some special cases it can be done by collapsing the ` n-vectors ` into scalars ( and using ` numpy.unique ` on the 1d result ) , e.g. if you had booleans you could use a vectorized ` dot ` with the ` ( 2**k )` vector to convert ( boolean vectors ) to integers , but I'm looking for a fairly general solution .
In general I see why it wants the stable sorting of mergesort to handle ties in the indices w / duplicates , but in my case I can just build the indices myself using quicksort and not worry about it being unstable .
Secondly , in numpy 1.6.2 unique uses mergesort ( rather than quicksort ) when you ask it to return indices and the mergesort implementation doesn't like custom dtypes .
To get around this I can make my own copy of unique ( which lives in arraysetops.py ) which removes the kind=mergesort .
So the next release of numpy should allow sorting structured array with kind= ' mergesort ' , and therefore ` unique ` should also work .
When you do ` intarray [ i ] = floatarray [ i ]` numpy has to truncate the floating point values to get them to fit into ` intarray ` .
So I've been trying to code a tabletop game that I made a long time ago - I'm working on the graphic section now , and I'm trying to draw the 9x7 tile map using nested For loops :
Set ` za ` to an empty list ` za = [ ]` , then ` za.append ( p31 [ i , j ])` , finally out of your for loop , ` print sum ( za )` ; but I'm sure there is a better way since you are using numpy .
But I need the total number of values , not the values themselves .. should I do sum ( numpy.where ( data < 200 )) ?
` ( ar = rand ( 5 , 1 )) , `
Note : I'm need to take absolute values of the result for my actual problem .
However I also need to find the indices of numbers for which I have got the minimum difference result value as 1 ( i.e indices of 7 and 6 in this example )
We need the indices of the elements , so that is modified to : #CODE
The list comprehension acts on this result and forms a new list of absolute differences and the corresponding pairs of indices : #CODE
Taking the minimum gives the tuple whose first value is the minimum absolute distance and whose second value is the tuple of the indices : #CODE
Finally , pull out just the indices from that result : #CODE
It just needs a couple lines to finish off the answer : ` diff = np.abs ( np.diff ( a_sorted )); k = np.argmin ( diff ); closest_pair = index [ k : k+2 ]`
Use correct indices : #CODE
One way to remember how slices work is to think of the indices as
Use ` np.array ` to construct the array , then ` reshape ` to mold it into the right shape : #CODE
I am only interested in pixels above a certain limit , but I do need their ( x , y ) indices and value .
@USER : ` x ` and ` y ` are arrays of x- and y-indices for the nonzero points , respectively .
I want to translate ` inputFile.csv ` using ` lat.csv ` and ` lon.csv ` such that my output file contains a list of values ( from ` inputFile.csv `) , latitudes , and longitudes .
Your second solution seems to work , but for the first solution I get " ValueError : shape mismatch : objects cannot be broadcast to a single shape "
Maybe your database can sum the values .
` TypeError : list indices must be integers , not tuple `
Calculate diff of a numpy array using custom function instead of subtraction
The indices refer to " coordinates " and the value in the matrix is the " height " I'm trying to plot this data using pcolor similar to the " Two dimensional spline representation " shown here :
To do so you would have to change the origin of the plot to be in the top left corner and possibly transpose the data .
I append them to a list and want to plot as points with x , y coordinate .
Assigning colour to data depending on sign of gradient ( Python )
As you can see , the triangle has 2 sections of positive gradient and 1 longer section with a negative gradient .
Queries whether the current entry in an vertical array has a positive or negative gradient with respect to the successive entry in the array .
Then , plots the y data against x , where y values with a positive gradient ( and its respective x value ) are plotted using one colour , and the negative points in another colour .
numpy array in array resize
Say I now want to reshape b so that its shape is ( 2 , 3 ): #CODE
The question is , is there a good way of doing the desired resize ?
Iterate over a matrix , sum over some rows and add the result to another array
Furthermore I need to store each rows sum in an array like this #CODE
Also , you might want to show the user how to slice the first 5 rows out of the list as it looks like those are the only ones OP wants to sum over ...
I need also to understand what a convolve or deconvolve method does to the image.I googled trying to figure it out but there are a lot of equations that I couldn't fully understand .
` col_sig_squared ` is an array with indices .
I think the most computationally efficient thing to do is to keep track of the indices that associate ` b ` with ` a ` as ` b ` is created .
For example , instead of sampling ` a ` , sample the indices of ` a ` : #CODE
Or , with Numpy ( which the OP is already using ): ` indices = np.arange ( len ( a )); np.random.shuffle ( indices ); indices = indices [: k ]`
After plotting in Matlab we do ` caxis ( max ( caxis() ) - [ 0.5 , 0 ])` to scale the color limits to go from the current max color limit to , say , 0.5 below this max .
If ` y ` were a numpy array ( which presumably it was in the original code ) then it'd return an array of bools , and that * would * have a ` nonzero ` method .
Is there any way to resize the output image so that I can get whole picture of image 1 ?
Doing parallel gradient descent is an area of active research , so there is no ready-made solution there .
The ` predict ` method of ` LinearSVC ` , ` SGDClassifier ` , ` Perceptron ` compute the same function ( linear prediction using a dot product with an ` intercept_ ` threshold and One vs All multiclass support ) so the specific model class you use for holding the average coefficient is not important .
Hi pv ., a quick question - would this work on Windows too ?
If I want to delete rows with specific indices in this matrix , I use ` numpy.delete() ` .
Left multiplication by the transpose of J will insert a zero-vector back to the i'th row of B , which makes this solution a bit more general .
To construct J itself , I used pv .
' s solution on a sparse diagonal matrix as follows ( maybe there's a simpler solution for this special case ? ) #CODE
So if ` d = pdist ( x )` , the ` k ` th tuple in ` combinations ( range ( m ) , 2 ))` gives the indices of the rows of ` x ` associated with ` d [ k ]` .
If you want to access the element of pdist corresponding to the ( i , j ) th element of the square distance matrix , the math is as follows : Assume i j ( Otherwise flip indices ) if i == j , the answer is 0 .
The bottom triangle is the transpose of the upper triangle because the distance matrix is symemtric , i.e. swapping j , i -> i , j gives identical results .
with ` - i* ( i+1 ) / 2 ` you remove lower triangle ( including diagonal ) in all lines before i ;
with ` - i ` you remove positions in line i before the diagonal ;
with ` - 1 ` you remove positions in line i on the diagonal .
What I'd like to get is a 9x9 matrix that is a 3x magnified version of the above , having 3x3 ones in the top left corner , 3x3 0s in the middle top , etc .
It's a matrix transpose operation , see [ here ] ( #URL ) , and for more detail see [ here ] ( #URL )
` X.T ` is the transpose of ` X ` .
When you transpose it you get an array of shape ` ( 2 , N )` which can be unpacked .
Also note that wherever possible , the transpose will return a new view ( the data won't be copied ) , so this is a very efficient operation .
T stands for transpose .
I get the following error ( I only included the end of log ):
In that case I would suggest you reshape and then use take , or you could use stride tricks to do that neatly .
If you have regular splits of course ( all the same size ) , you may just be better of with using ` reshape ` .
How do I compose a frequency list from unique arrays of different length in numpy
From this list , I want a count of the unique arrays ( like a histogram over the sequences ) .
I define a vectorized function to append a value to each list : #CODE
The ` where ` function broadcasts my column array then replaces some of the elements with zeros .
The problem is that ` np.putmask ` does not broadcast values , it repeats them :
When you explicitly broadcast , ` flat ` returns the desired flattened values : #CODE
But if you do not broadcast , #CODE
Yes , you are right , the new version of numpy this works as expected becuase copyto does broadcast , sorry ...
I expect the last print to give the sum of the rows of the two previous csv files .
So to stack the two arrays , the smaller one has to be promoted to the larger one's dtype : #CODE
Possibly an easier solution would be to concatenate the two csv files first and then call ` genfromtxt ` : #CODE
( in order to concatenate them I need to skip the headers of all but the first file )
@USER : I've edited the post to show one way to concatenate the files while skipping the first line of the second file .
pandas : flatten df with delimiter
If you want to stack up graphs on top of one another ( i.e. plot them on the same axis ) use : #CODE
I would like to get values of histogram ( not necessary plotting histogram ) ...
After being able to do this , I will have a look at the way to display histogram ( but that's an other problem )
You just need to use the histogram function of numpy : #CODE
I would suggest to create a large array first , of size say ` N*2*2 ` , fill it with your data and then reshape it to ` ( N , 2 , 2 )` .
Numpy : placing values into an 1-of-n array based on indices in another array
Suppose we had two arrays : some values , e.g. ` array ([ 1.2 , 1.4 , 1.6 ])` , and some indices ( let's say , ` array ([ 0 , 2 , 1 ])`) Our output is expected to be the values put into a bigger array , " addressed " by the indices , so we would get #CODE
I would then like the gradient of the scalar at every grid point .
In 2D I am doing this using ` np.gradient ` and I get two arrays with the gradient in the x and y directions : #CODE
The Numpy documentation indicates that ` gradient ` works for any dimensions :
Return the gradient of an N-dimensional array .
The gradient is computed using central differences in the interior and
The returned gradient hence has
I store the results and plot in a histogram .
And , you do not have to allocate space for a larger 2D array mainly filled with zeros .
Create a custom ideal projection for each tile , then calculate and store the area as an attribute for each tile .
If the matrices are sparse ( lots of zeros ) , use a sparse matrix class to save a lot of RAM .
How should I store the results of the dot products while I'm computing the other dot products ?
The result of dot products is only one scalar , so you can save it in memory , such as a list or array .
beta = inv ( X ' * X ) * ( X ' * Y )
Compute beta = inv ( X ' * X ) \ ( X ' * Y )
I would take advantage of numpy arrays he's using ( loops in C ): ` sum (( A > x ) & ( A < y ))`
@USER : Wouldn't surprise me if ` sum ( x < A < y )` worked as well then .
Using an iterator to sum a NumPy array would be orders of magnitude slower .
or ` sum ( bool ( i > x and i < y ) for i in A )`
I'm trying to make a 2^n x 2^n numpy array of all possible dot product permutations of a very large set of vectors .
File " C :\ Python27\lib\ site-packages \numpy\core\ fromnumeric.py " , line 171 , in reshape
return reshape ( newshape , order =o rder )
Why are you passing ` num , num ` to reshape , but not the actual thing you're reshaping ?
So that I can load 4 lines out of the 337 and plot them then another 4 and plot them contiguous next to the previous ones .
Personally , I'd plot the envelopes ( that is , say the max and min of every set of 10000 pts ) and maybe the envelopes of the diff , since OP is looking for outliers , but yours is the right basic approach , imho .
The full log is again available as a Gist .
To truncate by angle it is convenient to use a spherical coordinate systems .
Then , you can truncate setting the limits : ` r1 ` ` r2 ` ` t1 ` ` t2 ` ` p1 ` ` p2 ` : #CODE
To truncate by plane it is convenient to use a cartesian coordinate system ` ( x , y , z )` , where ` x**2+y**2+z**2=R**2 ` ( see mathworld ) .
The inverse ( and generalized inverse ) of a sparse matrix is usually dense , unless you can permute the rows and columns of the matrix so that it becomes block diagonal .
So your problem splits into two parts : ( i ) find a permutation that makes it block-diagonal , and ( ii ) compute the generalized inverse using linalg.pinv separately for each block .
be True if any of the elements were nonzero .
Get the censored time ( that is the period of observation minus the sum of all failure times )
Calculate a log likelihood based on additional arrays of parameters ( such as an array of hazard values ) .
For example , the log likelihood for a single ` MultiEvent ` ` M ` and constant hazard value ` h ` would be something like :
` sum ( log ( h ) + h*t for t in M.times ) - h* ( M.period - sum ( M.times ))`
Unless you know in advance the maximum number of entries for a multievent , you'll have problems , and even then you'll be wasting loads of memory / disk space filled with zeros for those events that aren't multi events .
An extension of this answer would be to create a numpy array ( or a pandas DataFrame ) based on a join between my MultiEvent table and the table holding my scalar variables .
You can ** flatten ** it so it will have a shape ( 4 , ) .
@USER -- I think that ` flatten ` creates a copy , which seems unnecessary ...
How to translate the coordinate system of a 2D Cartesian plane ( x , y ) to a matrix with indexes a [ i , j ] ?
Changed the question again , so a ) there is a clear question relevant to an algorithm and b ) a specific programming problem , and c ) unique to programming profession .
There would be no room to append to or extend our numpy array .
So any request to append to or extend our numpy array can only be satisfied by allocating a whole new larger block of memory , copying the old data into the new block and then appending or extending .
Essentially my compression does some wavelet and fft transforms , does some filtering here and there and returns an array with some numbers , I know the format of this array and I already achieve a high % of compression here , the next step is to first dump this array into a binary file .
The first array in the 2-tuple is an array of all the unique values in ` l [: , 0 ]` .
It also happens to be the rank , since ` np.unique ` returns the unique values in sorted order .
look at the sorted ` evals ` from eigh in Doug's answer -- post the top few and the sum if you like , here or a new question .
And here's a tutorial demonstrating how pincipal component analysis can be done using ` numpy `' s built-in modules like ` mean , cov , double , cumsum , dot , linalg , array , rank ` .
taking the eivenvectors eigenvalues of this cov matrix
It is free and explains PCA in 15 min ( many other algorithms as well .
Exactly , that's why I didnt take the time to translate it .
I don't know much about memory use , but I'd be surprised if ` modf ( a ) [ 0 ]` and ` a-numpy.trunc ( a )` both didn't use more memory than simply taking the mod directly .
If you know the syntax you need , why not roll that into your own ` rev ` function ?
I don't know how to translate ( for example ) ` 1 ` into ` [: , :: -1 , :] ` and ` 0 ` into ` [: : -1 , : , :] ` for an arbitrary number of dimensions .
which does exactly this by wrapping PIL's resize function .
I want to be able to resize arbitrary images , with any number of " color " channels .
I was wondering if there is a simple way to do this in scipy / numpy , or if I need to roll my own .
I will keep skimage's ` resize ` in mind , thanks !
If you don't want SVD , then see also my comment about using lu_factor instead of inv .
I have a numpy matrix and would like to concatenate all of the rows together so I end up with one long array .
You could use the numpy ` concatenate ` function : #CODE
You could also try ` flatten ` : #CODE
I would suggest the ` ravel ` or ` flatten ` method of ` ndarray ` .
` ravel ` is faster than ` concatenate ` and ` flatten ` because it doesn't return a copy unless it has to : #CODE
But if you need a copy to avoid the memory sharing illustrated above , you're better off using ` flatten ` than ` concatenate ` , as you can see from these timings : #CODE
Note also that you can achieve the exact result that your output illustrates ( a one-row 2-d array ) with ` reshape ` ( thanks Pierre GM ! ): #CODE
Note that ` ravel ` or ` flatten ` will transform your 2D array to a 1D array --- ie , switching from a ` ( N , M )` to a ` ( N*M , )` shape .
I understand that to calculate the area under the whole graph , I can either sum the values or use trapz , but I'm struggling when trying to restrict these calculations to a single region .
What's left is the integral over the fourier space of the FT of the function times the integral of an exponential , which gives something like ` sin ( kx ) / kx `
If you're using a simple sum ( or trapezoidal ) integration : #CODE
I have a 2D array of indices , where the shape matches the first two dimensions of the data array , and it specfies the indices I want to pluck from the data array to make a 2D array .
indices is a very nice function , but to a short test it is 2 or 3 times less performant than meshgrid
If ` x ` doesn't increase or decrease monotonically , you don't have a unique mapping .
I understand there's no unique mapping .
Is there a short-code efficient way of " glueing " two arrays together such that if the arrays differ in length then the glued product must be such that the values from the longer are filled between values from the smaller untill the the new product has the same length as sum of the length of the two arrays ?
0 1 1 0 1 1 0 1 1 and so on , essentially since ones has twice the length and the total is then of length 3 then to get to that length 3 vector one must cram two values from ones between every value from zeros .
I want this to be generalized to cases where ones ( or any other vector ) is N times larger , then N values must be filled between every value of zeros ( or any other vector ) .
This is pretty much what you have in the question , but the trick is to reshape B to be ( n , m ) instead of ( N , 1 ) ie ( 10 , 2 ) instead of ( 20 , 1 ) in this case .
The ` -1 ` in ` reshape ` is short hand for " whatever will make it work " it's a lazy way of doing ` B.reshape ( n , len ( B ) // n )` .
Y is a ( n x f ) matrix and C is ( n x n ) diagonal one ; n is about 300k and f will vary between 100 and 200 .
Y is initialized randomly and C is a very sparse matrix with only a few numbers out of the 300k on the diagonal will be different than 0.Since Numpy's diagonal functions creates dense matrices , I created C as a sparse csr matrix .
If somehow C could be represented as diagonal dense without consuming tons of memory maybe this would lead to very efficient performance but I don't know if this is possible .
There is a special sparse container for diagonal matrices .
Is your diagonal matrix by any chance positive-definite ?
This would allow to write the problem as ( B.T * B ) ^-1 with B = sqrt ( C ) * Y .
Since C is diagonal if all element are > 0 , it is positive definite .
The reason the dot product runs into memory issues when computing r = dot ( C , Y ) is because numpy's dot function does not have native support for handling sparse matrices .
I was mostly posting since I had to deal with the same problem on that day and did not see an explanation for WHY the dot ( sparse , dense ) function was not returning the result you were expecting .
Here's an example extracting values from array ` a ` which satisfies the criteria that that element has an absolute different of less than 0.75 from the corresponding element in array ` b ` : - #CODE
I am trying to extract the full set of indices into an N-dimensional cube , and it seems like ` np.mgrid ` is just what I need for that .
For example , ` np.mgrid [ 0:4 , 0:4 ]` produces a 4 by 4 matrix containing all the indices into an array of the same shape .
and I would like to fill in the first column with ones , when I try to do : #CODE
I used ` np.ma.average() ` as a robust solution to handle cases where the sum of all values is zero .
How do I reshape this array ?
Why won't numpy actually reshape the array into a one-dimensional array ?
` reshape ` doesn't work in place .
Your code isn't working because you aren't assigning the value returned by ` reshape ` back to ` img_buffer ` .
If you want to flatten the array to one dimension , ` ravel ` or ` flatten ` might be easier options .
I am trying to extract a subset of a a numpy array ` y ` specified by a set of indices contained in ` x ` , while still leaving some indices of ` y ` free .
The last dimension of ` x ` corresponds to indices info the first three dimensions of ` y ` .
Alternatively ( since it looks like you're dealing with binary data ) , if you want to save the raw data stream , you could use ` ndarray.tostring ` to get a string of bytes that you can dump to the file directly .
Also , you might want to include the complete stack trace and not just the error message .
However , if I reshape the same arrays to 2D or any ND and then I am able to plot them in 3D .
There are multiple ways to do this ( e.g. , you could load both arrays then concatenate them via ` x = numpy.concatenate (( x1 , x2 ))` , etc .
) , but what I'd do is concatenate the files on the fly , and pass the result to ` loadtxt ` .
@USER : Well , you'd have to open a ` zipfile.ZipFile ` , then iterate and concatenate its contents .
Following pv .
If you take the ` QR ` decomposition of a matrix ` A ` , the columns of ` R ` with a non-zero value along the diagonal correspond to linearly independent columns of ` A ` .
Error log while doing Logit ( Statsmodels ) File " / usr / local / lib / python2.7 / site-packages / numpy / linalg / linalg.py " , line 328 , in solve
Chasing this up , it seems that the ` counters = numpy.array ([ DATA ` line is converting peptides from a list ~3k long , to a 16 long string , so the resize obviously fails ...
The max function has to essentially find the row with the highest index that does not have a sum == 0 .
So , as you may notice , the rows have either all zeros or have some non-zero values .
I need to find the row with the highest index that does not have sum ( row )= =zero .
@USER : It makes more sense to use any() that sum() since it will exit at the first non-zero value in the sublist rather than requiring it to sum all elements .
This will return the indices that have non- " all-zero " rows : #CODE
an approximation of 3th grade diff .
Using integer indices will further reduce memory use .
to normalize it , and convert it to ints to get the random indices , at the end we
So it looks like ` numpy.random.permutation ` is the worst , not surprising , pythons own ` random.sample ` is holding it own , so it looks like its a close race between ` numpy.random.shuffle ` and ` numpy.random.sample ` with ` numpy.random.sample ` edging out , so either should suffice , even though ` numpy.random.sample ` has a higher memory footprint I still prefer it since I really don't need to build the arrays I just need the random indices ...
Unfortunately ` numpy.random.sample ` doesn't draw unique elements from a population so you'll get repitation , so just stick with shuffle is just as fast .
Note that N here is quite large as such you are going to get repeated number of permutations , by permutations I mean order of values not repeated values within a permutation , since fundamentally theres a finite number of permutations on any giving finite set , if just calculating the whole set then its n !, if only selecting k elements its n !
/( n - k ) ! and even if this wasn't the case , meaning our set was much larger , we might still get repetitions depending on the random functions implementation , since shuffle / permutation / ... and so on only work with the current set and have no idea of the population , this may or may not be acceptable , depends on what you are trying to achieve , if you want a set of unique permutations , then you are going to generate that set and subsample it .
For instance I would like to do sum ( random_arrays , axis=1 ) .
Also note that shuffle may generate non-unique permutations depending on the number of random arrays you need , if you truly want unique permutations than you are going to have to generate them manually and sub sample them , also note that ` numpy.random.choice ` was added in 1.7 Im currently at 1.6.1 , #URL Im not sure about its performance need to test it , but it may be slower since it gens new arrays ...
The permutations shouldn't be unique .
@USER I've updated to work with numpy , also sum can only be applied to numeric values , here you have string values , if you want to use indices , simply replace values by ` range ( leng ( values ))` and it should work .
At the moment , I'm simply looping through every line of strings , assigning values of 1 or -1 to the string if it is unique , and zeroing out the ones that do not exist ( for example , D is not present in the first line , so it is 0 ) .
Only 100000 lines * 10000 unique strings would take 1 GB . does this sound similar to the sizes you're using ?
Also note that matrices must contain some 80% zeros for sparse matrices to pay off .
It's a very crude guideline based on the memory overhead of sparse representations and the more complicated algorithms needed to perform operations like dot products on them .
Python ValueError : operands could not be broadcast together with shapes
ValueError : operands could not be broadcast together with shapes ( 375 , 375 ) ( 375 , 500 )
The second column is a list of indices from another list .
spreadsheet , lexsort returns an array of integer indices that
BTW : I agree that scikit-learn could offer such a tool by default , either by extending the features of the existing function / class pair or by introducing new ones for this case .
The best approach is not to set your mask explicitly if you don't need it and leave ` np.ma ` set it when needed ( ie , when you end up trying to take the log of a negative number ) .
we are asked to make S a diagonal matrix where the n*n terms have values and every other entry is 0
My hope is to load this data into a Hierarchical Indexed data structure with indices of the perl hash becoming indicies of my python data structure .
The bit in the middle , though , is quite repetitive and bulky ( especially because I have lots of parameters , and I might want to add or remove parameters ) , and I feel like there should be a more succinct / elegant way to initialize the results matrix , iterate over all of the indices , and set the appropriate parameters .
Is it the case that you're trying to find the best input parameters for one of many scalar outputs , e.g. , " Give me the inputs that minimize result [ i ]" , or do you have some way of evaluating the " goodness " of all the results at once , like the sum , or l1 or l2 norm ?
I'll be taking the strict min of the accuracy within each object category , and across objects , and combining the precision and recall measures in various ways .
like a > = 0.7 , and find min of b and max of c => min ( a ) +max ( b ) +a > = 0.7 ==>
{ min ( a ) +min ( -b ) + ( a-0.7 ) > 0 } > 0
and ` max grid ` : #CODE
Scipy : fill a histogram reading from a DB , event by event , in a loop
You sometimes don't want to fill a histogram after creating a huge list .
You want to read a DB and fill the histogram event by event .
So , if I have 10Bn entries in the collection , I can fill any histogram I need for analysis without putting all data in memory .
@USER Counter is similar to histogram , but you must have a dict in memory .
Sure , if you want to have the histogram as a ` numpy ` array , you'll have to construct it from the ` Counter ` manually .
It is not a histogram ( you can't define bins ) but it's close ...
rootpy's integration with matplotlib is also nice , especially if you want the histogram to show up in an IPython notebook , for example : #CODE
-1 ( sorry ); the [ ` ndarray ` docs ] ( #URL ) state : " Arrays should be constructed using ` array ` , ` zeros ` or ` empty `" -- you're not really supposed to call the ` ndarray ` constructor except when inheriting from it , and advising new users to do so is a bad idea .
Here's an example of sin wave with amplitude 7.0 and fft amplitude 3.5 #CODE
However it can also be equal to 1 or sqrt ( N ) .
The commented print is the place where you have corresponding indices of lat1 and lat2 .
We are seeking for ` 25 ` , ` 25 > 17 ` so there is no sense in seeking our element in the left part ( indices [ 0 , 3 ]) .
Yes , the problem was coming from the ` diff ` function , which I thought to be the same in both libraries .
I know about in-place modifications using ` += ` ` -= ` ` *= ` ` / = ` and numpy functions that take an ` out ` parameter , so you can do things like ` np.abs ( x , x )` to take the absolute value of ` x ` in-place .
Many NumPy binary operators have an ` outer ` method which can be used to form the equivalent of a multiplication ( or in this case , addition ) table : #CODE
I have a vector with a min of two points in space , e.g : #CODE
Unless you are specific , your question reads like " translate this code for me " .
pv , in your original post you used " splrep " instead of " splprep " which through me off as it gave errors .
ok , I found the solution which is a little modification of " pv " above ( note that splev works only for 1D vectors )
One problem I was having originally with " tck , u= scipy.interpolate.splprep ( data )" is that it requires a min of 4 points to work ( Matlab works with two points ) .
#URL ( det ) .
#URL ( det ) , despite the matrices being small .
My only guess is that if I want to sum columns correctly , I need to first cast the matrix as csc -- which makes sense .
uint16 has a max of ~65000 and my sums here are much bigger than that .
@USER yes , the thing is , numpy upcasts silently to uint32 for the sum method , the sparse matrix does not .
What I would like to figure out how to sum every two rows of an array .
This code appears to do gamma correction ( ` ** 2.4 ` ) .
What if I know my gamma is 2.2 , not 2.4 ?
Numpy structured arrays do not store arbitrary length strings , you need to pick a max length .
If you really cannot pick a max lenght , use ` object ` instead of ` str ` .
I have also tried zero-padding the array or list , but it is still inelegant ( requires adjusting the lookup locations indices accordingly ) and inefficient ( requires copying the array ) .
Is ` max ( n , 0 )` an acceptable substitute for your ` thing ( n )` ?
Normal getting of negative indices should be left unchanged .
Adding support for your slices would just require another check for ` isinstance ( key , slice )` and then setting ` key ` equal to a new slice that you create after looking at the old ones ` start ` / ` stop ` attributes .
Sorry , but how is this monstrosity of a hack of the standard Python syntax more elegant than bounds-checking your indices , which is much more clear and concise ?
Instead , negative indices are just ignored , so :
Then , I found that using append for large arrays takes more time , then I changed code like this : #CODE
That is actually the axis you sum along .
Here numpy.sum ( ..., axis=1 ) means sum along that axis only , rather than all elements : a ndarray with shape num_points x num_dimensions summed along axis=1 produces a result with num_points , which is correct .
I hope that you are trying to create a matrix by using the transpose , if it is so , the newaxis method defined in the question will be more efficient .
numpy histogram indexing
considering I have a 3D histogram or for simplicity a 3D numpy array of shape ( X , Y , Z ) #CODE
Getting the indexes can be obtained with nonzero ( ) like so : #CODE
How many values do you sum up ?
( max ( x ) / min ( x ) / max ( abs ( x )) / min ( abs ( x ))
The mean calculation doesn't do anything special really AFAIK ( just sum up , divide by length ) .
I tried taking the ` log ` first , then sum_by_group , then ` exp ` , but ran into numerical issues :)
There are some other similar answers here for min and max of elements by group :
now you can already do prod along the axis 1 except for those zeros , which is easy to fix : #CODE
Since you avoid boolean indices for every element in the original array for every group .
If ` X ` is a sparse matrix , ` X.min() ` also throws the error : ` *** AttributeError : ' module ' object has no attribute ' min '` .
After all , a sparse matrix implicitly contains mostly of zeros .
I can do it for my original image but after I take the fft and and fftshift I can not display it properly .!
The ` round ` call will round up or down , whereas a pure int cast is effectively a ` floor ` call .
Also have a look at ` np.clip ` if you want to clip values above and below a threshold ( e.g. 0 and 255 ) .
I think you can do it in one go , with a ( 3x100000 , 3x100000 ) matrix composed of 3x3 blocks around the diagonal .
But it should be ` A.transpose (( 1 , 2 , 0 ))` instead of ` A.T ` ( else you're solving for the transpose of your systems ) .
And could you elaborate more on how I would be solving the transpose of my system with ` A.T ` ?
You're solving for the transpose , see #URL
I set up ` A ` and ` b ` with the same dimensions as in your example , and filled them with random numbers ( but I added 10 to each diagonal in ` A ` to make the matrix positive definite ) .
How would I plot z = max ( abs ( xi ) , abs ( yi )) , i.e. the L1 norm ?
If you want to plot L1 norm with x and y , it's no more a revolution plot ...
I'm not sure why you consider a L1 norm cannot be represented in 3D by a revolution .
I think you need to fix either x or y in your formula z = max ( abs ( x ) , abs ( y )) to have only one dimension and draw a curve .
If we instead perform z = -max ( abs ( x ) , abs ( y )) then we are working with the L-infinity norm .
The 1D function gives a 2D representation which corresponds to the L-infity norm in 3D .
But the 2D function does not produce a single unique curve , so in that sense we cannot perform a revolution with it .
So if we speak with input and output : you would like to specify Zmin and Zmax , and get all X and Y giving a norm in the Z interval ?
The norm was an example to illustrate the issue , but we can keep on it if that helps .
If the " tri " plot comes from a Delaunay triangulation , then it seems the code is not handling , in a consistent way , the case of co-circularity of 4 points .
` after the zeros .
What I want to do is in each iteration take this array and concatenate it into another array of arrays so that at the end of the loop I have a matrix
You should also consider using ` np.vstack ` instead , since that stacks row vectors into matrices ( ` append ` can do it but it takes extra arguments ) .
I think you're looking for reshape and repeat #CODE
Yes I would like to but , How would I append the yH value to a 2D array ?
Often when building up a numpy array from an unknown number of smaller arrays , it's easiest ( and fastest ) to store the smaller arrays as a list and then stack them together at the end .
if you store the ` theta_Ridge_Matrix ` in a 3D array , you can also let ` np.dot ` do the work by using ` yH = np.dot ( x_train , theta_Ridge_Matrix )` , which would sum over the second last dimension of the matrix .
we will create an array , we name it mat #CODE
The shape of any array is changed by using the \verb " reshape " method #CODE
How do I translate the output ?
Sticking ` print ( len ( a ))` in the middle shouldn't change a -- len should be a pure function -- but that would mean that ` set ` has to store multiple objects and figure out which ones are the same when operations are done on them .
` append ` is an example of an operation that mutates list objects .
One option that people sometimes use is to work with log() of the values instead of the original ones .
As you can see it displays all adjacent cells , however the sum of them doesn't equal the perimeter of the patch .
Another way of doing this could be to join these data frames , it will remove the non-matching entries and I can drop count column afterwards .
My thought is to calculate a Reimann sum at each pixel and determine if the offset from the planar surface is equal to or less than the offset of the underlying topography from the planar surface .
This calculates the Riemann sum using the trapezoidal rule and cumulatively stores the total calculated area .
Okay , but you shouldn't need to use a Riemann sum , or I just don't understand the problem
This ends up looking like a diagonal slice + a horizontal slice + a vertical slice .
Then i want to append a function to the current cell value indexed in a empty array ( example function sum ) , which uses the values of all cells in the binary structure .
Given the structure s and an example function such as sum the value in the resulting array ( result_array ) becomes 7 ( or 6 if the current cell value is excluded ) .
This enables you to multiply the two sub-matrices together and sum , as desired .
The function sum is just an example .
I want to apply various functions to the cells vicinity values ( even custom made ones ) .
For the particular case of products , you could use the fact that ` log ( a*b ) = log ( a ) +log ( b )` to convert the problem back to one involving sums .
However the function sum is just an example .
I want to apply various functions to the cell vicinity values ( even custom made ones ) .
How does convolve work ?
` convolve ` returns an array composed of those summed values .
But ` convolve ` is always going to perform a sum .
I do not know of any ` scipy ` function which allows you to perform a calculation like ` convolve ` except with an arbitrary function .
However , the example above tries to show how you might extend the possibilities by applying ufuncs before or after ` convolve ` .
I want to extract elements from Foo using the values of Bar as indices , such that I end up with an 2D matrix / array ` Baz ` of the same shape as ` Bar ` .
numpy dot product of ith row with ith column
Still , each value in the matrix is produced by the product of the transpose of a column and a row .
sorry , i realized that i was actually looking for the diagonal , not the first row
So since you just want the diagonal elements , looks like you're after #CODE
OR , ( and this only works because you want a ` diagonal ` so this assumes that if A's dimensions are ` n x m ` , B's dimensions will be ` m x n `) : #CODE
Well if you found that question you can translate it directly into numpy ( I thought of the exact same thing as soon as I posted the answer and added it to my answer )
Numpy sum between pairs of indices in 2d array
I have a 2-d numpy array ( MxN ) and two more 1-d arrays ( Mx1 ) that represent starting and ending indices for each row of the 2-d array that I'd like to sum over .
Numpy sum of values in subarrays between pairs of indices In that question , they are wanting to find the sum of multiple subsets for the same row , so ` cumsum() ` can be used .
However , I will only be finding one sum per row , so I don't think this would be the most efficient means of computing the sum .
The line inside the loop previously read ` d [ i ] = sum ( a [ b [ i ]: c [ i ]])` .
Each set of starting and ending indices corresponds to a new row in the 2-d array .
probability density function from histogram in python to fit another histrogram
Firstly I have a histogram from data points .
I would like to interpret this histogram as probability density function ( with e.g. 2 free parameters ) so that I can use it to produce random numbers AND also I would like to use that function to fit another histogram .
I join Saullo .
Using a histogram to produce a smooth cumulative density function is not entirely trivial ; you can use interpolation for example scipy.interpolate.interp1d() for values in between the centers of your bins and that will work fine for a histogram with a reasonably large number of bins and items .
You could give your distribution gaussian tails based on for example fitting a gaussian to your histogram ) , or any other form of tail appropriate to your problem , or simply truncate the distribution .
This doesn't handle tails , and it could handle bin edges better , but it would get you started on using a histogram to generate more values with the same distribution .
, thank you for the quick reply , the interpolation was also in my mind , but as u said firstly it cant take care of the outliers and also that is not really a density functions but more a copy of the initial histogram .
Getting AttributeError : sqrt in vq.whiten
The difference between the two is that when the version which hasn't had the mean substitution and transpose called on it has one ` numpy.ndarray ` for each row while the one which has been mean substituted has one for each column .
Also , solving the ugliness problem of having to transpose after the list comprehension , I rediscovered ` np.column_stack() ` .
More likely , however , is that if the original file contained floats as values with relatively few significant digits ( for example " 1.1 , 2.2 , 3.3 ") , after you do normalization and scaling , you " create " more digits which are needed to represent the results of your math but do not correspond to real increase in precision ( for example , normalizing the sum of values to 1.0 in the last example gives " 0.1666666 , 0.3333333 , 0.5 ") .
NOT that it should be of max rank 2 .
Or must one necessarily reshape the numpy array to 2D ?
So I guess it is just up to me to " intelligently " reshape the numpy array
Since the ` log ` for ` x=0 ` is minus infinite , I'd simply check if the input value is zero and return whatever you want there : #CODE
@USER : You are right , I need o clip to a value above 0 .
otherwise you can simply use the log function and then patch the hole : #CODE
if somewhere you have to revert to the original value , you are going to experience some problem , changing zeros into ones ...
( By the way , it should be ` log ` inside the string expression , not ` np.log ` , numexpr is not really using the log function from numpy )
I have run the first solution and works properly , but it outputs a warning : RuntimeWarning : divide by zero encountered in log .
The where is supposed not ro run the log if the value is 0 !
You may also want to keep track of the actual data sizes ( max count , max i ) and trim the array later .
Unlike the list ` append ` method , numpy's ` append ` does not append in-place .
I accumulate the average change to produce a simple response image .
Updating each sliding sum to move to the next pixel requires only adding the newest pixel in the current window and subtracting the oldest pixel in the previous window , thus two operations per pixel regardless of window size .
I am not sure if there is a sliding window sum built into numpy , but this answer suggests a couple of ways to do it , using stride tricks : #URL .
This is a minor addition , but you may want to avoid calling intersect() for every pixel : either ( a ) only process pixels which are farther from the edge than the max integral size , or ( b ) add margins to the image of the max integral size on all sides , filling the margins with either zeros or nans , or ( c ) ( best approach ) use slices to take care of this automatically : a slice index outside the boundary of an ndarray is automatically limited to the boundary , except of course negative indexes are wrapped around .
My initial ` wind ` array is also a list of wind directions , although random ones .
Another idea I had was to truncate the end of the file such that it still looks like an npz file and python can read it , but I don't know if that's possible .
With struct you can dump or read a whole array of data pretty easily to disk , but you are going to have to figure out how to store metadata , i.e. number of items written , size of each item , shape of the array ...
Elementwise mean of dot product in Python ( numpy )
where ` C [ i , j ] = sum ( A [ i , k ] * B [ k , j ]) / count_nonzero ( A [ i , k ] * B [ k , j ])`
Further , if A or B are sparse matrix , making them dense ( replacing zeros with ones ) make memory occupation expolde !
Then you have to replace zeros with ones to avoid 0 / 0 ( in my exaple there are not zeros in ` np.dot ( np.sign ( A ) , np.sign ( B ))`)
If you want the project from another direction just change the axis parameter ( remember that python has the indices from 0 and not from 1 like matlab ) .
Numpy array admits a list of indices , for example #CODE
But this method don't work if we want to use a multiple slice indexing or indices plus a slice , for example .
If what you need is a monodimensional array with just one element , the simplest way is to reshape your array before unpacking it : #CODE
If you need it to work normally for multidimensional array and to keep one-dimensional when you read onedimensional array , I thik that the best way is to read normally with loadtxt and reshape you arrays in a second phase , converting them to monodimensional if they are pure numbers #CODE
you just have to adjust the reshape for your data .
As , I have mentioned in the question , without reshape it's working for files with multiple lines .
If i use reshape it's only working for the files with single line .
The simple way without using reshape is , to explicitly typecast the list #CODE
This works faster than the reshape !
numpy get all combinations of rows and column indices 2D array
Note : I know that it is easy to do basic slice selection to get a block from the array but in a more advanced use case the indices in cfilter and rfilter aren't necessarily right next to each other .
Note that all ` np.ix_ ` does is add axes appropriately to give index arrays that can be broadcast together : #CODE
You don't actually need nonzero for this .
But they can be broadcast if you stack them correctly .
I want to reshape this array to one that is of size ` ( 4 , 12 , 3 )` .
I have tried using ` reshape ` , but ` reshape ` requires that the array be the same size .
When you do a reshape what you're actually doing is just changing the order / way in which the bytes / elements are read
Yes resize does something similar also but to bo honnest I don't really understand the purpose of these functions , I don't find it very explicit and I don't understand how you can choose where to put the filling values .
I guess above doesn't work , since mgrid creates array of indices not values .
` reshape ` indeed gives 100**D D-vectors , but surprisingly they are not unique , which also means not all possible vectors are there .
In this case , it takes one extra increment to get that sum to round up instead of down .
numpy only perform function on nonzero parts while preserving structure of array
I want to perform a function on Foo such that only the nonzero elements are changed , i.e. for f ( x ) = x ( nonzero ) +5 : #CODE
I have a ' filter ' condition where I want to perform an operation on all rows for all matching columns , but if filter is an empty array , somehow my foo [: , filter ] gets broadcast into a 3D array .
Can someone explain what the proper use case of np.nonzero is compared to using booleans to find the correct columns / indices ?
To do multidimensional indexing in python you can either use indices in ` [ ]` , separated by commas or use a tuple .
This is all somewhat academic , because if you're just using ` filter ` for indexing you probably don't need to use nonzero , just use the boolean array as the index but if you need to , you can do something like : #CODE
i guess this is because PIL images are encoded in a RGB fashion , with 3 dimensions . would need to map this encoding to a representation with unique reprensentation for each pixel .
Try this :) ` int ( sum ( map ( lambda x : x [ 0 ] * x [ 1 ] , zip ( pixel , ( 0.299 , 0.587 , 0.114 )))))`
Numpy sum over planes of 3d array , return a scalar
I have a 3D array , lets say it's 3x3x3 and I want the scalar sum of each plane .
For example , if you want to sum along the last dimension of the array , you would do : #CODE
And you'll get a resulting 2D array which corresponds to the sum along the last dimension to all the array slices ` 3dArray [ i , k , :] ` .
You want to sum over two dimensions ( a plane ) .
I would like a vector of scalars , for each plane in the 3dArray , where each scalar is the sum of the entire plane .
@USER so you want to do the sum twice ( your matlab code does not do that as well ) .
On new numpy you can ( will be ) able to do ` array.sum (( 1 , 2 ))` as well , to directly sum along two axes at once .
I updated the answer to now sum over the planes .
I need to run each of their elements with matching indices through a function , then store the result at those indices in a third array z .
Next , you sort the distances from the shortest to longest and store the indices ( ` i ` in the function ) .
From ` i ` you select only the elements of specific type and location and finally you take first of the remaining indices ( if the array is empty , return ` NaN `) .
How to calculate the sum of all columns of a 2D numpy array ( efficiently )
What would be an efficient way to generate a 1D array that contains the sum of all columns ( like ` [ 18 , 22 , 26 ]`) ?
To sum over columns : #CODE
Or , to sum over rows : #CODE
Many unary operations , such as computing the sum of all the elements
Summing over an axis or axes of a numpy array is done with the ` sum ` function .
@USER , ah , now I see what you mean , though the question does ask for the sum .
Then NumPy ` sum ` function takes an optional axis argument that specifies along which axis you would like the sum performed : #CODE
You can use the sum of the squared errors .
try to minimize the sum of the absolute values of all the left hand sides ( which should be zero if the equation is solved exactly ) .
this code gives an error ` TypeError : cannot concatenate ' str ' and ' list ' objects ` for line ` new_line = d [ looking_for ] + ' | ' +columns [ 1 :] `
Also I need to keep track of new email addresses in fr , so I need to append the details of the dict d in a file .
Is that a log from a REPL such as iPython ?
" built-in method sum " weird message when printing a sum
Using Python NumPy to calculate the sum of the column of a matrix : #CODE
You have to make a call of the ` sum ` function .
i want to make a histogram spanning multiple files in a Folder .
I know that i can create a histogram using Numpy with :
How can i use this to create a histogram over multiple files , that gives me the absolute number of Occurrences of the Strings ?
:-) it's the eye of experience .
I'm trying to find the indices of all elements in an array that are greater than a but less than b .
I have a numpy array of zeros .
Of course there are [ related routines ] ( #URL ) argwhere , nonzero , where , and extract that can do the opposite eg can find the locations of non-zeros in an array
The key is , if you do an indexing using multiple numpy arrays ( advanced indexing ) , numpy will use pairs of indices to index into the array .
Of course , make sure ` x ` and ` y ` are both either C-order or F-order otherwise , the calls to ` reshape ` and ` flatten ` might give different orders .
You can broadcast ` np.power ` across ` A ` if you change its shape to that of a column vector .
The transpose and stack may be reversed , this may be faster : #CODE
This is likely only worth doing if the function you are calculating is quite expensive , or it is duplicated many , many times ( more than 10 ); doing a tile and stack to prevent calculating the power function an extra 10 times is likely not worth it .
Please compare with the broadcasting method above , not the one suggested by larsmans below ( which requires an extra reshape and transpose ) .
` reshape ` and transpose are very cheap .
So , this is the part that replaces existing IF / ELSE blocks under the outer IF Operating / Max / Min ?
It seems to return numbers at indices where the number in " res " is not " nan " and of course , passed to sum() .
" shape mismatch : objects cannot be broadcast to a single shape " error
Also ` np.ix_ ` doesn't do anything really amazing , it just reshapes it's arguments so they can be broadcast against each other : #CODE
Because you're expecting the " outer product " type of view .
The " outer product " view is probably the more useful one when you're talking about boolean indexing , but the default in numpy is element-wise indexing .
Because numpy supports several types of indexing and broadcasting allows numpy to easily support " outer product " indexing with the ` ix_ ` function , defaulting to element wise indexing seems like the reasonable choice .
Getting indices of both zero and nonzero elements in array
I need to find the indicies of both the zero and nonzero elements of an array .
Put another way , I want to find the complementary indices from ` numpy.nonzero() ` .
If so , you can get vector of logical values were zeros are , invert it ( fast ) and take vector of logic value with non-zero values .
Assuming you already have the range for use ` numpy.arange ( len ( array ))` , just get and store the logical indices : #CODE
then when you actually need the integer indices you can do #CODE
The diagonal of the covariance matrix are the variance of each parameters , so the uncertainty is it's square root
This method is intended for fitting " lumpy " ovals like a potato , not " twisted " ones like a pretzel :)
I guess it should be ` max ( i-7 , 0 )` .
What's the best way in matplotlib to make a series of subplots that all have the same X and Y scales , but where these are computed based on the min / max ranges of the subplot with the most extreme data ?
Each histogram will then have different ranges / ticks for the X and Y axis .
I'd like these to be all the same and set based on the most extreme histogram limits of the histograms plotted .
One clunky way to do it is to record the min / max of X / Y axes for each plot , and then iterate through each subplot once they're plotted and just their axes after they're plotted , but there must be a better way in matplotlib .
* ValueError : non-broadcastable output operand with shape ( 0 , 0 , 1 ) doesn't match the broadcast shape ( 0 , 0 , 0 )
I've thought about using a loop to import each text file as a 2D array , and then stack them to create a 2D array , although couldn't find the appropriate commands to do this ( I looked at vstack and column_stack in numpy but these don't seem to add an extra dimension ) .
An alternative if you dont want to transpose twice .
IF all data is of the same shape then just append to a list .
which is an array of shape ( 10 , 50 , 2 ) ( transpose if you want ) .
I'be been able to let the ' 1's represent a certain PNG tile in a dictionary , but how would i import a map file with other numbers and even symbols like $ or % or @ ect inside my dictionary instead of just all the 1s ?
Also , supposing you want to put the names of the files inside the array , thus creating an array of strings instead of an array of ones , you could loop over like this : #CODE
In the above code , ` final_image ` would be a large ` pil.Image ` object where you paste each tile , and ` tiledict ` is the dictionary from where you take these images from .
Remove leading and trailing zeros from multidimensional list in Python
I need to remove only the leading and trailing zeros from the inner lists , so that I end up with : #CODE
I tried iterating through the list and using len() to determine the length of each sublist then looking for zeros at the beginning and end , but this doesn't help when there are multiple zeros leading or trailing .
what if the last element is 10 , your code will strip it's ` 0 ` too .
For each value , create a new array by subtracting that value from each element and comparing the result with ` a ` , and then join those up : #CODE
( If you remember my original answer , I'd somehow blanked on ` reshape ` and was doing the same thing by multiplying ` m ` by a column vector of 1s , which obviously is a much stupider way to proceed . )
@USER : Reshape it from N dimensions to 2N dimensions , with the extras having a size of 1 , then transpose , then subtract that .
Fast Fourier Transform ( fft ) with Time Associated Data Python
How can I perform a fft on such data to ultimately achieve a Power Spectral Density plot frequency against |fft|^2 .
However , I would like to do a linear regression on both to show how much the two arrays correlate .
Only a few values are nonzero , but they have a nontrivial spatial distribution .
I want to delete all 2-D slices in that array which have only zeros in it .
but that one seems to work only from the start and leaves out the zeros at the end .
You can use transpose to rearrange your axes , but it sounds like you might really be looking for a sparse array
People have implemented multidimensional sparse arrays , which might be good depending on what exactly you need to do with the points afterward , if you check this out for example : #URL But if your code works for the z-direction , why not just transpose to rotate the dimensions twice to repeat the process , then once more to get back to the original order ?
The ` where ` gives the indices for the delete .
The numpy documentation for dtypes refers to C-API for how to export custom dtypes , but that document somehow only explains the existing dtypes not how to create new ones .
The reason why I can't combine them into a single matrix with one set of x and y axes ( by filling the gaps with zeros ) is that the combined matrix would be huge and there are large spaces in between the strips .
Same goes for the result of ` dot ( A , x )` .
Yes , I want it to be reduced to two dimensions .. or max three so that I can plot and interpret all these .
You can find the indices like this : #CODE
So far I am using ` dot ( diag ( b ) , C )` , but the dot product naturally has a lot of multiplications
Convert the matrix to an array , perform ( broadcast ) array multiplication , and convert back : #CODE
so , does the vector argument in the hadamard product get automatically expanded to a diagonal matrix ?
For 100,000 loops , einsum takes 1.16 seconds , dot takes 1.61 seconds and atleast_2d takes 2.03 seconds .
dot diagonal : 15.6 s ( henrikr , TS )
Actually NNlist is a list of np.arrays because it is created using append ( the size is not know a priori ) .
For example if the data is all small integers you might be able to use ` numpy.bincout ` or if the unique values in each of the arrays are mostly the same there might be some optimization that could be done over the whole list of arrays .
numpy diff on a pandas Series
Pandas implements ` diff ` like so : #CODE
Because np is taking ` np.asanyarray() ` of the series before finding the ` diff ` .
This is just babbling of course , I have myself a similar issue with min ( min ( .. )) measures and had trouble to generalize these considerations I'm putting here .
I hope this helps , I faced with 25 x 25.000 points in a pairwise comparison ( 25 x 25 x 25.000 points in total ) , and my code runs from 1 min up to 3-4 hours ( depending on the number of points ) .
How can we join two columns of a numpy array into a single column , separating the values by a space ?
Is there a nice numpy function to perform such a join ?
TypeError : cannot concatenate ' str ' and ' numpy.ndarray ' objects
A different way to do this ( just for fun ) is using ` tile ` ( doc ) #CODE
which will flatten ` val ` to a 1D ` np.array ` if you know it is 1D data or #CODE
Powell's algorithm uses two loops , if you parallelize the inner loop you can speed up the outer loop and thus the entire algorithm .
If yes , you can use gradient descent ( old , slow but reliable ) or conjugate gradient .
" walking " gradient : walk from x 0 in direction e 0 to x 1 ,
The user-supplied gradient function
How to get the sum of a slice of an n-dimensional array in numpy ?
I would like to calculate the sum of all the elements in a slice .
To sum the ` i ` th slice along the ` d ` th dimension : ` arr.take ([ i ] , axis =d ) .sum() `
- ) It still tells ' IndexError : arrays used as indices must be of integer ( or boolean ) type ' at the last line , but it might help me now .
It is better to first append to a list and then use ` np.concatenate ` ( or its specialized derivatives ) for speed reasons .
Fortunately , numpy includes a convolve function which we can use to speed things up .
The numpy implementation of convolve includes the starting transient , so you have to remove the first N-1 points : #CODE
Note that convolve does include a `' same '` mode which seems like it should address the starting transient issue , but it splits it between the beginning and end .
An easier way to solve the issue is to use ` mode= ' valid '` in ` convolve ` which doesn't require any post-processing .
BTW , here is a command to show the difference between the modes : ` modes = ( ' full ' , ' same ' , ' valid ') ; [ plot ( convolve ( ones (( 200 , )) , ones (( 50 , )) / 50 , mode=m )) for m in modes ]; axis ([ -10 , 251 , - .1 , 1.1 ]); legend ( modes , loc= ' lower center ')` ( with pyplot and numpy imported ) .
If you do choose to roll your own , rather than use an existing library , please be conscious of floating point error and try to minimize its effects : #CODE
1 . being applied to original problem , this would be terribly slow ( computing average ) , so this is just irrelevant 2 . to suffer from the problem of precision of 64-bit numbers , one has to sum up >> 2^30 of nearly equal numbers .
Yes , you're right that this takes 2x more operations than simple sum , but the original problem is compute __running mean__ , not just sum .
To go from a sum to an average is trivial , so let's ignore that .
When you have the sum at position x , then you get the sum at position x+1 by subtracting the left-most value in the window and adding the new value in at the right .
There are different implementations possible , in one case comlexity is O ( n log ( n )) , in other case complexity is O ( n ) , but there is IMHO no benefit from such summation .
@USER How do those have greater control than ` concatenate ` ?
I thought ` concatenate ` can do an arbitrary axis , whereas ` hstack ` , ` vstack ` , and ` dstack ` concatenate along a particular axis .
By " greater control " I meant that , say , with ` hstack ` you always will concatenate along 1-axis .
So , using ` hstack ` you won't " accidentally " concatenate along the wrong axis , so you would in this sense be more " in control " of the situation .
but this requires me to know how many dimensions the original array has ( and concatenate [ 0 ]' s when working with higher dimensions )
I have the desired indices of ` C ` stored in CSC format which is specified here .
I check mathexchange and while making the tags for the post , it didn't have any the ones that would seem relevant like scipy and numpy or even sparse .
So for example , you can use the same basic logic of @USER , but instead of having a list of ' i ' and ' j ' elements you have a list ( or array ) of four-tuples of i_start , i_end and j_start , j_end that define sub-matrices of C then you can use those indices ( though rules established in those links ) to figure out the the sub-matrices of A and B you need to solve for the desired blocks of C .
So the slice ` [ 0:1 ]` is the half-open range ` [ 0 , 1 )` , meaning just the index ` 0 ` ; the slice ` [ 0:2 ]` is the half-open range ` [ 0 , 2 )` , meaning the indices ` 0 ` and ` 1 ` .
` rand ( ' seed ' , x )` switches to a completely different PRNG .
If you use ` rand ( " seed " , 4 )` this actually switches to an earlier version of the PRNG used previously in Octave , which PRNG is not MT19937 at all , but rather the Fortran ` RANDLIB ` .
If it's the latter , why are you trying to pass 2D indices into a 1D array ?
I wanted the sum to be the same as the existing distribution , but the values to be different .
My insight is that it's possible to re-generate the values a few times until you get ones you like .
Again , this solution is tailored to my problem , where I wanted to generate values close to a sum , without going over .
I feel it's valid to ' let the computer do the work ' , or to hand-pick the best option from a few reasonable ones .
Make sure you convert the max to a ` float ` : #CODE
( even if you did actually transpose it ... which you did not ) #CODE
When you say " For each Xi " , do you mean there are multiple rows of ` Y ` , or do you mean " for each unique value of Xi " ?
this will do elementwise comparison and return a boolean list at indices which have your values .
if you just asked how to concatenate two ` csr_matrix `' s you would get totally different answers ...
Regarding your question - I'm sure there are many ways to concatenate two sparse matrices ( btw this is what you should look for in google for other ways of doing it ) , here is one , but you'll have to convert from csr_matrix to ` coo_matrix ` which is anther type of sparse matrix : Is there an efficient way of concatenating scipy.sparse matrices ?
EDIT : When concatenating two matrices ( or a matrix and an array which is a 1 dimenesional matrix ) the general idea is to concatenate ` X1.data ` and ` X2.data ` and manipulate their ` indices ` and ` indptr ` s ( or ` row ` and ` col ` in case of ` coo_matrix `) to point to the correct places .
OK , I tried rotating about the exact centre of the DCT pixel ( ` fftshift ` places it at ` ceil (( dim-1 ) / 2 . )` for even and odd-sized dimensions ) .
How about if I have an arbitrary number of indices ?
Change ` empty ` to ` zeros ` to make your code work that way .
However in order to do more useful things with this I would like to be able to approximate this array of values with a continuous function so I can calculate the field and its gradient at points not on my grid .
However , when I do the array conversion on it , it only seems to convert the outer list into an array .
the one liner returns a list where each element is the result of applying the lambda function to each array in items . the lambda function just does a dot product of each array with -1 times the same array and then squares the results .
Calculate the sum of squares between all elements of each of ` N ` ` M ` -dimensional vectors and store the results in a list : #CODE
We don't really need to finish the calculations of the whole vector if it already has a bigger sum than best .
You are looking for ` apply ` ( ` merge ` is like a database join . ): #CODE
Of course I can compute this " by hand " through outer products , something like : #CODE
Having in mind that outer product is Kronecker product in disguise your problem should be solved by this simple functions : #CODE
If you apply it to 2 vectors of sizes ( 5 , 8 ) and ( 4 , 8) , you get a new one of ( 20 , 64 ) that you then try to reshape to ( 5 , 4 ) ...
I will , however , want to take its transpose , with preferably O ( 1 ) memory and time .
Can I hold nan's instead of zeros to indicate " empty " cells ?
You are looking for an outer ` join ` , here is a simple example : #CODE
And to get the indices : #CODE
I prefer ` nonzero ` to ` where ` for this , because ` where ` does two very different things depending on how many arguments are passed to it .
I use ` where ` when I need its unique functionality ; when I need the behavior of ` nonzero ` , I use ` nonzero ` explicitly .
If there a max file size for np.fromfile() ?
But all values are read as zeros when the count is set to 9e7 : #CODE
I've had this problem of numpy writing zeros with large arrays ( either with tofile or save , or even using other libraries such as pyfits ) .
Sets are unordered collections of unique elements , so set ([ 2 , 8 ]) and set ([8 , 2 ]) are exactly the same .
Then how to append or combine them into an array with 2 columns and 99 rows ?
` vstack ` , ` hstack ` and ` transpose ` are your friends for things like this .
Is it possible to load them all as memory-mapped files , and then concatenate and slice through all of them without ever loading anythin into memory ?
To avoid this you can easily create a thrid ` memmap ` array in a new file and read the values from the arrays you wish to concatenate .
In a more efficient way , you can also append new arrays to an already existing file on disk .
1 ) concatenate along ` axis=0 ` #CODE
You can define a third array reading the same file as the first array to be concatenated ( here ` a `) in mode ` r+ ` ( read and append ) , but with the shape of the final array you want to achieve after concatenation , like : #CODE
2 ) concatenate along ` axis=1 ` #CODE
You may repeat this process to concatenate in pairs of two .
Finding the indices of the top three values via argmin() or min() in python / numpy without mutation of list ?
I don't think I can just delete the value , because otherwise it would shift the slices over and the indices is what I have to recover the original ID .
Does anyone know how to get argmin() to spit out indices for the lowest three ?
Numpy includes an ` argsort ` function which will return all the indices .
Then you can change the frequency to seconds using resample , specifying how you want to aggregate the values ( mean , sum etc . ): #CODE
I am not that sure of my implementation of the gradient function , but it looks reasonable .
The first optimization ( without gradient ) ends with a whole lot of stuff about division by zero .
The second optimization ( with gradient ) ends with a matrices not aligned error , which probably means I have got the way the gradient is to be returned wrong .
Here I'm using an approximate gradient ( seemed to work fine with your data ) , but you can pass ` fprime ` as in your example ( I don't have time to check its correctness ) .
function append_fields() or rec_append_fields() to append a field with some
ValueError : operands could not be broadcast together with shapes ( 10 ) ( 10 , 3 )
where ` 10 ` is the length of my existing array , and ` ( 3 , )` is the shape of the field I want to append .
ValueError : operands could not be broadcast together with shapes ( 4 ) ( 4 , 2 )
In light of the fact that my new field doesn't have the same shape as my array , I suppose that my desired append is impossible , suggested by @USER .
How can I append a field like this ?
I should note that for my application , I can append an empty field as long as it's possible to somehow change the shape later .
How can I append a field that is similar in shape to one of the fields already in the array ?
I think that if the new field must have a shape , then the best thing is to create a new array and then merge or join the two similarly shaped structured arrays as in : #URL
This means that the system of equations you are trying to solve does not have a unique solution ; ` linalg.solve ` can't handle this .
By " no unique solution " do you mean it may have multiple solutions ?
I am resampling a real signal , and since I have at my disposal its fft from ` rfft ` , I want to use ` irfft ( signal , new_length )` .
This is a working code snippet that resamples a signal of length 4 using complex fft : #CODE
What do you do with the indices at the edges ?
More details : I have a nontrivial distribution of ones and I want to check how many neighbours of them are ones as well and doing stuff on them depending on how many neighbours ( in all dimensions ) they have with the same value .
For example : Delete all ones with exactly 5 neighbours , count all ones with 3 or more neighbours and so one .
If you need the neighbours in the other axis , maybe diff the result of np.swapaxes ( a ) and merge the results together somehow ?
However you must reshape .
Whereas the ` norm ` function correctly calculates the norm for the entire matrix giving me a single value that is not what I'm looking for .
but this is ( or rather , should be ) merely a difference in dimensionality assumptions , and taking the covariance of the transpose vector should work fine , right ?
I mean , the vector has 4 " features " , so given that I want to measure the variance between the features and store them in appropriate places ( for example , cov ( 1 , 3 ) , I need a covariance matrix , do I not ?
If you want to pass in the transpose , you'll need to set ` rowvar ` to zero .
At this point though you need to find a domain relevant way to encode the information .
Keep in mind that the fft returns a complex array .
Because ` 2*sin ( a*pi*x ) == i* ( exp ( i*a*pi*x ) - exp ( -i*a*pi*x ))` , s_2 has all of it's power in the imaginary component of k-space ( notice the y axis is on the order of 1e-12 ) , s_1 is shifted slightly so you see a little bit of signal in the real component of k-space , but most of the power is still in the imaginary component .
I just need the equivalent in Python - presumably there is a similar load / reshape tool available .
I did a mean , max , min sum on the cube and it matches my fortran code .
Thanks for introducing me to ufuncs and accumulate !
@USER .Davidson .Pilon Now that you figured out how to do it with ufuncs and accumulate , please post a piece of working code as your own answer , or editing the question , so that others will benefit also .
When I padded the signal myself with zeros to such a length , it took 100ms .
s += usage can be vectorized with cumsum , but the floor on the sum is problematic .
One could hunt along those indices and do a ` cumsum ` in blocks of ' Trues ' , but I'm not sure if it would be more efficient than your loop .
Add a linear function of the coordinates to your function to give some nonzero , but very small slope to the flat areas .
If you need to keep track of which pairs you've checked , so that you don't keep the same ones , use some sort of memoization on top of that .
How do I retrieve the ( logical ) indices of the integers or slice objects ?
@USER Actually , the data is stored as a list : ` [ 0 , slice ( None )]` ; from this list I can retrieve the slice objects ( or the indices of those ) using a list comprehension , but I thought it would be quicker to ( mis ) use a numpy array for that purpose .
If you need to access the indices of your arrays .
@USER The OP is talking about indices .
You can use ` np.where ` to get indices and ` np.logical_and ` to set two conditions : #CODE
But this returns the values whereas the use of np.where will return the indices of the values .
To make sure they appear as an ordered pair I'd have to find the indices where ` x ` and ` y ` appear in ` cluster ` and compare them , which seems very clunky and inelegant , and I'm certain there must be a better solution out there .
And how come the last example does not return any kind of sum and does not raise any error either ?
If the argument is a generator , Python's builtin ` sum ` get used .
` _gentype ` is just an alias of ` types.GeneratorType ` , and ` _sum_ ` is alias of the built-in ` sum ` .
If you try applying ` sum ` to ` gen ` and ` lst ` , you could see that the results are the same : ` 6.0 ` .
The second parameter of ` sum ` is ` start ` , which defaults to 0 , this is part of what makes your result a ` float64 ` .
Could you explain better why the sum is an integer when the dtype of the array is ` int64 ` but a float when the dtype is ` uint64 ` ?
Second , your example matrices are poorly conditioned : the diagonal elements are orders of magnitude larger than the off-diagonal ones .
Also be sure to specify the array min and max to avoid scaling .
multinomial pmf in python scipy / numpy
There's no built-in function that I know of , and the binomial probabilities do not generalize ( you need to normalise over a different set of possible outcomes , since the sum of all the counts must be n which won't be taken care of by independent binomials ) .
My implementation of the multinomial coefficient is somewhat naive , and works in log space to prevent overflow .
Also be aware that n is superfluous as a parameter , since it's given by the sum of the counts ( and the same parameter set works for any n ) .
Furthermore , since this will quickly underflow for moderate n or large dimensionality , you're better working in log space ( logPMF provided here too ! )
You can then use ` numpy.non-zero ` ( to filter your array to the indices of those datetimes where , for example , year is 2012 ): #CODE
However it seems that in this case it does not count correctly the number of degrees of freedom ( DOF ): The chi-square statistics ( the first return value of ` chisquare `) is the same before and after removing the zeros , so only DOF could change .
Grabbing tensor indices with name of tensor attached
This returns the indices of the tensor meeting a specific criteria , however it does not name them as components of the tensor , i.e. they come back as ` [ 0 , 0 , 1 , 1 ]` versus ` x [ 0 , 0 , 1 , 1 ]` for a tensor ` x ` .
Is there a built in or slick way to grab the components of a tensor that meet a certain criteria where the components are written with their indices and the name of the tensor attached ?
The value held in your tensor ` x ` at position with indices ` [ 0 , 0 , 1 , 1 ]' ?
It at least removes one loop ( and those nested indices ) , but it's not much faster than the original , ~20% faster according to ` %timeit ` in IPython .
I couldn't get ` np.fromfunction ` to work either , as it passed ( 2 ) 3 x 3 ( in this case ) arrays of indices to ` np.interp ` , the same arrays you get from ` np.mgrid ` .
You could invert your sorting order and then transpose the resulting matrix .
As a result , you get the horizontal edges in edges_horizontal , which is an 2d array ` [[ p_{0} , p_{1} ] , ..., [ p_{n} , p_{n+1} ]]` , in which p_i are indices into the ` coordinates ` array .
If ` y ` contains indices that are valid for ` x ` , then : #CODE
How to flatten a numpy array of dtype object
It's a common requirement to flatten a nested list , have a look at [ this ] ( #URL )
I m doing some log analysis and examining the length of a queue every few minutes .
that takes a floor value .
Here is where the fourier part starts , I take the fft and turn it into a powerspectrum .
So I have no idea what could be going wrong , I think its the fft , as nothing else can really go wrong .
The green dots are the fft spectrum , the lines are the fits , the red dot is where it thinks the cutoff frequency is , and the blue line is the chi-squared fit , looking for the lowest value .
Your library FFT routine might include a scale factor of 1 / sqrt ( n ) .
Check the documentation for the fft you used , as the proportion of the scale factor allocated between the fft and the ifft is arbitrary .
I was working with the normalized , orthonormal hermite polynomials , and as far as I remember , my errors were much smaller in absolute terms , but I guess not in relative terms .
There is a manner to resize all images in a standard size ( i.e. 200x200 ) or a manner to have a 3d array with matrix ( a , b ) where a !
You have to resize all your images to a fixed size .
Edit : the above won't work , try instead ` resize ` : #CODE
Now I tried to do the same thing with a hog feature , which produce a numpy array with difference shapes too and i need to reshape for SVM . data = a.reshape (( n_samples , -1 )) and this code doesn't work a = np.dstack ( listagrigie )
a= np.rollaxis ( a , -1 ) and i think resize cannot help me here !
Note that in the last one , your list ( if you are setting a column ) would have to be a list within a list ( i.e. the outer list acts as a row , the inner lists act as columns ) .
Make the sum of integers in a matrix maximal by multiplying rows and columns by -1
Having a Matrix ` M ` of size ` m , n ` over integers , what would be a good algorithm to transform it such that the sum of all elements is maximal ?
Rough , overall idea : What I have thought about is to move each minus sign from one such negative number to the positive number whose value is smallest , such that the minus would have the least influence on the sum .
@USER : That problem reduces to some linear algebra mod 2 .
In this specific case , M is already the maximal sum ( 27 ) #CODE
A variant of your method can do this in O ( 2^ ( min ( m , n )) poly ( m+n )) time .
Calculate the length ( L2 norm ) of each matrix row ; in other words : ` sum ( multiply ( M , M ) , 2 )`
Note that the default ` ord ` for ` numpy.linalg.norm ` is L2 norm .
I am trying to norm my data but I keep getting ValueError : dimension mismatch .
I think the problem you're having is because you are declaring ` list_of_arrays ` in the stack , and its size must be known at compile-time .
What I did was just changing the time from absolute to relative and dynamically adjusting the range of the axis .
and would like to stack them in grid format , ending up with #CODE
Ok , so considering I have N block matrices with ` bm x bn ` dimension and want to stack them in a ` m x n ` matrix , provided ` N = m x n ` , I would then have
it seems like its ` [[ 1 , 2 ] , [ 2 , 3 ] ,... ]` and he wants to flatten it
The short version is : ` append ` is faster than ` extend ` , and Joran Beasley's suggestion ` itertools.chain.from_iterable ` is slightly faster than either but only if you replace the ` map ` with a list comprehension .
So , you may find that ` append ` wins in that case .
I suspect part of the difference between my results and theirs was improvements in iterators between 2.6.0 and 2.7.2 / 3.3.0 , but the fact that we're explicitly using 2-element elements instead of larger ones is probably even more importantly .
They still aren't competitive with ` append ` or ` chain.from_iterable ` , but they're in the right ballpark .
have you any idea , how I can bin 3 arrays to a histogram .
Howto bin series of float values into histogram in Python ?
How to correctly generate a 3d histogram using numpy or matplotlib built in functions in python ?
2D histogram with Python
If you had a _systematic_ method of shifting them ( say you know you should only get integer data and you reshape them into a spiral ) it would be ok , but adding _random_ shifts is not .
Using log scales is ok , using an arbitrary transform on your axes is not .
All the duplicated x , y points are now reduced to a unique set and their z values have been averaged :
With a transpose ?
the given axis , higher order differences are calculated by using diff
Update : In recent pandas , you can use the dt accessor to broadcast this : #CODE
Python NumPy doesn't resize correctly
I can't get this numpy array to resize correctly .
32 > = 25 , so your change isn't surviving the resize .
or if you really wanted , you could copy the data over before a resize : #CODE
[ I can't remember how resize handles memory , but I wouldn't worry about it . ]
Note that this behavior is different from a.resize ( new_shape ) which fills with zeros instead of repeated copies of a .
F = sin ( x ) *cos ( x ) .
I'm trying to get some grip on Python's fft functionality , and one of the weird things that I've stumbled on is that Parseval's theorem doesn't seem to apply , as it gives a difference of about 50 now , while it should be 0 .
where the reshape is to have broadcasting do its magic .
With numpy arrays you can access multiple indices directly by indexing into the array with a list : #CODE
You can avoid the explicit inner loops and compute the matrix of euclidean distances directly by pre-extending the original matrix so that no checks are needed on element indices : #CODE
Summing over pair of indices ( or more ) in Python
For example , what if I want to sum over three indices ?
It's more like " I want an efficient and easy to understand way to sum over indices in general " .
It also works for any number of indices .
For small samples , the second code I showed is quicker but it becomes unbearably slower with larger samples , unlike yours that is fairly quick because you're using that trick of realizing that the original sum can be express as a sum of multiplications .
Do you know how to extend this result to consider more indices ?
So if it were yi + yj - yk , you could sort , then go through for each k , and have a rolling interval for the min yi and max yj that gives a sum greater than yk . that shows how much yk contributes .
To clarify : do you want to replace _all_ ` None ` s with zeroes , or just the ones that got added to the end of ` todayorders ` because it's shorter ?
This is a fine answer to the question as stated , but it's not actually very good for what the OP actually wants , which is effectively to stack two ` numpy.array ` objects of different shapes .
And , if you know the size ( which you do it's ` max ( len ( todayorders ) , len ( lastyearorders ))`) , in some cases it's faster or simpler to pass an explicit ` count ` as well .
The mcint package performs a monte carlo integration : running with a non-trivial ` W ` that is nonetheless integrable so we know the answer we get ( note that I've truncated r to be from [ 0 , 1 ); you'll have to do some sort of log transform or something to get that semi-unbounded domain into something tractable for most numerical integrators ): #CODE
Therefore , I'm trying to create a loop to go through the ' allorders ' array and stack them via : #CODE
Sampling on a spline to a given max chordal deviation
Numpy has a diff operator that works on both numpy arrays and Python native arrays .
There's also ` numpy.ediff1d ` , which allows you to explicitly prepend or append to the diff using the ` to_end ` and ` to_begin ` parameters , e.g. : #CODE
Create a loop that runs function ( with parameters which are indices of data set ) on all items of the data set ?
This basically sees whether two circles ( with coordinates that correspond to the indices n and m ) connect .
The n and m parameters refer to the indices in the data sets x and y , which come from a numpy.random array : #CODE
m and n are indices in these lists .
Therefore , n and m correspond to indices in the array , but I'm not sure how ?
What I've been doing now is manually inputting the indices to see whether any two circles in this array connect - is there a -for loop that can do this in a more efficient way ?
Also , n and m aren't the x and y coordinates in the circle - they are actually indices that correspond to two different circles .
The idea , however , is that the function tells me whether [ i ] two [/ i ] circles connect - m and n aren't the x and y coordinates of the circles , they are actually indices that correspond to two different circles within the array .
@USER again , this error is perfectly normal , it means you are trying to get a boolean ( True / False ) value out of a list / array , which is impossibly , you can use any ( list ) or all ( list ) to check if any or all of the list are True . but again - you must tell me the error / stack / code being used , else i can't really help more than this .
Python : When running a for loop , how to append each solution to a list , which is appended to a new dictionary ?
However , from reading your code , it seems more like you might be wanting to just iterate over the indices of the first array , which means you probably want something like ...
In my application , the ` arr ` list is not " too ragged " ( i.e. the max length of any interior list is not extremely different from the min length of any interior list ) .
since that's where ` 5 ` and ` 16 ` share the same indices ; but instead i get a
Numpy's ones and zeros array creation -- how to do the same for an arbitrary value ?
but to be honest , I tend to either add to the zero matrix or multiply the ones matrix instead , as initialization is seldom a bottleneck .
Well , ` ones ` is basically ` empty ` + ` fill ` with some error catching , so the only place that any performance difference * could * show up is in small array sizes , because asymptotically they do exactly the same thing .
Right now , my implementation of the method looks up the pressure indices of the two closest altitude values for the given lat , lon , time .
Then it uses those altitude values and the two pressure indices to build up the 4D hypertetrahedron in the temperature matrix and interpolate ...
1.86ms , but I need it to be run approximately 50,000 times in 5 seconds , i.e. max run time 0.1ms .
It's slow because 1 ) outer loop in python ,
` trinormals ` then is an array of shape ` ( n , )` holding a value assigned to each trinagle , and you want ` vertnormals ` to be an array of shape ` ( m , )` holding , for each point , the sum of the values assigned to each triangle that point is a vertex of .
The second issue is the broadcast on the last step never matched as it was trying to broadcast ( m , n ) v ( 3m , 1 )
The two lines bellow are the the same when the elements of ` index ` are unique , but different when ` index ` has repeated values : #CODE
Didn't realise that project existed - so thanks - will have to keep an eye on that .
Here , since we have 5 parameters , my initial guess is a list of all ones of length 5 .
( ` scipy ` defaults to using ones if you don't provide a guess also ) .
In my experience , 1D is the norm in numpy for vectors .
If you stick with 1D vector of shape ` ( n , )` , you can reshape on the fly for specific operations requiring the 2D shape : #CODE
Numpy will automatically reshape your 1D vectors to shape ` ( 1 , n )` when broadcasting it for an operation with a 2D array involved .
However , when I stack back the copied rows and columns using ` numpy.vstack() ` and ` numpy.hstack() ` , then everything again converts to strings type .
I have a list of datetime objects and would like to find the ones which are within a certain time frame : #CODE
The ` .bisect_ * ` functions return indices into the ` dates ` list : #CODE
` bisect ` does not traverse the whole list , it's methods bounded by O ( log n ) complexity , while this answer requires you to access the whole list ( O ( n ) complexity ) .
If your proportion is always one-dimensional ( i.e. concatenate in one dimension only ) , you can use this : #CODE
histogram matching in Python
I am trying to do histogram matching of simulated data to observed precipitation data .
@USER , I am trying to the CDF matching histogram matching as shown in the image here ..
How to get the values from a NumPy array using multiple indices
what if indices are 2D ?
I really like your answer ... great use of outer product !
As I have never seen the use of [ None ] before is there any difference between that and reshape ( 1 , -1 ) ?
Anaconda comes with many of the packages in question ( scipy , numpy , sklearn , etc ... ) as well as the conda installer to install additional packages should additional ones be necessary .
I don't understand why the inner-most loop does not fully translate to C code ( i.e. the last line , celle [ l ] = ... ) , see output from ` cython -a feedback ` :
dot product between scipy sparse matrix and numpy arrays
I am trying to find the dot product between a scipy sparse matrix and a numpy.ndarray . tensor refers to theano.tensor .
I cant compute the dot product .
Apart from that , it seems like I am not able to add a constant ndarray to a sparse matrix and further more , I cant apply the tanh function .
Also , could you separate the ` dot ` operation , the addition , and the ` tanh ` operation to better drill down on the error ?
The error ralated to the dot product is the type Error .
2 ) use * instead of dot as both X and W_hidden are matrices .
Here , i try to make a facetious reshape program that will take a size 20 array and reshape it to a 5x4 array : #CODE
Yes , i am aware of ` reshape ` .
If you want the former , the easiest way to do it is to reshape the input array , then just iterate that : #CODE
And it would be a bit silly to suggest that " the way to write your own ` reshape ` is to first call ` reshape ` , and then "
But this gives me all zeros : #CODE
Without having to make a one-item list and then flatten it .
Give me a couple of min .
Unionize the non-common column names ( as in outer join ) .
Though when I try ` join= ' outer '` it raises an exception signaling that it's not implemented .
This assumes that the merging happens on the indices of each of the dfs .
New columns are added in an outer-join style , but columns that are common ( and not part of the index ) are used in the join via the ` on= ` keyword .
The cardinality of the " join space " is the issue and may need to be worked around .
@USER : Yes , I want an outer join but I want it to use the indices of the left and right df .
@USER : It's possible in the sense that there's always a unique index for all the dfs in question .
I can explain what's unclear if you let me know what - I basically want an outer merge that adds columns together based on a unique index
From the documentation , this seems like an outer join on an index with merge , but they behave very differently ...
I would like to be able to easily log heterogeneous data , for example : images , numpy arrays , matplotlib plots , etc , from within the algorithms , and do that using two keys , one for the current frame number and another to describe the logged object .
scipy.sparse dot extremely slow in Python
It seems that the problem is that ` X `' s ` dot ` method , it being an array , doesn't know about sparse matrices .
Alternatively , sparse matrices have a ` dot ` method that knows about arrays : #CODE
I'm using SciPy's weave inline with some C++ code of mine to transpose huge matrices ( about 200.000 x 15 ) .
This doesn't affect the way the transpose function works , but later on I have to convert certain rows into numpy float arrays anyways , so I was wondering if this could be done in the C++ code instead .
` arr = np.arr ([[ ' 0.5 ' , ' 0.1 '] , [ ' 0.2 ' , ' 0.2 ']] , dtype=float )` and then to transpose : ` arr.T ` , although you don't have to transpose to get the first row : ` arr [: , 0 ]` should do that .
I have a ( 3 , 3 ) numpy array and would like to figure out the sign of the element whose absolute
gives me the indices of the elements that I need , ` s = [ 0 , 1 , 1 ]` .
As a side note , you don't need to flatten ( reshape ) an array to use vectorize .
I can use a loop to do this but I get a feel that it would be better to first convert this array into a 3D array by ` reshape ` and then use the ` mean ` method on the 3D array along the third axis .
The trick is in the ` reshape ` .
` reshape ` returns a view of the original array , not a copy , so the conversion to 3D only requires altering the ` shape ` and ` strides ` attributes of the array , without having to copy any of the actual data .
To be sure that reshaping does not copy the array , but returns a view , do the reshape as #CODE
And in general there are only problems if you have been doing ` transpose ` , ` rollaxis ` , ` swapaxes ` or the like on your array .
It's just computing a dot product hence directly linear with the number of non-zeros ( i.e. approximatively the number of words in your test set ) .
Translate numpy's array reshape to OpenCV equivalent
I am having a problem to translate numpy's ndarray functions to their equivalent
OpenCV C++ calls to reshape / split a n-dimensional cv :: Mat into appropriate slices .
Can someone help me to translate the lines in question to C++ ?
The algorithm needs to be able to cope with removing several data-points and adding false ones as well .
What I want is to , in other words , map the red-circled positions ' new coordinate values ( the ones written beside them ) so that I can obtain the old coordinate from the new ( e.g. ` ( 1 , 1 ) - ( 170.82191781 , 162.67123288 )`) .
Make sure to define it in a module that includes ` __unittest = True ` so that it will not be included in stack traces : #CODE
Scipy inv ( A ) vs A.I
Under the covers , the getter for ` A.I ` is ` A.getI() ` , so these are equivalent , and both are probably equivalent to ` inv ( A )` .
( Having free functions ` foo ( A )` and methods ` A.foo() ` that do the same thing is pretty common all over ` numpy ` . ) Except that , as unutbu points out , there's more than one method called ` inv ` ; ` A.I ` will be equivalent to one of them , but not necessarily the same one you imported directly into the main namespace .
instead of numpy dot product , use simple * product .
Python will broadcast and the correct result will be obtained .
the addition doesn't work , because through numpy dot product , the size of the result matrix didn't match the expected and so the addition could not take place between two matrices that have different shapes .
If a sparse matrix is 99 % 0 , ` sparsematrix + 1 ` is 99 % ones -- dense .
` y = dot ( x + bias , npvec )`
to ` dot ( x , npvec ) + bias * npvec `
Next find the indices where d is greater than your threshold : #CODE
Problems using gradient boosting classifier in Scikit-learn
I'm trying to use the gradient boosting classifier ` ensemble.gradientBoostingClassifier ` in Scikit-learn for python , but I keep getting these error messages : #CODE
It is not important which direction the iteration goes along the diagonals ( if 1 and 2 switched placed , and 3 and 5 etc . in ` A `) only that the diagonals are orthogonal to the main diagonal of A ( the one produced by ` np.diag ( A )`) .
Oh , you mean , first diagonal as [ 0.048 ... ] , second as [ 0.599 .., 0.105 ] and so on ?
@USER note that ` diagonal ` accepts an offset argument to pick a different diagonal .
We can find out the indices associated with that order using ` np.argsort ` : #CODE
Side note : ` argsort ` does not use stable sorting , so for larger arrays you might get switching in the diagonal order .
redo some / all ( which ? i have no exp ) parts in cpp although i really would like to avoid this
Are the BLAS and FFT routines as fast as the ones packaged with MatLab ?
Getting rid of the fft is certainly a big improvement .
Essentially I am looking for something that would take the place of Ellipsis , but instead of expanding to the needed number of ` : ` objects , it would expand into the needed number of zeros .
fast way to get the indices of a lower triangular matrix as 1 dimensional list in python
You actually do not need to have an array to get the indices , there is also ` np.tril_indices ` , which takes the shape as arguments .
` np.tril_indices ` takes an integer for the dimension of a matrix and returns the tuple of indices directly
Performing the mathematically identic calculation with a product instead of a sum leads to values that are alright .
Why not do these sums in the log domain to avoid the precision problem ?
Eventually , ` 1+exp ( x )` gets so big that adding 1 to exp ( x ) gives the same thing as exp ( x ) .
In double precision that's somewhere around ` exp ( x ) == 1e16 ` : #CODE
See my last edit on what would be required to make the algorithm run in ` O ( N log N )` .
However , for what you want to achieve , ` O ( N log N )` or possibly ` O ( N )` would be sufficient .
But finally you would arrive at an algorithm that is ` O ( N log N )` .
To get it work ` O ( N log N )` is less trivial than I thought .
In a normal list , finding the position can be done by bisection in ` O ( log N )` .
I would be pleased if anyone could prove me wrong and show up a way to to insert an element in a sorted linked list in ` O ( log N )` .
Insertion in such a tree can be done in ( quite fast ) ' O ( log N )` .
Once you set it up , which is ` O ( N log N )` , the removal can be done in ` O ( log N )` per iteration .
The total thing is still ` O ( N log N )` .
Since B is going to be symmetric along the diagonal and sparse , I could use a triangular matrix ( upper / lower ) to store the results of the matmul operation and a sparse matrix storage format could further reduce the size .
Since you are after the product of a matrix with its transpose , the value at ` [ m , n ]` is basically going to be the dot product of columns ` m ` and ` n ` in your original matrix .
The product of its transpose with it is of course of shape ` ( 12 , 12 )` and has 16 non-zero entries , 6 of it in the diagonal , so it only requires storage of 11 elements .
If your original matrix has ` C ` non-zero columns , your new matrix will have at most ` C**2 ` non-zero entries , of which ` C ` are in the diagonal , and are assured not to be zero , and of the remaining entries you only need to keep half , so that is at most ` ( C**2 + C ) / 2 ` non-zero elements .
If your matrix is stored in ` csr ` format , then the ` indices ` attribute of the corresponding ` scipy ` object has an array with the column indices of all non zero elements , so you can easily compute the above estimate as : #CODE
If instead of column indices of non-zero elements we have row indices , we can actually do a better estimate .
For the dot product of two columns to be non-zero , they must have a non-zero element in the same row .
When you sum this for all rows , you are bound to count some elements more than once , so this is also an upper bound .
The row indices of the non-zero elements of your matrix are in the ` indices ` attribute of a sparse csc matrix , so this estimate can be computed as follows : #CODE
Numpy's ` nonzero ` will do this :
That is , the output is the indices of the non-zero elements in the array .
I see also from the example in the docs that I can then transpose this to reach my x , y final requirement .
@USER If you need a weighted moving average , then yes , probably ` correlate ` is the way to go .
But because ` correlate ` uses a generic algorithm , if your window is of size ` m ` , at every point it must perform ` m ` multiplications and ` m-1 ` additions .
Sum between pairs of indices in 2d array
This question was asked a few months ago here : Numpy sum between pairs of indices in 2d array
I have a 2-d numpy array ( MxN ) and two more 1-d arrays ( Mx1 ) that represent starting and ending indices for each row of the 2-d array that I'd like to sum over .
Numpy sum of values in subarrays between pairs of indices In that question , they want to find the sum of multiple subsets for the same row , so ` cumsum() ` can be used .
However , I will only be finding one sum per row , so I don't think this would be the most efficient means of computing the sum .
The cumsum , or reduceat or similar tricks may be extremely competitive if you are summing small enough arrays , or the sum is not very small portion of the array .
To be honest , I doubt the timing for the op solution is right ... especially for the large arrays , it should be very competitive .... maybe you did use the python sum ( you could also use ` .sum() ` to safe a lookup ) ?
In fact it's so competitive , that it is the fastest , once you replace the ` sum ` by ` np.sum ` .
If you multiply ` a ` by a vector of all ones , each element of the output vector will contain the sum of the corresponding row of ` a ` .
To get the ` d ` you need , you can mask out the elements that are excluded from each row , and then multiply by a vector of all ones to get ` d ` .
Then use the ' Clear ' button to strip it , and try again .
Bivariate splines would be fine , but as far as I can tell all of the functions for evaluating bivariate splines in ` scipy.interpolate ` take x , y values and return z , whereas I need to give z and return x , y ( since x , y are points on a line , each z maps to a unique x , y ) .
try plotting my example coordinates - in this case the line curves back on itself , so there can be no unique mapping from X --> Y or from Y --> X
Your code is hard to read , try following the norm and using 4 space indentation : #URL
Oh sorry I didn't know that was the norm , thought t'was 2 spaces like Ruby .
On a separate note , I cannot test it right now , but when using ` numpy.linalg.lstsq ` , I you don't need to ` vstack ` a row of zeros , the following works as well : #CODE
Use the most significant 4 bits of every RGB value , as indices into a three-dimensional look up table .
This relates directly to my interpolation example above , and is harder to come across , although worth keeping an eye open for it .
My fix was just to check the std if it is larger than some minimum ( ad-hoc chosed ) .
Since Numpy and Scipy use zero-based indices , row 0 is the first row and ` 1:3 ` denotes the first and second column .
` Asp [ 0 , 1:2 , 3 ]` is invalid because you've got three indices , ` 0 ` , ` 1:2 ` and ` 3 ` .
If you scroll down [ here ] ( #URL ) , you will get to it , and ` nonzero ` is part of the list , linking to [ this page ] ( #URL ) that is not linked elsewhere that I know of , so definitely hard to find .
If you had 6 points in the outer radius , you would get an hexagon .
The linear system to solve is of the shape ` A dot x == const 1-vector ` .
I took my chunk and added a row of zeros to the top and bottom of the array using vstack .
Finally I removed the outer rows and columns to return the image to its original size .
I was looking for teeny weeny holes , the ones gone are much bigger than I was expecting ,
( in fact , I need the transpose of this result )
` a [ rows.reshape ( -1 , 1 ) , cols ]` is a single advanced indexing expression basing on the fact that ` rows.reshape ( -1 , 1 )` and ` cols ` are broadcast to the shape of the intended result .
I know that there is the ` nonzero ` function in numpy but what I need is the first index so that I can use the first index in another extracted column .
But if all you are looking for is finding the first element that satisfies a condition , you are better off using ` max ` .
For example , in matlab , ` find ( A = 9 , 1 )` would be the same as ` [ idx , B ] = max ( A = 9 )` .
I am trying to compress a huge python object ~15G , and save it on the disk .
Due to requrement constraints I need to compress this file as much as possible .
I am guessing perhaps pandas.Series does not implement reshape function and I am calling the version from np.array ?
You can call ` reshape ` on the values array of the Series : #CODE
I actually think it won't always make sense to apply ` reshape ` to a Series ( do you ignore the index ? ) , and that you're correct in thinking it's just numpy's reshape :
It is tempting to catch the ` reshape ` s and don't allow them , but a lot of the cool things you've come to love in numpy rely on altering the dimensions of the underlying data , e.g. get rid of ` reshape ` and ` tile ` does not work any more .
But the problem is not having access to ` reshape ` from within your object , but knowing if the call to ` reshape ` your object got is coming from a user , that should get a ` NotImplemented ` exception , or from another numpy method relying on ` reshape ` to do its thing .
In numpy , ` matrix ` is supposed to be a 2D object , but you can reshape it to 1D or 3D , because if not you wouldn't be able to , for instance , ` np.tile ` a ` matrix ` .
The reshape function takes the new shape as a tuple rather than as multiple arguments : #CODE
It would've been a lot faster if you made sure each block had the same number of lines ( even if filled with zeros ) .
That way you could just reshape an array read with ` loadtxt ` .
I don't have time right now , but it's not hard to change the algorithm to avoid the zeros .
BTW , I gave you a real example of a block . zeros were easier to reach than other numbers ...
i had to reshape dist because it didn't have the same shape of imglLab and imgRLab , i mean imglLab is ( 288 , 384 , 3 ) and disp was ( 288,384 ) .
Also if i print disp ( 288 , 384 , 1 ) i get the same error , its like there is no value there but the dimension is the same like the others ones .
Using ' hstack would be consistent with a ` reshape ( h , 3 , w )` , for yours you would want to use ` np.dstack ` .
But you don't need to stack three copies of the same array : simply reshape it to ` ( h , w , 1 )` and when you create ` data ` change the indexing of mgRlab ` to ` imgRlab [ rows , cols - disp [ rows , cols , :] , planes ]` .
maybe it gives me trouble because there is no value there or how can i change the shape of those images like ' ( h-1 , w-1 , 3 )' because i have tried with ' reshape ' but it doesn't work
I think the trick is to ` reshape ( -1 , nbcols )` what you get from ` np.genfromtxt ` , so your assignment should look like : #CODE
your suggestion with reshape is a good solution .
Finally reshape back to the shape you want .
Since you specified above that you want remainder columns to be tacked on to the last column , the last line folds in the remainder , if there is one ( if so , cols % j is nonzero , look up modulus operator if you're not familiar ) .
The slowdown is not because of initialization , but writing zeros to the entire array .
So why not just make a copy of the new matrix instead of filling with zeros and then adding a new matrix ?
The following ` git diff ` illustrates an easy fix : #CODE
If you give an array or list as second argument you basically give the indices ( before ) which to ' cut ' #CODE
For example , ` numpy.array ([ 1 , 1 ])` would become ` numpy.array ([ 0.70710678 , 0.70710678 ])` whose elements are sqrt ( 2 ) / 2 .
I searched around on google and stack overflow , but wasn't really able to find a general answer to this question .
If your matrix is real valued you can do this without making a copy by simply passing the transpose to cholesky .
I don't think the transpose fix will work if your matrix is complex-valued , since the transpose of a complex hermitian matrix is not generally equal to its conjugate transpose .
So for this , if you know that b only has one outer array , then you can do : #CODE
I've searched around but can't seem to find any efficient way to select a portion of a 3d array depending on indices .
Lets say for example that I have some 3d array with dimensions 200 x 200 x 200 and I want to select and change the value of all elements where all indices are greater than 100 #CODE
I want to select and change the value of all elements where all indices are greater than 100
Strictly speaking , that will change the values where all indices are greater than * or equal to * 100 .
You can customise the ` atol ` ( absolute tolerance , defaults to ` 1e-08 `) and the ` rtol ` ( relative tolerance , defaults to ` 1e-05 `) parameters .
You can then set ` rtol=0 ` to only use the absolute tolerance .
I have some data stored in a ` std :: vector ` .
Since I want to do some more things with the results in C++ I want to convert the result back from ` ndarray ` to ` std :: vector ` .
How can I convert ` std :: vector ` to ` ndarray ` and vice versa ?
NumPy min / max in-place assignment
Is it possible to perform min / max in-place assignment with NumPy multi-dimensional arrays without an extra copy ?
Say , ` a ` and ` b ` are two 2D numpy arrays and I would like to have ` a [ i , j ] = min ( a [ i , j ] , b [ i , j ])` for all ` i ` and ` j ` .
So in the code above , it will create a new temporary array ( min of ` a ` and ` b `) , then assign it to ` a ` and dispose it , right ?
ValueError : too many boolean indices
Text mode means it expects Unicode data , and it will encode it into bytes format for you .
Binary mode means it expects data in bytes , and will not encode it .
For example , you can get the sorting indices from your second column : #CODE
It seems not to be a good idea to sum these types inside a ` math.exp() ` , as shown below : #CODE
Using ` exp ` , ` sqrt ` and ` pi ` from ` numpy ` reduces the difference but it makes your code much slower , probably because these functions may also return a ` ndarray ` : #CODE
I want to join these 2-d arrarys to form the 3-d during an iteration but I've looked at functions like meshgrid() , dstack() , concatenate() but can't seem to get any of them to fit right into the code .
Given a tuple of ordered 1D-arrays ` ( arr1 , arr2 , arr3 , )` , which would be the best way to get a tuple of min / max indices ` (( min1 , max1 ) , ( min2 , max2 ) , ( min3 , max3 ) , )` so that the arrays span the largest common range ?
As Peter de Rivay explains , there is a problem in your solution with broadcasting -- but mathematically what you want to do is some kind of outer product over addition of two vectors .
Therefore you can use the outer operation on the add function .
special vectorized operations like reduce , accumulate , sum and outer .
Second this 6 element array is broadcast to a 6 by 6 matrix by repeating the rows
Third the minimum of the original matrix and this broadcast matrix is computed .
The transpose is needed because of how pcolormesh expects to receive the input data .
I didn't know about ` clip ` .
I would almost have pointed you to a rolling window type of trick , but for this simple non-overlapping thing , normal reshape does it just as well .
it seems resize actually does manage to do that memory moving around ( if you are lucky )
@USER No , it only sets up the return value of ` main() ` and exits , never even calling ` sin ` .
Simply calling the ` sin ` function the appropriate number of times does not make for an equivalent test .
I just ran oprofile on the original code as supplied above ( with a ` printf ( " %f\n " , x )` at the end ) , and it spends about 65% of the time in " sin " .
C has standard functions for sin / cos , that are generally derived for accuracy .
If the whole test was made to do something more complicated that requires more control structres ( if / else , do / while , etc ) , then you would probably see even less difference between C and Python - because the C compiler can't really do " sin " any faster - unless you implement a better " sin " function .
I question whether numpy's sin is even correct .
A string integer specifies which axis to stack multiple comma
entry into as the second integer ( the axis to concatenate along is
The string ' 0 , 2 ' tells numpy to concatenate along axis 0 ( the first axis ) and to wrap the elements in enough brackets to ensure a two-dimensional array .
`' n , m '` tells ` r_ ` to concatenate along ` axis=n ` , and produce a shape with at least ` m ` dimensions : #CODE
Anything you can do with ` r_ ` can also be done with one of the more readable array-building functions such as ` np.concatenate ` , ` np.row_stack ` , ` np.column_stack ` , ` np.hstack ` , ` np.vstack ` or ` np.dstack ` , though it may also require a call to ` reshape ` .
Even with the call to reshape , those other functions may even be faster : #CODE
Good eye , @USER ...
Would you know of a way to ** average ** the duplicates instead of sum them ?
I believe it has something to do with the ` 1.0 - np.sum ` , a substraction of the sum .
I want to reshape a 2d ` scipy.sparse.csr.csr_matrix ` ( let us call it ` A `) to a 2d ` numpy.ndarray ` ( let us call this ` B `) .
But I put some prior knowledge into , that you didn't have : the number of rows exceeds the number of cols significantly , thus I will loop over cols , reshape each and compute the maximum .
Using matrix multiplication you can do en efficient slicing creating a " slicer " matrix with ones at the right places .
This would in theory be possible by using the inspect module or other ways of checking stack frames , but it would be an extremely bad idea .
concatenate two one-dimensional to two columns array
so is there a simple solution to concatenate along axis 0 two one-dimensional arrays ?
Note that ` a ` and ` b ` are both one-dimensional ; there's no axis 1 to concatenate along .
Alternatively , you could reshape ` a ` and ` b ` first : #CODE
Even running all combinations of planes in the LUT and image and then discarding the ` planes**2 - planes ` unwanted ones is faster than fancy indexing : #CODE
You could translate the 2-d array to a 1-d array by hand probably , but if ` img ` is really large it probably doesn't matter , and if its worth the juggling around ...
But from the result you have to query the diagonal : a [ 0 , 0 ] , a [ 1 , 1 ] , a [ 2 , 2 ]; to get what you want .
I've tried to find a way to do this indexing only for the diagonal elements , but still did not manage .
in my machine your solution is still the best option , and very adequate since you just need the diagonal evaluations , i.e. plane1-plane1 , plane2-plane2 and plane3-plane3 : #CODE
I am almost certain that indexing with a 2D array will not work for any of the formats , although I believe LIL will take two 1D arrays for indices , and return a row vector .
In some circles this operation is known as the " asof " join .
See also : #URL for a " O ( n log n ) NA-friendly time series as of using array operations " by McKinney .
If you are trying to save an array as a tiff ( with no axis markers ect ) from mat , you might be better off using PIL .
( this assumes you are using ` ipython -- pylab ` , so ` rand ` is defined )
Something must be occurring in ` permutation ` that is randomly ordering the columns ( local to each child process ) the same ... ideas ?
anyway , I may be thinking the wrong way , but maybe just to make sure seed it with the process ID or such that is unique for each process ...
The following ` g ( core )` function succeeded in generating a random permutation for each core .
I find this a little easier on the eye .
I have a 2D ` numpy ` array made from zeros and ones that I use as a mask for other arrays .
that way ` tmp1 ` and ` tmp2 ` should give me the max and min of the rectangles I am searching for .
So ` mask ` is the 2D array of zeros and ones ?
When I simply print what I receive on C++ side as seq , sequence1 , I'm getting values far above unsigned short range and zeros sequence is not filled with zeros .
That wouldn't explain the zeros not being zeros though ...
That random garbage is whatever happens to be next on the heap , which can easily be nonzero values .
My poorly expressed point was that you should still see ** some ** zeros .
I have a symmetric matrix ( adjacency matrix for an undirected graph ) and I have a particular eigenvalue ( the maximum eigenvalue ) and I want the eigenvector associated with it ( left or right , either one , since I believe the left is simply the transpose of the right for symmetric matrices ) .
I believe ` which ` controls what eigenvalues you get , and then the eigenvectors are the ones corresponding to those eigenvalues .
But it threw an error : ` ValueError : operands could not be broadcast together with shapes ( 1 , 3 ) ( 3 , 2 , 2 )`
I'm trying to solve the bottleneck in my application , which is an elementwise sum of two matrices .
Frequency for sin ( x ) , which is 1 /( 2 * pi ) #CODE
Frequency for sin ( 2x ) , which is 1 /( pi ) #CODE
So for a more complicated signal , you cannot rely on just looking for the ` max ` of periodogram to find the dominant frequency , because the harmonics may fool you .
Of course that will correlate for almost all practical purposes with out of memory ...
The output of the code writes an image with the pixels of each zone assigned a unique color .
How should I permute the nonzero outputs into pixel values for np.where() ?
Rather than iterating through all ` 2**24 ` combinations of RGB colors , you can vastly reduce your search space using only the cartesian product of your non-zero histogram bins : #CODE
If you cannot rely on ` convert ( ' L ')` giving unique colors ( i.e. , you are using other colors beyond the ones in the given image ) , you can pack your image and obtain the unique colors : #CODE
The 2nd suggestion ( the 24-bit packed with unique ) works great !
+1 Had to try it myself to be convinced that , as you say , ` np.average ( d , weights=e [ ..., None , None ] , axis=1 )` refuses to broadcast ` d ` and ` e ` and does not work .
More particularly , regarding arrays , and basic functions that are dealing with arrays , like sort , concatenate ..., which solution is more efficient ?
why does slicing an ndarray reshape it ?
This will not work if an odd length of a side ever comes up , but depending on exactly how you want to handle the downsampling for those cases , you should be able to get rid of a full row ( or column ) of pixels , reshape your image to ` ( rows // 2 , 2 , cols // 2 , 2 , planes )` , so that ` img [ r , : , c , : , p ]` is a 2x2 matrix of values to interpolate to get a new pixel value .
Since exp is monotonic , you could use the logarithm of the gaussian as your error function , e.g. #CODE
This will result in 1 call to log per iteration , rather than 1 call to exp per dataset row each iteration .
Then this data ( arrays 2 dimensioanl ) should be added to the other 2 D array Full of zeros .
So finally I am getting array , which have only 2 arrays concatanatted ( arrays with zeros and last inputted data array )
Do you need the array with zeros to be in between the arrays containing data ?
I can even get each individual record by calling list indices .
But when I try to get the size of the list by calling np.size ( data ) ( I imported numpy as np ) I get the following stack trace .
I can't even divide that list into a multiple parts using list indices or convert this list into a numpy array .
Or , if you don't need all the fields in the CSV file , only pick out the ones you need rather than adding them all to the ` data ` ?
Maybe you want ` any ( group.TYPE == ' start_codon ')` , or ` ( group.TYPE == ' start_codon ') .any() ` or ` sum ( group.TYPE == ' start_codon ') == 1 ` or something ?
But I can't do this if the user has passed a slice , e.g. something like ` p [ -1 :] ` , since the method just gets individual indices , e.g. as if the user wrote ` p [ 4 ]` .
and what I need it to be ordered like is : the one having the highest cumulative sum comes first .
Assuming that by cumulative sum you mean total ( there's a cumulative sum function which returns something else ) , then you can do this both using the standard sort : #CODE
Therea 24 joblib.dump files , and each weights 45 megabyte ( sum of all files = 931mb ) .
Smaller ` z ` should be an advantage to ` interp_2 ` , longer ones to ` interp_1 ` .
How to use an array as its own indices in Numpy
So it is something about using a matrix member as its own indices that causes the problem .
Only integers can be used as array or matrix indices .
Which is faster , numpy transpose or flip indices ?
However if numpy transpose uses some features I'm unaware of to do the transpose in just a few operations , it may be that transpose is in fact faster than my pass-through function idea .
transpose simply returns a ` view ` if possible -- So it's a fast operation -- That said , if you * can * just switch the way you index your array , that's probably the fastest you can do if you're operating one element at a time .
In NumPy , transpose returns a view with a different shape and strides .
I'll stick with transpose since it's clearly a simpler solution .
` skimage.measure.find_contour ` should do the basics , though it doesn't simplify the polygons at all , and doesn't distinguish inner rings ( holes ) from outer rings .
ifft ( fft ( audio )) is just noise
whether i just nest them ( ` iff ( fft ( audio ))`) or try window-by-window ( ` window ` the audio , do the ` fft ` , do the ` ifft ` , then invert the window , replacing zero with eps , then merge the samples back ( trying ` abs ` here and there in the pipelines )) i get only noise .
i know the ` ifft ` is only inverse to the ` fft ` with infinite precision arithmetic , infinitely many samples , etc ( right ? ) i'm working with 64bit floating point and 44kHZ sample rate . but , i would expect to be able to at least hear the original audio .
Depending on the library you're using , it might just dump the memory buffer of the array out to disk as a .wav without re-casting things back to floats .
Try writing ` ifft ( fft ( audio )) .real ` or ` abs ( ifft ( fft ( audio )))` and see if it changes anything
i changed ' int8 ' to ' uint16 ' . since i was adding notes ( read in as ` .wav `) to make chords , i thought i should use the same dtype when writing the output . nope . ifft and fft are inverses , the universe makes sense again !
Because we have redefined ` local_dict ` , ` a ` and ` b ` no longer show up there as local variables , so the value of the global ones is used instead .
How to simplify array into only unique values in Python
How would you simplify this 3D ` numpy.ndarray ` to a collection of unique RGB triples ?
Not quite unique , but you can sort ` inner ` to get only unique .
Sorting ` inner ` doesn't get only unique values ; you still end up with 3 copies of ` [ 138 , 121 , 81 ]` .
To make it unique , you can just use a ` set ` : #CODE
As a side note , there's also ` np.unique ` which will give you the unique elements of your array as well .
Now we have something that will generate the first to indices -- We just need to construct a tuple to pass to ` __getitem__ ` that numpy will interpret as ` ( X , Y , :) ` -- That's easy , we're already getting ` ( X , Y )` from indices_generator -- We just need to tack on an emtpy slice : #CODE
Now we can loop over all_items looking for the unique ones with a set : #CODE
There are ` flatten ` and ` unique_everseen ` functions that do exactly what you want .
Now , you can flatten the 3D array to 2D , and uniquify the 2D array with ` unique_everseen `
The only real trick here is using a ` set ` to hold the values seen so far : sets only hold one copy of each unique element ( and can determine whether an element is already in the set very quickly ) .
I'm getting a ` NameError : global name ' flatten ' is not defined ` error on this .
Tried ` intertools.flatten ( arr3d )` but it throws ` AttributeError : ' module ' object has no attribute ' flatten '`
The documentation says that ` yerr ` can be a 2d array , with the first column being the min error and the second being the max .
@USER , Not exactly , because I need to keep track of indices in subsequent calculations .
min function in numpy array
I am trying to find a min value from one dimensional numpy array which which looks like : #CODE
I tried as suggested on NumPy : get min / max from record array of numeric values view function , it failed to recognize ' S7 ' as valid field .
Should I have specified the data type while reading the values or while using the min function ?
should I have specified the data type while reading the values or while using the min function
If you won't need to do many other numerical operations and you have a reason for preferring the data to reside in ` str ` format , you can always use the native Python ` min ` and ` max ` operating on a plain ` list ` of your data : #CODE
But it's just another option to consider if you need to or if you have a special reason for working with strings ( such as , you're only ever calculating the min and max and all you do with them is display them ) .
You're making some pretty strong assumptions about the strings here : ` min ( ' 00.0 ' , ' 0.9 ')` doesn't give what you'd want ...
Also , you shouldn't need to convert to a list in order to use the builtin ` min ` function .
@USER Yes , ` min ` would work on a flattened NumPy array , but if it's two-dimensional , even if one of the dimensions has size 1 , it doesn't work .
If you use ` tolist() ` on a 2-dimensional array , then ` min ` will give you the column-wise mins .
An alternative is to use the python builtin ` min ` function in conjunction with the ` key ` keyword : #CODE
Just calculate the impulse response and use any of the appropriate scipy.signal filter / convolve functions .
For smaller ones it does not : #CODE
Why does the standard deviation when I use the built in random.expovariate scale proportionately with number of event in a given interval while the expovariate std_deviation scales at a rate of log base 10 ( count ) ??
The mean / variance of a Poisson distribution are both ` lambda ` , hence the ` stdev ` is ` sqrt ( lambda )` .
So ` std = sqrt ( 1 /( 1 / rate ) ^2 ) = sqrt ( rate^2 ) = rate ` which is exactly what you are seeing here .
and then dot this with each of your vectors ; which I assume is an ` Nx3 ` matrix ` M ` #CODE
You have to decompose your vector , lets call it ` u ` , in the sum of two vectors , ` u = v + w ` , ` v ` is in the plane , and so can be decomposed as ` v = a * v1 + b * v2 ` , while ` w ` is perpendicular to the plane , and thus ` np.dot ( w , v1 ) = np.dot ( w , v2 ) = 0 ` .
If you write ` u = a * v1 + b * v2 + w ` and take the dot product of this expression with ` v1 ` and ` v2 ` , you get two equations with two unknowns : #CODE
How do I correlate my original data with clustered data
Maybe it is because I am trying to append a string to a float ?
A great example of what a stack overflow answer shouldn't be .
I have an optimization problem that involves minimizing a function whose gradient I know , but the actual value of objective function at any point is unknown .
Obviously I can use gradient descent , but I'd prefer not to reinvent the wheel here .
So , it's pretty straightforward to calculate the gradient of h ( x ) , but hard to get explicit values for h ( x ) .
So , it's pretty straightforward to calculate the gradient of $h ( x ) $ , but hard to get explicit values for $h ( x ) $ .
EDIT : sorry , what i meant was h ( x ) is the gradient ..
For example f ( x ) = sin ( x ) .
maybe he meant to minimize the square of the gradient ?
Yes sorry , i was treating h ( x ) as the gradient and not the function itself ...
I think that this answer is quite good : instead of minimizing the functional h , just try to find x such that some norm of the gradient is zero .
The gradient of y is df / dx .
You can track down the location of the minimum ` x ` such that ` h ( x )` is min ) , but not the value of ` h ( x )`
This is an extremely specialized root finding problem , because in general we know that we want to go in the direction opposite the gradient .
If we're just doing root finding , and all we know is that we want to find where the gradient is zero , we can really only get the search direction from the inverse jacobian / hessian .
You just need to reshape ` A ` so that it will broad cast properly : #CODE
Do you mean generator is faster than ` append ` ?
You said that yours only supported `` float `` so as you'd already generalised `` cond `` I thought yielding the line would allow you to use the same `` genfromtxt_cond `` irrespective of the line data .
It runs okay now and you can use genfromtext as a generator like so ( using a random CSV log I have sitting around ): #CODE
It's relatively simple to add a block to skip leading lines and yield only N lines from the file , conversion bails out of a line as soon as reasonably possible ( you could even remove conversion and return the original string to save on ` append ` s ) , and the whole thing is extremely lazy so memory use is minimal .
I have an intra day series of log returns over multiple days that I would like to downsample to daily ohlc .
The outer key references the columns you want to apply the functions to .
To select particular columns you put the indices within the same square brackets separated by a comma : #CODE
As a concrete example , say the goal is to broadcast ` x ` into an array ( of predefined shape ` xshape `) if ` x ` is just a number and to return an error if ` x ` is an array with the wrong shape .
If I try to make a histogram with ` r.hist ` , it doesn't work at all , yielding the error : #CODE
take a look at #URL which uses std python ds ... take same approach using pandas df
Use numpy.ones instead of just ones : #CODE
I just built ` class MultiIndexList ( list ): def __getitem__ ( self , indices ): try : return list ( self ) [ indices ] except ( TypeError ): return [ self [ i ] for i in indices ]`
The values should be distributed over the matrix lines according to an array of indices .
broadcast an array to different shape ( adding " fake " dimensions )
In python ( using numpy ) , I can broadcast an array to a different shape : #CODE
I have a subroutine which expects a ` 2D ` array to be passed in -- I would like to " broadcast " my 1-D arrays up to 2-D as I've demonstrated above .
As a side note , I thought that this functionality might be provided by the ` reshape ` intrinsic , -- Something like : #CODE
The reshape intrinsic will allow you to copy the 1D array to a 2D array .
if you don't mind copying the array , here is how to use reshape : #CODE
wait -- I'm confused -- How does this allow me to use ` reshape ` to do what I proposed ?
` reshape ` can take an optional argument , called ` pad ` , which can be used to provide the ' extra ' elements needed when you reshape into an array with more elements than you started with , say from 3x4 to 2x4x2 .
You can do reasonably well using ` unique ` and ` bincount ` : #CODE
I am trying to stack two images together , so i can show both in a single window .
I need both images to have the same number of channels to stack them
More specifically , the operation ` absolute ( data-dc )` eats up all the memory .
The memory leak occurs when calculating ` absolute ( data-dc )` .
The max memory is around 3GB for the program .
` std ` is also a huge bottleneck .
Notice that because the tuple is the container for indexing elements , it cannot be combined with other indices , for example ` A [( ) , 0 ] == A [[ ] , 0 ]` and ` A [( ) , 0 ] !
Because an array ` A ` can be indexed with fewer indices than ` A.ndim ` , indexing with an empty tuple is a natural extension of that behavior and it can be useful in some situations , for example the above code snipit will work when ` A.ndim == 0 ` .
#URL > error : ( -215 ) npoints > = 0 npoints == std :: max ( ipoints.checkVector ( 2 , CV_32F ) , > ipoints.checkVector ( 2 , CV_64F ))
Is there a way of doing this without copying the array , perhaps by forcing ` genfromtxt ` to create an empty column to which I can append derived values ?
My problem is that i call this with one of the fields being a ` datetime ` object , which prevents the stack and numpy.lib.recfuntions ` add_field ` from merging the arrays .
If I run your code to generate ` d ` and ` dx ` with ` eig ` I get the following : #CODE
Please provide another format , e.g. , load it as numpy array and then dump it to a file , or pickle it or whatever .
( BTW : the grid cells have the dimensions 0.25x0.25 and the polygons 1x1 at max )
However , running this code on a huge amount of polygons ( each one could intersect dozens of grid cells ) takes around 15+ minutes ( up to 30+ min depending on the number of intersecting grid cells ) on my machine which is not acceptable .
I want to recognise the shape in the image like round for face or recognising shape of eye nearly something like round or square shape .
When you ask for a transpose , numpy doesn't actually transpose the data , only the strides used to access it , unless you specifically trigger a copy with ` copy() ` .
I'm trying to create an n x m numpy array populated by a set of cosines like cos ( v_t ) , cos ( 2 * v_t ) , cos ( 3 * v_t ) , ..., which I've tried to do with the following ; #CODE
This seems to reshape the cos ( v_t ) array .
@USER It does reshape the ` cos ( v_t )` array , but starting from a longer ` v_t ` array to achieve what I think you are after .
All I really know is that I'm trying to do something with a continuous signal ( sum of cosines ) , and part of the process involves multiplying the signal vector by a random matrix R and the matrix psi , which is the cosine basis matrix ( i.e. matrix with elements cos ( t ) to cos ( nt )) .
I'd like to concatenate ' column ' vectors using numpy arrays but because numpy sees all arrays as row vectors by default , ` np.hstack ` and ` np.concatenate ` along any axis don't help ( and neither did ` np.transpose ` as expected ) .
Yeah , that's good , thanks but I thought that there would be something that could be done along the hstack way ( I mean in a more straightforward way . To lower the cognitive load , it would be better to see and concatenate these vectors as column vectors )
It contains the ` ids ` of the observations .
As long there are as many ids as rows in the array .
Is there a numpy / scipy dot product , calculating only the diagonal entries of the result ?
As you can see , I am keeping only the n diagonal entries , however there is an intermediate ( n x n ) array calculated from which only the diagonal entries are kept .
I wish for a function like diag_dot() , which only calculates the diagonal entries of the result and does not allocate the complete memory .
is equivalent to the individual sum of the scalar product of rows of X and columns of Y , the previous statement is equivalent to : #CODE
Nested for loop to numpy convolve
EDIT : The images values are in binary range so lowest value is 0.0 and max value is 1.0 .
Specifically you're getting negative indices when x , y are smaller than radius .
As a basic sanity check , do the results at all correlate with what the command line tool gives ( e.g. lower gain for louder files ) ?
Ok the values seem to correlate losely to the command line tool .
A few points : 1 ) In " view " , instead of a [: ] , I think you should use a [ ... ]; 2 ) in " logic " , I think you should use np.any and np.all instead of the python ones ; 3 ) It would also be good to do a comparison for the False result , since that will be dramatically different for some of these cases ( especially " gen ") .
That may seem odd , but you have to remember that numpy supports many data types and has a bigger machinery to select the correct ones and select the correct function to work on it .
thanks , that proved helpful to me in a situation where I wanted to plot with a categorical x axis ( the names of rooms ) , but I wanted to order it such that the y values ( exhibits per room ) would be increasing to make it easy on the eye .
Another use case is float -> int indices , ` y.take ( xfloat.astype ( int ) , mode= " clip " )` : unsafe without ` astype ` but common ( well , in c ) and useful .
numpy matrix trickery - sum of inverse times matrices
I'd love to be able to just calculate the Cholesky factor of U and V in the iteration , but I don't know how to do that because of the sum .
So having a python loop , and having to sum all the results together , is taking 390 ms more than 200 times what it takes to solve each of the 200 systems that have to be solved .
In my current project I need to " convolve " two three dimensional arrays in a slightly unusual way :
But this computes only the elements ` max ( dimA , dimB )`
" But this computes only the elements ` max ( dimA , dimB )`" - doesn't for me , it seems to me that it is your shape that is wrong .
With mode = ' same ' I get an array of dim max ( dimA , dimB ) and with " full " I get an array with a dimension of dimA + dimB - 1 so one row , column is missing .
The output of convolve should be dimA + dimB - 1 .
Just use fftconvolve and then copy the result into a slightly larger zeros array if you need that .
If you compute the residual between the results of both functions you will get zeros .
Then you need to change the arguments of the sin functions .
sin takes radians for arg .
I am trying to extract the indices of all values of a 1D array of numbers that exceed some threshold .
Sampling histograms such that the sum over the sample is uniform
I have a list of items from which I want to randomly sample a subset , but each item is paired with a histogram over D bins and I want to sample the items in such a way that the summed histogram is approximately uniform .
The absolute values of the summed histogram are not important nor does it need to be exactly uniform , it just needs to be approximately uniform .
As an aside , the items that I want to sample are image patches and the histogram are label histograms from a manual segmentation of the image .
What you could do is first choose weights for your items so as to make the weighted sum ( approximately ) uniform , and then take a weighted sample of the items .
To expand on @USER Karonen's solution , what you want to do is compute weights for each histogram and then sample according to those weights .
Let D_ij be the weight of the jth bin in the histogram of the ith item .
Then if each item is weighted with weight w_i , the " summed histogram " would have weight sum ( i in items ) w_i D_ij .
The above is basically saying that ` z = ` absolute value of difference across all weighted pairs of bins .
Finally you just sample from histogram ` i ` with weight ` w_i ` .
If you wanted the resulting weighted distribution to be " as uniform as possible " , I would solve a similar problem with weights , but maximize the weighted entropy across the weighted sum of bins .
When using the LP solution , a bunch of the histogram weights may bind to 0 because the number of constraints is small , but this will not be a problem with a non-trivial number of bins , since the number of constraints is O ( n^2 ) .
Using ` np.argmax ` instead will get me the indices I need , which I can then plug into the original ` cumsum ` array .
For code readability then I guess I should change the name of my ` indices ` variable .
Gaussian blur image histogram of Y channel
I'm new to computer vision and image processing , anyway I'm trying to calculate the histogram of image y_channel which has previously been blurred with cv2.GaussianBlur and converted from BGR to YCr-cb color space .
Why do you expect the histogram to be Gaussian ?
@USER I edited the question , I guess the face luminance should look like a Gaussian histogram .
@USER yes you're right I tried in different light conditions and I've got a positive result ( by positive I mean a Gaussian look alike histogram ) .
Under different conditions the Y histogram looks a lot like a Gaussian one .
IndexError : too many indices
I seem to be getting the error ' too many indices ' returned , but cannot figure out why .
In my class definition , the origin refers to the top most tile ( tile ` a `) , thus the ` left ` dimension defined in the class is the vector along ` [ a b d g ]` and the ` right ` dimension is the vector along ` [ a c f j ]` .
Also , the algo has a lot of matrices manipulation ( fft , filters , etc . ) , so using numpy / scipy should result in faster run time .
Could someone help me translate this short code snippet into python ?
Weighted sum of adjacent values in numpy array
What is the easiest / fastest way to take a weighted sum of values in a numpy array ?
Note that computing ` D2u ` is equivalent to taking a dot product with this kernel centered around the current position : #CODE
Can this be vectorised as a dot product ?
Are you sure you're importing ` convolve ` from ` scipy.ndimage ` ?
I believe I've seen this error before with the 1d-version of convolve .
Return elements not referenced by array of indices in Python
The list of indices of ` my_array ` that are not in ` my_indices `
In order to return the list of indices in ` my_array ` that are not in ` my_indices ` , you could use list comprehension : #CODE
This will return the indices ` [ 1 , 2 ]` .
Thanks , but in my first question I am looking for ** indices ** of ` my_array ` , not its elements , and in my second question I am looking for elements not ** referenced ** by ` my_indices ` .
The problem I see is that , for example , when I resize the window the stream is interrupted till I stop resizing .
@USER thanks ... but btw how can I resize the image as I was doing with set without causing it to crash ?
I tried changing the size , with the new module and the old one , however as soon as I resize it , it doesn't refresh any longer and it starts crashing again .
By " resize it " do you mean resize the window running the program ?
When I resize this window I stop getting updates too , as commented earlier .
I see that ` OnPaint ` and ` NextFrame ` are always being called , but when I leave the mouse button down during the resize ( without further resizing ) , no update actually happens in the panel showing the frames .
Just about 8000 lines per 30 min .
I'm trying to put Poisson continuous error bars on a histogram I'm making with matplotlib , but I can't seem to find a numpy function that will given me a 95% confidence interval assuming poissonian data .
I have a system of diff eqs in a m*m matrix S .
Eventually what I'll need to do is calculate a range of individual totals ( distributed evenly over the absolute total number of entries ) -- and then plot those averages using any of the plotting libs for python .
I have tried the following example to sum a range of 5 entries from the above list of 20 ( which works ) , but I don't know how to dynamically pass the 5 at a time until completion , all the while retaining the calculated averages which will ultimately be passed to pyplot .
Then reshape x into an array A with 5 columns and calculate the mean for each line .
In one case ` izip_longest ` will fill the remainder with ` None ` whereas ` imap ` and ` izip ` wil truncate .
( could be 500k , 1 million , or just a few thousand . It's always variable . ) Would the remainder be rounded off using something like a mod by ?
Could I strip the non 100 end piece , or divide by 100 to get the exact metric ?
reorder list of vectors by norm
I allow the user to pass in a function that will operate on an arbitrary stack of pixels after I've interpolated and aligned all the data .
Also , I assume that when your program terminates , it has some code to tear down things , dump log files , etc .
It is painful to add new functions , changing existing ones , especially the main program .
From this I've gathered that ` len() ` will return the number of rows , so I can always us the transpose , ` len ( A.T )` , for the number of columns .
There is also the shortcut index-trick ` c_ ` , which would handle the reshape for you automagically : #CODE
It's probably most efficient to just use ` sum ` : #CODE
How to configure pandas to use one numpy.nan , and not multiple , unique float ( ' nan ') s ?
Is there a way to configure pandas to use ` numpy.nan ` as its NaN constant , or at least a single module-global constant ` pandas.nan = float ( ' nan ')` , rather than a brand-new and unique ` float ( ' nan ')` for each NaN it needs to represent ?
( Alternatively , is there any justification for not using a single , globally unique NaN constant , the way that ` numpy ` does ? )
numpy / pylab min , max
max ` and ` ?
min ` stand for unknown functions .
The ` numpy.max() ` and ` numpy.min() ` seem to find the max and min of an entire array , which is not what I want .
possible duplicate of [ NumPy min / max in-place assignment ] ( #URL )
The problem is that ` reduce ` tries to accumulate the result " locally " , while we want a " global " test like ` np.all ` .
is a list of indices where
How to find all zeros of a function using numpy ( and scipy ) ?
This function can have many zeros , but also many asymptotes .
I need to retrieve all the zeros of this function .
Do you have a better strategy ( still efficient ) to find all the zeros of a function ?
Can you also provide a bit more detail about what kind of function you are dealing with ? for example , this simply can't be done for ` sin ( 1 / x )` in the region around ` x=0 ` .
For example , if you want to discretize cos ( \pi x ) , the parameter that multiplies x ( if x has units of length ) must have units of 1 / length .
So the characteristic size of cos ( \pi x ) is 1 / \pi .
Are you implying that ` brentq ` can't find the roots of cos ( x ) between 0 and 2*pi ?
` brentq ` would fail in the interval [ 0 , 2*pi ] because the signs of the cos function are the same at those two points .
Is your claim that cos ( x ) is a multidimensional function ?
If you are sure that your function is not completely pathological ( ` sin ( 1 / x )` was already mentioned ) , the next one is what's your tolerance to missing a root or several of them .
I would like to concatenate these two arrays on axis 1 :
The last two numbers are the residual sum of squares for the numpy and lapack solutions , respectively .
Right , I guess it is probably not possible . the dot function has some extra arguments to do such things , so I thought it might work here too .
Just use ` loadtxt ` and ` reshape ` ( or ` ravel `) the resulting array .
You can usually just dump the IronPython assemblies and your custom assemblies in that case .
Provided A has the right properties , you could transform it to the diagonal form A0 by calculating its eigenvectors and eigenvalues .
In the diagonal form , the solution is ` sol = [ exp ( A0*b ) - exp ( A0*a )] * inv ( A0 )` , where ` A0 ` is the diagonal matrix with the eigenvalues and ` inv ( A0 )` just contains the inverse of the eigenvalues in its diagonal .
Finally , you transform back the solution by multiplying it with the transpose of the eigenvalues from the left and the eigenvalues from the right : ` transpose ( eigvecs ) * sol * eigvecs ` .
simple but weird vstack / concatenate problems ( python )
It's unclear what you're trying to achieve , but did you perhaps mean to stack them horizontally : #CODE
No combination of vstack , hstack , or concatenate seemed to give a good result .
I'd prefer to merge / join / stack the arrays before structuring them , I think .
From what I can see , all of the typical matrix functions ( ` ones ` , ` rand ` , etc ) in Numpy return arrays , not matrices , which means ( according to the documentation ) that ` asmatrix ` will copy the data .
The main memory is taken up by the Q , P , and mat matrices , as they are all 2.2 million by 2000 matrices ( size = 2.2 million , numlabels = 2000 ) .
While the mat matrix is being computed , I get the glibc error , and my process automatically goes into the sleep ( S ) state , without deallocating the 131GB it has taken up .
Interestingly , on one GDB run I forgot to set the stack size limit to " unlimited " , and got the following output : #CODE
When I set the stack size limit to unlimited " , I get the following : #CODE
A special thanks to pv . for helping out on this to pinpoint the exact issue , and thanks to everyone else for the brainstorming !
Note that here using the zeros function from numpy makes all the element 0 that when initialized .
What is the best way to reshape the following dataframe in pandas ?
I think the answer given by Chang She doesn't translate to dataframes that have a unique index , like this one : #CODE
The `" names "` column just assigns a unique name to each row in the dataframe .
In which case you can just turn the columns into a MultiIndex and use stack then reset_index .
Edit : use merge to join original ids back in #CODE
Also , what if the dataframe has a unique index to start ?
After the call to stack , " s1 s2 s1 s2 ...
How do I stack vectors of different lengths in NumPy ?
How do I stack column-wise ` n ` vectors of shape ` ( x , )` where x could be any number ?
it doesn't support basic methods like ` sum ` or ` reshape ` , and you should treat this much as you'd treat the ordinary Python list ` [ a , b ]` ( iterate over it to perform operations instead of using vectorized idioms ) .
Several possible workarounds exist ; the easiest is to coerce ` a ` and ` b ` to a common length , perhaps using masked arrays or NaN to signal that some indices are invalid in some rows .
To add to larsmans ' solution , to find the largest of your " jagged " arrays , you could use ` max_entries = max ([ len ( x ) for x in [ a , b ]])` , and to automatically generate the mask , use ` np.concatenate ([ np.zeros ( len ( b ) , dtype =b ool ) , np.ones ( max_entries-len ( b ) , dtype =b ool )])` .
I'm afraid you'll have to roll your own .
The ` allclose ` function from the numpy module , checks whether two arrays are the same within machine precision a given relative and absolute tolerance .
If your software depends on the difference between ` [ 0 , 1 )` and ` [ 0 , 1 ]` then you should probably roll your own random number generator , possibly the one mentioned here in order to ensure that it meets these stringent requirements .
I made a typo in the norm , fixed now .
You are missing braces in the denominator of the argument to ` exp ` in the second Boltzmann term or you have simply forgotten to replace ` k*t ` there with ` kt ` .
Taking ` v = 0.0009 ` and ` E = 5.18e-23 ` , the expression ` exp (( E + e*v ) / kt )` ( I corrected the typo pointed out by Hristo Liev in your Python expression ) is ` exp ( 709.984 .. )` which is beyond the range you can represent with double precision numbers ( up to ca . 1E30 8) .
At ` E = gap ` you would have ` exp ( E / kT )` of approximately 6.24E +100 .
I would like to " horizontally " concatenate ` x ` and ` y ` to produce a new array of records ` z ` , having shape ( r , c x + c y ) .
Ideally , it should be agnostic also about the number of arrays to join .
from numpy import nonzero #CODE
Negative indices are interpreted as counting from the end of the array
The underlying problem that I am attempting to solve is find the point wise difference in terms of the parallel and perpendicular components and create a 2d histogram of these differences .
I have also set the diagonal offset to ` 1 ` to avoid including the ` 0 ` distances of each point to itself in the histogram .
You are still doing double computation on the ` sqrt ` step .
Another micro-opt : you can use ` hypot ` instead of the ` sqrt ( a**2+b**2 )` It probably won't have much effect here , though , because that's not where most of the time is being spent ..
[ [ nan nan nan 1 . ]] invalid value encountered in divide return c / sqrt ( multiply.outer ( d , d ))
How can I append even amount into dictionary and access it for finding correlation
` min ( tuple ( r [: : -1 ]) for r in a ) [: : -1 ]`
There's no registry or anything for these shorthands and you're free to invent new ones as you see fit .
You have to transpose one of them to get a 2x1 matrix .
If you change the ` axis ` you use for the outer calls to ` np.any ` and ` np.all ` , you could check whether every row in ` a ` is close to some row in ` b ` .
Numpy ndarray subclass - forcing reshape in __array_finalize__
` reshape ` will try to create a view of the data with the new shape , and if it can't it will create a copy of your data with the new shape .
However , when you do so you'll double count the diagonal , thus you have to subtract that part out .
I need to remove all arrays that compare to smaller ones .
Just that the first n-1 indices are not equal ?
At best you will have O ( N log ( N )) scaling to this , I believe the only implementation of the required ideas is the scipy KDTree , but of course does not do what you want it to do .
Is it true that we delete any array that is > ANY of the other arrays , even the deleted ones ?
The overall performance of this algorithm is n2*log n in Stage 1 + { n lower bound , n * log n - upper bound } in Stage 2 .
You should look into Numpy's ` roll ` function .
I think this is equivalent to your first block of code ( though you need to decide what happens at the edges - ` roll ` " wraps around ") : #CODE
We are first going to calculate the norm of each row .
Take the square root of the sum of the squares .
We now have the norm of each row .
I understand , but for me at least the generic way * is * to " line up " the elements with something like ` roll ` , maybe creating some intermediate arrays if it's getting messy .
Extending this to other indices should be pretty straightforward .
picking out elements based on complement of indices in Python pandas
I just assume that ` df_a ` and ` df_b ` are subsets of ` iris ` , so I'd like to pull out elements from ` iris ` based on the indices of ` df_a ` and ` df_b ` .
All of your examples actually work ( but the decorator ones giving a type , which cannot ) .
I can only think of indecent ones .
All you need to do is form the array they request , with 24x12x6 values , sum all of them , and divide by the number of elements in that array .
In my case that was just one folder up ( I had this code in the dump folder inside the pymunk source tree ) , but it could have been anything .. for example , the absolute path would in my case be ` sys.path.insert ( 0 , ''' c :\ svn\pymunk\trunk ''')`
To find matches between lists of arrays , the most straightforward method is to broadcast the lists to the same ` n x m ` shape ; this can be done with ` np.tile ` but using ` stride_tricks ` is faster : #CODE
( up to permutation of the lines ) from ` M ` .
But why not just getting the eigenvalues and the eigenvectors of the first matrix , and using the eigenvectors to transform all other matrices to the diagonal form ?
Because these eigenvectors are not necessarily eigenvectors of the second matrix , so the diagonal might not be the eigenvalues ( ! ) of the second matrix .
Add a random vector to cos ( w* t-4 *pi / 3 ) for noise .
An alternative , MATLAB-style approach would be to reshape a as ( 3 , N*M ) and M as ( 3 , 3*N*M ) and take a dot product , but that tends to eat up a lot of memory .
For multidimensional arrays , ` np.dot ( M , a )` performs a sum product over the last axis of ` M ` and the second-to-last axis of ` a ` .
` a ` has shape ( 3 , 4 , 5 ) , but we want to sum over the axis with shape 3 .
, i.e i have took amount in a seperate list , can i append tat also to a dictionary and how can i access it .
If you are sure you don't need more than a fixed number of places after the decimal dot , then : #CODE
Here I'm converting to an int and back to avoid leading zeros : ` -02 ` -> ` -2 ` etc .
@USER : what I have now is still dirty , as it's leaving extra zeros in the base-10 exponent ( e^{-01 } etc ) .
Every time I concatenate data , I need to take the array , sort it along axis 0 and do other stuff , so I cannot construct a long list of arrays and then np.vstak the list at once .
Sure you could do things like dynamic resizing ( if you want to do crazy things , you can try to use the resize array method too ) .
I'm actually avoiding doing this list approach because each time that I concatenate something I also need to perform other operations on the array ( like sorting and many other things ) .
About the resize I like the suggestion but " Referencing an array prevents resizing ...
I need to compute the min , max , and mean from a specific list of faces / vertices .
You're not really timing the min and max functions , you're timing how long it takes to sort out the nested references
The confidence interval is then ` mean + / - z*sigma ` , where ` sigma ` is the estimated standard deviation of your sample mean , given by ` sigma = s / sqrt ( n )` , where ` s ` is the standard deviation computed from your sample data and ` n ` is your sample size .
@USER , about the suggested calculus for the confidence interval , wouldn't be ** mean + / - z * sigma / sqrt ( n ) ** , where n is sample size ?
As pointed out above , you only have two vectors so you'll only get a 2x2 cov matrix .
IIRC the 2 main diagonal terms will be sum ( ( x-mean ( x )) **2 ) / ( n-1 ) and similarly for y .
The 2 off-diagonal terms will be sum ( ( x-mean ( x )) ( y-mean ( y )) ) / ( n-1 ) .
The ` Name ` field is not unique , so I don't think it can be used as an index .
I think this situation is ambiguous since the ` test ` dataframe doesn't have an index that identifies each unique row .
First , doesn't test already have a unique index that identifies each row , i.e. the default index ?
Why can't you do : ` test [ ' index '] = list ( test.index )` or something like that to create an arbitrary unique index for each row ?
The original index on ` test ` has a unique value for each row in ` test ` , but not for each value in the * original * ` iris ` dataframe .
My code ` range ( len ( test ) / 2 ) * 2 ` is two lists [ 0 .. 149 ] concatenated together , which can be seen in the output from ` test [ -10 :] ` ( the original and new indices don't match up ) .
Or , you can concatenate all the arrays into a big array , and save it to the file .
It creates has methods for adding to it's data , a method to initialize its VBOs and VAO and a draw function , which gets a stack and draws on it .
And I initialize the handler with some very simple code , where translateMatrix ( x , y , z ) returns a translate matrix and applyTrans ( base , trans ) applies trans to every vertice ( x , y , z , w ) in base .
I've now made sure that it stays ' uint16 ' as long as possible ( till the max of index data gets higher then 2^16 ) and added a check to see if indexData is ' uint16 ' or ' uint32 ' and added the appropriate flag on glDraw command .
This seems to have fixed it , as I can now easily add a few thousand ( tried with 5.000 max ) robots to one VBO and it still works .
The max value of indexData when it was still OK was 32831 , max value when it started acting up was 33023 .
So that there are more 50s and less ( nearly no ) zeros and hundreds .
You have to decide for a way to " round " them while keeping their total sum .
But the append function , and a for-loop are both ruinously inefficient as I understand it , and I'm both trying to improve my code and the number of runs is going to be large enough that that kind of inefficiency isn't doing me any favors .
When you reach the size of ` results ` , then do the ` numpy.append ` ( or ` concatenate `) on a new array .
I know that I can sum all the columns per group with ` agg ` like this : #CODE
But what I'm looking for is a twist on this : instead of summing all entries of a particular ` Name ` for each column , I want to sum just a subset of the columns ( ` SepalWidth , SepalLength `) for each ` Name ` group . thanks .
In general , for a dataframe ` df ` , aggregating the groupby with ` sum ` gives you the sum of each group #CODE
In your case , you want the sum of the two values , so you'd sum this across the rows .
Could you explain more what ` grouped_iris [ cols ] .transform ( sum )` is doing here exactly and how ` transform ` is generally used ?
One more question : Why did you pick ` reshape ` over ` flatten ` ?
I'll amend the answer to use ` flatten `
After fitting a linear function to a set of data , how can I find the error on the gradient of the function ?
Does this mean the standard error of the gradient or intercept ?
It's the standard error of the entire fit , so the error in both ( ` displacement = gradient * time + intercept `) as displacement is a function of both .
I mat a problem when solving inverse of a matrix .
What I want to do is assume each of those cells is connected to all of its adjacent cells ( " + " pattern ) , and then create a path between every pair with cost == to the largest value between the two of them ie : ` max ( cell1 , cell2 `)
Sorting it by the first column , using the second for tie-breakers will take O ( N log N ) time : #CODE
Finding the row by the value in the first column can be done with ` searchsorted ` in O ( log N ) time : #CODE
Think of the array ( even multidimensional ones ) as a contiguous block of one-dimensional memory .
array , and if too small , it will try to resize the array .
@USER : If you do not specify ` count ` , then ` np.fromiter ` will have to resize the numpy array when the data outgrows the pre-allocated output array .
It calls the NumPy array method ` resize ` to extend the resultant arrays as necessary .
So if I wanted to do a four dimensional broadcast , say Arr4 = Arr1 + Arr2 + Arr3 I would need to reshape all of them to four dimensions first ?
More than just the number of dimensions , you should figure out what the shape of the final result should be and reshape the input arrays so that they can broadcast to the final shape .
I've been able to broadcast successfully , however I now hit memory issues when I start moving into higher dimensions so I think I may need to develop a less " brute force " method .
Your code would work on lists but not on Numpy arrays ( plus on arrays does not do append but vector addition )
E's pure python solution is your best choice , but if you have even as few as a hundred ranges to concatenate , this stars being noticeably faster .
Let me explain ( code is below , but I'll have a go at explaining it here ): I generate my random exponential distribution and plot a histogram of the data , producing a nice exponential curve .
I can achieve this in a basic way by changing the number of bins in my histogram , but this only changes the plot and not the original data .
This code creates the following histogram :
So , to summarise , I would like to be able to specify where this plot intercepts the y-axis by controlling how I've generated the data , not by changing how the histogram has been plotted .
Yes the y-intercept of the histogram will change with different bin sizes , but this doesn't mean anything .
` ValueError : operands could not be broadcast together with shapes ( 2100,210 0 ) ( 5 )`
Maybe you can get the parameters of sin by FFT .
I'm a bit confused on how to do these operations on dataframes that have a unique index .
Assume the dataframe ` df ` has a unique column , ` runner_id ` which records the time and speed of each runner completed one of two races , races ` A ` and ` B ` .
Each runner is unique and so the DataFrame can have this shape for two runners ` bob ` and ` mary ` : #CODE
Since the runners are unique , it's very convenient to index the dataframe ` runner_id ` .
Is there a good form to keep ` df ` in that allows unique indexing but also convenient melt-ed representation for ggplot ?
Hadley's package ` reshape ` ( and ` reshape2 ` where the original ` melt() ` is found are popular for a reason ) .
Each value is unique , because there is only one ' runner ' with the given name per race .
If ` a ` is always going to be 2x2 , but the elements of it can be huge , doing a slow outer 2x2 loop doesn't make any difference , as long as you're still doing fast iteration over the inner loops ( which ` np.mean ` does ) .
That means the speed of the outer loop's iteration is irrelevant , while the speed of the inner loops ' iterations is crucial .
Your outer array has up to 4 dimensions of up to size 10 .
Anything you do to improve the speed of the 10000 outer iterations will make less than a 2% difference in your code .
Cool idea , however , wi're looking to get a histogram of the periods so that we can tell in what percentage it suits our needs .
Why do you think roll does as a copy as a opposed to a view ?
Don't quite see the advantage over roll ( this is basically what it does afterall ) .
Currently you could probably slightly beat both speed wise with a concatenate call probably .
If , as your , example suggests , the points are generated in lexicographical order , you only need to grab the columns to ` f ` , ` g ` and ` h ` and reshape them : #CODE
A more robust solution in case the order of the points is not guaranteed , would be to use ` np.unique ` to extract indices to the grid values : #CODE
I cannot escape from the matrix inversion , the only shortcut would be to just get an idea of the main diagonal elements , and ignore the off-diagonal elements ( I'd rather not , but as a solution it'd be acceptable ) .
My code works fine for smaller matrix sizes but crashes for larger ones ( with plenty of available memory )
The size of the matrices I use is substantial ( my code runs fine for 1000000x10 float dense matrices but crashes for 1000000x500 ones - I am passing these matrices to / from subprocesses by the way ) .
Once again , it works fine for smaller values of one run-time parameter but crashes ( or hangs in python3 ) for larger ones .
Each column represents a unique longitude ( 102.5 , 5.5 , 116.2 , 100 ) and each column represents a unique latitude ( 45.5 , 65.3 , 43.2 ) .
I have arranged it now , so they are all unique pairs and there is an additional data point to demonstrate how the data should be arranged when NaNs are present .
But ` lats ` and ` lons ` aren't integer values - so they won't fit neatly into a grid of size ` ( max ( lats ) , max ( lon ))` have I missed something ?
If ` len ( vals )` is too long , do you want to truncate ` vals ` ? and if ` len ( vals )` is too short , do you want to fill the rest of the array with ` 0 ` s ?
I see that my lats have fewer unique than the lons ( all are unique ) .
It appears [ reshape ] ( #URL ) doesn't support this directly .
The lat / lon / value data should be unique pairs and are all consistent in their ordering ; though , no one list is in strictly ascending or descending order .
I've reorded things correctly ( so they are unique ) and added a value to demonstrate how the output should be when NaNs are present .
I have an estimated normal bivariate distribution , defined as a python matrix , but then I will need to transpose the same computation in Java .
Another approach for univariate is to use the inverse of bivariate approximation ( so , approximate a normal as a binomial ) , but extending this to the multivariate I can't figure out how to keep in count the covariances .
#URL RuntimeWarning : invalid value encountered in sqrt
x1 = ( abs ( E ) / sqrt ( E*E - d1*d1 )) * ( abs ( E + eV ) /( sqrt (( E + eV ) **2 - d2*d2 ))) * ( 1 /( 1 + exp ( E / T )) - 1 /( 1 + exp (( E + eV ) / T )))
#URL RuntimeWarning : overflow encountered in exp
x1 = ( abs ( E ) / sqrt ( E*E - d1*d1 )) * ( abs ( E + eV ) /( sqrt (( E + eV ) **2 - d2*d2 ))) * ( 1 /( 1 + exp ( E / T )) - 1 /( 1 + exp (( E + eV ) / T )))
dealing with zero log values in numpy / pandas
When I take the log values , is the preferred way to do this ?
If you want to plot ` 0 ` in a log plot ( which should be at $\infty$ , you could consider the ` pyplot.symlog ` function .
The ` ix ` attribute is a special thing that lets you do various kinds of advanced indexing by label choosing a list of labels , using an inclusive range of labels instead of a half-exclusive range of indices , and various other things .
The difference is that in that case , ` b.ix [: 3 ]` is a slice on indices , not on labels .
What you've requested in your code is actually ambiguous between " all labels up to an including 3 " and " all indices up to but not including 3 " , and labels always win with ` ix ` ( because if you don't want label slicing , you don't have to use ` ix ` in the first place ) .
if the svd of X^T is : #CODE
if you want them to be row vector , then just take transpose on both sides .
@USER Lin I tried this , but its giving ValueError : operands could not be broadcast together with shapes ( 10 ) ( 10 , 17 )
The * should be replaced with dot for matrix multiplication .
The data is a stack of 80 images ...
I updated the code , now it is running by itself :) on i7 CPU it is very slow ... and this is only for one stack of images , usually I have 260 stacks and the resolution is much greater than in this demo xD
By looking at the code it seems that you can get rid of the two outer loops completely , converting the code to a vectorised form .
To void giving the details of my data etc ., here is an example with a simple sin wave : #CODE
To make an orthogonal version of the DFT , you need to scale the result of the un-normalised DFT by 1 / sqrt ( N ) .
In this case , the transform is orthogonal ( that is , if we define the orthogonal DFT as F , then the inverse DFT is the conjugate , or hermitian , transpose of F ) .
In your case , you can get the correct answer by simply scaling ` aft ` by ` 1.0 / sqrt ( len ( a ))` ( note that N is found from the length of the transform ; the real FFT just throws about half the values away , so it's the length of ` a ` that is important ) .
The correct answer will not be arrived at with that simple scaling , as in your case the DC component will be added in twice , although ` 1.0 / sqrt ( len ( a ))` is still the correct scaling to produce the unitary transform .
` rfft ` , apart from repeated terms excluded , and an almost 2x speed-up , returns the exact same you would get from ` fft ` .
And it must therefore be scaled in the same way to get a unitary transform : by the square root of the length of the data , not of the unique values in the transform .
Henry is right on the non-normalization part , but there is a little more to it , because you are using ` rfft ` , not ` fft ` .
Coming from IDL , I was used to having that done automatically by the fft routine .
I don't think that permutation matrices are well suited for this task , as they like randomly change the sparsity structure .
CSC format keeps a list of the row indices of all non-zero entries , CSR format keeps a list of the column indices of all non-zero entries .
After fooling around with the ` indices ` parameter you can run ` .sort_indices() ` to get it to a more standard form .
The command ` d ` is the command for the debugger used to go down the stack to a ' newer frame ' .
If ` f_int ` wants single dimensional data , you should flatten your input , feed it to the interpolator , then reconstruct your original shape : #CODE
` PEP8 E712 ` requires that " comparison to ` True ` should be ` if cond is True : ` or ` if cond : `" .
( Or to put it otherwise : there is no unique way to assign three values to eight variables , therefore the assignment is ill defined . )
I don't feel like I should have to tell it to transpose the vector when the orientation should be implied in X [: , 4 ] .
Essentially , ` a :b ` represents the list of all the elements at indices ` a ` ( inclusive ) to ` b ` ( exclusive ) .
The slice notation represents all indices along the first dimension ( just 0 and 1 , since the array has two rows ) , and the 4 represents the fifth element along the second dimension .
The trick , if you want to get out something of the same dimensions you started with , is then to use all slice indices , as I did in the example at the top of this post .
numpy.random.multivariate_normal ( mean , cov [ , size ])
numpy.random.multivariate_normal ( mean , cov [ , size ])
So when I compute numpy.mean ( data , axis=0 ) and numpy.cov ( data ) and use the mean and cov values in numpy.random.multivariate_normal ( mean , cov ) .
To fix , take the transpose of data in ` np.cov ( data.T )` to get the ` X ` x ` X ` covariance matrix : #CODE
Note that this causes the python interpreter to exit rather than just printing a stack trace .
I would like to have a numpy array that contains all of the atomic positions , so that I can perform numpy operations like vector manipulation and such as a translation function ` def translate ( data , vec ): return data + vec ` .
For your translate function : #CODE
Let's say you have a template containing an subimage which is deformed ( partial , and / or rotated ) , it's the exact same image ( to the human eye ):
How to retrieve frequencies of the number of unique occurrences of pair letters for every possible pair of columns from a numpy matrix in python
What I would like to have is FOR EVERY POSSIBLE pair of columns , retrieve the frequency of the number of unique occurrences of every pair of letters from the row within each pair of columns .
` s ` is the matrix , and ` s.T ` is its transpose .
And now in ` matches [: , i , j ]` you have the unique pairs between columns ` i ` and ` j ` , and you can then do : #CODE
KeyError : ' max ' becose i need parse as " max " , " min " , " avg "
I could list the lines where the changes were made , but those can be found by looking for " float ( " and " exp ( " in the first version of the code .
Basically , you modify the objective function you want to minimize , which is normally the sum of squares of the residuals , adding an extra parameter for every fixed point .
Calculate euclidean norm of 100 points of n-dimensions ?
I want to compute euclidean norm over all such points .
You can do this to compute the euclidean norm of each row : #CODE
In CSC format you have two writable attributes , ` data ` and ` indices ` , which hold the non-zero entries of your matrix and the corresponding row indices .
Using ` numpy ` I first get sorted array of unique elements .
I did not mention it , but there is only 1 unique element in some arrays .
I would like to resize image A to a quarter of its dimensions ( i.e. 1000 x 1000 pixels ) with a pixel size 20 x 20 meters ( image B ) by spatially aggregating four neighbouring pixels in non-overlapping blocks , starting from the top-left corner of the image , while the value for each pixel in image B will be a result of their arithmetic average .
How to concatenate two numpy arrays inside a function and return it considering the following program #CODE
The concatenate function creates an entirely new object and this is why the original object , array ([ 1 , 2 , 3 ]) is not changed .
You need to concatenate two arrays is that correct ?
I have myarray1 , myarray2 , data1 , data2 and I need to concatenate ( myarray1 and data1 ) and ( myarray2 and data2 ) .
If transpose needed : #CODE
I'm assuming the ` AttributeError ` is because pandas will not automatically try to convert / truncate the float values into an integer ?
linear regression on log-log histogram in numpy
I'd like to compute and draw a linear regression on this histogram to find out the parameters of the linear regression , as well as the r square .
Can you fit a line to the log of the values ?
No I can't , and I suspect that is because I have a few zeros on the y axis .
So some bins in the histogram have count 0 ?
To concatenate them you must either remove the first element of every array in ` c ` or the last of every array in ` b ` .
Is the ` max ` function being invoked part of the standard python library ?
When I plot the histogram I get a pdf defined by 2x as opposed to a normal uniform distribution .
U has shape ` ( 2,100 00 )` , and ` u.max ( axis=0 )` gives you the max along the ` 0 ` axis , returning an array with shape ` ( 10000 , )` .
It is basically select a max and the axis determines how the max is chosen .
The rest of the code is meant to set the histogram plot .
There are 100 bins in total in the histogram and the bars will be colored blue .
The maximum height on the histogram y-axis is 2 and normed will guarantee that at least one sample will be in every bin .
Of course the easiest way to get three periods is to concatenate 3 copies of the inverse FFT in the time domain : #CODE
But that does not stop the FFT function from decomposing it into a sum of many periodic sinusiods .
And the sum of periodic sinusoids with periodicity of N will also have a periodicity of N .
In that case you would " extrapolate " zeros to the left and the right .
So , if your input is sin ( W*t+p ) for arbitrary W and p you will only need linear prediction with order two .
Since creating the array is an expensive operation , I want to create it only when necessary , and to do it as efficiently as possible ( specifically , to append data in-place when possible ) .
I'm fine with it raising an error if refcehck fails ( I can catch it and then create a new copy ) , but I want it to fail only when there are " real " external references , ignoring the ones I know to be safe .
The ` resize ` method has two main problems .
Now the resize will do one of two things depending on your implementation .
` resize ` is acctually not very efficient .
I believe in general , resize has to allocate new memory and do a copy every time it is called to grow an array .
This should be very fast because you only need to grow the array log ( N ) times and you use at most 2* N-1 storage where N is the max size of the array .
What I did not mention above is that I sum all the values that fall into one grid pixel and then take the mean from the total count .
First , you should modify all your lats and lons into , to make them ( possibly fractional ) indices into your grid : #CODE
If you run this code on your example data , you will get two empty arrays returned : that's because the order of the quadrilateral points is a surprising one : indices 0 and 1 define one diagonal , 2 and 3 the other .
If you really are doing things this other way , you need to change the second call to ` lattice_points_in_triangle ` inside ` lattice_points_in_quadrilateral ` so that the indices used are ` [ 0 , 1 , 3 ]` instead of ` [ 0 , 2 , 3 ]` .
Thus , the red dots represent the original corners - the blue ones the intermediate pixels between them .
Here , the two runs are at indices ` [ 0 ; 3 ]` and ` [ 7 ; 11 ]` .
The main idea is to create a helper array with the indexes using ` arange ` and ` reshape ` .
Just so you understand what is going on , you could get the same result with the dot product of your indices and the strides of the array : #CODE
Your other option , which I hadn't even considered , is to implement the sum in sparse format yourself , so that you can take full advantage of the periodic nature of your array .
I have edited my answer with a x 10,000 times faster version , which is still kind of slow , you are looking at about 1 min . to add the vector to your whole matrix .
Note that while memory-mapping can be plenty fast , you should keep an eye out for thrashing .
This is just gradient descent if you have access to a library implementation , the gradients are cheap to compute and because it's only a first order method , you'll not have the same performance overhead .
I don't avoid standard backprop , I use backprop to calculate the gradient of the error wrt . the network's weights .
I still need an optimization algorithm that can traverse the error surface ( using the gradient calculated with backprop ) to find its minimum .
Backprop , at least as I would use the term , means using gradient descent .
I.e. your update is a step size multiplied by the negative of the gradient .
What is the algorithm used to calculate the error gradient wrt . weights called ?
Also , is there a technical or conceptual reason for " backpropagation " to refer only to training done with gradient descent ?
It just happens that this is equivalent to gradient descent .
It means adjusting weights by some learning rate times the negative of the gradient of the error wrt the weight .
I've tried a bunch of dumb things like unicode ( string ) that all throw : UnicodeDecodeError : ' ascii ' codec can't decode byte 0xb0 in position 3 : ordinal not in range ( 128 )
So I replaced \xb0 with \xC2\xb0 and I could encode into unicode .
Replacing ` \xb0 ` with ` \xc2\xb0 ` allows utf-8 to correctly encode the string .
Now lets say I want to run this function a bunch of times for getting a sum :
Your last reshape doesn't work , because ` y ` has no shape defined .
( Or efficient , as each min call will make temporaries . )
You can tell if you replace the ` min ` with ` max ` .
Therefore , I calculate a couple of coordinates within the polygon and translate their lat / lon combination to their respective column / row combo of the grid .
Yes , histogram 2d is probably even more appropriate for this problem .
Yes , the y array may contain zeros .. how can I skip the problem and just put a zero in ya / yb when the corresponding value in y is a zero too ?
Skip the problem and use zeros : ` ind = y !
You are passing to ` newdist ` 4 values , of which only 2 are unique , there may be some performance to scrape there too .
So you are still looking at a good 10 min . of processing , which is not really that much when compared to the more than 2 days that your current solution would require .
You could use np.unique to get the unique values in ` x ` , as well as an array of indices ( called ` inverse `) .
Since the labels start at 0 , the bincount won't be filled with a lot of zeros that you don't care about .
Finally , column_stack will join ` y ` and the bincount into a 2D array : #CODE
Use the Set constructor to create a unique list of items
So , I have a random array of unique integers ( normally millions of elements ) .
I have another array ( normally a tens of thousands ) of unique integers which I can create a mask .
Now given that the IDs are unique in both arrays , is there any way to speed this up significantly .
i.e. it tries to compute the dot product along the outermost axis of ` A ` .
The ` axes =( 0 , 2 )` indicates that ` tensordot ` should sum over the first axis in ` v ` and the third axis in ` A ` .
Follow the documentation link and it says that ` obj ` may be a list of indices and ` values ` may be a list of values , so any number of inserts could be accomplished in one step .
So , what your saying is I cant write C style loops over indices of elements ?
@USER you _can_ do loops , over either the indices or the elements ( see Ionut's updated answer ) , but it'll be an order of magnitude slower than using equivalent Numpy or Scipy routines .
I just find it difficult to comprehend without indices .
@USER ` a = array ([[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]]); b = sum ( a , 1 )` will give ` b = array ([ 6 , 15 ])` ( because 1+2+3=6 , 4+5+6=15 ) .
It doesn't hurt to write it out using ` for ` loops to get your thoughts in order , and then translate that to vectorized Numpy .
how to obtain residuals from numpy svd
I have fitted a set of points using numpy svd and have the coefficients for the fitting function .
Using ` digitize ` gets the result I want , but I have to use some ugly loops to get it to work : #CODE
or use ` unique ( inds , return_inverse=True )` and ` bincount() ` twice : #CODE
I have two arrays , one for the outer radius of each shell ( ` rad [ ]`) and one for the density of each shell ( ` den [ ]`) .
The array ` mass [ 0 : -1 ]` in your second line will be evaluated as ` den [ 0 ] * ( rad [ 0 ] **3 )` followed by nothing but zeros .
I am trying to figure out how to calculate covariance with the Python Numpy function cov .
Does anybody know how to make the Numpy cov function perform like the one I wrote ?
When ` a ` and ` b ` are 1-dimensional sequences , ` numpy.cov ( a , b ) [ 0 ] [ 1 ]` is equivalent to your ` cov ( a , b )` .
( where , again , ` cov ` is the function you defined above . )
Cython : unsigned int indices for numpy arrays gives different result
I was using unsigned int indices to speed up access according to : #URL
The indices are extremely unlikely to effect the answer since it is just accessing a fixed length of memory that the data attribute of the numpy array is storing .
But when you used Python ` int ` indices , it already had a Python object ; it used ` PyObject_GetItem ` to get the ` np.float32 ` scalar .
I'm using a data set that consists of mostly nominal values from SFDC ( e.g. EE Names , Title , Role , Lead Source , Account Name , etc . ) and am trying to correlate the features to a boolean class of whether a Sales Lead was converted to a Sales Contact .
I could map each of the unique classifications to a new field ( feature ) with a boolean mapping scheme , but then i'll generate an extremely large number of new features and I'm not sure if that will give a meaningful output .
` sum ( np.mgrid [: 6 , : 6 ])` ?
It is hard to tell why a function call would reshape a numpy array without seeing the code .
I have seen pv post about performing tensor rpoduct interpolation with pchip , but it results in a pesky divide by zero error I can't get rid of , even with his updates on github .
For context : the goal is to effectively merge the two arrays such that each unique combination of corresponding values between the two arrays is represented by a different value in the resulting array , except zeros in the new_data array which are ignored .
EDIT2 : the data array only contains whole numbers , that is : it's initialised as zeros then with each usage of this loop with a different ` new_data ` it is populated with more values drawn from ` available_values ` which is initially a range of integers .
What would be a better way to make sure that for the duration of this loop each unique combination of corresponding values in ` data ` and ` new_data ` results in a consistent integer value at that index of ` data ` ?
Let's say ` new_data ` and ` data ` take values 0-255 , then you can construct an ` available_values ` array with unique entries for every possible pair of values in new_data and data like the following : #CODE
@USER indices
Then the first sum , ` Sum_v ( Av * Xi_v )` can be computed with ` np.dot ` : #CODE
Similarly , the second sum , ` Sum_v ( Bv * Wj_v )` can be computed as : #CODE
However , we want the first sum to result in a vector varying along the ` i ` -index , while we want the second sum to result in a vector varying along the ` j ` -index .
The third sum is a simple dot product between two 1-dimensional arrays : #CODE
For the sake of concreteness , let's say the first two axes of ` Z ` represent the ` i ` and ` j ` -indices , and the last axis of ` Z ` is the one you wish to sum over .
` Z = np.asmatrix ( ... reshape ( N , N , N ))` throws a ` ValueError ` .
You're right , but it still has the advantage of ` matrix ` times ` ndarray ` gives you a ` matrix ` and automatically does inner product over the innermost indices .
Personally I'd rather keep my matrices as a ` matrix ` because then I can't accidentally do ` A*X ` as it gives a warning since the innermost indices do not have the same dimension .
The strictish typing does help prevent mistakes , and allows for outer products , etc without as much though ( for me ) .
This will also help you avoid having to broadcast carefully .
Note that I've ` transpose `' d the 1d matrices , since a 1d matrix is a row vector by default .
@USER A couple more pointers : I have made ` test ` a single element array so that it can broadcast against any shape , tried it first with a 0-D array and it failed .
I have a txt file ( which is basically a log file ) having blocks of text .
@USER -- is this the output you want , or do you want only the indices that are greater than their immediate successors -- in which case it is even simpler ...
If you take the diagonal of that matrix : #CODE
and you will have the indices of all the values fulfilling your desired condition .
Why does this sum not converge in Python
Python however calculates wrong sum values .
It's just a function definition of a saturation curve . massuptake ( D1 , D2 , D3 , time ): =1- ( 8/ ( Pi^ ( 2 ))) ^ ( 3 ) evalf ( Sum ( evalf ( Sum ( Sum (( exp ( - ( D1* (( 2 * ii+1 ) /( 75 )) ^ ( 2 ) +D2* (( 2 * jj+1 ) /( 75 )) ^ ( 2 ) +D3* (( 2 * kk+1 ) /( 4 )) ^ ( 2 )) *Pi^ ( 2 ) *time )) /(( 2*ii+1 ) ^ ( 2 ) * ( 2*jj+1 ) ^ ( 2 ) * ( 2 * kk+1 ) ^ ( 2 )) , ii=0 ..
I would like to create a function which generates the 64x4 matrix , a matrix of 256 elements , that includes every set of the above eleven values for which the sum is 1
The leaves of the tree contain all combinations that sum to one .
Select the next number that does not make the sum exceed 1 .
Repeat from this node until you reach a leave node ( sum == 1 ) .
And if you don't limit yourself to subsets of 4 elements , then we have to get rid of the ` 0 ` ( there are infinite subsets adding up to 10 with an arbitrary number of zeros ) , and then we are calculating the partitions of 10 , which are The Answer to the Ultimate Question of Life , The Universe , and Everything , or exactly 42 .
A stack trace would go a long way , perhaps .
By running variance image I mean the result of calculating sum (( I - mean ( I )) ^2 ) / nPixels for each sub-window I in the image .
The output should be a 1-1 mapping of indices of elements from x to indices of elements from y .
I prefer to do it without sorting the elements first , but if they are sorted then I want to get the indices in the original , unsorted lists ` unsorted_x ` , ` unsorted_y ` .
edit : to clarify I'm not trying to find the best fit across all elemets ( not minimizing sum of distances for example ) but rather the best fit for each element , and it's okay if it's sometimes at the expense of other elements .
I'm not trying to find the best fit across all elemets ( not minimizing sum of distances for example ) but rather the best fit for each element , and it's okay if it's sometimes at the expense of other elements .
` tree = KDTree ( x [: , None ]); tree.query ( y [: , None ] , k=1 )` finds the nearest ` x ` for all ` y ` ( based on quadratic norm , you can change that ) .
EDIT 2 A solution using ` KDTree ` can perform very well if you can choose a number of neighbors that guarantees that you will have a unique neighbor for every item in your array .
How can I use numpy / scipy to flatten a nested list with sublists of different sizes ?
An other option might be to ` sum ` the lists : #CODE
return sum ( lst , [ ])
There are a lot of zeros ( probably 70% of the values in the numpy array are 0.0000 ) I am not sure of how I can somehow exploit this though and generate a tiny file that my c++ program can read in
` std :: ostream :: write ` ` std :: istream :: read ` are useful for binary output / input in c++ .
e.g. is it all zeros or something else that has a concise text representation ?
There are a lot of zeros ( probably 70% of the values in the numpy array are 0.0000 )
Suppose you have an array with a small amount of nonzero numbers : #CODE
First , isolate the interesting numbers and their indices : #CODE
Is there a way to adjust the code to give me both indices ( since it's a 2D array ) .
` indices = np.where (( a < 4 ) | ( a > 12 ))` Aside : this is a duplicate .
I understand this can be done by adding an extra column that contains values to allow grouping , for example you could join the above DataFrame to ` [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ]` and groupby the added column .
If you write Python like you write Java , of course it's going to be slower , idiomatic java does not translate well to idiomatic python .
Cython : unsigned int indices for numpy arrays gives different result
Here I use unsigned int indices to access the array response but the results are left on np.float32_t intermediate variables that then I use in the calculation .
These small differences accumulate with other operations I do afterwards producing different benchmark results for my system
I have an array with 301 values , which were gathered from a movie clip with 301 frames .
The movie clip is running at 30 fps , so is in fact 10 sec long
From the numpy fft page #URL :
When the input a is a time-domain signal and A = fft ( a ) , np.abs ( A ) is
if rate is the sampling rate ( Hz ) , then ` np.linspace ( 0 , rate / 2 , n )` is the frequency array of every point in fft .
You can use ` rfft ` to calculate the fft in your data is real values : #CODE
signal x contains 4Hz 7Hz sin wave , so there are two peaks at 4Hz 7Hz .
This has just taken me most of the day to get to work on Gentoo ( see this bug report ) , I'm not sure my experiences translate all that well to the Mac unfortunately .
` AttributeError : sqrt ` what's going on here then ?
The ` AttributeError ` occurs because NumPy , seeing a type that it doesn't know how to handle , defaults to calling the ` sqrt ` method on the object ; but that doesn't exist .
How does that accidentally remove ` sqrt ` from the ` numpy ` namespace ?
I need to make a multidimensional array of zeros .
What ` x.any() ` actually does : it's the same as ` np.any ( x )` , meaning it returns ` True ` if any elements in ` x ` are nonzero .
Your code first tests ` x.any() ` , which evaluates to ` True ` , as ` x ` includes a nonzero value .
If I understood this , ` sum ` performs a total sum along an axis and , given that ` a > 10 ` returns ` bool ` and in python true values are equal do one , then ` sum ` in this case is equivalent to counting , right ?
I wrote some code that has a few boolean statements at the beginning , and depending on which ones are True / False different problems are solved , and different plots are created ( currently using imshow and some animation ) .
I have thought about ` reshape ` and ` resize ` without success .
you can ' slice ' an collection by ` col [ #URL in fact the slice operator ( ` [: :] `) used in ` col [ #URL can be represented as a ` slice ` object ( ` slice ( start , end , step )`) and you can ' slice ' the collection using that object like you do with integer indices .
to truncate an ( possibly nested ) array , you have to select every item except the last one - which is ` [: -1 ]` - hence ` slice ( -1 )` , and you have to do the selection for n times , n being the dimension of the array .
thanks , but no . the resize operation shuffles the values around in the array , matching the original memory layout I think .
Multidimensional indices are passed to the numpy object as a tuple .
Update , since you posted a solution with ` np.where ` at the same time as mine : that's probably the best solution , though it does make a copy that these ones don't .
Numpy nonzero / flatnonzero index order ; order of returned elements in boolean indexing
I'm wondering about the order of indices returned by numpy.nonzero / numpy.flatnonzero .
While in most cases this is enough , there are some when you need a sorted list of indices .
Is it guaranteed that returned indices are sorted in case of 1-D or I need to sort them explicitly ?
Given the specification for advanced ( or " fancy ") indexing with integers , the guarantee that ` A [ nonzero ( flag )] == A [ flag ]` is also a guarantee that the values are sorted low-to-high in the 1-d case .
` result ` takes the shape of ` ind ` , and contains the values of ` x ` at the indices indicated by ` ind ` .
This means that we can deduce that if ` x [ flag ]` maintains the original order of ` x ` , and if ` x [ nonzero ( flag )]` is the same as ` x [ flag ]` , then ` nonzero ( flag )` must always produce indices in sorted order .
The only catch is that for multidimensional arrays , the indices are stored as distinct arrays for each dimension being indexed .
Can't you just concatenate the arrays ?
not really a numpy user , but it seems it's one function - [ concatenate ] ( #URL )
To stack two structured numpy arrays " vertically " , use np.vstack .
My machine has 16 real cores , so 4 processes with max 400 % load each will almost get the maximum performance out of the machine .
The scaling is not perfectly linear anymore and the speedup factor is not the number of workers involved , but the absolute calculation time becomes significantly reduced compared to ` OMP_NUM_THREADS=1 ` and time still decreases significantly with the number of worker processes .
If you try and run something like ` svd ` in parallel ( which uses LAPACK ) perhaps each worker still behaves as though it is executing on multiple cores , and does ' sub-optimal ' things like writing to each other's caches etc .
Thanks , but I get following error messages : ' Exception RuntimeError : RuntimeError ( ' cannot join current thread ' , ) in ignored '
All I need to do is reshape it .
One solution I came up with was to append a dummy element and then pop it off , but that is very ugly .
And the case with all lists of length 2 won't even let you reshape to ` ( n , 3 )` .
@USER You don't need to tile , use broadcasting instead !
@USER If you have many more rows than this example , surely keeping everything in one array and accessing it with indices would be easier ?
But , ` resize ` looks like it just might be the thing I'm looking for ...
If I do the operation in-place , then I mess up other objects which have " views " into this array , but other references to my array will see the resize too .
In my previous question , I learned to resize a subclassed ` ndarray ` in place .
Unfortunately , that no longer works when the array that I am trying to resize is the result of a computation : #CODE
In any event , after the view casting , the output no longer owns the data making it impossible to reshape in place ( as far as I can tell ) .
As a kludge you could resize ` d.base ` and then reassign ` d = d.base.view ( Foo )` .
What happens if I resize ` d.base ` and don't reassign ?
To resize you need to use ` refcheck=False ` .
You might be able to avoid the need to resize if you use [ numexpr ] ( #URL ) .
I would also claim that , based on my experience , ` resize ` should generally be avoided .
You should not have any problem working with the view created if you avoid ` resize ` ing .
There's a hacky feeling to it , and it's hard to work with ( as you might begin to understand , having encountered one of the two classic errors when using it : ` it does not own its data ` . The other is ` cannot resize an array that has been referenced `) .
Since your decision to use ` resize ` comes from an answer to your other question , I'll post the rest of my answer there .
@USER , and why do you need that in the first place , why do you have to shrink the data * after * the ufunc so much that it is worth to even consider resize ?
@USER -- I have something like ` a + b + sqrt ( d*d + e*e + f*f ) ...
If I can resize the arrays in my datafile reader ( which is my plan ) , I shouldn't have other views of the array floating around to be a big problem anyway .
In my case , that's not the greatest if you want to do ` a + b * sqrt ( c ) ** 4 ` or whatever .
First , I populate a grid ( numpy array ) with zeros , each position in the grid corresponds to a length of 0.5cm .
I know the radius of the circles so I can calculate the area of the circle , because I know the area of the circle I change the the zeros in the grid to which fall in the area of the circle ones .
I then count the frequency of ones in the gird and use this to calculate the area of the combined circles , because I know the length of each position in the grid I can calculate the area .
I want to change the the area with marked with orange to ones .
I can currently change the the orange squares horizontal and vertical to the circle centre , but the diagonal boxes from the centre are causing me trouble .
For getting the squares diagonal from the center you probably have to use the Pythagorean theorem .
If you are after the intersection , note that your circle centers are ` sqrt ( 17 ) ~= 4.123 ..
` units apart and the sum of the two radii comes to ` 3.5 ` so there is actually no overlap .
Use a ` grid ` of ` zeros ` and ` | ` instead to get points inside ** any ** circle .
I know I could split my array into three separate ones , one for each region , but how do I combine them again afterwards ?
If you actually want to know which of the b points is the nearest , you use ` argmin ` in place of ` min ` .
As the performance is critical in my project I prefer not to construct a permutation vector , I am looking for a way to bind the labels with the samples .
The mixing of indices with float datatypes makes me uneasy .
If so I would go with the random permutation vector - I don't think your solution is any faster ( even without my data type reservations ) because you're still allocating memory when creating your samples_and_labels array .
I can never remember how ` translate ` and ` maketrans ` work .
@USER -- I had a hard time remembering how ` translate ` and ` maketrans ` work for quite a while too , but I've gotten used to it .
You need to strip the `"` from you time string !
@USER : ` a [ indices ]` returns such ordered array .
Now when I try to sum f the array isn't added up .
I print f And I see 1500 items of correct values but not the sum of these .
For example how many times can [ 3 , 1 , 1 , 0 ] be found , and return the indices of those rows ?
Find the ones which match : #CODE
+1 for using None as the 0th axis index to force a resize !
Basically get the output from featureselection function and concatenate it to the numpy array data #CODE
Alternatively , I could get the indices along that axis which correspond to the maximum values from : #CODE
My question -- Given the array ` indices ` and the array ` a ` , is there an elegant way to reproduce the array the array returned by ` a.max ( axis=2 )` ?
I would like to do this because I would like to extract the indices based on the data in 1 array ( typically using ` argmax ( axis= ... )`) and use those indices to pull data out of a bunch of other ( equivalently shaped ) arrays .
Nevertheless , you may be interested in functions ` ravel ` , ` ravel_multi_index ` , ` unravel_index ` , ` flat ` , and ` flatten ` .
If I want to generate random permutations of x , then what is the difference between shuffle ( x ) and permutation ( x ) ?
When used on a ` panda.Index ` , only ` permutation ` works and ` shuffle ` doesn't .
@USER ` permutation ` coerces its argument into an ndarray ( by copying ); ` pandas.Index ` is sufficiently different from an ndarray that ` shuffle ` won't work on it , but will work on an ndarray created from it .
All you need to know is , for every bin in ` x ` , what are ` n ` , ` sy ` and ` sy2 ` , the number of ` y ` values in that ` x ` bin , the sum of those ` y ` values , and the sum of their squares .
Generalizing matrix transpose in numpy
When matrix transpose is applied to ` a ` , we get : #CODE
I would like to consider each of ` [ 1 , 2 ]` , ` [ 2 , 3 ]` , and ` [ 3 , 4 ]` as list items in ` b ` , only for the purpose of performing a transpose .
In general , I would like to be able to specify what a list item would look like , and perform a matrix transpose based on that .
Then the output given below would be of doing a matrix transpose on higher dimension elements .
More generally , once I describe what an ' item ' would look like , I would like to know if there is a way to do something like a transpose .
The transpose , ` b.T ` is the same as swapping the first and last axes , ` b.swapaxes ( 0 , -1 )` : #CODE
It lets you add dimensions as you like , you just have to choose the right set of slicing indices .
I'd also mention the ` transpose ` method , which can replace a bunch of ` swapaxes ` calls because it lets you apply a permutation to the axes instead of just a transposition .
In this example , each row is a sample and the sampling depth is the sum of the row .
I want to randomly sample ( with replacement ) the matrix by ` min ( rowsums ( matrix ))` samples .
The rarefaction function goes row by row randomly sampling with replacement ` min ( rowsums ( matrix ))` times ( which is 6 in this case ) .
In C++ I would go for every row and build a lookup table of positions that are > 0 , generate ` min ( rowsums ( matrix ))` integers within the range equal to the number of items in the lookup table .
I would accumulate how often each position in the lookup table was drawn and then put those numbers back into the right positions in the array .
I need to reduce a nonzero array of the type : #CODE
sum along axis 0 to get all columns sum , then create a bool array to select the columns : #CODE
You could assign a unique sequential number to each row , then choose a random sample of those numbers , then serially extract each relevant row to a new file .
As a first experiment I tried to use the scipy.spatial.KDTree.query_pairs implementation , and with a sample of 5000 point it takes 5 second to return the indices .
This process is O ( n * log ( n )) and should take certainly less than your 1 / 10th of a second requirement .
I tried using the boolean ` np.in1d ( A , B )` directly instead of converting it to indices but it didn't work .
Are IDs in A unique ?
( adding zeros to a2 , and increasing its length ? )
Do I have add zeros to other arrays ?
As for your second question , just reshape it to a 3 dimensional array : #CODE
Or maybe ask on a more CS stack exchange ..
where ` indices ` will contain the column indices in ` obs2 ` corresponding to each observation in ` obs1 ` .
Note that I had to transpose ` obs1 ` and ` obs2 ` .
The moment you extract ` r ` with arbitrary indices , the data is copied .
That ` .nonzero() [ 0 ]` makes it a list of indices , rather than a ' mask ' with True and False values , this could be more efficient if we are talking about very long lists .
Now ` cond [ i , j ]` has the boolean value for the window centered at ` im [ i+1 , j+1 ]` , and is two items shorter in each direction than your original image .
Being a scientist , ` sqrt ( -1 )` should return a complex number , so I'm inclined to go with SciPy only .
getting indices of non-unique items in an array
Given an integer array ` I ` with ` 0 = I [ j ] 1000 ` , with non-unique integer values ` I [ j ]` , and a ' values ' array ` V ` with ` V.shape == ( 1000 , )` , how can I create an array ` R ` with ` R.shape == I.shape ` such that ` R [ j ] == V [ I [ j ]]` if ` I [ j ]` is unique in ` I ` , and ` R [ j ] == np.nan ` if the value of ` I [ j ]` occurs more than once in ` I ` ?
As an example , given ` I = np.array ([ 1 , 2 , 1 , 3 , 2 ] , dtype=int )` , the result should be ` V = array ([ nan , nan , nan , V [ 3 ] , nan ])` , as ` 3 ` is the only unique element of ` I ` .
To try getting around the problem you discussed , I tried using std :: vector .
@USER Weckesser I convolve a windows obtained with scipy firwin to the signal as proposed in [ this ] ( #URL ) .
You can check this by comparing ` convolve ( b , convolve ( b , x ) [: : -1 ]) [: : -1 ]` with ` convolve ( bb , x )` , where ` bb = convolve ( b , b )` .
@USER First , you need to understand that a matrix dot product would be written as ` ( ' ij , jk -> ik ')` ( see #URL ) .
Here is the error message you get when the last dimensions are 3 and 4 : ` ValueError : operands could not be broadcast together with remapped shapes [ original -> remapped ]: ( 2 , 2 , 3 ) -> ( 2 , newaxis , 3 , 2 ) ( 2 , 2 , 4 ) -> ( 2 , 4 , 2 )` .
@USER ` hits ` has 501 ` True ` s in the diagonal , and you are counting twice every hit , both as ` ( i , j )` and as ` ( j , i )` .
For each color channel of each pixel , I want to compute fft of that pixel across time in the series .
Note that the matlab fft operates on the first nonsingleton dimension , so ` F = fft ( video )` is doing what I'm after .
do you mean ` fft ` or ` fft2 ` ?
` fft ` .
I think you should spend some time working out how to solve a simpler problem with the tools linked above : for instance , unknown function u ( x ) , u ( 0 )= u ( pi )= 0 , d^2u ( x ) / dx^2 = cos ( x ) - u ( x ); after that d^2u ( x ) / dx^2 = f ( x ) - u ( x ) where f ( x ) is a linear interpolant of some data points .
strange behavior with numpy roll and masked array
The plot seems to result corecct , but I receive the message : " overflow encountered in exp " .
Your problem is ill conditioned because your array ` times ` contains big numbers that when used in ` exp ( -a*time )` are giving values close to ` 0 .
` exp ( -a* ( time-time0 ))`
E's basic idea of stacking all the surrounding values in a single dimension , but I think there are better ways of creating the stacked array and converting the return of ` np.argmin ` to pairs of indices : #CODE
The only I know of in python is to loop over the data in x and group them according to bins ( max ( X ) -min ( X ) / nbins ) then loop over those blocks to find the std .
` left_v.T.dot ( right_v )` should give a diagonal matrix , but it doesn't ,
` left_v_2.T.dot ( right_v_2 )` gives an anticipated diagonal matrix .
Is the sciPy manual a bit imprecise while describing ` eig ` ?
About ` vl ` , the ` eig ` docstring says : #CODE
Or , taking the conjugate transpose ( i.e. Hermitian transpose ) of both sides ( which is what .H means ) , and assuming ` b ` is the identity , #CODE
So the rows of the conjugate transpose of ` vl ` are the actual left eigenvectors of ` a ` .
Now , the ` eig ` docstring also says in the description of the return values : #CODE
and that is potentially misleading , since the conventional definition of a left eigenvector ( e.g. #URL or #URL ) is a row vector , so it is the conjugate transpose of the column of ` vl ` that is actually the left eigenvector .
I understood that you confirm that the conjugate transpose of ` vl ` is the left eigenvector according to commonly used convention .
Now the question is why ` eig ` doscstring provided this ambiguous information ?
@USER , ` Q / V * ( CO2_To-y [ 0 ]) + TG_CO2 ` is a number , ` r1 ` is a numpy array , so their sum is a numpy array .
What you can do is use ` r1 [ 0 ]` or ` sum ( r1 )` - actual approach depends on your needs .
Generating a binomial distribution around zero
I want a binomial distribution but I want it centred around zero ( I know this doesn't make much sense with respect to the definition of binomial distributions but still , this is my goal . )
so basically a binomial such as f ( 0 ) = max ( f ( x )) .
` n*p ` is the mean of the binomial distribution .
The most frequent element is the mode , and [ according to wikipedia ] ( #URL ) it is either ` floor (( n ? + ? 1 ) p )` or ` floor (( n ? + ? 1 ) p ) ?
But the limit of the binomial distribution is just the normal distribution when ` n ` becomes large and with ` p ` bounded away from 0 or 1 . since ` n*p ` is not going to be an integer except for certain values , why not just use the normal distribution ?
This the all true for large N , but the op might want a binomial with a small N , in which case the normal distribution would not be a good approximation .
To get a binomial distribution with mean shifted close to 0 , you can call #CODE
Or should I just give up and pass in a 2D array and reshape it myself ?
Despite some index-rearrangements the crucial trick here is to use ` repeat ` because in the loop the indices ` b , c ` " freeze " for ` ni ` steps , while ` k ` grows .
What if I need to concatenate several of these types of matrices together ?
If you try to stack views , you'll trigger a copy .
Or you could use a simple run-length-encoding scheme , plus maybe a higher-level list of runs for , or list of pointers to every Nth element , or even a whole stack of such lists ( one for every 100 elements , one for every 10000 , etc . ) .
But for mostly-uniformly-dense arrays , the easiest thing is to simply store a ` dict ` or ` defaultdict ` mapping indices to values .
how to perform an inner or outer join of DataFrames with Pandas on non-simplistic criterion
we would like to produce a SQL-style join of both dataframes using a non-simplistic criteria , let's say " df_b.c > df_a.a " .
my current approach for inner join is to produce a cartesian product
for outer join , I'm not sure of the best way to go , so far
I've been playing with getting the inner join , then applying the negation
HYRY answered the specific question here but I needed something more generic and more within the Pandas API , as my join criterion could be anything , not just that one comparison .
For outerjoin , first I'm adding an extra index to the " left " side that will maintain itself after I do the inner join : #CODE
then we do the cartesian and get the inner join : #CODE
then I get the additional index ids in " df_a " that we'll need , and get the rows from " df_a " : #CODE
How would you produce a " join " of df_1 and df_2 on " c > a " ?
How would you produce the " left outer join " of same ?
I use the outer method of ufunc to calculate the result , here is the example :
Inner join , because this only calculate the cartesian product of ` c ` ` a ` , memory useage is less than cartesian product of the whole DataFrame : #CODE
to calculate the left outer join , use ` numpy.setdiff1d() ` to find all the rows of ` df_a ` that not in the inner join : #CODE
And I should obtain the value of ` y ` for ` x= 5.1 ` , but I think that exist better methods , and probably they are the correct ones .
If you use ` dtype= np.float32 ` and use separate arrays for the vertices and colors , then you don't need to ` flatten ` ( copy the data to contiguous memory ) or use ` astype ` ( create a whole new array ) , and the pointer is just ` vertices_gl = verticesArray.ctypes.data ` , etc .
Can you broadcast dictionary definitions over numpy arrays ?
My professor insists that we use numpy's broadcast functionality instead of for loops as much as we can , and I want to know if it's possible to broadcast dictionary definitions .
Einsum has nothing to do with dot or tensordot , its completely utterly distinct . you could do einsum logic by hand , cast to another type if possible or implement einsum for object types .
Einsum basically supersedes tensordot ( not dot , because dot is normally using optimized linear algebra packages ) , code wise it is completely different .
Of course you can translate einsum notation to tensordot notation too I am sure , and that is probably a bit faster since the loops would end up being mostly in C ...
Your code does not cover everything einsum does ( repeated indices ) , otherwise it is likely somewhat faster if you care about that .
But translate this into C , and it is by far the best solution .
First it separates the ` einsum ` arguments in tuples of ( indices , tensor ) .
It also prints out the new indices signature .
The indices signature is discarded and only the tensor is returned .
at the section # do something to vector , i want to normalize the image ( as vector ) i need to do linear normalize or histogram equalization ? and one more question , i can use your peice of code as function , and if i have an array of images how i put all of them to one big matrix and every image is a vector ?
because i want to make face space , each vector is an image , i want to use histogram equalization because i want to learn how face recognition works !
Vectorized way of calculating row-wise dot product two matrices with Scipy
I want to calculate the row-wise dot product of two matrices of the same dimension
You're taking the dot product of the first and second rows of ` a ` , and the dot product of the first and second rows of ` b ` , not the dot product of every i-th row of ` a ` and ` b ` .
as jorgeca said , the for indexing is wrong : in that code snippet you are doing : dot ( a [ 0 , :] , a [ 1 , :]) , dot ( b [ 0 , :] , b [ 1 , :]) , see #URL
You'll do better avoiding the ` append ` , but I can't think of a way to avoid the python loop .
However , if performance and not code style is your primary concern , you might still be better off with dot and the loop ( depending on your specific data and system environment ) .
In contrast to einsum , dot can take advantage of BLAS and will often multithread automatically .
I have to work with adjacency matrices of shape ( 10^6 , 10^6 ) and perform operations including addition , scaling and dot product .
please don't just dump code without some exclamation .
That returns the indices at which the elements first occurred in the input .
Then ` argsort ` those indices .
In the above case a line of zeros arises , as the matrix has only rank ` 3 ` .
Step 1 : Find ` np.argmax ( b , axis=1 )` indices .
Step 2 : Find ` b [ indices ] a [ indices ]`
" For the max value in a row in b , ` b > a == True ` .
You want to actually use the sorted indices on the original array , ` b [ ... ]`
You want the sorted indices , not the indices that sort the indices , so use ` sort ` not ` argsort ` .
In the example , the first item is Aug-09 , so that should come first in the unique list with order preserved
Use ` sort ` on the indices , not ` argsort ` .
Correctly indexing a multidimensional Numpy array with another array of indices
I'm trying to index a multidimensional array ` P ` with another array ` indices ` . which specifies which element along the last axis I want , as follows : #CODE
How can I correctly index ` P ` with ` indices ` to get an array of shape ` ( 20 , 10 , 2 )` ?
If that's not too clear : For any ` i ` and ` j ` ( in bounds ) I want ` my_output [ i , j , :] ` to be equal to ` P [ i , j , : , indices [ i , j ]]`
For any ` i ` and ` j ` I want ` my_output [ i , j , :] ` to be equal to ` P [ i , j , : , indices [ i , j ]]`
[ I'm assuming that your ` rand ` is the ` numpy.random ` module . ]
Finally , you can reshape : #CODE
Slice numpy matrix using vectors as indices
In this case is not possible to access the same section using the same vector ( Note that the indices in numpy are based on 0 ) .
My goal : find Q such that Q = inv ( chol ( S )) * X , where chol ( S ) is the lower cholesky factorization of S .
Why not just use the equation : ` Q = inv ( chol ( S )) * X ` , here is my test : #CODE
I don't know why ` scipy.linalg.solve_triangular ` is slower than ` numpy.linalg.solve ` on your system , but the ` inv ` version is the fastest .
" Several widely-used textbooks lead the reader to believe that solving a linear system of equations Ax = b by multiplying b by a computed inverse inv ( A ) is inaccurate .
It says that solving Ax =b as inv ( A ) *b is as stable & accurate as " better " methods ( LU , etc . ) if b isn't " bad " , i.e. , if it isn't ( nearly ) orthogonal to " small singular subspace " ( span of left singular vectors with small singular values ) .
And even if b * is * " bad " , it'll still be * accurate * ( inv ( A ) *b is as close to the real solution as A\b ) but not stable ( A* ( inv ( A ) *b ) will be much farther away from b than A* ( A\b )) .
SO , if you just want a solution , inv ( A ) *b will be accurate ( and usually stable ) .
These files have common indices with a different value for different methods , I am not sure how to phrase this well so here is a three dimensional example : #CODE
It assumes that your indices is known ( or computed
Although if you create the frames in such a way its a simple matter to loop through setting indices and combine_first .
As requested in a comment below , here is a histogram of some sample data from the sensor to illustrate the dynamic range :
Can you show the histogram of the data from the sensor ?
For example you can make the max value to white by ` img_16bit / ( img_16bit.max() / 255.0 )` .
You may also find the max & min value of sensor data and map them to 255 & 0 .
@USER , added histogram that shows I am using the full range of 16-bit ...
So , most of the data are in range 25000 - 45000 , maybe clip the data to this range and map it to 0 - 255 will get better result .
Looking at this a different way , I'm wondering if it even matters if I encode 16-bits worth of grayscale .
Also , for what it is worth , that histogram has 256 bins .
When map min max value to ( 0 , 255 ) , it looks like :
This also works if , instead of a single index , you provide an array of indices : #CODE
So if ` cells ` has the indices of several cells you want to find neighbors to : #CODE
Now , the numpy dot product : #CODE
What's the difference between dot ( a , a ) and dot ( b , a ) ?
Why dot ( b , b ) doesn't work ?
I also have some differencies with those dot products : #CODE
How to flatten a numpy slice ?
Does any one know how to convert a multidimensional slice to a list of indices ( or a uni-dimensional slice ) on the flattened array ?
But you can construct a list of flat indices from multidimensional ones , doing something like the following : #CODE
In a more general setting , once you have an array of indices for each dimension , you need to buld the cartesian product of all index arrays , so ` itertools.product ` is probably the way to go .
Very interesting , to generalize this , you need to know the shape of your matrix ( easy ) and how many steps of your stride will fit in your matrix ( for the repeat and tile functions ) .
And should extend them with tile or repeat ?
nice , the mask and sum idea is much better than my using ` np.histogram ` :)
I actually came up with it after reading your histogram code
Do you mean ignore them in the std calculation ?
So you can call ` std ` on this : #CODE
But I would recommend converting your image into a numpy array immediately , and doing a histogram along one axis , instead of using your version of counting .
So my question : is the ` a ` in line 8 the same object / data as before ( not counting the diagonal of course ) or is it a copy ?
I'm confident in your skills to translate this code to Python .
i.e. ` if ( * it ) { std :: cout << " if " << std :: endl ; std :: cout << it.pos() << std :: endl ; } else { std :: cout << " else " << std :: endl ; std :: cout << it.pos() << std :: endl ; } ` but for example I get the following in the output ` if [ 411 , 280 ] else [ 412 , 280 ]` .
I do not want to convert my data into single expanded list like ` d ` first and use histogram function .
( Python ) How do you get the mean and std of a column in a csr_matrix ?
Hi David , I should have edited my question , I'm looking for finding the mean and std of a vector rather , but in its sparse form , is there a command in scipy that gets you those two values ?
It's the sum of the 988 values , divided by 988 ?
You can use the sum function that I linked to in my first comment .
I just looked at [ the source code ] ( #URL ) , and ` np.delete ` does something similar to what you do behind the scenes , but instead of indexing with a boolean array , it creates an array of ` np.intp ` , and then does a ` np.setdiff1d ` with the element indices to delete .
How to compute " EMD " for 2 numpy arrays i.e " histogram " using opencv ?
In your particular case , to ensure that ` A * inv ( A )` is close enough to the identity matrix , you could compute a matrix norm of ` numpy.dot ( A , A_inv ) - E ` and ensure that it is small enough .
Note that the matrix norm you propose is similar to the RMS value I compute in my answer - the difference being that I then divide by the size of the matrix to get a sense of relative scale .
However I would suggest that rather than looking at the individual off-diagonal elements , you take their rms sum ; this reflects in some sense the " energy " that leaked into the off-diagonal terms as a result of imperfect calculations .
If you then divide this RMS number by the sum of the diagonal terms , you get a metric of just how well the inverse worked .
Please note that the histogram does not follow the Cartesian convention where x values are on the abcissa and y values on the ordinate axis .
You can take reshape out of example 1 , but reshape is really quite fast .
which takes the ` numpy.array ` objects ` x ` , ` t ` , and ` M ` ; and the scalar floats ` beta ` , ` gamma ` , and ` mu ` .
I get this error even when ` SIR ` just returns ` x ` , and if I strip all arguments apart from ` x ` and ` t ` from it : #CODE
` def SIR ( x , t , beta , gamma , mu ): `
` x = integrate.odeint ( SIR , x0 , t , args =( beta , gamma , mu )); `
Don't reshape it to be a 2-D array .
Another method would be to use ` needle ` as a kernel and find the max of the convolution .
By looking at the diff value , right ?
` np.max ( diff )` will give you the max value .
But for it to actually work , the big trick was to not cross correlate the images , but the images - 128 .
The residual value returned is the sum of the squares of the fit errors , not sure if this is what you are after : #CODE
In version 1.7 there is also a ` cov ` keyword that will return the covariance matrix for your coefficients , which you could use to calculate the uncertainty of the fit coefficients themselves .
Another thing you can do is look at those squared deviations as a function ( the sum of which is ` res `) .
Also , it supports elements but does not join them together in the end if each element has multiple features .
Calculating norm of columns as vectors in a matrix
I am looking for the best way of calculating the norm of columns as vectors in a matrix .
You can calculate the norm by using ufuncs : #CODE
I'd like to copy data from one 3D array to another 3D array at the indices where a condition is true for a different 2D array .
Your array indices there will evaluate to ` False ` or ` True ` , which are equivalent to 0 or 1 respectively , every time .
@USER The OP is using Numpy , so ` c == cond ` is still an [ array of booleans ] ( #URL ) .
Something about broadcast errors .
You could try ` np.copyto ( b , a , where=cond [ ..., None ])` which adds one empty axis to ` cond ` of size ` 1 ` , so then the third dimension will be broadcast to all values for the third dimensions of ` a ` and ` b `
Write simple functions -- ones that either always return an array , or always return a scalar .
extracting indices from sorted list
Is there a way to get the indices of the sorted list from the original list using numpy ?
The easiest way is to augment the array with the position indices and then sort the 2-D array .
Suppose I have a list of indices which are ` index= np.array ([ 4 , 2 , 3 , 1 ])` .
I get an IndexError : arrays used as indices must be of integer ( or boolean ) type .
I get an IndexError : arrays used as indices must be of integer ( or boolean ) type .
In other words , I'll use this plot to select the points I'm interested in , and then I'll need those indices to work for other arrays from the same table .
It might also be better to encode screens as a number , not as a one-hot-encoding of different ranges .
Here , ` ( signal % 3 == 0 ) | ( signal % 5 == 0 )` evaluates the criterion on every element of ` signal ` , and ` np.where() ` returns the indices where the criterion is true .
If you have to append to ` positions ` in a loop , here is one way to do it : #CODE
Subject to certain constraints , the smaller array is broadcast across the larger array so that they have compatible shapes .
Had not understood that concept yet , eventhough the error message already pointed me to it : " ValueError : operands could not be broadcast together with shapes ...
However , my question is for addition , where broadcast seems to work as pointed to in the accepted answer .
Now I would like to generate the grid of ` N*N*N ` dimension and rotate and translate the molecule on the grid .
Until a bit longer there are some differences for out of bound / negative or boolean indices though .
I've been searching for a way ( more efficient that just writing loops to traverse the matrix ) to create matrices from elements given in a wrapped diagonal order , and to extract values back out in this order .
If we breakdown how numpy finds diagonal indices we can rebuild it to get what you want .
I saw the plot and was wary of the max at 15 but that is what the professor wrote .
you will need to log out and back in #CODE
@USER these plots look very similar to me ... note your example has a linear axis and HYRY's answer is plotted on a log axis .
Round a Python list of numbers and maintain the sum
But , I need the overall sum to be maintained , i.e. the sum of the original array rounded to 2 decimal places must be equal to the sum of the rounded elements of the array .
I need an efficient way of amending my new rounded array such that the sum is also 187976.61 .
But below I provide some quite generic solution - you need to store , accumulate and use the sum of differences in rounding .
The first step is to calculate the error between the desired result and the actual sum : #CODE
If the chances are high that you should change only at a few ( or one ) of these indexes you could use a heap ( or a min / max if there's only one place to change ) .
` d ` is the numerical difference between the rounded sum and the sum of rounds , this tells us how many places should we change the rounding .
If ` d ` is ` 1 ` or ` -1 ` the best place can be found easily with ` min ` or ` max ` .
So why is there a ` max ` , if ` nlargest ` would do ?!
Because ` min ` and ` max ` are much more efficiently implemented than that .
Note that ` numpy.rollaxis ` brings the specified axis to the first dimension and then let's us iterate over arrays with the remaining dimensions , i.e. , if we want to shuffle along the first dimension ( columns ) , we need to roll the second dimension to the front , so that we apply the shuffling to views over the first dimension .
But we can broadcast our first column of ` a ` up to the correct shape by simply inserting a newaxis so that it becomes a 2-D array : #CODE
ValueError : operands could not be broadcast together with shapes ( 12 , 4 ) ( 12 )
At the moment I smoothen the signal with a hann window to get rid of eventual noise and to flatten the peaks .
I tried to stack the arrays and then trnaspose the result .
I think quick_hist is not histogramming dist correctly in log .
I don't think that your approach is very robust , because when you take the ` sqrt ` there are two solutions , one positive , one negative , and you are only considering the positive .
numpy : How to join arrays ?
I need to join an array ` a ` with an array ` b ` :
perhaps you could try to use numpy.concatenate() to join the arrays together , and then find the mininum and maximum of each row ... then create c as a matrix of the min and max of each row .
It has trouble , i.e. it doesn't work , with inputs like ` ranges = np.array ([[ 2 , 7 ] , [ 3 , 12 ] , [ 4 , 11 ]])` , not sure if there is a better way than what I came up with to get the ` max ` of the ends ...
Any solution that has ` sort ` in it is at least ` O ( n log n )` , but I believe it can be done in ` O ( n )` .
I also think that the best you can get is ` O ( n log * n )` but you'll need a fancy data-structure .
I found this ` roll ` function but it seems like it only does the opposite , which shifts the last ` n ` elements to the beginning .
Isn't ` roll ` exactly what you want ?
Why not just ` roll ` with a negative number ?
Can you tell me , why circshift ( eye ( m ) , 2 ) and numpy.roll ( numpy.eye ( m ) , -2 )) yield different result ?
Thus there is a sense in which the ECDF retains all possible information about a dataset ( since it must retain the entire dataset for calculations ) , whereas a histogram actually loses some information about the dataset by binning .
I need a scrollbar to roll under the elements , see that when create more than 30 elements from first window the screen dont fill in correct mode : #CODE
The way we use it in some goodness of fit tests is to stack the arrays , so they are defined on all points , points from both arrays .
@USER I never really use ` np.matrix ` , but from a quick test they broadcast just as 2D arrays .
For example , using the ` A ` and ` B ` from my answer , ` A*B ` gives an outer product ( resulting in a 3x3 matrix ) but ` B*A ` gives an inner product ( resulting in a scalar ) .
If you use ` np.asarray ( A ) * np.asarray ( B )` and vice versa , you get the outer product in both cases , which is the standard numpy broadcasting rule : ` ( 3 , 1 )` and ` ( 1 , 3 )` yields ` ( 3 , 3 )`
Multiplication of a vector and its dual is allowed in various ways ( i.e. - broadcasting is an outer product ) .
Maybe I'm wrong , but broadcasting is a useful programming tool and is only really mathematically defined when you'd like to perform an outer product .
If you are going to run ` min_coords ( array )` in every element of arrays ` a ` and ` c ` , you might consider to " stack " nine copies of the same array , each copy rolled by some offset , using ` numpy.dstack() ` and ` numpy.roll() ` .
Is it a Digital Elevation Model , and you are trying to obtain some sort of gradient or other terrain-oriented data ?
Also , I may have misunderstood , but didn't diegogb want to use the neighbor indices from ` a ` to get values from ` c ` to produce ` b ` ?
I am trying creating an NumPy array filled with an object , and I was wondering if there was a way I could broadcast to the entire array for each object to do something .
FYI , @USER the types of ' element-wise ' functions ( I used ` + ` as my example ) that broadcast over an array are called [ ` ufunc ` s ] ( #URL )
then to get the ` mxnx3 ` matrix using tile #CODE
AttributeError : ' numpy.ndarray ' object has no attribute ' append '` This error is showing up when trying to convert the list to NumPy array just after the declaration ` pv_za_temp = [ ]`
and want to add a ninth column of zeros you can do this : #CODE
This generates return concatenate (( arr , values ) , axis=axis )
` concatenate ` assumes ` axis = 0 `
` append ` flattens array
I add a new column with ones to a matrix array in this way : #CODE
Don't use that list comprehension , use [ ` np.ones `] ( #URL ) or [ ` np.ones_like `] ( #URL ): ` append ([ np.ones_like ( Z )] , Z.T , 0 ) .T `
specifically the problem is to generate a random 2-dimensional walk of 200 ( but for testing say 2 ) steps with a max distance , then 100 ( try just 2 ) of those walks per stage , and commencing each stage at the largest distance from the origin of the previous stage .
How to accumulate unique sum of columns across pandas index
Every page_id may appear in one or more dates ( not unique ) and is large in size ~1 million .
Here I use ` pandas.factorize() ` to convert the ` page_id ` to an array in range 0 and N . where N is the unique count of elements in ` page_id ` .
+1 , it would be really nice to see the performance diff ...
it is the same thing as problem a except at point A , after you get the list , do another for loop for each word in there . for python , there is a find function for strings to see if a substring exists in there if it does exists , then append it to the list element :
Then to get the indices of where each of the sub_lists were found #CODE
Here is the Recursion I have understood : #URL The problem is that each argument of the new array must be the sum of the two arguments beside that !
Then instead you should make ` i ` into a numpy array , and simply take its sum : #CODE
` pyplot.hist() ` documentation specifies that when setting a range for a histogram " lower and upper outliers are ignored " .
Is it possible to make the first and last bins of a histogram include all outliers without changing the width of the bin ?
While I guess a ` clip ` keyword to ` hist() ` would be nice , I think no-one has bothered to implement this because it's so basic .
Yes , @USER is right , clip is the best solution .
Looking at ` matplotlib.axes.Axes.hist ` and the direct use of ` numpy.histogram ` I'm fairly confident in saying that there is no smarter solution than using clip ( other than extending the bins that you histogram with ) .
The first two columns are indices .
I can divide my dataset into blocks via the indices , i.e. first block is 0 0 second block is 0 1 third block 0 2 then 1 0 , 1 1 , 1 2 and so on and so forth .
The numbers in the indices columns can vary
Also , unless you specifically want to roll your own PCA implementation , you can do this much more easily with numpy by using ` np.cov ` ( for covariance calculation ) and ` np.linalg.eig ` ( to compute the eigenvalues and eigenvectors of the covariance matrix ) .
You need to pass the absolute path of the file in to ` Image.open ` , unless your cwd is in the same place as the images .
This will work because ` filepath ` value will be an absolute path , `" C :\ \Users\\Karim\\Downloads\\att_faces\\New folder\\ 10.pgm "`
I got another error ` ValueError : need at least one array to concatenate `
You then use an outer loop that requires ` O ( len ( A ))` operations , making your original algorithm roughly ` O ( len ( A ) *len ( B ))` operations .
I make use of numpy's unique an in1d functions .
` B_unique_sorted ` contains the unique values in ` B ` sorted .
` B_idx ` holds for these values the indices into the original ` B ` .
Note : I need to look for ( unique vals from B ) in A because I need the output to be returned with respect to ` B_idx `
Note : I assume that ` A ` is already unique .
and their respective indices in the original ` B ` #CODE
Summing values of 2D array on indices
I need to extend this question , which sums values of an array based on indices from a second array .
Then ` A [ i ] = sum ` over ` C ` such that ` index ( B ) == i ` .
N } C [ j , k ]` such that ` C [ k ] == i ` , i.e. a rowsum conditional on the indices of B matching i .
Next let's break down the outer ` i ` loop .
Each column of ` A ` is the sum of a subset of the corresponding row of ` C ` .
We'll get around this by multiplying the ` B == i ` mask by the current row of ` C ` , resulting in zeros where ` B == i ` is ` False ` , and the value in the current row of ` C ` where it's true .
Then we can sum over each row to get the current column of ` A ` ( This will be broadcast to a column when it's assigned to ` A [: , j ]`) : #CODE
` dot ` will cast the boolean ` B == i ` array to the same dtype as ` C ` behind-the-scenes , so we don't need to worry about explicitly casting it to a different type .
After that , we're just performing matrix multiplication on the transpose of ` C ` ( a 5x5 array ) and the " mask " 0 and 1 array above , yielding a 2x5 array .
` dot ` will take advantage of any optimized BLAS libraries you have installed ( e.g. ` ATLAS ` , ` MKL `) , so it's very fast .
Well , you can compress the code using list comprehensions : #CODE
But this is helpful and I think I use the generator to create the parameters first and have just one loop instead of several nested ones .
An alternative approach is to use ` itertools ` , convert the resulting list to ` numpy array ` and reshape it #CODE
It might be that this gets more memory consuming than all the other approaches as the size increases , but it ' s only two lines and creates unique elements for each triplet instead of just creating multiple pointers to the same object .
Arrays should be constructed using ` array ` , ` zeros ` or ` empty ` ...
I have two arrays that are used as indices #CODE
I am trying to find a simple way to move through the list finding all the rows with common radii and angle , average the rssi and append the radius , angle and averaged rssi to a new list .
I think that they are all the ones that you can find in ` math ` .
How can I prevent the TypeError : list indices must be integers , not tuple when copying a python list to a numpy array ?
np.append needs the array as the first argument and the list you want to append as the second : #CODE
Returns : append : ndarray
Note that append does not occur in-place : a new array is allocated and filled .
So you have to assign the ` np.append ` result to an array ( could be ` mean_data ` itself , I think ) , and , since you don't want a flattened array , you must also specify the axis on which you want to append .
To answer your question , as others have said , you cannot access a nested list with two indices like you did .
( error : " Array must be square ") Do you have any idea how I can deal with det ?
I tried replacing ` b = np.zeros ([ d1 , d2*d3 ])` by ` b = np.zeros ([ d2*d3 , d1 ])` I got ` ValueError : could not broadcast input array from shape ( 2760 ) into shape ( 112 )`
You can simply transpose ` b ` #CODE
I got your point and I find it more logical , but when trying the code you've suggested to get rid of the second error I got another error : ` AttributeError : flatten `
again I got an error similar to the second error : ` ValueError : could not broadcast input array from shape ( 10304 ) into shape ( 2760 )`
no it doesn't work ` ValueError : could not broadcast input array from shape ( 10304 ) into shape ( 2760 )`
If a positive shift of a say 3 is given , all values in the array shift up and 3 new slices of zeros appear at the bottom of the array .
The opposite would occur if a negative shift is given , all slices shift down along the height axis and new slices of zeros appear at the top .
Second , while it is clear that you want zeros to fill in the shifted data , you don't specify what you want to do with data on the other side : should it disappear beyond the array's boundary and be lost ?
For the former , numpy has the ` roll ` function , which does similar to what you want , but instead of filling in with zeros , it copies the data from the other side of the array .
You can simply replace this with zeros afterwards : #CODE
If you don't want to lose data , then you are basically just adding slices of zeros at the top or bottom of the array .
If you wanted the zeros appended at the bottom , simply change the order of the parameters in the call to the stacking function .
I want to find the max value of a 3d array in python .
To have the max element a multi-dimensionnal array , you can use ` flatten() ` : ` maxval= pp.max ( pix.flatten() )`
You are using the builtin ` max ` function that does not understand multidimensional NumPy arrays .
Edit : removed the ` lambda ` , the default ` max ` should be OK .
How do I concatenate them easily ?
I haven't tried to use ` np.savetxt ` in the context below , perhaps it could be used so long as the file is opened in append mode , but here is the solution for what I was trying to do .
What I need to do in the code is loop through all the blocks in the dataset and return a scalar number for each block after some computation , then sum up all the scalars , and store it in a variable called ` cost ` .
" Returns the sorted unique elements of an array "
If you know that the indices do not have holes or have few holes , might be worth to remove the part where you define ` idx1s ` and ` idxs2 ` and change the for loop to #CODE
The ` aligned_W ` array is a ( probably repeating ) copy of the data of ` W ` for the corresponding indices , but there is no constraint for all of the data having to be fetched by the fancy indexing that I know of .
I am trying to generate a random array of 0s and 1s , and I am getting the error : shape mismatch : objects cannot be broadcast to a single shape .
When I run it by itself , it returns ` function rand at 0x1d00170 ` .
The reason it prints ` ` when you run this program is because your line ` print rand ` is printing out the function object .
Try : ` print rand ( 4 , 2 )` instead .
I'm looking for the modular inverse , so inv * det won't work ( I think , modular arithmetic is not one of my strong points either ) .
` map ` with ` max ` is cleaner IMO .
[ 0.2 , 0.4 , 0.8 ]]) the result is >>> map ( max , s )
Maybe instead of the second for loop just use the max function
Interesting , would you recommend this method over using ` map ` / ` max ` ?
Based on your link it seems that ` amax ` is the same as ` max ` .
axis=1 refers to working on rows in this 2d case ( axis=0 , in contrast , would be getting you the max in each column )
Increment given indices in a matrix
together they make indices that should be incremented : #CODE
The matrix will be small ( like , 5 5 ) , and the number of indices will be large ( somewhere near 10^3 or 10^5 ) .
The trick is to convert ` a ` and ` b ` into a single 1D array of flat indices .
If the shape of the array is more complicated , it might be easier to use ` np.ravel_multi_index() ` instead of computing flat indices by hand : #CODE
My guess is because somehow the convolve function does not see Y as a 1D array .
To convert it to a 1D array , slice it as ` Y [: , 0 ]` or reshape it with ` np.reshape ( a , len ( a ))` .
This doesn't answer your question about performance , but you really shouldn't " roll your own " when it comes to projections .
This creates the array of N random locations , and no i am trying to create a loop that will append a 1 to an empty list if x > 85 or y > 85 or x 15 or y 15 , then append a zero to the same empty list if x or y is anything else .
Then i would find the sum of the list , which would be my count of how many of the random location fall within the edges .
In Python numpy , there is an unwrap function that :
Unwrap radian phase p by changing absolute jumps greater than discont
I didn't realize it because I'm only interested in relative phase , and wasn't taking attention to the absolute phase .
This works because sin ( phases ) / cos ( phases ) == tan ( phases ) .
Furthermore , I don't have to think hard about whether python's ` % ` operator keeps the sign of the divisor or the dividend and how this affects the result , or whether I have to add and then subtract a constant before and after performing the mod operation in order to keep the resulting phase in the range I want .
Python ' AttributeError : ' function ' object has no attribute ' min ''
` min ` and ` max ` are not attributes of lists , they are their own functions .
You'll need to use min ( x ) .
then ` x ` is a function , and functions ( in general ) don't have ` min ` attributes , so you can't call ` some_function.min() ` .
Numpy max slow when applied to list of arrays
Is there any way to speed up the max function or is it possible to populate a numpy array efficiently with the results of my calculation such that max is fast ?
I'm not claiming to understand this though :P Maybe for 2d lists , ` stack ` is faster , for 1d , ` array ` is faster ; again due to possible shape mismatches ?
I would like to replace all empty strings like the ones in the second row above , with some default value like 0 .
This boolean array is then used to assign `' 0 '` only to the appropriate indices in the original ` t ` .
I have two numpy arrays ( A and B ) , and I want to get the indices of A where the elements of A are in B and also get the indices of A where the elements are not in B .
which takes advantage of the fact that ` A ` is in order , and gives me ` [ 1 , 3 , 5 ]` , the indices of the elements that are in ` A ` .
This is great , but how do I get ` D = [ 0 , 2 , 4 , 6 ]` , the indices of elements of ` A ` that are not in ` B ` ?
I was hoping that there was a function that would give me the indices directly , but this works just fine .
if the number of dimensions is less than four , call ` atleast_3d ` and append an extra dimension on the end , otherwise just return the array unchanged .
It took me a while to decode though .
For the record : ``` ( 1 , ) * ( 4 - arr.ndim )``` creates a tuple of ones .
Also , you can use numpy helpers like ` flat ` , ` flatten ` , ` nditer `
In which case you would get a unique cluster for each leaf node for the cluster that it is in .
I keep getting the error ` ValueError : operands could not be broadcast together with shapes ( 219812 , 2 ) ( 219812 )`
that returned the same error I am getting : ` ValueError : operands could not be broadcast together with shapes ( 2 ) ( 219812 )`
@USER Then try ` u1 , u2 = odeint ( deriv , uinit , time ) .T ` .
You could also unpack the array ( it works row wise , so you need to transpose ` u ` so that it has shape ( 2 , n )) , using ` u1 , u2 = u.T ` .
One thing I notice is that your ` deriv ` equation accepts a ` t ` argument and never uses it .
This also reduces the size of the difference array , reducing the amount of iterations required by your ` min ` call .
Unless the expected min actually is zero , this will be significantly slower than OP's existing solution .
` x ` must still be sorted before you can do this , since this is basically just non-numpy ` min ( ediff1d ( x ))`
The timing of ` unique ` was in hopes that it would be comparably fast to ` sort ` , and you could break out early without the calls to ` diff ` and ` min ` if the length of the unique array was shorter than the array itself ( as that would mean your answer was ` 0 `) .
But the overhead of ` unique ` is more than any gain to be made .
So it seems the only potential improvement I can offer is replacing ` ediff1d ` with ` diff ` : #CODE
I was expecting ` ediff1d ` to be faster than ` diff ` , since it is for 1d arrays , but apparently ` diff ` is faster .
Where the x , y are the indices of the corresponding z value .
The eigenvector corresponding to the greatest eigenvalue ( ` lambda = 1.50 `) is ` x =[ 0 , sqrt ( 2 ) / 2 , sqrt ( 2 ) / 2 ]` just as in the SVD and LTSQ .
sum{i} ( Ax + By + Cz + D ) -> min
sum{i } [ A^2 x^2 + B^2 y^2 + C^2 z^2 + D^2 + 2ABxy + 2ACxz + 2ADx + 2BCyz + 2BDy + 2CDz ] -> min
You could reshape the data to group it into groups of 10 , 50 , or 100 .
` arr.reshape ( -1 , 10 )` tells NumPy to reshape the array ` arr ` to have a shape with size 10 in the last axis .
Note that using ` reshape ` in this way requires that ` len ( intensities )` is evenly divisible by the size ( e.g. 10 , 50 , 100 ) that you want to group by .
You can calculate moving averages using ` convolve ` as mentioned on stackoverflow here .
This is nice , and note that you can use any window besides a flat one ( using a gaussian ( size ) for example instead of ones ( size )) .
If you can explain _why_ you added the flatten , transpose , etc ., it might be easier to figure out what you _should_ be doing .
` numpy.convolve ` effectively replaces this undefined space with zeros , which you can see if you set a second non-zero value : #CODE
If you think of convolution as mirroring one of the functions along the y-axis , then sliding it along the x axis and computing the integral of the product at each point , it is easy to see how , since outside of the area of definition numpy takes them as if padded with zeros , you are effectively setting an integration interval from 0 to t , since the first function is zero below zero , and the second is zero above t , since it originally was zero below zero , but has been mirrored and moved t to the right .
Update : Your functions ` alfa ` and ` gamma ` , defined as follows : #CODE
You're right that they look the same , and even behave the same in many circumstances , including the ` roll ` function , but be careful in some cases where ` ndim ` might matter ( for ` a.shape ` is ` ( n , )` , ` a.ndim ` is ` 1 ` ; but for shape ` ( n , 1 )` , ` a.ndim ` is 2 ) .
Yes , ` roll ` will be effectively be applied along the non-1 axis if there is only one , which is why my comment was nothing beyond pedantry :) .
But if your array is ` ( n , m )` ( or higher ) it will roll along all the axes ( the flattened array ) which might give unexpected results .
You can reshape ` weights ` to a dimention ( 3 , 1 ) array and then multiply it to ` values ` #CODE
This operation is called the outer product .
Handling of duplicate indices in NumPy assignments
I doubt that anything is guaranteed , but some experiments with exotically strided arrays point to a simple left-to-right loop over the array of indices .
I think it would be good to iterate all indices + the assignment array together using the newer iterator .
At this point , is it reasonable to assume that ` a.sum() ` is ` a `' s previous sum + ` x.sum() ` ?
In your case , when assigning to an array using duplicate indices , the result is intuitive : assignment to the same index takes place multiple times , thus only the last assignment " sticks " ( it overwrites previous ones ) .
you start working with duplicate indices
you stop paying attention to the crucial fact that your operations involve duplicate indices .
you start using the same indices in different contexts , e.g. as above
However , when the list of indices contains repetitions , the assignment is done several times , leaving behind the last value :
Even though 0 occurs twice in the list of indices , the 0th element is only incremented once .
@USER Can you at least post the matlab code for the ` alfa ` and ` gamma ` functions ?
I still think that the problem ( or at least one problem ) is in the definitions of ` alfa ` and ` gamma ` .
@USER are you referring to the alfa ( 2 , k ) and gamma ( 2 , k ) ?
cuz if that is the problem then i should do that in the definition of alfa and gamma itself .
I don't see anything with ` alfa ( 2 , k )` or ` gamma ( 2 , k )`
So , ` alfa ` calls ` gamma ` which calls ` alfa ` which calls ` gamma ` and so on forever .
Difference between nonzero ( a ) , where ( a ) and argwhere ( a ) .
In Numpy , ` nonzero ( a )` , ` where ( a )` and ` argwhere ( a )` , with ` a ` being a numpy array , all seem to return the non-zero indices of the array .
Why have a whole function that just transposes the output of ` nonzero ` ?
What about the difference between ` where ( a )` and ` nonzero ( a )` ?
` where ` gives the option to return from two different arrays , I think that's the main reason for it over simply ` nonzero `
I can't comment on the usefulness of having a separate convenience function that transposes the result of another , but I can comment on ` where ` vs ` nonzero ` .
In it's simplest use case , ` where ` is indeed the same as ` nonzero ` .
` where ` is different from ` nonzero ` in the case when you wish to pick elements of from array ` a ` if some condition is ` True ` and from array ` b ` when that condition is ` False ` .
Again , I can't explain why they added the ` nonzero ` functionality to ` where ` , but this at least explains how the two are different .
` nonzero ` and ` argwhere ` both give you information about where in the array the elements are ` True ` .
` where ` works the same as ` nonzero ` in the form you have posted , but it has a second form : #CODE
As far as having both ` nonzero ` and ` argnonzero ` , they're conceptually different .
` nonzero ` is structured to return an object which can be used for indexing .
However , as it is , it's not very nice conceptually to figure out which indices correspond to 0 elements .
` np.argnonzero ` is nice to get the indices which are not zero .
For example , if your array truly is mostly zeros as you've shown , and you want to check whether it has values that are nonzero , you might look at things like : #CODE
I'm trying to manipulate them to give me the sum of the entries of the solutions ` y1 , ..., y3 ` for the first block of equations , the sum of the entries of the solutions ` c1 , ..., c6 ` for the second block of equations , the sum of the entries of the solutions ` x1 , ..., x13 ` for the third block of equations , and the sum of the entries of the solutions ` z1 , ..., z100 ` for the fourth block of equations .
Why does a memmap need a filename when I try to reshape a numpy array ?
The stack trace there is a bit different , but the code ends up failing at exactly the same point with exactly the same exception : #CODE
So , its a lot of zeros or small numbers in a DFT matrix or small quantity of high frequency energies .
By default the max of your data gets mapped to 1.0 , the min to 0.0 .
A = zeros ( 10 , 10 ) %
Of course if you just want to flatten out a matrix or 2-D array of zeros it does not matter .
` numpy.ravel ` does flatten 2D array , but does not do it the same way matlab's ` (: )` does .
Knowing that A (: ) is equivalent to reshape ( A , [ numel ( A ) , 1 ]) , you can get the same behaviour in Python with : #CODE
I thought it would be as simple as finding the maximum in the cross-correlation function of ` f1 ` and ` f2 ` , and I broadly followed the advice here : How to correlate two time series with gaps and different time bases ?
So using the formula given here : #URL I can write a cross-correlation function that pads the data to add ones to the left and zeros to the right : #CODE
So it looks like Jamie was correct , the issue is in how numpy ` correlate ` does the padding of signals .
Unfortunately , a fast check at #URL doesn't bring up any specification about absolute value neede for the input signals .
What the maximum of the cross-correlation finds is the shift at which the sum of the products of your two signals is a maximum .
Since functions on matrices can be written as functions on the eigenvalues of a matrix , I see why the logarithm has a problem there , because log ( 0 ) is not defined .
I guess that you just need to make sure that your random Hermitian matrix has nonzero eigenvalues .
I can't compare the speed to your method since I don't know how you turn your ` ( 60 , 6 )` array into a ` ( 4 , 4 )` , but this works to take the dot of a sequence : #CODE
byloop does 3 , byreduce 2 dot products .
You now have log ( n ) Python calls instead of n , good for a 2.5x speed-up , which will get close to 10x for n = 1024 .
It might be doable to wrap them with f2py or cython , or --- it might be easier to roll your own , in cython or fortran / f2py .
It looks like there might be an error in specifying the denominator of your second ` sqrt ` parameter .
However , if ` x 1 ` or ` x -1 ` , both of the ` sqrt() ` functions will be passed a negative argument , which causes the error ` invalid value encountered in sqrt ` .
Because ` sqrt ` returns NaN for nagative argument , you function f ( x ) is not calculatable for all real x .
I change your function to use ` numpy.emath.sqrt() ` which can output complex values when the argument 0 , and returns the absolute value of the expression .
% 4.2f makes some numbers join .
I'm not talking about ceil or floor , I just need different tie breaking .
However , as I've transitioned to noisy data , the std is no longer sufficiently well behaved as to be useful .
` a // b ` is floor division .
It's basically ` floor ( a / b )` , but it preserves the number type .
It is the explicit floor division operator .
In Python 3.x this was changed , and the ` / ` operator does floating-point division and the ` // ` operator does floor division .
Python - ValueError : operands could not be broadcast together with shapes
ValueError : operands could not be broadcast together with shapes ( 90 , 90 ) ( 8100 )`
You should either flatten the input image , too , or not flatten the original images ( I don't quite understand why you do it ) , or resize ` mean_image ` to 90x90 just for this operation .
Most methods , like ` mean ` , ` cov ` , etc accept the ` axis ` argument , and you can list all the dimensions to perform it on without having to flatten .
As a final comment , if speed is an issue , it would be faster to use np.dstack to join your images : #CODE
where 675,103 9 are the taxi ids .
Most of the documentation I found is relative to functions like correlate and convolve .
However , for a given random variable x these functions just seem to calculate the sum #CODE
is the sum ( integral ) .
This is because they are defined in terms of the mathematical convolution operation , which is simply the integral that you've written as a sum above .
ValueError : operands could not be broadcast together with shapes ( 90 , 90 ) ( 8100 )`
File " C :\ Python27\lib\ site-packages \numpy\linalg\ linalg.py " , line 1016 , in eig
eigenvalues_in , eigenvectors_in = linalg.eig ( cov )
File " C :\ Python27\lib\ site-packages \numpy\linalg\ linalg.py " , line 1016 , in eig
I have finally looked at the tutorial you supplied , and it seems that the author suggests you flatten the images .
So we reshape ` c ` to be 2d : #CODE
I'm trying to calculate the ratio of two log values ` a ` and ` b ` and then convert it back to non-log values .
Since these are log values , the ratio ` c ` is : #CODE
since these are log values , the ratio is : #CODE
I do all the arithmetic in log values , so ` a ` and ` b ` are just the result of arithmetic on log values .
I just take ` log ( ... )` of things and add them / subtract them to end up with two values whose ratio I want to compare
@USER you can convert cutoff value into log and compare it with other logs anyway
` val == exp ( c ) == 4.4291364817936896e-1179 ` .
The problem is clearly that exp ( - 2713.259620000026 ) not represented well by a float .
But since you know the log of the number , you could do something like : #CODE
This makes the ` cov ( X , Y ) = 0.2 ` , and the variances , ` var ( X )` and ` var ( Y )` both equal to 1 .
Then we plot a histogram of the correlation coefficients : #CODE
Ambiguous Error using clip , where to replace negative numbers with zero
I want to replace negative values in a multidimensional array with zeros .
Of course in this case you could just loop over the outer array and call clip on each of the inner arrays .
It appears that most of the work goes into finding the nonzero elements and retrieving them with indices ( the call to ` nonzero ` and subsequent use of its index . ) Can this be optimized or is this the best that can be done ?
The nonzero is a good try ( not sure if it helps much or at all though ) .
Furthermore , using nonzero adds an extra step .
I want to select and append all the values that are marked by * .
You cannot apply C functions such as log on numpy arrays , and numpy does not have a C function library that you can call from cython .
Unless you have a very unique use case , you're not going to see much benefit from reimplementing numpy functions as C functions .
The reason I started delving into this is that I encountered a non-intuitive behavior when working with duplicate indices : #CODE
More interesting stuff about duplicate indices in this question .
If you assign values to an array as an in-place operation in which all the indices are the same , the last one is going to clobber the rest .
If internally you're actually writing back to memory the result of the operation , you'd expect so see the sum of all the values in ` x ` . hmmm ...
There is no " set / getitem in-place " in python , these things are equivalent to ` a [ indices ] = a [ indices ] + x ` .
( EDIT : As lvc writes , actually the right hand side is in place , so that it is ` a [ indices ] = ( a [ indices ] += x )` if that was legal syntax , that has largly the same effect though )
However , simple conversion will truncate the string at length 4 or 1 ( why ? ) , e.g. , #CODE
Of course , one can always iterate over the entire array explicitly to determine the max length , #CODE
Not a solution , but ` max ( len ( x ) for x in a )` is probably faster than constructing a list and calling ` np.max ` .
I edited the question just before your comment :D ` max ( a , key=len )` is even faster .
So ` a.data ` has the non-zero entries , in row major order , ` a.indices ` has the corresponding column indices of the nono-zero entries , and ` a.indptr ` has the starting indices into the other two arrays where data for every row starts , e.g. ` a.indptr [ 3 ] = 4 ` and ` a.indptr [ 3+1 ] = 5 ` , so non-zero entries in the fourth row are ` a.data [ 4:5 ]` , and their column indices ` a.indices [ 4:5 ]` .
3 corresponding lists R , P and Z which contain the radius , the angle and the linear value for field strength at each unique point .
( ` dot() ` would sum along the wrong axis . )
More generally , if you want to scale the rows of a sparse matrix with a vector ` x ` , you could multiply ` b ` on the left with a sparse matrix containing ` 1.0 / x ` on the diagonal .
= N ` as long as the diagonal matrix for ` x ` has shape ` ( M , M )` .
Note also that I misstated the question somewhat , and I also need to apply ` xlogx() ` to ` b ` before summing along the axis ( 0 log ( 0 ) is defined to equal 0 ) , so I will need to operate on b.data anyway !
I am doing gradient descent ( 100 iterations to be precise ) .
In the above we can see for the same vector we have different norm ?
Also using dot ( x , x ) instead of an l2 norm can be much more accurate since it avoids the square root .
Your exact error is caused by machine errors but since your vectors are not actually equal ( you are showing two logically equivalent vectors but their internal representation will be different ) the calculation of the norm is probably being processed with different precision numbers .
Or you can unpack ` u ` from the start , and ignore the values for ` x2 ` , ` y2 ` , and ` z2 ` ( you must transpose the output first with ` .T `) #CODE
At the first glance I thought this function would append to the original array , but it creates a new instance instead .
I now need to reshape this array : #CODE
it takes about 18 seconds to read 1.25GB of data ! and 12-14 seconds to reshape !
I have been trying to find the min of this function but i cant seem to get the proper syntax .
What happens if you try ` y = lambda x : sum ( abs ( y2+x ) -y1a ` ?
y = lambda x , y2 , y1a : sum ( abs (( y2 ) +x ) -y1a )
numpy cov ( covariance ) function , what exactly does it compute ?
I.e sum of outer products .
[ 1 ] which in this case is effectively ( but not exactly ) the outer product because ` ( x-m )` has ` N ` column vectors of length ` M ` and thus ` ( x-m ) .T ` is as many row vectors .
The end result is the sum of all the outer products .
But , technically these are both just standard matrix multiplications and the true outer product is only the product of a column vector onto a row vector .
Right , and this is equivalent to my sum of outer products ?
It turns out the type of ` mat [ ' ProteinComplex ']` is a scipy multidimensional array ( ndarray ) .
Convert a std :: vector to a NumPy array without copying data
I have a C++ library which currently has some methods inside which return a ` std :: vector ` defined like #CODE
numpy.array ' s underlying data structure is just a C-style array as is a C++ std :: vector so I would hope that it is feasible to have then access the same data in memory .
As @USER notes , your function definitions are inside of a loop , define them before you run the loop , but also give them unique names .
With a unique name , you will still be able to use the ordinary ` plot ` function if you wish , as well as you'd have caught the scoping error ( that it's defined in the loop ) because the unique name ( ` my_plot ` , e.g . ) would have been undefined .
You have your code and it should be a loop that will grab things from the data to generate your examples and then it generates the example . create an array outside the loop and append your vector into the array for storage !
Bug or meant to be : numpy raises " ValueError : too many boolean indices " for repeated boolean indices
The numpy documentation ( #URL ) says about boolean arrays as indices :
Ha , it seems " too many boolean indices " doesn't refer to the number of times you tried to access elements with boolean indexing , but rather the length of the boolean index array .
It's saying that your array ( ` t < 5 `) is an array with too many " boolean indices " to index the other array ( ` t [ t < 8 ]`) because the other array is too small .
Yes , I guess I just didn't think of that ` t < 8 ` is just a boolean array , since I only use boolean arrays as indices .
numpy fromfile ( count = -1 ) returns array of zeros on Mac OS for huge filesize
this gives me an array of zeros .
So it makes an array of the expected size , but its full of zeros .
So , how to call sum function in numpy module from C-code , for example ?
Calculating gradient with NumPy
I really can not understand what numpy.gradient function does and how to use it for computation of multivariable function gradient .
I need to compute it's 3-dim gradient ( in other words , I want to compute partial derivatives with respect to all variables ( q , chi , delta )) .
How can I calculate this gradient using NumPy ?
Since you want to calculate the gradient of an analytical function , you have to use the Sympy package which supports symbolic mathematics .
Also ` theano ` can compute the gradient automatically
However I have a matrix that I know to be diagonal beforehand .
Do these scipy functions check if the matrix is diagonal before they run ?
Obviously the exponentiation algorithm can be much faster for a diagonal matrix , and I just want to make sure that these are doing something smart with that - if they aren't , is there an easy way to do it ?
I think ` scipy.sparse ` has some special support for diagonal matrices ( stored as a 1-d array holding just the diagonal ) .
If a matrix is diagonal , then its exponential can be obtained by just exponentiating every entry on the main diagonal , so you can calculate it by : #CODE
It happens to be the same on the diagonal for diagonal matrices .
If you know A is diagonal and you want the k-th power : #CODE
Check if a matrix is diagonal : #CODE
[ I don't know if there's a more efficient way inbuilt to test whether a matrix is diagonal , but it wouldn't surprise me . ]
HYRY's method is quadratic w.r.t the diagonal length ( probably because of the new array memory allocation ) , and so if your matrices are of little dimension , the difference might not be as big .
But this is linear w.r.t to array size , so quadratic w.r.t to diagonal length .
You have to reshape your data to a 2D grid , see my edit on how to go about that .
How can I access the elements of the array initially read from ` loadmat ` without producing a 0-d array , and preferably with more elegance than using multiple ` [ 0 ]` indices ?
In fact , then I know how many values are stored in the same grid cell and what the sum is of them .
Just to clarify to future readers , the seemingly simple solution which is stated not to work , indeed does not work , because ` rows , cols ` contains duplicate indices .
@USER ah ok , I assumed unique indices .
I will keep this answer in case it is helpful for others who might have unique indices .
Sorry , thx for fixing the format and I added full stack trace .
I have a large matrix , I'd like to check that it has a column of all zeros somewhere in it .
If there is a column of zeros , do you need the column index , or do you just need " yes or no " ?
A bad way :: Try to invert it :-P ( if it has a column of all zeros , it's singular )
Create an ` equals 0 ` mask ( ` mat == 0 `) , and run ` all ` on it along an axis .
Warren Weckesser's solution is likely to be faster , because ` any() ` probably stops checking non-zero lines very early , whereas ` mat == 0 ` performs the comparison on every single element of the array , which is slower .
@USER yes i agree . also ` mat == 0 ` creates a temporary bool array with the same shape as ` mat ` , whereas Warren's ` mat.any ( axis=0 )` only creates a temporary 1d array of size ` mat.shape [ 1 ]` , which is another reason for my solution to be slower ( but perhaps a bit more readable for some ) .
You can use nonzero function .
it returns the nonzero indices of the given input .
to see the indices more cleaner , use ` transpose ` method : #CODE
So I'm getting : RuntimeWarning : invalid value encountered in sqrt
vector length is the square root of its dot product . you can modify your normalize function by changing the second line to ` row_sums = ( data*data ) .sum ( axis=1 )`
However , to scale every vector in your dataset to unit norm use : ` norm_data =d ata / np.sqrt ( np.sum ( data*data , 1 )) [: , None ]` .
You need to divide by the L2 norm of each vector , which means squaring the value of each element , then taking the square root of the sum .
I am looking for a way to concatenate the values in two python dictionaries that contain numpy arrays whilst avoiding having to manually loop over the dictionary keys .
NumPy : How to collapse N-dimensional array along a single dimension using argmin / max output ?
an N-D array using the ` argmin ` or ` argmax ` indices for that dimension ?
The most straight-forward solution I could think of was to use logical indexing to zero out the entries that are not selected by the desired index , and then to sum over the dimension of interest , e.g. as follows : #CODE
and , trivially , get the ` max ` across any given dimension using the corresponding ` argmax ` : #CODE
Some elements ( but not very few ! ) have extremely large absolute values compared to the others ( by a factor 1000 or larger ) .
I have already tried to use ` plt.axis ([ mean ( xvals ) - k * std ( xvals ) , mean ( xvals ) + k * std ( xvals ) , mean ( yvals ) - k * std ( yvals ) , mean ( yvals ) + k * std ( yvals )])` , trying out different values of ` k ` , but none seems to work for all my plots .
where I've assumed that the image is of type ` np.uint8 ` ( has a max of ` 255 `) .
i.e if entry occurs 1st time i need to append 1 if it occurs 2nd time i need to append 2 and likewise i mean i need to count no of occurences of an email address in the file and if an email exists twice or more i want difference among dates and remember dates are not sorted so we have to sort them also against a particular email address and i am looking for a solution in python using numpy or pandas library or any other library that can handle this type of huge data without giving out of bound memory exception i have dual core processor with centos 6.3 and having ram of 4GB
The combiner function does not reduce , but instead calculates your function ( the diff in days ) between all elements in that chunk , eliminating duplicates as you go , and taking the latest data after each loop .
My first thought was also to put data in database but it wont do the trick i need to track 1st occurence of each email address also and count too and what about getting the diff of dates whats the soln for that ??
My file has more than 20million rows and unique values are turned out to be more than 5million if i start comparing those i will got
so the first time an email appears its email is then the reference date for subsequent appearances ? for the 2nd email it's easy , count is 1 and days is diff of days , what about 3rd email . does days get updated to be the diff between 3rd date and 1st date or is the number of days somehow involved ( maybe the 3rd days is max of current and 3rd date - reference date ? )
I am not sure about memory but it realy took very less time around 10-12 min to give the output
I'm calculating the dot product between a scipy.sparse matrix ( CSC ) and a numpy ndarray vector : #CODE
You cannot use numpy's dot with scipy.sparse , quite simply .
Going up the stack , I get to sysconfig.py , line 435 , which seems to generate the path name by calling get_makefile_filename , which is in sysconfig.py , line 251 .
calling dot products and linear algebra operations in Cython ?
I'm trying to use dot products , matrix inversion and other basic linear algebra operations that are available in numpy from Cython .
Functions like ` numpy.linalg.inv ` ( inversion ) , ` numpy.dot ` ( dot product ) , ` X.t ` ( transpose of matrix / array ) .
To give an example , imagine you have a function in Cython that does many things and in the end needs to make a computation involving dot products and matrix inverses : #CODE
dot product ( ` np.dot `)
taking transpose ( equivalent of ` x.T ` in numpy )
The example you linked to is the only one I found and it's too sparse for me to figure out how to call things whatever ` dot ` is calling from ` linalg ` etc .
If you use Cython's memoryview syntax ( ` double [: : 1 , :] `) you transpose is the same ` x.T ` as usual .
Alternatively , you can compute the transpose by writing a function of your own that swaps elements of the array across the diagonal .
So is Scipy ( via f2py ); nicer functions ( such as ` inv ( x )`) are written in Python .
Note that you don't actually need to set order= ' F ' in Python - you can use a C-ordered array , set the dimension to shape [ 1 ] , shape [ 0 ] in the call to dgemm and set the transpose parameter to ' t ' .
getting the opposite diagonal of a numpy array
So in numpy arrays there is the built in function for getting the diagonal indices , but I can't seem to figure out how to get the diagonal starting from the top right rather than top left .
@USER you will always loose with those timings , because diagonal creates a view ( or will in newer versions ) .
Unfortunately , this operation is running in a tight loop , so any amount of performance I can squeeze out would be highly beneficial .
For each coefficient i have to generate a 4-D tensor [ nalpha X nmach X nbeta X nalt ] of zeros ( for preallocation purposes ) , so I do : #CODE
If you are counting on having to resize your array , there is very likely not going to be much to be gained by preallocating it .
It will probably be simpler to store your arrays in a list , then figure out the size of the array to hold them all , and dump the data into it : #CODE
Is there a easy way to store the indices of the values for later use without creating a array and append the tuple of the indices in every loop ?
Here my classical looping approach to store the indices : #CODE
later use the indices : #CODE
But how do I get the indices from this boolean Matrix because without the indices I can't access the TT array ?
My diagonal is already zero .
The question I really wanted an answer to was " how do I get the ` x ` , ` y ` indices of an array ?
Do you even need the indices ?
If you only care about the elements , and not the indices , the following can be used : #CODE
However , if you want to some funky things with indices , this method cannot be used ( use ndenumerate -- your linked SO answer -- for that ) .
The single object in the tuple is the numpy array of indices .
A function for returning a list of tuples that correspond to indices of ALL elements of an ndarray sorted ?
I'm aware of ` numpy.argsort() ` , but what it does is return indices of elements in an array that would be sorted along a certain axis .
Instead , you want to flatten your array into a 1-dimensional array of values , then argsort that .
No need for the list comprehension , ` np.unravel_index ` will take an array of indices as first argument , somehting like ` np.vstack ( np.unravel_index ( np.argsort ( A , axis=None ) [: : -1 ] , A.shape )) .T ` is more _numpythonic_ .
Extract and set thick diagonal of numpy array
How can I extract and consequently set a diagonal having a " thickness " equal to ` width ` with a constant value ?
I know the ` fill_diagonal ` function which fills the main diagonal with the given value .
Similar to that I want to fill the main diagonal AND its surrounding diagonals .
See banded diagonal matrix .
By changing the sign of one of the ` arange ` s ( and using ` np.abs `) , you can measure the distance from the diagonal : #CODE
So you can " select " all elements that are a certain distance from the diagonal by writing a simple inequality : #CODE
It says " too many indices " .
I noticed that it's quite different from the constant you get from Python's max int : #CODE
Have you just tried taking the max and min of a few thousand samples , and seeing which scaling factor gets near ` 1.0 ` ?
The C standard says ` rand ` returns an ` int ` in the range 0 to RAND_MAX inclusive , so dividing it by RAND_MAX ( from ` stdlib.h `) is the proper way to normalise it .
Because ` rand ` has been part of ISO C since C89 , it's guaranteed to be available everywhere , but no guarantees are made regarding the quality of its random numbers .
Python's ` sys.maxint ` is a different concept entirely ; it's just the largest positive number Python can represent in its own int type ; larger ones will have to be longs .
overhead is a major issue since this is called many times in a loop that needs to generate random numbers ( in order to sample from multinomial in part ) .
` B [: , None ] -B ` automatically broadcast the result to a 4x4 matrix that you can simply multiply by ` A `
Made the " random " call local to the function , i.e. do ` rand = np.random.rand ` and later ` u = rand() ` , because in Python lookups in the local namespace are faster , which can be significant in tight loops such as this .
And then to find the min and that specific distance : #CODE
Then we could just ask for the min of an array to be returned .
`` argmin `` asks for the position of the min ( The operations are all element-wise ) .
If you mean you want to sum your result array you can just use ` np.sum ( result_array )` .
The output should be correct ( Matrix times vector should be a vector full of zeros minus one devided by ten , voila ) .
I'm quite not sure about the general form of an NxM matrix and the usage of transpose ( that would need another minute to think about ;-) )
` numpy.zeros ` defines a matrix filled with zeros .
` numpy.ones ` defines a matrix filled with ones .
But I suppose that ` matrix ` should be something else than a matrix of zeros , or the transpose operation isn't needed .
clearly the not working way would be easier for dealing with data because I can append the rows to ` array ` and the other way must be done manually .
Incorrect sum of when averaging multiple numpy arrays
I would expect that for the first file , the sum of the first array and the running sum should be the same .
When masked arrays are added together , so are their masks , which will give a different sum .
For arrays , ` a = [ 1 , 2 , 3 ]; b = [ 1 ]` , then ` sum ( a ) + sum ( b )` is not the same as ` sum ( a+b )` because ` b ` gets counted three times in the latter case .
In numpy , you'll need to explicitly use ` eigh ` instead of ` eig ` to get the same speed with a symmetric matrix .
In MATLAB : ` tic , fft2 ( rand ( 1024 )); toc ` is 25 ms but in Python the same thing is 125 ms .
Am I again , like with ` eig ` , missing something basic about how Python is doing the 2D FFT ?
How can I use numpy array indexing to select 2 columns out of a 2D array to select unique values from ?
I want to identify unique elements in the 1st or 2nd column but not in the rest of the columns .
Using ` np.unique ` will get unique values in the array and I can index a single column like so #CODE
How can I index it so that I can find all the unique values in [; , 0 ] and in [: , 1 ] ?
There is no guarantee of a unique solution here - consider ` [[ A , B , 90 ] , [ C , D , 90 ]]` .
Begin with a random node with the absolute angle set to zero .
When you traverse an edge in the downward direction , add the angle associated with that edge to the current absolute angle .
If the graph is connected , then this procedure should associate an absolute angle to each node .
Otherwise , you will have to attempt to restart the DFS at each node in the graph to obtain disconnected sets of absolute angles .
If some absolute angles are negative , just subtract the minimum angle from the entire list of absolute angles .
We can encode our graph as follows : #CODE
Get a list of unique nodes then for each node iterate through the list of angles and find where it occurs ?
I'd like to add two numpy arrays of different shapes , but without broadcasting , rather the " missing " values are treated as zeros .
And yes , you can eliminate that , by just calling the ` resize ` method ( or the ` resize ` function , if you want to make copies instead of changing them in-place ) .
Enlarging an array : missing entries are filled with zeros
Silly question - but is there a numpy function that returns the ' shape ' of the convolve function when it is working in ' valid ' mode .
Basically I have an issue working out which ' x ' values match which y ones - and comparing it with the original data ?
The docstring for ` convolve ` explains the length of the result for the " valid " mode .
I guess I don't want to have to delve into what convolve does if i can help it - time pressures !.
Argmax tells you where max came from .
I have problem during low-pass interpolation , I have to fill 1D array with zeros before processing it .
I want to have array like this [ 1 0 2 0 3 0 4 0 5 0 6 ] so it is L-1 zeros in array where L is the number of all values inside array before zero stuffing .
You can assign an unpadded list of values into a slice of another list of zeros : #CODE
This can translate to ` numpy ` too , though as Jaime pointed out in a comment , it's even easier to use ` numpy.insert ` : #CODE
( My current sigmoid function is tanh if it matters )
The problem is with the implementation of mean and sum functions .
To overcome the problem you need to divide the array and partialy compute the sum : #CODE
In [ this article ] ( #URL ) , in page 2 , right after formula ( 1.6 ) they describe what they call the _pairwise summation algorithm_ , similar what your solution presents , and describe how it reduces rounding error from O ( N ) to O ( log N ) .
As the sum of your values grows , you start losing the accuracy of smaller digits .
I need to identify the indices of these smallest real values , not the values themselves .
Also it doesn't give me the indices that I am looking for .
The only values that should be throwing this out are the negative infinite ones .
Plot contours of the absolute value of the phase ( going from -180 ? to 180 ? ) so that there is no discontinuity .
Here is the complete code to append to your example : #CODE
You're getting a 1D array , which let's you forget whether you need column or row vectors , because it just works ( in dot products , setting rows or columns of a 2D array ... ) .
-1 is the default size so it will work with any size list ... other wise you would need to put the size there ( 3 in this case ... ) the argument to reshape is a ( width , height ) tuple ...
numpy divide row by row sum
How can I divide a numpy array row by the sum of all values in this row ?
Use the ` keepdims ` argument on ` sum ` to preserve the dimension : #CODE
Without the ` axis ` argument , ` sum() ` returns the sum of all the values in the array .
Returns array of indices of the maximum values along the given axis .
slice K in columns , apply f and , join them once again , thus obtaining f ( K ) somehow .
` view ` and ` reshape ` create flat structured views so that each row appears as a single element to ` in1d ` .
If I've understood the structure of your data correctly , I think you can solve this with a dictionary comprehension that calls the builtin ` min ` function and gives it a ` key ` function .
The ` values ` list is next innermost layer , which is iterated over implicitly by ` min ` .
` results = dict (( key , min ( values , key=lambda x : x [ -1 ] .f() )) for key , values in myD.iteritems() )`
While it still won't work with this , the simplest way I found to join many fields in a single dtype is ` dtype (( np.void , a.dtype.itemsize * a.shape [ 1 ]))` .
the numpy module can actually broadcast through your array and tell what parts are the same as the other and return true if they are and false if they are not : #CODE
So you can transpose ( flip x and y ) the array and then compare the two rows with an ` ` gate .
Use commas instead of separate indices in brackets : #CODE
What you're doing instead is failing because a [ 1:2 ] still returns a list of lists , so your next index is an index on the outer list ( which only has one element ) , not the inner list that you want : #CODE
( You wouldn't have this problem if you were using simple indices instead of slices , but you should still use the comma syntax because it's much clearer .
The first value of the slice is the index of the first value of the array that you want -- and the indices start at 0 .
@USER I have a generalized version of the stack as another answer now .
The reason for this is because ` np.concatenate ` will then stack along the new dimension , which is ` 0 ` since the ` None ` is at the front .
where the new axis is now added at the end , so we must stack and maximize along that last axis , which will be ` a.ndim ` .
Which is equivalent to the ` dstack ` I mentioned in my comment above ( ` dstack ` adds a third axis to stack along if it doesn't exist in the arrays ) .
@USER : look at the stack trace .
well , F_cont has three arrays and I want to pick the values from each array giving the same indices of I_data for each array .
So only the first element of one dimension gets used and the reshape command has no effect .
` reshape() ` doesn't reshape in place , you need to assign the result : #CODE
create a multichannel zeros mat in python with cv2
i want to create a multichannel mat object in python with cv2 opencv wrapper .
i've found examples on the net where the c++ Mat :: zeros is replaced with numpy.zeros , that seems good . but no multichannel type seems to fit ..
It's also possible to patch ` MaskedArray.__float__ ` so that it raises an exception , this way you would see stack trace , which would include your code .
Subsample 1-D array using 2-D indices in numpy
The data I'm using is being extracted from a ` netCDF4 ` object , which creates a numpy masked array at initialization , but does not appear to support the numpy ` reshape() ` method , making it only possible to reshape after all the data has been copied = way too slow .
I just tried Jaime's brilliant answer , but it appears that ` netCDF4 ` won't allow for 2-D indices .
After trying this ( awesome solution ) , It appears ` netCDF4 ` doesn't like 2-D indices .
One should be clear whether there could be no solution ( since eg the argmax answer will not work in that case ( max of ( 0 , 0 , 0 , 0 ) = 0 ) as ambrus commented
Since ` argmax ` will stop at the first ` True ` ( " In case of multiple occurrences of the maximum values , the indices corresponding to the first occurrence are returned . ") and doesn't save another list .
handling zeros in pandas DataFrames column divisions in Python
What I need to get outer product : #CODE
The motive is that I need to get sum of element-wise multiplied two arrays : #CODE
I think the OP already knows that ` outer ` or ` tensordot ` can easily handle the length 2 case , and was looking for a cleaner way to handle the case where ` l ` has arbitrary length .
Size your array with room to spare , and then concatenate in large chunks if needed .
Since ndarray can't dynamic change it's size , you need to copy all the data when you want to append some new data .
You can create a class that reduce the resize frequency : #CODE
Not necessarily : in the code above we don't have the definitions of the functions ` lil_matrix ` and ` zeros ` .
On the other hand ` b ` is equal to the value returned by the function ` zeros ` , when you pass to it ` n*m ` .
For each of the cycles described above , if we have a boundary value , i.e. either ` i ` or ` j ` are at the first or last iteration of the cycle ( e.g. either ` i ` or ` j ` are ` = 0 ` , or ` i= n-1 ` or ` j= m-1 `) then the values of ` A ` and ` b ` are not any more the ones described at the beginning of this answer , but are changed according to the two lines after the line starting with ` if ` .
Other than ` lil_matrix ` and ` zeros ` , the other function which you need to investigate , because its definition is not included in this code , is ` linspace ` : this is used at the beginning to create the variables ` x ` and ` y ` which then are used in the ` if ` block .
I'm trying to create a 2d histogram where each bin of the histogram is coloured based on the percentage of successes in that bin ( i.e. # of successes in bin divided by total points in bin ) .
This could leave ` nan ` in the final histogram , so you might want to make a decision for those cases .
In the past I've found ` np.count_nonzero ` to be much faster than the ` sum ` trick , but here -- probably because of the need to use ` np.appyly_along_axis ` -- that version is instead much slower , at least for this ` a ` .
Macports was causing me one of the worst experiences I had while installing the scientific Python stack on Mac .
I'll be interested to see if there's a slick solution on the science stack ( numpy / scipy / sympy / mpmath / pandas etc . ) , though .
@USER : the det in Numpy / Scipy is floating point only .
Ultimately I wonder which part of the science stack * should * have this in it ?
If it's 1 ( mod 2 ) , the matrix is invertible .
To actually find the inverse , you can just take the normal inverse over the integers , multiply by the determinant ( so that you don't have fractions ) , and mod each element by 2 .
I should point out that in general cyclic finite fields , the " multiply by the determinant " part will need to be undone by multiplying by the inverse mod p ( it is unnecessary mod 2 because the only possibility is 1 ) .
As a general rule , indices for a multidimensional array should be in a tuple .
To index the first dimension with a 1 and a 3 , and then the second with a 1 and a 3 also , you could transpose your array : #CODE
You are computing the outer product of two vectors .
You should ` append ` ( i , j ) rather than overwriting previous ones
You need to reshape with .reshape() or add an index by reindex by using np.newaxis .
By using one of the alternate DataFrame constructors it is possible to create a DataFrame without needing to reshape my_array .
Note that indices for desired slices should be contained within ' np.array() ' .
The corresponding error is ` TypeError : list indices must be integers , not tuple ` .
A better way to express a multitude of dot products ?
I think it should be possible with a simeple reshape , but i don't see how atm .
inner on the last dimension and broadcast on the rest : ( i ) , ( i ) -> ( )
tensor dot operation in python
The question how to perform their tensor dot product in python .
Concerning the addition : I have no clue what the result of a dot multiplication of these should be .
copy the source of ` plot_trisurf ` to your script and add the line ` tri.set_mask ( ... )` ( tri is a ` matplotlib.tri.triangulation.Triangulation ` instance ) using the algo of your choice ( some max edge length criteria or find triangles who centroids are within some radius .. whatever suits your actual data ) to create the boolean mask after triangulation is done .
The biggest problem is that you have to reshape and then cut off ( because the last file line is shorter , the array is not ' rectangular ' so it fills with ` -1 ` s .
So you could make things more efficient by looping through ` y ` ( 10K unique ? categories ) instead of looping through ` x ` ( 2.6M records ) .
You can access the same items as ` y [ mask ]` by doing ` y [ mask.nonzero() ]` which is a different type of fancy indexing , by giving a list ( or array ) of indices , as in ` y [ np.array ( 1 , 2 , 4 , 5 )]` which returns a copy of the 2nd , 3rd , fifth and sixth item in ` y ` .
Do you have any idea when rank becomes quicker than det by any chance ?
%timeit det ( np.random.randint ( 0 , 2 , size =( 25 , 25 ))
det ( np.random.randint ( 0 , 2 , size =( 25 , 25 ))) is about 42 us and matrix_rank ( np.random.randint ( 0 , 2 , size =( 25 , 25 ))) is about 190 us .
Replacing ` matrix_rank ` with ` det ` computes the determinant using lapack's ` dgetrf ` , which computes only the LU decomposition of the input matrix ( #URL ) .
The asymptotic complexity of both of the ` matrix_rank ` and ` det ` calls are therefore O ( n^3 ) , the complexity of LU decomposition .
@USER : actually it saves space whenever the matrix contains a lot of zeros , not only when there are empty rows .
The " compressed row " name refers to a compressed representation of row indices , compared to the more straightforward coordinate sparse matrix format .
The binomial distribution is similar to normal distribution , but discrete , and ranges only over positive values : #URL
You can take a normal distribution and take the absolute value to " clip " to positive values , or just discard negative values , but you should understand that it will no longer be a normal distribution .
Using the parameter ` zsort= ' max '` when you call ` ax.bar3d() ` solves your problem ( see here ): #CODE
And I have to use the append to have them in a list .
( Ironically when I started using pandas I assumed the " merge " operation did just this , instead of being a form of join . )
All points are equal to the ones in this shapefile :
You have to divide them up in the different shapes and render the ones you are interested in .
:) For the second error , will you simply truncate the list ` y_train_actual [: 40663 ]` and still get a meaningful mean squared error ?
You're trying to compare the predicted y_valid values to the actual ones from the test set's ground truth ?
The data is of a unique type , so I can't use available programs for analysis .
I have built this random dataset of ( aprox . ) 12,000 indices between 0 and 199 , 999 , 999 , and an equally long list of random floats between 0 and 1 : #CODE
Then I construct an array of indices of total window size ` 2*win+1 ` around each of the ` indices ` , and a corresponding array of how much is contributed to the moving average by that point : #CODE
All that is left is figuring out repeated indices and adding contributions to the moving average together : #CODE
You can now get the list of indices at which , e.g. the moving average exceeds 0.5 , as : #CODE
Take a look at this part of your stack trace : #CODE
It can take 3 values : " auto " , " svd " and " eigen " .
By default , it is set to " auto " , which has the following behavior : use the svd mode if n_samples > n_features , otherwise use the eigen mode .
Since in your case n_samples > n_features , the svd mode is chosen .
However , the svd mode currently doesn't handle sparse data properly . scikit-learn should be fixed to use proper sparse SVD instead of the dense SVD .
I have a given array ` [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 2 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 2 , 1 , 0 , 2 , 3 `] ( arbitrary elements from 0-5 ) and I want to have a counter for the occurence of zeros in a row .
what does this have to do with 6 , 4 and 1 zeros in a row ?
The reason it doesn't work is because ` np.where ` returns a list of indices , not a boolean array .
I have a function , which I want to plot a histogram of a dictionary with the keys on the x-axis and the values on the y-axis , then save the file in a location specified when calling the function .
This operation will be used in calculating the loss for a minibatch for a gradient descent algorithm in Theano , so that's why I need to keep it as a composition of numpy functions if possible , rather than falling back on native Python .
The original objects being scored are vectors and the scoring function is a matrix multiplication , which is why we flatten things out in the first place .
I'm writing code that solves the intersection of a few functions which involve cos and sin and other various trig functions in python .
` sin ` , ` cos ` , and ` pi ` are also in the ` math ` module .
If you're using ` sin ` and ` cos ` on numpy arrays , it's better to use the numpy version , but otherwise , the ones from the math module are good .
I've been working with svd , and as a result , a get an image whose matrix representation consists of floats .
[ min ( I (: )) max ( I (: ))]; the minimum value in I displays as black , and
If you are calling it with an empty matrix for [ low , high ] it will just use whatever the max and min values in the array are .
There are a variety of linear and non-linear ways to do this ( implementing gamma correction for example ) The ` colormap ` class then coverts scalars in the range ` [ 0 , 1 ]` -> ` rgb ` values , and can do that by any mapping you want ( gallery )
Perhaps you want a smooth roll off at the edge ?
Missing numpy . before ` eye ` and missing : at the end of the ` for ` line , but you example is actually working .
making multi-dimensional nulls or ones from numpy array
I want to create an array filled with ones or zeros based on this array #CODE
Should your final result be a 7x7 array with zeros where you haven't designated ones ?
For zeros , just use ` np.zeros ` .
if you want it to be in the write format you can always reshape it : #CODE
The reason to use yum would be to resolve dependencies , it also does some extra work to ' roll back ' in case there's an install problem ; in short , yum is less likely to install a broken package into a non-working state .
You can create a 2dim array , and flatten it : #CODE
If all you want is the indices where ` r 120 ` but neither ` g 100 ` nor ` b 100 ` , there are much simpler approaches .
The above code uses your rule for what constitutes a white pixel -- ` ( r > 120 ) & ~ ( g > 100 ) & ~ ( b > 100 ))` -- and gives the indices where the white pixels are , as ` r_idx ` and ` c_idx ` are your ` row_array ` and ` col_array ` without the wasted space for the zeros .
This is the function : cos ( 6.72 * ( 10**7*t ) + 3.2 *sin ( 5* ( 10**5*t ))) I know what the plot is supposed to look like , but mine is completely wrong .
File " / usr / lib / python2.7 / dist-packages / numpy / fft / fftpack.py " , line 164 , in fft
File " / usr / lib / python2.7 / dist-packages / numpy / fft / fftpack.py " , line 48 , in _raw_fft
I need to multiply the window FUNCTION ( .54 + .46cos ( 2pit / T )) by my original function acc ( cos ( 6.72 ( 10^7t ) + 3.2sin ( 5 ( 10^5t )))) in order to make the final result appear better .
Then , I need the absolute value of the fft() of that .
The post suggest that use the transpose within Scipy as Matlab applies the fft over the columns of the matrix , numpy applies the fft over the last axis ( the rows ) by default
As the title states , I would like my interface to convert to / from C++ ` std :: vector ` and numpy ` ndarray ` .
You make an array with shape ( r , index [ r ]) , however remember that indices start with 0 .
These pair should be used as indices for p and the corresponding value of i+ ( j-1 ) *dy should be positioned in the p array which will finally give the output mentioned in the post
So perhaps you wanted to append your value to the array rather than assigning to a location that doesn't exist .
I'm trying to rebuild a song in python , but I cannot concatenate the notes of the same .
ValueError : operands could not be broadcast together with shapes ( 0 )
This takes all the notes ( ` pausa ` , ` La ` , etc ) and stacks them h orizontally ( so that they're all in one row together ) , so you never need to flatten or concatenate anything .
` sum ([[ x , sum ([ x , A [ n+1 ]]) / 2 ] for n , x in enumerate ( A ) if n < len ( A ) -1 ] , [ ])`
numpy.unique generates a list unique in what regard ?
If you input an array with general objects to ` numpy.unique ` , the result will be unique based upon what ?
Can some one pls help me to understand how I can do the indexing of some arrays used as indices .
I want to use these arrays as indices and put the value 10 in the corresponding indices of a new empty matrix .
I'm trying to translate some Matlab code into Python ( using NumPy ) .
I'd hazard a guess that a ` p ` -long head of ` x ` is being used as indices to select ` p ` entries of ` w ` , and that those entries in ` w ` are being replaced by corresponding entries in ` v ` ( less a scalar ` theta `) .
Those zeros are required if only the first column should be modified .
This question is related to ( but not the same as ) " numpy.unique generates a list unique in what regard ?
which simply sorts the terms and then takes the ones which aren't equal to the previous one .
this code should use ( r , index [ r ]) as indices and put the value of i+ ( j-1 ) *dy to the corresponding indices and record that in a new matrix / array which should look like this- #CODE
Is there any way I can set the output like output = np.zeros ( r , index [ r ]) so that it will create the output the exact size and use ( r-1 , index [ r ] -1 ) as indices and put the value of i+ ( j-1 ) *dy in the corresponding indices position of the output array which should give the exact output as you mentioned in your answer
You want to try to find out what ` max ( rs )` and ` max ( index )` would be , and make the shape of your output array be ` ( max ( rs ) , max ( index ))` .
thanx for your answer again . this works for me though I had to add +1 with max ( rows ) .
With an array of ` 1 ` s and ` -1 ` s , performance wise nothing is going to beat using ` np.dot ` : if ( and only if ) all items match then the dot product will add up to the number of items in the row .
For example I have a numpy array with 3 dimensions and I will regularly have to sum it along a specific dimensions .
How I can make , for example , matrix transpose , without making a copy of matrix object ?
Taking the transpose of an array does not make a copy : #CODE
But you also have to be careful to avoid unintentionally modifying the original array ( if you still need it ) when you modify the transpose .
Just looked a the result on OS X and - surprisingly - the output is all zeros .
The output was also all zeros on RHEL with numpy 1.6.2 .
The data of the first column of X is in the range of 1e-6 to 1e-2 ( if you exclude the zeros ) .
what is the absolute value there ?
Here I had to take the transpose , view it as a flat array , and then subtract 1 from the indices to match the way you seem to be counting .
Thank you for your answer . the transpose , flat array is something new for me as I am learning still .
You can try to concatenate them also with ` numpy.r_ ` #CODE
` VOID_nonzero ` from numpy / core / src / multiarray / arraytypes.c.src is calling itself recursively until it runs out of stack space ( over 87000 stack frames on my system ): #CODE
You don't need to find ( using ` where `) and save the indices , you can access the data directly from the array : #CODE
If not , you can first pass it through ` set() ` to get it down to unique variables .
And then modify its values , but don't ` append ` ( that changes the size of the array ) .
Then here's an example of a function that will ' roll ' the elements of your array back by one ( or any number ) and then replace the first value with a new value .
For example , if you want to append the sum of the original array to the beginning : #CODE
Numpy interp not interpolating / extrapolating the values , only finding if the data point is above or below the max empirical value
numpy interp returns 6.5 for any value below - 48.89 ( the max value in the emprical data ) and 0.5 for any value above .
I don't have a picture readily available , but you may leave out the outer point and see how well ( or bad ) it is interpolated by ` p ` .
Numpy provides a unique method , but it says that it cannot sort datetime .
And then append a new columns for ` index ` : #CODE
Now you can get the unique indices : #CODE
And get the unique dates : #CODE
` angle ` and ` absolute ` get you from rectangular to polar , but I don't know of a way to go back .
This function , x_to_x , creates a matrix , M , such that I can take the dot product of M with data on grid a and produce data on grid b .
The ` transpose ` operator changes the axes around so that the first axis become the channel axis .
Break leaves the inner most loop directly and goes to the next step of the outer for loop .
Additionally you can also extract the date a single float : convert each date as the number of days since the min date of your training set and divide by the difference of the number of days between the max date and the number of days of the min date .
If this were Java , I'd recommend that you bump up the max heap on your JVM .
But where would you store the intermediate result of the pseudoinverse that's computed as part of the dot product if 2 / 3 of your RAM is taken by ` phi ` ?
Such as doing a ` numpy.linalg.norm ([ 1 , 1 , 1 ] - [ 1 , 1 , 1 ])` 2 times , and then doing ` norm ([ 0 , 0 , 0 ] - [ 0 , 0 , 0 ])` , and then ` norm ([ 2 , 2 , 2 ] - [ 1 , 1 , 1 ])` 2 times , ` norm ([ 2 , 2 , 2 ] - [ 0 , 0 , 0 ])` , then ` norm ([ 3 , 3 , 3 ] - [ 1 , 1 , 1 ])` 2 times , and finally ` norm ([ 1 , 1 , 1 ] - [ 0 , 0 , 0 ])` .
Yes , unfortunately , ` norm ` doesn't allow an ` axis ` arg .
` keepdims ` argument is added in numpy 1.7 , you can use it to keep the sum axis : #CODE
I'm doing something similar that is to compute the the sum of squared distances ( SSD ) for each pair of frames in video volume .
Or can I get the ` join ` method to insert the commas for me ?
First you should use ` join ` this way to avoid the last comma issue : #CODE
2 ) If you are only after the convolution of multi-exponential decays with Gaussian kernels , use the analytic result , which is easily and efficiently evaluated using scipy.special.erf : The convolution of exp ( -t / tau ) ( t > = 0 ) with the normalized Gaussian 1 /( sqrt ( 2*pi ) *sigma ) *exp ( -t^2 /( 2*sigma^2 )) is
1 / 2 * exp ( - ( 2*t*tau - s2 ) /( 2*tau^2 ) ) * ( 1 + erf ( ( 2*t*tau - s2 ) /( sqrt ( 2 ) *tau*sigma ) )) .
3 ) You can efficiently convolve vectors using np.convolve , which is based on the FFT approach mentioned in your post .
but if the array is nested ( as above ) , then i should point to each sub-list , right ? and then calculate the mean for each and append it to the np_array list ?
It works fine , but I need to figure out a way to compress the data as the tiff file is > 4gb ( at this point the script crashes ) .
I have very sparse arrays , 640*480 all zeros with some dozen ones per frame , the original data file is 4MB so some compression should be possible .
At the moment not , but in the end I would like to make a tiff stack movie , then I will need the frame number .
I did have to switch around data_height and data_width and transpose the final result to get what I want .
This gives me indices of the ` n ` smallest elements .
Is it possible to use this same ` argsort ` in descending order to get the indices of ` n ` highest elements ?
Isn't it simply ` ids = np.array ( avgDists ) .argsort() [ -n :] ` ?
@USER Well , then make it ` ids = np.array ( avgDists ) .argsort() [ -n :] [: : -1 ]` .
The advantage of this method is that ` ids ` is a view of avgDists : #CODE
In reality , the negation is * O ( n ) * and the argsort is * O ( n log n ) * .
This means the timing discrepancy will * diminish * for larger data sets - the * O ( n log n ) * term dominates , however your suggestion is an optimisation of the * O ( n ) * part .
It replaces the O ( n ) negation with an O ( 1 ) slice , but the dominant term remains the argsort at O ( n log n ) .
Use log binning ( see also ) .
Here is code to take a ` Counter ` object representing a histogram of degree values and log-bin the distribution to produce a sparser and smoother distribution .
What I have is a lot of time series data with many identifiers ( indices ) .
How do I reshape the data such that I can write functions that will calculate auto correlation times ?
Because we want to keep the document ids and term ids .
I want to sum the rows of this matrix without including the -1 values .
Approach #2 : use the same boolean array ( well , flipped -- before we wanted to know which ones to keep , and now which ones to ignore ) as a formal mask for a ` MaskedArray ` .
We can see the first subrecord ends with the first 3 bytes of certain float number and the second subrecord begins with the rest 1 byte as 2147483639 mod 4 =3 .
Convert DataFrame with index-OHLC data to index-O index-L index-H index-O ( flatten OHLC data )
Heres how to create mod columns #CODE
Do you mean select the max values from the second array and third array ?
I select the max value and replace them While they select those non-zero and then replace them .
You can find the indices where ` y ` and ` z ` are nonzero using the nonzero method : #CODE
1 . with a boolean array 2 . with indices #CODE
( 30,000 to 3000 ) and needs to be a log scale
To determine the point , work out which axis ( linear or log ) will require more ' space ' along the x axis , set its limit as its max , and then convert that to the coordinate space of the other axis to use it as its limit .
It can be a novel concept for you , but they're quite useful in fact ( you can stop worrying whether you need to multiply by the transpose , insert a row or a column in another array ... )
By the way , you could write a fancier , more general norm like this : #CODE
One simple way to do that is to square all the elements , then sum along the last axis , and then take the square root .
If you're expecting more than log N matches per average lookup , you can do it with two sorted lists and ` bisect ` .
For example , if you can spare N^2 space , you can keep the time at log N by having , for each range in the first list , a second list , sorted by end , of all the values with a matching start .
If you use ` numpy ` or a database instead of pure Python , this can't cut the algorithmic complexity from N to log N but it can cut the constant overhead by a factor of 10 or so , which may be good enough .
I tried to implement you numpy solution but keep getting and error for ` comparisons = Location < RangeArrays [ Chr ]` : ` TypeError : list indices must be integers , not str ` I've never used numpy or arrays before and am not sure what I'm doing wrong ...
@USER : ` e ` has shape ( 10 ) and ` nu ` has shape ( 5000 ) ., so numpy can't figure out what you want to happen that gets fed to the ` sqrt ` .
what is reshape doing and what is ` np.ones ( nu.shape )` ?
I would suggest reading the documentation [ ` ones `] ( #URL ) , [ ` reshape `] ( #URL ) and playing around in an interactive shell .
Perform a reverse cumulative sum on a numpy array
Can anyone recommend a way to do a reverse cumulative sum on a numpy array ?
Where ' reverse cumulative sum ' is defined as below ( I welcome any corrections on the name for this procedure ):
More specifically , I have a roughly 20000x3 array and I need to know the indices of the 1x3 elements that are entirely contained within a set .
If you need the indices , then use #CODE
The idea overall is good , but you should strip down the list comprehension .
` searchsorted ` is log ( N ) in the size of master and O ( N ) in the size of triangels so it should also be pretty fast , maybe faster depending on the size of your arrays and such .
The last line takes care of the virtually insertions , but you're right you need to clip to take care of the size issue .
I've added a line to clip .
I don't know how to flush denormals to zero from within Python other than creating a C extension though ...
I've been looking around a bit since I have run into this problem several times and I can't seem to find any way to flush the denormals .
I don't know of a way to flush denormals to zero in numpy .
You could resample ` df2 ` to 5 min and fill it .
My biggest issue is that further down the code I divide my spectra by a template spectrum constructed from the sum of all my spectra in order to analyse the differences and since I do not have enough decimal places I am getting rounding errors .
assign all items of an array except those of the given indices
My goal is really to fill a given array with given values except for indices I specified in another variable .
What the asker wants is to pass random numbers to ` a ` for indices of ` a ` that are not in ` b ` .
` assignment in the first part of this answer , except instead of passing ` a ` to ` np.in1d ` , ` np.arange ` is used to make an array that gives indices , not elements , of ` a ` to ` np.in1d ` .
Your solution seems to be the right one but I need a little change so as ` b ` is understood as a list of indices not values .
In fact my example is too confusing ; I should have initially filled ` a ` with , for example float random numbers while keeping ` b ` unchanged because it is the list of indices in ` a ` which values I want to keep unchanged .
When dealing with NumPy arrays , use their ` mean ` and ` sum ` methods , and avoid using the Python ` sum ` function : #CODE
Avoid the Python builtin ` sum ` function because
it is slower than the NumPy ` sum ` method , and
it is not as versatile as the NumPy ` sum ` method .
does not allow you to specify the axis on which to sum .
We managed to eliminate two calls to Python's ` sum ` with one call to NumPy's ` sum ` or ` mean ` method .
If you look at the equation 5 update rule for ` H ` , first notice that indices for ` V ` and ` ( W H )` are identical .
we get a matrix indexed by the two remaining indices , alpha and mu , in that order .
That is the same as the indices for ` H ` .
If i count instead of sum i dont end up with the extra bin .
A spherical covariance matrix has all items in the diagonal equal , and all off diagonal elements zero .
While I the ` cm ` is not diagonal , since some variables are correlated .
If you just have a diagonal covariance matrix , it is usually easier ( and more efficient ) to just scale standard normal variates yourself instead of using ` multivariate_normal() ` .
Maybe it is not right to call it " spherical " , since the matrix is not diagonal .
The parameters will be broadcast against each other , and each broadcasted set of parameters will be sampled independently .
Therefore I use a converter to decode the strings : #CODE
I am trying to translate a project I have in MATLAB to Python+Numpy because MATLAB keeps running out of memory .
Basically I'm making a 2d histogram of a dataset , and want to save it after some processing .
The problem is that the numpy.save function throws a " ValueError : setting an array element with a sequence " when I try to save the output of the histogram function .
Alternatively , if you only need the histogram itself , you could save just that .
Comparing elements of an array to a scalar and getting the max in Python
I am trying to create a lat / lon grid that contains an array of found indices where two conditions are met for a lat / lon combination .
returns all found indices from lon / lat that are present at -180 / -82 .
Now lets assume that the indices [ 10000,100 01,100 02 ,.., 10025 ] of lat / lon are on the same gridpoint .
For those indices I need to check whether array [ 10000,100 01,100 02 ,.., 10025 ] now met a condition , i.e. np.where ( array == 0 ) .
With cts.nonzero() I only get the index in the histogram .
But then all information of each point contributing to the value of the histogram is lost .
So it seems that you need a list of lists ` a ` where ` a [ i ]` is a list of all indices to ` lat ` and ` lon ` that fall into the ` i ` th 1 bin ?
But it returns at each lon / lat ( d1 / d2 ) point a list of indices where the condition was met .
I like your histogram approach @USER , but unfortunately all the information I'm interested in is lost .
This is what a histogram does :
The indices where you have a nonzero count would be at : #CODE
Ok , with your histogram I get at least the total number of each pair .
But I need the indices of such pairs in lat / lon to check a third array of the length of lat / lon for some condition .
Therefore I have to store the indices in an array , right ?
If your array only consists of permutation of indices you can use a base-convertion #CODE
I think this is better than doing ` hash ( str ( a ))` , because the latter could confuse arrays that have unique data in the middle but zeros around the edges .
operand 1 did not have enough dimensions to match the broadcasting , and couldn't be extended because einstein sum subscripts were specified at both the start and end
It looks like ` einsum ` loops through the string argument and the ops several times , identifying the indexes , and broadcast types ( right , left , middle , none ) , and op dimensions .
For each location in the result matrix , instead of storing the dot product of the corresponding row and column in the argument matrices , I would like like to store the element wise product , which will be a vector extending into a third dimension .
One idea would be to convert the argument matrices to vectors with vector entries , and then take their outer product , but I'm not sure how to do this either .
What I am doing is taking apart each row and column pair that will have their outer product taken , and forming two lists of them , which then get their contents matrix multiplied together in parallel .
You can replace you reshape with ` m1.A [: , None , :] ` , which makes it more compact and ( I think ) more readable .
In other words , each element of the outer array will be a row vector from the original 2D array .
I had the same issue to append a raw with a different length to a 2D-array .
The only trick I found up to now was to use list comprenhsion and append the new row ( see below ) .
A comprehension could be used to reshape those arrays into 1d ones .
Is definitely a lot faster , but is there any possibility to remove the outer for-loop too ?
Is definately a lot faster , but is there any possibility to remove the outer for-loop too ?
@USER I made an attempt to remove the outer loop too , check it out .
Note that the ` dstack ` assumes that each ` comp.ndim ` is ` 2 ` , because it will add a third axis , and sum along it .
This will slow it down a bit because you have to build the list , stack it , then sum it , but these are all either parallel or numpy operations .
Thus , arctan ( tan ( x )) does not yield x if x is an angle in the second or third quadrant .
If you plot arctan ( tan ( x )) from x = 0 to x = Pi , you will find that it has a discontinuous jump at x = Pi / 2 .
It's after the tan ( theta / 2 ) , not at the end of the whole equation .
The common practice is to sum 2*pi in the negative results of ` arctan() ` , which can be done efficiently .
The OP's suggestion to replace arctan ( x ) by arctan2 ( 1 , 1 / x ) , despite not yet explained , produces the same results without the need to sum 2*pi .
Because the common practice is to use arctan ( x ) and sum ` 2pi ` to the negative results
There are thousands of numbers below the ones shown here .
I know that numpy is configured for multiple cores , since I can see tests using numpy.dot use all my cores , so I just reimplemented mean as a dot product , and it runs way faster .
Any insight would be helpful , I'd prefer to just use mean since it's more readable and less code , but I might switch to dot based means .
Be sure to make a function called something like ` mean ` to use rather than doing the dot thing everywhere , so you can change it later if ` numpy.mean ` improves .
Basically , because the BLAS library has an optimized dot product that they can easily call for ` dot ` that is inherently parallel .
Not only is this simple and clear to read and write , since numpy knows you want to do a matrix dot product it can use an optimized implementation obtained as part of " BLAS " ( the Basic Linear Algebra Subroutines ) .
In particular , ` dot ` is implemented on top of BLAS when possible , and BLAS is automatically parallelized on most platforms , but ` mean ` is implemented in plain C code .
And , of those which aren't , how do you know which ones can be nicely manually-threaded and which need multiprocessing ?
I want to stack these two lists together and write them to a text file in the form of columns ; therefore , I want to use numpy.savetxt ( if possible ) to do this .
Using two indices ` [ i ] [ j ]` is two operations .
If you ever want to delete more than one columns , you just pass indices of columns you want deleted as a list , like this : #CODE
Together with that i want to show a plot of the fft of that whole signal .
The fft of the whole signal is as expected with the peak on 4 .
We should just vectorize the forloop / append construct : #CODE
However , I would like to know the indices in the string array , A where the string ' apple ' matches .
Yes , but what I want are the indices where the string ' apple ' matches in the string array .
Basically , this stores the ` x ` coordinates of our ` ( x , y )` data points in the array ` t ` ; and the resulting ` y ` coordinates ( result of y=f ( x ) , in this case ` sin ( x )`) in the array ` s ` .
It appears you want to use a combination of NumPy's ` max ` , ` min ` , and ` where ` functions .
There is an " ugly " option that involves a " dummy " coordinate of zeros , which allows using ` scipy.spatial.KDTree ` : #CODE
If you need the indices themselves , use ` numpy.searchsorted() ` .
Distances are easy to compute once you have the indices .
Thanks for your answer , ` numpy.searchsorted ` is a good approach to find the indices in 1D arrays , and the weights ( distances ) are also easily computed before .
If you need the indices themselves , use ` numpy.searchsorted() ` .
Distances are easy to compute once you have the indices .
Please see the application event log or
Well , have you checked the log that the error message points to ?
Unable to unwrap image in OpenCV
How do I receive an array ( like numpy ones ) into a function ?
improving Numpy dot performance by removing arrays copy
Is there an enhanced numpy / scipy dot method ?
Numpy dot product very slow using ints
On #URL " Although C is only 40 by 40 , inspecting the memory usage during the operation of dot will indicate that a copy is being made " .
@USER : if you cannot fit into memory a matrix full of zeros that is of the same size as the result of the matrix product would be , what you are trying to do is impossible ( and whether Numpy makes copies or not is irrelevant ) .
@USER SVD yields either square or diagonal matices .
Your matrix is not square , as then it won't lead to memory errors , so it has to be diagonal .
Otherwise you can concatenate strings with ` + ` as `" Subarray .
" +str ( i ) + " .txt "` , but you have to make sure that all the elements that you concatenate are strings .
The stuctures i discribed are in a mat file which i have already loaded with the way you suggested.But after that i cant acces the atributes of the substructures .
@USER , well try to ` print mat ` and show some part of it so we can help you access it .
If you inspect the id of the view ( with ` id ( arr1 [ 0 ])`) and then assign ` arr2 = arr1 [ 0 ]` and inspect its id ( with ` id ( arr2 )`) , it's perfectly possible that those ids are the same .
The ` arr1 [ 0 ]` was created , its id found , and then immediately it went out of scope , so the memory ( which is how CPython assigns ids ) could be reallocated to a new object .
I take from this that you should * only * compare the ids of objects in scope .
Note that if I use ` C ` ordering , it does maintain contiguous data when I reshape , but not when I add a new axis : #CODE
update For those who might find this in a search , to keep the current array order in a reshape , ` a.reshape ( 3 , 1 , 4 , order= ' A ')` works and keeps a contiguous array contiguous .
And then concatenate ( ? ) the various columns that would be created for each list in A , creating the final dataframe , with the same index as pxc .
I don't think there is a way to use ` np.take ` without going to flat indices .
As he points out , the ` [ 0 ] [ 1 ]` element is what you'd want for ` cov ( a , b )` .
with a and v sequences being zero-padded where necessary and conj being the conjugate .
While these values look much better than what I'm getting from ` correlate ` , I'm still not getting the same answers as the octave function .
I would like numpy to see data as a 643 length 1-D array of 2890x10 matrices and calculate a dot product ( sum-product ? ) between data and vector .
The size of the dot is determined by the percentage overlap .
You might also need the ` reshape ` package to , well , reshape your data for usage in ` ggplot2 ` .
It doesn't matter if the last such set of indices in each row or column runs off the end of the array - numpy's behavior of simply giving the portion within the slice that exists is sufficient .
I want a way to generate these slice indices programatically for arbitrarily sized matrices and submatrices .
and similarly to generate ` y_coords ` , so that the series of indices is given by ` itertools.product ( xcoords , ycoords )` .
I don't understand why the ifft ( fft ( myFunction )) is not the same as my function .
All the documentation I can see says there is some normalisation that fft doesn't do , but that ifft should take care of that .
I simply need to remove the rows that are repeated in an array but maintain one of them , I can't use unique because I need to maintain the order .
I want to check if a solution exists where the transpose has a 0 solution .
so this means on transpose the first row ( which would be the normal first column ) works with your algorithm , the second and third do not !
In my case the X's can be any length but always 2 vectors in total ( so the transpose will just be a bunch of ay1 + by2 equations , number of equations dictated by length of X ) .
If this cross-product has three nonzero components all of the same sign , then none of the the normals from it can be in the dreaded octant .
In general , the tricky things happen when the normal contains zeros :
Matlab ` ones ( n , m , p )` appears to create an array of ints .
I just cast ` kernel = kernel.astype ( int )` , which , of course , resulted in all zeros .
Replacing Matlab's kernel with ` kernel = rand ( 5 , 5 , 5 ); ` didn't change its execution time , though , so I don't think Matlab is cheating here :)
You need to strip off the trailing `' ; '` from the lines .
You are right , but I can't strip trailing ` ; ` for some lines as there are missing values .
A python list has an ` append ` method which does this , what is the equivalent in numpy .
The commenters on the question are right : numpy arrays are different from Python lists and so the numpy append method is often not a good choice .
This can be particularly tricky when trying to append to a numpy array quickly .
The append method for a numpy array returns a copy of the array with new items added to the end .
Because the array might be moved by resize , references to the array ( e.g. , doing ` b = a ` before the resize ) are not safe after resizing .
Issue with gradient calculation in a Neural Network ( stuck at 7% error in MNIST )
Hi I am having an issue with my calculation of checking the gradient when implementing a neural network in python using numpy .
I am using ` mnist ` dataset to try and trying to using mini-batch gradient descent .
Howerver that does not explain the problem with the gradient as it is calculated using back_prop .
I get %7 error rate using 300 units in the hidden layer using ` minibatch gradient ` descent with ` rmsprop ` , 30 epochs and 100 batches .
And the cost function for the gradient checking : #CODE
What kind of results are you getting when you run gradient checking ?
Often times you can tease out the nature of the implementation error by looking at the output of your gradient vs the output produced by gradient checking .
And the top-layer gradient becomes : #CODE
While I am having some trouble getting my head around your backprop routine , I suspect from your code that the error in gradient is due to the fact that you are not calculating the top-level dE / dw_l2 correctly when using square error , along with computing fprime on the incorrect input .
Similarly when computing fprime for the lower layers , you want to use the input to your transfer function ( i.e. dot ( X , weights_L1 ) + bias_L1 )
Based on what you are saying , the cost function is bad defined but still the gradient should giving good answer .
I was using then the wrong cost function , but that would explain the problem with the gradient calculation.But not the reason I am getting only 7% error with 300 hidden units and 30 epochs
With 7% error it's unlikely that the gradient is incorrect .
You made me realize that I was using the wrong gradient calculation .
Therefore the gradient was being miscalculated .
What you are looking for is ` reshape ` and ` argsort ` , I think .
Using the ` reshape ` member function you can change the shape without changing the sequence : #CODE
As you can see , each row now has the required indices .
` 4 - np.argsort ( r )` : argsort would create indices in the rang 0-3 .
Then using another reshape , you bring the data back into its original shape #CODE
The patteren here is ` a [ #URL #URL Using the ` reshape ` and ` argsort ` techniques shown above you can create the new values .
My application is fairly perfomance-critical , and so I tried an alternate , which is to first concatenate all the sparse matrices into one large sparce matrix , and then use only one call of ` dot() ` like so : #CODE
I don't care about the time required to concatenate the sparse matrices , only the time to compute the result .
As far as I understand , sparse matrix multiplication time with a dense vector should be linear in the number of nonzero elements , which is unchanged in the two cases above .
The performance difference in this case is not as large as with my own sparse matrices that have some structure ( probably because of caching ) but it's still worse to concatenate the matrices .
I can't reproduce that : the single dot product is 5 times faster for me .
For example , if a point is just outside the hull but , say , midway along a 45deg diagonal , then your test will fail .
Instead , sum the angles between the test point and all the points of the convex hull : if it's inside the angles will sum to 2pi , and if it's outside they'll sum to 0 ( or I might have some detail of this wrong , but that's the basic idea ) .
O ( N log N ) to construct the convex hull
O ( log h ) per point-in-polygon query .
You can see in below illustration ( Green dot is the X point you are trying to determine .
In some circumstances , it may be more efficient to use the low memory footprint ` numpy.ogrid [ ]` , which creates in this case a row and a column that are automatically broadcast so as to cover a 2-dimensional grid .
( BTW , are you sure about the syntax ? Isn't ` mgrid ` one of the square-bracketed ones ? )
And yeah , mgrid is one of the square bracketed ones ( #URL )
Python Pandas find non zero indices
` df.apply ( numpy.nonzero() , axis=1 )` and ` df.apply ( numpy.nonzero() , axis=0 )` to get the indices of the non-zero columns so that I could remove there inverses from the DataFrame.That gives me a list of tuples I'm not clear how to get at .
thanks @USER that will work for the latter , but I actually have a more pressing need to get rid of the zeros .
If you are sure that everything you want to drop is either the literal empty string ( `''` , ` None ` , ` np.NaN ` , or ` 0 `) and that you don't want to keep ` 0 ` , then you can just fill the ` NaN ` and convert to boolean and check whether the sum is 0 .
So putting ` astype ( bool )` means that it converts the entire array to either ` True ` or ` False ` ( which are equivalent to ` 1 ` and ` 0 ` respectively ) and then you can just sum to find the number of ` True ` values in a row or column .
What does the norm do ?
You are passing None for the ord parameter to linalg.norm() so you get the Frobenius norm .
The code appears to be normalising the input , by dividing by the norm .
I didn't quite understand the math of Frobenius norm ...
I thought you wanted to know what norm did ?
what does the norm do ?
Broadly speaking a norm is a measure of how large an object is .
Abs() is the most common norm for scalars .
For vectors you have L1 , L2 , Linfinity norm and so on .
The most common is L2 , the Euclidean norm .
In your code , the use of norm is pointless .
Rough Outline of the code : First compute the indices for each patch , so to be able to
Those patches are processed , each patch individually and afterwards are merged together to an image again , with the precomputed indices .
It can be helpful to think of a numpy array as a ( glorified ) chunk of memory , and then strides are a way to map indices to memory address .
" Hints " in case you insist : ` strides = img.itemsize * np.array ([ 1 , Y , Y , 1 ])` , use ` .reshape ( ..., order= ' F ')` on ` contiguous_patches ` and finally transpose it ` .T `
Numpy : clip / cut 2d masked array
` 2Hz ` can be close to ` 0Hz ` especially on a log scale .
@USER by default should be normal scale , not log I think
Also , just so everything is clear , your FFT plot is probably of the real part of the fft .
Please use tags pertaining to your programming environment , such as ` python ` and ` pyfits ` , instead of overly generic ones such as ` file ` and ` function ` .
Numpy dot product
I guess that's because self.yuv [ i , j ] is not a vertical vector . transpose doesn't help .
Your matrix has shape ` ( 3 , 3 )` while your image has shape ` ( rows , cols , 3 )` and ` np.dot ` does " a sum product over the last axis of a and the second-to-last of b .
The simplest solution is to reverse the order of the operands inside ` np.dot ` and transpose your conversion matrix : #CODE
Just for reference : [ Looking at numpy source ] ( #URL ) ` fromfunction ` just creates ` indices ` array and passes it to user function .
I need to do ` dot product ` on each pair of some arrays where there are some empty sub arrays , in different index .
as you can see I have some empty arrays in these arrays , so when I do the dot product , those empty arrays make everything to ` [ ]` .
It returns the index of nonzero / non_empty elements .
Distance / absolute error between two numbers
Maximum is always bigger than the minimum ( more to the right on a 1d axis , not by absolute value ) .
Surely ` abs ( max - min )` would work .
In fact , if the maximum is always bigger then ` max - min ` would work .
You could do that with ` abs ( max - min )` : #CODE
I'm not sure which indices i need to change to achieve the minimum and not the maximum values .
It will give you the indices of the 3 smallest elements .
To create an array with all zeros : ` a = numpy.zeros ( 100000 )` .
What I have found so far is a group of functions in Numpy ( ` fft ` , ` fftn ` ,.. ) which compute the discrete Fourier transform , of which the square of the absolute value is the power spectrum .
The simplest way to deal with this is by making your array twice as large along every dimension , padding with extra zeros , and then discarding the extra data .
Find peak of 2d histogram
I make a 2d histogram of some ` ( x , y )` data and I get an image like this one :
Here ` H ` contains histogram values and ` xedges , yedges ` boundaries for histogram bins .
How can i append zeros to the end of the 1-D array such that the final size becomes 1x4000 and its able to fit in the 2d array ?
gradient descent using python numpy matrix class
I'm trying to implement the univariate gradient descent algorithm in python .
I would like to add two rows of zeros to array X so that X and Y are both ( n+2 , m ) .
The ones you also use when inverting a matrix ?
Example steps without checking for zeros ( i . e . how to do it if all values are ? 0 ): #CODE
You have to check for zeros , of course , ( and maybe swapping lines ) to avoid dividing by them , but that's the normal way of applying Gauss .
My transform is ` C = inv ( A [: , : n ]) * A ` which means that rows are combined without permuting columns .
Here's a solution based on detecting the distance between unique dates #CODE
You can always use the reshape command .
You are getting the indices wrong .
If you want to use ` np.delete ` , you need a more correct list of indices .
On the other hand , this can be more easily done with indices : #CODE
Or , inspired by the suggestion of @USER , you can also create the indices with ` np.in1d ` , here in one line : #CODE
If you must use ` np.delete ` , just convert the list of indices from ` bool ` to a sequence : #CODE
And ` A * mat ( vec ) .T ` would be your only choice .
The issue is that the center estimation I can get by averaging the global maximum obtained with different bin sizes is always slightly off than the one I would set by eye , because I'm only accounting for the biggest bin instead of a group of biggest bins ( like one does by eye ) .
It's a combination of Bi Rico's comment here ( May 30 at 18:54 ) and the answer given in this question : Find peak of 2d histogram .
So instead of using the detect-peaks function as I did above , I simply add the following code after the Gaussian 2D histogram is obtained : #CODE
So if I'm using ' - 99999.99 ' as the expected value to indicate a missing data point then I won't get a match with what gets pulled into the slice array since those values have a greater scale length and the additional digits in the scale are not just zeros for padding .
If you wrote a variable all in one go , you can then set the ` missing_value ` attribute to an array of indices where the values are missing .
Find the columnwise indices of the minimum values of a generator in Python + Numpy
and I want to generate a vector containing the row indices of the matrix , be created by the above generator , that contains the minimum values .
I was expecting to figure out a way to do it just like if I was to obtain just the min values : #CODE
one thing I forgot to mention is that the " list_of_vectors " is a generator as well , so I couldn't transpose .
But your answer got me thinking how to generate the transpose instead and I could reach the desired speed !
When you do ` y == ' y10ut '` and ` y ` is an array of ` dtype ` string , numpy returns a boolean array with the indices of ` y ` where the condition is met .
I'm trying to get the max of a vector given a boolean value .
memory leak in matplotlib histogram
However , when substituting the call to pylab with a direct call to the numpy histogram method then memory usage is constant ( it also runs significantly faster ) .
I was under the impression that pylab is using the numpy histogram function .
But I guess drawing histogram wasn't your intention
Here we are using ` 100,000 ` indices uniformly picked from ` [ 0 , 1000 )` .
np-optimisied - uses custom ` numpy ` indexing / other tricks to beat the above two implementations ( except for ` min max prod ` which rely on ` ufunc.at `) .
You should be able to break your array into " blocks " using some combination of ` reshape ` and ` swapaxes ` : #CODE
There was [ another question ] ( #URL ) a couple of months ago which clued me in to the idea of using ` reshape ` and ` swapaxes ` .
` -1 ` tells reshape to fill in whatever number is necessary to make the reshape valid .
The question " How can I use numpy array indexing to select 2 columns out of a 2D array to select unique values from ?
" is similar , but slicing ` data [: , 5 :] ` as suggested throws ` IndexError : too many indices ` with a record array .
I know the question has been answered , but just wanted to log this , as it is related - it's something between Extracting specific columns in numpy array and Select Rows from Numpy Rec Array ( but not quite How to return a view of several columns in numpy structured array ) , this is a syntax I was looking for a while , and I finally found it ; let's say this is the data : #CODE
( dot is normally handled by lapack anyway ) in other words , the stability of usual addition applies ( possibly with extended precision registers , but that depends on the hardware and operation )
@USER thanks , do you know if lapack does anything special to ensure dot product stability ?
If you look at , for instance , some of the ATLAS code for doing a dot product , basically the only tricky that they do for accuracy , when doing a sum of single precision numbers , is to actually accumulate the sum in double .
` sum ` and ` dot ` are BLASs not from the LAPACK .
Sum and dot depend only on the order of evaluation .
For the cumulative rounding during a " cumsum " or " dot " function you do have choices :
note the single precision roundings within " dot " cancel in this case as each almost-integer is rounded to an exact integer
Is there a way to flatten a numpy array in diagonal order efficiently ?
I am looking for an efficient way ( preferably a vectorized fast built-in function ) to flatten a numpy array in diagonal order .
For the same reason I also do not want to prepare in advance a list of all the indices in the correct order .
It would be even better if I could specify which subset of diagonals I would like to flatten , e.g. flattening only 1st and 2nd diagonals will give ` [ 1 , 5 , 9 , 2 , 6 ]` .
With fancy indexing , you'll need to prepare an array of the indices in advance .
Perhaps I will also try to write a fast diagonal flattening with iteration in Cython .
numpy.diag returns the diagonal along a certain index .
So this should give you the desired output : ( Note that the 0th diagonal is the normal diagonal , so if you want subdiagonals , you may need to use negative values for the diagonals . ) #CODE
The following function is based on ` indices ` comparisons , based on the fact that each diagonal has an index relation , for example at the main diagonal ` i == j ` , and so on ...
Yes , I had the order of arguments to ` reshape ` wrong .
I have a 2D numpy array that I need to take the max of along a specific axis .
B should now be array ([[ 1 , 1 , 2 ] , [ 1 , 1 , 2 ]]) since it preformed the operation on b for only the indexes of the max along the columns of a .
The above works with argmax as you requested but you might have wanted to increment all max values . as in : #CODE
I have two arrays , that I am trying to combine using concatenate : #CODE
from numpy import linspace , sin , pi , int16
string concatenation is usually done using join : #CODE
this is giving me an array of zeros . apparently the astype ( str ) doesn't do that in my python .
AttributeError : ' numpy.ndarray ' object has no attribute ' append '
I googled this and found this question / answer on ` append ` , but I didn't get anything .
Well , looking at the link to the other question you asked , it looks like ` numpy.ndarray ` has no attribute called ` append ` , but it looks like NumPy does .
Or you can try to concatenate .
Ordinarily , intermediate values in calculations can be recovered in post-processing , and , given that ` dy ` might be evaluated at timesteps other than the ones you request , and that ` dy ` might be evaluated multiple times for the same time value , getting x from postprocessing is often the best option .
I am simulating a system of the type x '' = u , where u is a sum of different forces ( e.g. u = u1 + u2 + u3 ) .
The call to ` np.sqrt ` , which is a Python function call , is killing your performance You are computing the square root of scalar floating point value , so you should use the ` sqrt ` function from the C math library .
It might be able to solve diff eqs .
If x and y are two volatility numbers , the sum of the volatility is ( x^2 + y^2 ) ^ 0.5 ( assuming certain mathematical condition but that's not important here ) .
If you think about it , a ` Volatility ` object is just a ` float ` , the difference lies in how it behaves under addition with another ` Volatility ` instance - its not a simple sum .
@USER they correspond to ` c ` when viewed as two dimensional indices
c are indices in b .
Then , the only thing that remains is to stack that array back against ` c ` using ` column_stack ` : #CODE
Most things these days run on x86 hardware , so little-endian is the norm .
In the version I'm running ( 0.12.0 ) , there is no ` out ` parameter because there are two output parameters : ` distances ` and ` indices ` , which can both be used for output .
indices : ndarray , optional
Used for output of indices , must be of type int32 .
I generated a unique numerical code for each word , so I could use numpy.bincount ( since it only works with integers , not strings ) .
I think the issue here involves your unique numerical code system , not the size of the initial arrays .
You can then just use ` numpy.bincount ` on ` lookup_vals ` , and if you need to get back the original string unique integer , you can just use the the values of ` lookup_vals ` as indices to ` original_keys ` .
In numpy , there is a ` flatten ` operation which allows you to , for example , flatten a ` m x n ` matrix down to an array of ` mn ` elements , and a ` reshape ` operations which goes in the opposite direction .
There is a ` reshape ` method on one-dimensional matrices , but it appears to create copies .
How can I broadcast between 1D and nD arrays to obtain a ( 1+n ) D array output ?
One example is when trying to relabel the image colors so that every unique color is given a different value .
What if you feed it a simple impulse signal ( all zeros except a single 1.0 ) and have a and b be just as simple - all zeros except a single 1 ( note that a0 has to be 1 ) ?
If you want the same behaviour of matlab you need to specify the first axis or squeeze the array ( if the second dimension has a size of 1 ): #CODE
NumPy array , change the values that are NOT in a list of indices
I know that I can replace the values located at positions ` indices =[ 2 , 3 , 4 ]` using for instance fancy indexing : #CODE
But how to replace the values at the positions that are not in ` indices ` ?
Subtracting your ` indices ` set from a universal set of indices ( depends on the shape of ` a `) , but that will be a bit difficult to implement and read .
Some kind of iteration ( probably the ` for ` -loop is your best bet since you definitely want to use the fact that your indices are sorted ) .
Creating a new array filled with new value , and selectively copying indices from the old one .
You can ` resize ` the dimension of ` a ` to match the ` shape ` of ` b ` and fill in the missing values .
Mapping an array into other with zeros at the begining and the end
but leaving the first and last columns as zeros
Without using a for loop or iterating on the outer array .
What's wrong with outer for loop ?
For example , if I have a very high resolution PNG image , how can I manipulate it with Python ( crop , split , run through an fft , etc . ) ?
If you know the bounds of your image , I'd suggest creating an empty uint8 array ( less memory usage ) for the whole thing , and then populate it by scaling each sub-image to the min / max of the data .
Also see [ Subset sum problem ] ( #URL )
Let ` M [ i ]` be the number of sublist found so far to have sum equal to ` i ` .
Starts with only ` M [ 0 ] = 1 ` because there is one list with has sum equals zero , that is the empty list .
Update the number of ways you have to compose a list of each sum when considering
This is similar to those Knapsack and subset sum problems .
it means there are 3 sublists with sum equals 11 .
To account for the fact that you allow the 10 sublists to have similar sum .
Not just exactly the same sum .
You might want to change termination condition from " terminate if any M [ j ] > = 10 " to " terminate if sum ( M [ j : j+3 ]) > = 10 " or something like that .
In your example , rounding error is introduced when calculating ` dot ( u , v ) /( norm ( u ) *norm ( v ))` .
For your test values , the calculation is effectively ` 2 /( sqrt ( 2 ) *sqrt ( 2 ))` .
The computed value for sqrt ( 2 ) is rounded to a value slightly larger than the infinite precision value .
The ` decimal ` module solution by @USER .J calculates ` 2 /( sqrt ( 2 ) *sqrt ( 2 ))` to higher precision .
Calculating ` 2 /( sqrt ( 2 ) *sqrt ( 2 ))` using ` decimal ` and different precisions highlights another issue .
` AcuteAngle3 ` avoids your original problem but it is possible that ` dot ( u , u ) *dot ( v , v )` is rounded to a value that is slightly smaller in magnitude than the real value and you could try to take the arccos of a value greater than 1 .
Error when trying to apply log method to pandas data frame column in Python
I suspect that the numbers are stored as generic ' object ' types , which I know causes log to throw that error .
Find unique rows in numpy.array
I need to find unique rows in a ` numpy.array ` .
` np.unique ` when I run it on ` np.random.random ( 100 ) .reshape ( 10 , 10 )` returns all the unique individual elements , but you want the unique rows , so first you need to put them into tuples : #CODE
This precisely describes how performing unique over rows is performed .
This actually does not work for my data , ` uniques ` contains unique elements .
And then reshape back into a 2D array ( ` -1 ` is a placeholder that tells numpy to calculate the correct number of rows , give the number of columns ): #CODE
NOTE : A previous version of this did not have the ind right after a [ , which mean that the wrong indices were used .
Also , if you want to find unique rows in an ndarray regardless of how many dimensions are in the array , the following will work : #CODE
An interesting remaining issue would be if you wanted to sort / unique along an arbitrary axis of an arbitrary-dimension array , something that would be more difficult .
Then again , finding unique values in a floating point array is risky business anyway ...
Calling ` unique ` with ` return_index=True ` should always be more expensive than doing it without , as it has to do extra work to compute the index array .
Based on the answer in this page I have written a function that replicates the capability of MATLAB's ` unique ( input , ' rows ')` function , with the additional feature to accept tolerance for checking the uniqueness .
It also returns the indices such that ` c = data [ ia , :] ` and ` data = c [ ic , :] ` .
When trying to sum that column via aggregation I get an error stating ' Must produce aggregated value ' .
Sidenote : to make a Panel object , you'd need to broadcast category to be 2d as well .
The problem here is that pandas is checking explicitly that the output not be an ` ndarray ` because it wants to intelligently reshape your array , as you can see in this snippet from ` _aggregate_named ` where the error occurs .
Trying to calcuate mean and std using float32 numpy arrays .
so at some point in the execution it will max my memory .
lexsort zeros before negatives ?
will get lines corresponding to indices ` [ 0 , 1 , 4 ]` .
The same result would be obtained by passing directly these indices , like ` c = a [ [ 0 , 1 , 4 ] ]` .
When fewer indices are provided than the number of axes , the missing indices are considered complete slices ...
For example , if O2 chooses OB and O1 chooses OA , total sum will be smaller .
That would sum to 7 instead of 8 as it stands with your expectation .
Even then , ` A [: , newaxis , :] * A [ newaxis , : , :] ` should yield a 2-dimensional array , so I don't understand ` sum ( 2 )` , because there is no third axis ( or newaxis is a slice , not a single number ) .
I am trying to translate every element of a ` numpy.array ` according to a given key :
set diag elements to zero , then use ` argsort() ` to find the top-K index in a flatten array , and use ` unravel_index() ` convert 1D index to 2D index : #CODE
a dot product or SVD .
` zero ` gives an array of zeros with dimensions you specify : #CODE
The tuple you give to ` zeros ` and other similar functions gives the ' shape ' of the array ( and is available for any array as ` a.shape `) .
The link you give shows the same behavior , except that the ` reshape ` method takes arbitary numbers of positional arguments rather than a tuple .
The number of arguments given to ` reshape ` , and the number of elements in the tuple given back by ` a.shape ` , is 2 .
That said , I would recommend possibly looking at the Continuum Anaconda distribution to just get the majority of the python scientific stack through a simple installer :
When the indexed array a is multidimensional , a single array of indices refers to the first dimension of a .
You are creating a 3D array , where first 2D array ( withing 3D array ) is given by extracting rows from ` palette ` as given by indices of ` image [ 0 ]` and the second array is given by extracting rows from ` palette ` as given by indices of ` image [ 1 ]` .
Python plot log scale set xticks ?
I need to find the positions of all the zeros in these strings and then label them .
Also , I expect the positions of the zeros to be relatively sparse ( ~1% of all bit positions ) .
At worst , the lookup table needs 2 16 ~ 65536 entries - but if zeros are very sparse ( e.g. , at most two zeros in any group of 8 bits only need about ~64 ) .
Then from the DataFrame , just ` stack ` and find the zeros : #CODE
Now create 5-bit bitstrings from each integer and join them together : #CODE
To decode it again there are a number of options , but as everything is the same length the ` cut ` method is useful : #CODE
It should be a bit quicker just by not using so many intermediary bitstrings - it's all done in the join method .
If you are using Python 2.X , change ` sum ( col )` to ` float ( sum ( col ))` , or use ` from __future__ import division ` , or make some other change to ensure that the division is not integer division .
MLP Neural Network : calculating the gradient ( matrices )
What is a good implementation for calculating the gradient in a n-layered neural network ?
you can compute the gradient for one instance ( which is actually a special case of 1 . ) .
The ` dE / dY ` of the output layer is computed ( either for softmax activation function and cross entropy error function or for linear activation function and sum of squared errors ) as ` Y-T ` , where ` Y ` is the output of the network ( shape = [ num_instances , num_outputs ]) and ` T ` ( shape = [ num_instances , num_outputs ]) is the desired output .
_A heads-up for reader_ : for the conjugate gradient method , only the ` dE / dX ` method is required in the ` backprop() ` method
:) But usually nonlinear conjugate gradient also requires ( dEdW , dEdb ) to update ( W , b ) so that E will be minimized .
The implemented code snippet is a scaled conjugate gradient algorithm that uses the Polak-Ribiere formula for updating the search direction .
Hessian = np.dot ( d , gradient ( x + ? *d ) - gradient ( x )) / ?
... and what are gradient ( x ) and initial_gradient in this case ?
I guess gradient ( x ) is actually ( dEdW , dEdb ) for each layer organized in a flat vector .
I didn't fully understand that implementation but I think it is definitely not the standard conjugate gradient which usually takes the derivative of an objective function with respect to some parameter x to update the parameter x ( where x could also be W or b ) .
Originally , ` xdata ` and ` ydata ` are derived from a non-uniform cylindrical grid that has a 4 point stencil -- I thought that the error might be coming from the fact that the same point was defined multiple times , so I made the set of input points unique as suggested in this question .
What are the relative ranges ( e.g. max and min ) of the data you're interpolating ?
Right now I am using using openpyxl.reader.excel to read the excel file and then append to numpy 2D arrays .
If fancy indices ( and scalars are fancy in this regard too , which may be weird but ... ) are all * consecutive * numpy can guess where you want to put the dimensions resulting from the fancy index .
If you have lists or arrays as indices , they must all be of the same shape , or be broadcastable to a common shape .
If there are indices which are slices , then every entry in the base shape array will be multidimensional , so the base shape gets extended with extra entries .
I want to replace all the zeros with the median value of the whole array ( where the zero values are not to be included in the calculation of the median )
Python multiplication Matrix with Matrix transpose with array
J is matrix , J ' the matrix transpose of J ,
I'm assuming you are using ` zip ` because other posts about how to transpose a list of lists in python recommend using this .
This is not what you are using ... you are using ` numpy ` , so you want to use the ` .T ` attribute which returns the transpose of your array .
Additionally , ` dot ` is a numpy function , not a method of a nmpy array : #CODE
@USER You wrote " Additionally , dot is a numpy function , not a method of a n [ u ] mpy array " --- well , dot is a numpy function but it's also a method of numpy arrays , as in
By " merge " , I mean output a new array with the sum of each i , j in each of the arrays in each position .
In the real case they are long series of ` sin ` and ` cos ` functions ...
Like sin ( x ) ^m*cos ( x ) ^n , with m , n arbitrary positive integer coefficients ?
having seen that you want to integrate stuff like sum ( a*sin ( bx+c ) ^n*cos ( dx+e ) ^m ) , for different coefficients for the a , b , c , d , e , m , n , i suggest doing all of those analytically .
( should have some formula for that since you can just rewrite sin to complex exponentials
Another thing i noted when checking those functions a bit better , is that sin ( a*x+pi / 2 ) and sin ( a*x+pi ) and stuff like that can be rewritten to cos or sin in a way that removes the pi / 2 or pi .
because if its only sin n cos n things like that , but with different numbers in it , analytical still is the way to go ? or do you expect future applications to contain besselfunctions n other stuff that might get messy ?
( if it is just with one sin function , it should be as described , but not sure when it is used in the entire code etc )
However you may first need to transpose ` input_array ` to match dimensions .
Apples error log prints #CODE
When the sums overflows , but the mean is within range , how to get an accurate idea of the size of sum fast with integer arithmetic ?
Natural log works in mathematics , but is slow in programming .
I would want to detect the zeros ( really every other zero ) so that I can make the array : #CODE
Byt what happened to negative numbers on the left and positive ones on the right in your example ?
The insert function adds 0's to the beginning of the array , and append adds 0's to the end of the array .
In higher dimensions , where " row " and " column " stop really making sense , try to think of the axes in terms of the shapes and indices involved .
If you do ` .sum ( axis=n )` , for example , then dimension ` n ` is collapsed and deleted , with all values in the new matrix equal to the sum of the corresponding collapsed values .
Furthermore , ` c [ x , y , z ]` is equal to the sum of all elements ` c [ x , y , : , z ]` .
As I wrote , the problem comes from a memory leak in the diagonal function of numpy v .
if I should try to guess what is going on is is the folloing : python have 3 lairs of buckets inside each other where it allocate memory , and it can only ' give back a chunk of memory ' to the os if the outer bucket is completely empty ...
@USER , ` x += ` would work as long as ` exp ` did not read the memory location that was being written to by ` x += ` part .
I thought to first make an array of zeros , then somehow append the array in the loop to have an array of the correct 0's and 1's .
Then I could just sum the array .
Goal : sum the elements that have the same two labels .
If we didn't need to worry about zeros in the divisor , we could simply do this : #CODE
Since we do need to worry about zeros , we can make a " mask " to solve this .
@USER A solution to the least squares problem Ax =b always exists , and it's unique iff the columns of A are linearly independent .
@USER Almost every instance of a gradient method ( e.g. the Levenberg-Marquardt method for non-linear optimization ) depends on an initial guess .
Matlab and numpy norm the eigenvectors in the same way , but the sign is arbitrary and can depend on details of the linear algebra library that is used .
Unfortunately , numpy appears to be reordering them so as to always list the smallest eigenvalue first ; this means that , when I vary a , the eigenvectors get reordered unpredictably , and the dot product numpy.dot ( v , z*w^ 0.5 ) is no longer assigning the same coefficient to the same eigenvector .
It doesn't matter how I do this assignation , since all of the Z's are identically distributed , but the assignation needs to be unique and it cannot depend on the corresponding value of \lambda_i , since that will depend on the parameter a .
Another way of seeing the problem is to look at the characteristic equation , which is a polynomial whose zeros define the eigenvalues .
There is no order of these zeros wrt to the parameter !
Specifically , there might be situations where zeros are identical ( degeneracies ) .
I guess your best bet would be to reorder the eigenvalues of two nearby parameter points by minimizing the distance between eigenvectors ( thus identifying similar ones ) .
However , within the set of allowed eigenvectors spanning the degeneracy , there will be two which satisfy the dot product condition and which can be used as the basis spanning the degeneracy , thus maintaining the mapping .
Why don'y tou simply create a different subplot for each graph and stack them vertically ?
It's pretty clear that ` f ` runs in linear time , and ` f2 ` runs in quadractic time because that's the time complexity of a matrix-vector dot product .
The Cython profiler tells me that , the numpy.min , mean and zeros are expensive time .
Padding with zeros
In the case of " zero " boundary conditions , there's a very simple way to avoid indexing problems : Pad the ` sediment_transport ` grid with zeros .
ValueError : operands could not be broadcast together with shapes ( 654,102 3 ) ( 655,102 4 ) any ideas ?
I know that I can get the nearest index with ` np.abs ( a-value ) .argmin() ` , and I found out that ` min ( a [ np.where ( a-value = 0 . ) [ 0 ]])` returns the desired array value .
Hence , ` np.where ( a == min ( a [ np.where ( a-value = 0 . ) [ 0 ]])) [ 0 ]` would probably give me the desired index .
Sure , I can create an array of zeros and make a loop ( and it is what I am doing by now ) , but it does not sound neither Pythonic nor efficient ( as every looping in Python ) .
contains at least one nonzero value .
nonzero value is the first one in the array , there should be no need to consider
` first ` in order to know that it contains at least one nonzero value .
Sebastian and jorgeca's lead I've done some more benchmarking using ` np.all ` on an array of zeros , which ought to be equivalent to calling ` np.any ` on an array where the first element is one .
This solution works for vertical and 2d arrays , however it outputs horizontal arrays as vertical ones .
The values of the histogram .
Extract unique rows from a matrix in numpy with the frequency of each row that was created
How can i use the unique ( a , rows ) from MATLab at python ?
The answer there explains how to get the unique rows .
One way to transform to a 1D array of floats would be to take dot product with a D-dimensional random vector .
You can count the number of each unique row using fancy indexing and evaluating a condition like : #CODE
Numpy : outer product of n vectors
` outer ` only takes two vector arguments .
` np.ix_() ` will do the outer broadcast , you need reduce , but you can pass the ufunc ` np.multiply ` without lambda function .
` a [: , np.newaxis ]` is another way to add axis , I don't know if it is better than ` reshape ` , but just wanted to point it out .
I would need to find indices ( start , stop ): #CODE
Shift it one to the left ( to get the next state at each index ) using ` roll ` : #CODE
The edge cases aren't correct because ` roll ` is being used .
@USER that's very True , I have this vague memory of thinking I should correct this at the time , need to somehow update the first and last ( ? ) element after the roll ...
So in order make a histogram out of my data , I just do : #CODE
What I want in the end is to multiply a matrix of size NxN by a diagonal matrix that would be fed in as a 1D matrix ( thus , a.dot ( numpy.diagflat ( b )) which I have found to be synonymous to a * b ) .
The kernel is significantly faster because SGEMM does a full matrix-matrix multiply ( O ( n^3 )) , and expands the diagonal into a full matrix .
It simply does a single multiply for each matrix element , and never expands the diagonal to a full matrix .
One approach is CUBLAS SGEMM , but matrix-matrix multiply is overkill when you know one matrix is diagonal .
Actually , my calculation was wrong for the diagproduct kernel : it does not promote the diagonal vector to a 2D matrix , so you would only have 2*400MB + 40KB of data , which should fit in your 1GB GPU .
I would like to create a rank 3 array , using numpy , such that the array resembles a stack of 9x9 rank 2 arrays .
Each of these arrays will be completely filled with ones , twos , threes , etc .
So , looking at one face of the cube we see ones , at the opposite face nines .
To see this is what you want , you can slice for example ` c [: , : , 0 ]` to get a matrix of ones or ` c [: , : , -1 ]` to get a matrix of nines .
Now I want to replace the zeros by the nearest previous element in the same row ( Assuming that the first column must be nonzero . ) .
Without going crazy with complicated indexing tricks that figure out consecutive zeros , you could have a ` while ` loop that goes for as many iterations as consecutive zeros there are in your array : #CODE
I think I prefer using an outer concat tbh .
If you know each of these are the same length then you could create the DataFrame directly from the array and then append each column : #CODE
An alternative here is to create each as a DataFrame and then perform an outer join ( using ` concat ` ): #CODE
Can you provide the complete traceback , instead of the last couple of stack entries ?
It does not seems to be likely case here , but can you try absolute path for a file you are saving ?
If I want the row indices of the maximum value in any given column , ` [ x , xi ] = max ( M )` will return these indices for me as a row vector .
The above will return row vector ` [ 3 2 1 ]` as ` xi ` ; A vector of the indices of each row which contains the maximum value for that column .
For the above example , the first such vector would be the above ` [ 3 , 2 , 1 ]` , ( the indices of the rows with the highest values for each given column ) .
The second such vector would be ` [ 2 1 3 ]` , ( the indices of the rows with the second-highest values for each column ) .
Please take a look at ` numpy.argmax ` , which returns indices of the maximum values along an axis .
Note that this indices are 0-based so you may want to add / subtract 1 to it to make it 1-based as in matlab .
Here the axis is 0 because you want max indices in each column .
Change it to 1 if you want max indices in each raw .
Also , there is ` numpy.argmin ` if you want min .
Based on your clarification , you want nth largest indices in each column , which is very easy with ` numpy.argsort ` .
In which case , Get the indices of the n largest elements in a matrix is almost the same question as this , except there the OP wanted the largest values of the whole matrix , not individual maximums .
The following procedure results in values which do not always match the assigned ones : #CODE
To summarize your problem , adding ` v ` to an array of zeros and then substracting ` v ` doesn't always yield an array of zeros : #CODE
In fact , the indices ` i ` , ` j ` and ` k ` are numpy arrays , and so it's using fancy indexing .
What should happen when a triplet of indices is repeated ?
It turns out that by an implementation detail ( that can change any time ) only the last triplet ( in C order ) of indices will get assigned , and in the end ` vector_field [ i , j , k , :] ` doesn't really contain ` v ` .
You are correct , the repeated indices ( due to the discretisation of the volume ) is the reason .
The very inefficient method that works is ( where minVal and maxVal are the min and Max values in the original 2 byte numpy array , and paddingVal in the original will be set to 0 ): #CODE
Just so you're fore-warned , the version I posted in my comment implicitly takes the floor of the values instead of rounding .
OneHotEncoder to encode my training and test data .
You probably have some categorical features with a much to large cardinality ( for instance a free text field or unique entry ids ) .
I mean OP should not use OneHotEncoder on features that have unique values such as a sample identifier or a free text field .
but unfortuanly the det of ` np.cov ( np.vstack (( x , y )) .T )` is 0 , which means that a inverse matrix does not exsists .
I currently use euclidian distance aka sum of squared differences . but i want to compare the results to other distance measures , to see what fits best into my program .
Some distance measures ( Euclidean ( ssd is square of Euclidean ) , L1 norm , etc ) you can use on two arbitrary vectors but the Mahalabonis distance is derived statistically and needs to learn the covariance matrix from a set of datapoints .
To assemble / grow / accumulate ` N ` column by column I thought of using #CODE
Which you can stack : #CODE
That's better than your loop since ` append `' ing to a list takes amortized constant time , while ` hstack ` takes linear time .
Generating a list of unique numpy arrays
I'm trying to speed up the following code which computes a sum of integrals .
The ` pL_t ` function looks like a sum of gamma distributions , in which case , you should be able to evaluate the integral as a sum of partial incomplete gamma functions : #URL
I'm using pandas to do an ` outer ` merge on a set of about ~ 1000-2000 CSV files .
Each CSV file has an identifier column ` id ` which is shared between all the CSV files , but each file has a unique set of columns of 3-5 columns .
There are roughly 20,000 unique ` id ` rows in each file .
I think you'll get better performance using a ` concat ` ( which acts like an outer join ): #CODE
I am trying to return the mean and max of a for only those cells that have a row / col position in b and it would be nice to know the index of the max value in a , given the mask .
Python : Numpy tile in the 3rd dimension
I would like to tile my array in 3 dimension in order to obtain the following result : #CODE
Here I want to append NumPy arrays with the Triples ` ( name , timestamp , value ) .
When I stop the iteration my list only needs max 10 MB , depending on when I stopped it , but after some few calls to ` self.allEvents.append ( np.fromiter ( ... ))` 300MB are used and I have absolutely no idea why .
Do I simply use axis=1 [ or 2 or 3 ] to stack it in each direction ?
` max ` accept optional ` key ` parameter .
or as a list comprehension : ` Z = [ itemgetter ( * ' xy ') ( max ( lst , key=itemgetter ( ' y '))) for lst in A ]`
I'd use ` itemgetter ` and ` max `' s ` key ` argument : #CODE
Thanks Wes , Tom - let me know if you want me to log this in github or if you want to add it with the results from your tests .
Create a stacked 2D histogram using different weights
Say I want to build up histogram of particle data which is smoothed over some bin range , nbin .
Ordinarily , a histogram of particle positions is a simple case ( using numpy ): #CODE
Is there a way to weight the histogram by some constant such that the plotted heatmap will be a true representation of the density rather than just a binning of the total number of particles ?
It will not broadcast a constant value , so even though the mass is the same for each call to ` np.histogram2d ` , you still must use something like #CODE
If you naively add heatmaps together , the final result will accumulate particle density in the wrong places .
numpy : 1d histogram based on 2d-pixel euclidean distance from center
I want to compute the histogram of intensity values of a grayscale image , based on the distance of the pixels to the center of mass of the image .
All numpy binning functions ( ` bincount ` , ` histogram ` , ` histogram2d ` ... have a ` weights ` keyword argument you can use to do really weird things , such as yours .
EDIT : if I change ( around the 800 line ) the xranges to ' range ' and extend back to append ( see lines 792 and 798 ) I will now get this error : #CODE
I am trying to create a 100 by 100 matrix that will be populated with 1's and - 1's ( at random ) , and the diagonal of the matrix should be all zeros .
so far , I have created a matrix with all 1's and - 1's , but I can't get it to have zeros along the diagonal .
Also , when a matrix is created with numpy , how do i access the values ( if i wanted to sum up values in a matrix ) ?
To create the matrix with ones : #CODE
` #URL New in 1.4.0 and likely a lot faster since the indices are never constructed .
This will give you the random choice between -1 and 1 with 0 for the diagonal .
So reshape the array as follows : #CODE
Yup , sure is - apologies for the dup , didn't think to search ' flatten ' :/
Probably because ( on Python 2.x ) you can just do ` from compiler.ast import flatten ; flatten ([[ 0 , 1 , 3 , 7 , 8 , 11 , 12 ] , [ 8 , 0 , 1 , 2 , 3 , 14 ] , 2 ])` -> ` [ 0 , 1 , 3 , 7 , 8 , 11 , 12 , 8 , 0 , 1 , 2 , 3 , 14 , 2 ]`
And then use any of following methods to concatenate them into one list :
would You try ` sum ` #CODE
You should provide initial value for sum ` [ ]` to start sum from
Vectorizing a numpy array call of varying indices
I have a 2D numpy array and a list of lists of indices for which I wish to compute the sum of the corresponding 1D vectors from the numpy array .
( edit ): Note that the indices are not ( x , y ) coordinates for array_2d , instead indices [ 0 ] = [ 1 , 2 ] represents the first and second vectors ( rows ) in array_2d .
The number of elements of each list in indices can be variable .
Is the last element of ` indices ` supposed to be ` [ 2 ]` ?
I edited the post and explained a bit more what indices represent .
Originally yes , elements in indices can have variable size , but the operation performed on the corresponding vectors in array_2d should work equally well ( things like sums or averages ) , and output a single vector .
First to all , I think you have a typo in the third element of indices ...
The easy way to do that is building a sub_array with two arrays of indices : #CODE
and finally , you can take the sum of sub_arr2d ...
That wasn't a typo , I edited the post a bit to better explain what indices means .
A possible idea is just using concatenation of the indices array along the ( 2nd ? ) axis , but that doesn't seem like a very elegant solution .
When you roll the axis , the memory layout of the data is not changed , only the strides of the array .
Suppose I wanted to flatten the 2nd and third axis .
You could reshape your array after rolling the axis .
I am trying to create a sigma sum in python .
The sum should look like so .
so , if I wanted h67 , the sum would be : #CODE
Looks like you want a dot product .
I don't think you need to pass two indices to your function .
Finally , you need to return ` hi ` , not ` sum ` .
It looks like a vector product followed by a sum along the resulting array .
The numpy sum() function just return the sum of all the elements in an array .
The parameter you are giving to it is just a case and not an array to sum .
So you are returning the sum of one element : this element .
I need to calculate a std mean from a time series ( monthly frequence ) , but i also need to exclude from the calculation the " incomplete " Years ( with less then 12 moths )
Now we deal with getting the indices in the correct data type and aligned .
File " / usr / lib / python2.7 / dist-packages / numpy / linalg / linalg.py " , line 445 , in inv
z = zeros ( s , x.dtype.char )
Thank you , I have tried scipy function for reading the image , but result is the same , see the log error output
I set flatten = true for scipy.misc.imread function , same memory error occurs , thanks
Suppose I have a two-dimensional array -- just an array of 2d position vectors , and suppose I want to find the norm of each vector , and if it is over a certain value , do whatever .
` np.where ` seems to be a great solution for this but it won't work unless I transpose the array : ` np.where ( sum ( a.T **2 ) 10 , a * 2 , 0 )` ( Just an arbitrary example ) .
This really seems verbose and the transpose doesn't make much sense .
I can't , though , say , for example , create a new numpy array from each element sum it ?
Say you have a numpy array of 2d vectors and you want to calculate the norm of each vector .
That way I know if I want to concatenate blocks of data I should be using numpy.hstack .
So , to test out methods , creating a 3x30000 array with 10000 unique rows : #CODE
While unifying , I need to build a list of unique vertices ; if a point already is in the list a reference to it must be stored .
On my machine , ` unified , ref = unify ( raw_data )` requires about 51.390s , while ` uniq , inv = unify2 ( raw_data )` requires about 0.133s ( ~ 386x speedup ) .
I'm trying to import an image to an array , sum the values of each row , then plot this in a histogram .
an ` and ` can be redefined as a min function providing keys -1 , 0 , 1 for 0 , nan and 1 respectively . using the same keys ` or ` is implemented as max .
` g = nx.from_numpy_matrix ( mat , create_using= nx.DiGraph() )`
I think writing ` mat ` is O ( nlgn ) as we have n rows , reading from a database ( btree search ) is O ( lgn ) , and writing ` mat ` is O ( 1 ) .
I think it would be easier to just write a preprocessing script which rewrites your old data file ( by adding zeros into the missing columns ) into one that can be read by np.genfromtxt .
The issue I am running into is how to properly substitute for initial conditions given a these equations were I don't know which values are present ( i.e. which ones must be substituted ) .
If you convert your 2D coordinates into ` target_map ` into flat indices into it using ` np.ravel_multi_index ` , you can use ` np.unique ` and ` np.bincount ` to speed things up quite a bit : #CODE
To get the unobserved values filled , we'll use the ` unstack ` and ` stack ` methods .
Unstacking will create the ` NaN ` s we're interested in , and then we'll stack them up to work with .
` stack ` does have a ` dropna ` argument .
I'd like to scale the array so that the max value of the a dimension is 1 like this :
As max ([ 0.2 , 0.1 , 0.1 , 0.1 ]) is 0.2 , and 1 / 0.2 is 5 , so for the first element of the int tuple , multiple it by 5 .
As max ([ 0.3 , 0.5 , 0.3 , 0.4 ]) is 0.5 , and 1 / 0.5 is 2 , so for the second element of the int tuple , multiple it by 2
If you have an array of indices , you can use it to vectorize the calculation : #CODE
Note that a is a NumPy array from the start , and the the indices I use are in the form ` a [ list_of_rows , list_of_columns ]`
Then rather than you iterating over indices in sfcube : #CODE
But it doesn't work as the dot function very rarely returns 0 which should be in case the vectors are orthogonal .
I simply fix j-1 random elements for the coefficients of the orthogonal vector and then in order to find the last coefficient i solve a simple equation which is the dot product of the previous coefficients of the random elements with the coefficients of the vector divided by the last coeffient .
The results from the last dot product computation i am getting are in this form : #CODE
How big is the result of ` dot ` ?
So the dot product is not always 0 .
Can i force dot product to a specific precision suth that i am getting rid of this weird results .
You can actually make it even faster , treating the multiply , then sum part as a dot product : ` from numpy.core.umath_tests import inner1d ; data2 = np.random.rand ( * data.shape ); quotients = inner1d ( data2 [: , : -1 ] , data [: , : -1 ]); data2 [: , -1 ] = -quotients / data [: , -1 ]` .
@USER but is not 1 in a trillion it happends ~32 times per 173 dot product computations
There's also ` np.c_ ` to column stack : #CODE
How can I find the indices of the values out of the range I have set ?
For example if my list is [ 5 , 5 , 8 , 9 , 0 , 10 , 3 ] and low=4 I want to return the indices bellow 4 which for this case would be [ 4 , 6 ]
To get the indices of the ` True ` s you can use ` np.where ` or ` np.nonzero ` : #CODE
Note that what gets returned is a tuple of arrays , so for the 1D case you will probably end up doing something like ` indices = np.where ( y low ) [ 0 ]` .
Have you googled for " numpy transpose " ?
So , it is not directly the transpose I guess .
Using transpose swaps the first and last axes ( 0 and 2 ): #CODE
I found out that a file with 5000 lines max can be processed .
I can ` sum ` all the elements along a specific axis , using ` numpy.sum ` , that is #CODE
That is ` sum along row ` , which add elements of each column one by one .
So you would have to calculate the stride corresponding to the axis , and then step through the array while calculating the sum .
If you'd like to know a bit more , you could read chapter 15 ( no real need to read all the previous ones ) of the [ guide to numpy ] ( #URL ) , which starts of with a section on numpy array iterations as it's done in C .
So you would have to calculate the stride corresponding to the axis , and then step through the array while calculating the sum .
For each sum , you start at an offset in the array ( the first would be 0 ) , that increases with yet another step size .
If you'd like to know a bit more , you could read chapter 15 ( no real need to read all the previous ones ) of the guide to numpy , which starts with a section on numpy array iterations as it's done in C .
Upvoted , furthermore , I found out that most of core methods in ` numpy ` and ` scipy ` are actually implemented using ` C / C++ ` , such as ` convolve ` and ` correlate ` , Why is that ?
Specifically , are these max values available through the numpy API ?
If you're on Linux , use the translate utility tr
To get the items you want you should use a ` 2D ` array of indices .
You can use an ` outer ` to make from your 1D ` idx ` a proper ` 2D ` array of indices .
Recalling that ` True*True=True ` and ` False*True=False ` , the ` np.multiply.outer() ` , which is the same as ` np.outer() ` , can give you the ` 2D ` indices : #CODE
But for now , let's assume I've got an NumPy array filled with zeros #CODE
Now I want to change some of these zeros with specific values .
Now I've used a diagonal index , but what in the case my index is not just diagonal .
However , NumPy array indexing works differently : It still treats all those indices in a 1D fashion , but returns the values from the vector in the same shape as your index vector .
This will fetch all elements on the main diagonal of your matrix .
Below is the interesting part of travis log .
This library , for instance , uses travis-ci for testing and depends on the full scipy stack ( numpy , scipy , matplotlib , pytables , pandas , etc ) , which is installed via ` apt ` with ` language=c ` .
Here's an approach from Dan Balchard using Miniconda , that will pre-install matplotlib and the rest of the scipy stack on your Travis CI test machine .
Then post the stack trace for us .
You should see the ` fwrite() ` as in your stack trace near the end .
When I installed debugging symbols , it looks almost identical to [ another stack trace I found ] ( #URL ) .
With the code you have posted , ` ynew ` ends up being an empty array : you never append anything to that list .
int_range= np.linspace ( min ( xnew ) , max ( xnew ) , len ( xnew ))
The max is 255 and the min is 0 so it is RGB .
Importantly this displayed a 3D surface with the colouring not dependant on the height ( it is a gradient in on direction ) .
It's less of a hassle to roll the dimension to a set position ( the first is the most convenient ) and then have always the same iteration machinery , see my answer .
To iterate over the others , roll that dimension to the front and do the same : #CODE
I've looked into random.sample , and numpy.random.shuffle and numpy.random.permutation , all of these work , but usually they return the whole permutation or at least generate the entire range ( n ) .
So it might take O ( n ) to generate the permutation but you'll only require O ( m ) memory .
Generating indices using np.triu_indices
I suspect that there is a way to implement this without needing to reference indices [ i ] in the zip call .
If you want the triangular values offset from the main diagonal by ` k ` columns , then do #CODE
The only thing left to get the same as the OP function returns , is to stack the three arrays together and transpose them : ` np.vstack ( indices+ ( adjmat [ indices ] , )) .T `
` indices + adjmat [ indices ]` gives me a result I understand , but I do not understand why ` indices + ( adjmat [ indices ] , )` performs a union , instead of vector addition ( adding nothing to the second array in indices ) .
` np.vstack ` needs a tuple ( or list ) of arrays to stack , so by adding the single element tuple ` ( adjmat [ indices , )` to the ` indices ` tuple , we concatenate them and create a three element tuple , that gets stacked , and then transposed to fit your original output .
Since ` np.fromiter ` always creates a 1D array , to create higher dimensional arrays use ` reshape ` on the returned value .
Get indices for all elements in an array in numpy
I'm trying to get a list of the indices for all the elements in an array so for an array of 1000 x 1000 I end up with [( 0 , 0 ) , ( 0 , 1 ) ,..., ( 999,999 )] .
So now all we need to do is to fetch the last two columns of those rows and stack them to the ` Z2 ` array : #CODE
As you can see I would like to make a sum over some indices of my multispace python array .
Off course I have a problem of not multiplying the same indices so I have redefying it .
Then when the multiplication is done I guess that I will have to do some sort of sum to reduce the dimensionality .
Maybe by making multiple dot product on every dimension we want ?
There is a ` np.tensordot ` than does the multiple ` dot ` ...
The syntax can be a bit tricky to get your head around , so I've simplified the names of your variables and indices a bit : #CODE
output by giving the output indices as well .
where ` trTab [ 0 , :] ` holds all possible ids ( integers ) , while ` trTab [ 1 , :] ` their translations that will be used later .
The ids in both rows of ` trTab ` are unique .
Then I need to translate all ids in the first column of a frame numpy array , say #CODE
` frame ` can be of different number of rows , in fact I'll need to translate a long sequence of frames .
The ids in ` frame ` 0'th column do not have to be in order , and not all possible ids need to be used .
The fancy indexing using the ` translate ` array will take the right elements in the the second line of ` trTab `
But that would amount to trying to translate something that is not in the translation table .
but translated gives just the indices , not the translated values .
The variable maxLoc will hold a tuple containing the x , y indices of the upper lefthand corner of the best match .
I've got a 900 x 650 2D numpy array which I'd like to split into 10 x 10 blocks , which will be checked for nonzero elements .
1-says " IndexError : too many indices " pointing at [: , col ] 2- I changed [: , col ] to [: ] [ col ] and it seems to do the job , but I don't know where to insert key=natural_keys to sort it the right way .
As I said , compiler says " IndexError : too many indices " if I run your line . now if I run this ** m [ np.array ( m [: ] [ 1 ] .argsort ( axis=0 ) .tolist() ) .ravel() ] ** , it extract the first row of my array .
However , I know that there must be some way to do this with just a single call to dot , right ?
I'm not sure if there's a neat way to slot the normal dot product method into a larger array .
The d*d is of course the standard dot product operation since it's just elementwise multiplication of all entries in the array .
The ` axis=1 ` argument sums along the second axis ( horizontally , as the array is printed ) , which is the second part of the dot product operation .
I understand why it is faster than ` diag ( dot ( d , d.T ))` ( because that does unnecessary calculations ) , but can you explain why it is much faster than ` sum ( d**2 )` ?
Moreover , ` np.einsum ` is especially designed for taking a sum of products .
Numpy has a hidden little gem , ` inner1d ` , which does the dot product , with broadcasting , over the last dimensions of its arguments .
You can use ` np.concatenate() ` specifying which ` axis ` to append , using ` np.newaxis ` : #CODE
I want to generate the following complete matrix , maintaining the zero diagonal : #CODE
I generated a lower triangular matrix , and I want to complete the matrix using the values in the lower triangular matrix to form a square matrix , symmetrical around the diagonal zeros .
I want to generate the following complete matrix , maintaining the zero diagonal : #CODE
You can simply add it to its transpose : #CODE
I would like to identify the row with the highest average , excluding the diagonal zeros .
Note that the presence of the zeros doesn't affect which row has the highest mean because all rows have the same number of elements .
As pointed out by a lot of people , presence of zeros isn't an issue as long as you have the same number of zeros in each column .
Just in case your intention was to ignore all the zeros , preventing them from participating in the average computation , you could use weights to suppress the contribution of the zeros .
You can always create a weight matrix where the weight is 0 for diagonal entries , and 1 otherwise .
The presence of zeros can indeed affect the columns ' or rows ' average , for instance : #CODE
You can correct the calculated mean using the ` lcm ` ( least common multiple ) of the number of lines with and without the diagonals , by guaranteeing that where a diagonal element does not exist the correction is not applied : #CODE
My approach would be to reshape the array to flatten all of the higher dimensions and then run the mean on axis 1 .
I am guessing this will perform similarly to the other proposed solutions , unless reshaping the array to flatten all dimensions triggers a copy of the data , in which case this should be much faster .
If you just wish to get rid of the blanks , rather than slice on them , then just compress your array with the selection criteria being a check for not nan .
I do not necessarily need to slice the array but at least I'd have to know the indices of start- and end of each non-nan-section , so I could refer to the sections by start- and end-index ( this would even be possible without the slicing ) .
To support this , store the node-name as a list to give them mapping indices .
Given the indices - what is the most efficient to way to create the numpy array from this structure ?
I thing the structured array is what you mean by using mapping indices ...
Is there a way to create the structured array such that it is accessible as b [ ' x '] [ ' y '] , and thus bypass indices completely ?
I would like to identify the column with the highest average , excluding the diagonal zeros .
I [ added this answer ] ( #URL ) the takes into account the exclusion of the diagonal of zeros
You can always create a weight matrix where the weight is 0 for diagonal entries , and 1 otherwise .
When you transpose the array , instead of shuffling data around , only the strides are changed , which means that now it is columns that are stored in consecutive positions in memory .
I need to append the name of image which has been processed in Python to the output .csv file .
How can I identify the minimum value in a numpy array , excluding the diagonal zeros ?
I want to be able to identify -9 as the min ( x ) , not 0 .
` min ( -9 , 0 ) == -9 `
Set the diagonal to the maximum value of its ` dtype ` .
Take the ` min ` .
Set the diagonal back to zero .
You only need to worry if you min won't be negative .
The following-the-numpy-docs way is to use a list comprehension on the flat iterator of the broadcast object .
I'm surprised to see , that the ` broadcast ` call can actually be omitted .
I am not sure why you reshape out- its what is causing the zero rank array to return to a scalar , but if its a problem .
> I am not sure why you reshape out ...
Without reshape ` step ( 1 )` would return ` array ([ 1 ])` , which is a 1-D array .
With reshape , it returns ` array ( 1 )` , which is a scalar .
Order record array by unique occurances
I'm having difficulty piecing together some logic to order a numpy record array by unique occurrences , such that the first record occurs most frequently and the last as least frequent .
Argsort will find the indices sorted from lowest to highest : #CODE
Hence , we have our " outer " dictionary , whose keys are all the possible states .
Have you tried giving the absolute path to the LAPACK lib ?
The idea behind it is to sum the values corresponding to a particular value in ` column 1 `
I essentially want to sum up the values ` 12+9 ` for since it has the same column 1 .
To sum it up I am assuming I can create a list with #CODE
append the value to that list #CODE
and use numpy to sum it up #CODE
Then you can get sum for each key by : #CODE
You can group and sum based on the values in ` col2 ` with just a couple of lines of code : #CODE
I am using scipy to create csr sparse matrix and want to substract this matrix from an equivalent matrix of all ones .
For example ` outer ( a , b )` or ` a.dot ( b )` but I'd like to differentiate for my own convenience .
Thus , your tests should be done with ` array ([[ 1 , 2 , 3 ]])` , instead , which is not equal to its transpose .
" :) You can treat rank-1 arrays as either row or column vectors . dot ( A , v ) treats v as a column vector , while dot ( v , A ) treats v as a row vector .
Keeping it to 2 says that it will be a ( 4 , 1 ) and transpose will be ( 1 , 4 ) .
array ([ 9946 , 9867 , 10155 , 10127 , 9905 ])` to show the even distribution of numpy's rand function .
Is there any way you can strip this down to something small enough to post complete code and source data that still exhibits the problem ( i.e. , an [ SSCCE ] ( #URL )) ?
To see if you are really at a line which begins with Mod# just do ` if line [: 3 ] == " Mod# "` provided that you have used strip on it before .
Consider separating the operations : first create DataFrame ( s ) from xml , maybe then merge / concat / reshape , then do computations .
Python - fast way to sum outer products ?
I'm looking for a fast way to calculate a sum of n outer products .
What I'd like is to calculate the outer products of each vector of size v in A and B and sum them together .
The best I can do is create a loop where the outer products are constructed , then summed later .
This creates an n x v x v array ( the loop is within the list comprehension , which is subsequently converted into an array ) , which I can then sum together by using ` np.sum ( outers , axis = 0 )` .
But , as an aside , I do not find that ` A.T * B ` or ` A * B.T ` broadcast successfully .
( Python ) How to get diagonal ( A*B ) without having to perform A*B ?
We wish to get only the diagonal entries of ` C ` , which can be done via ` np.diagonal ( C )` .
However , this causes unnecessary time overhead , because we are multiplying A with B even though we only need the the multiplications of each row in ` A ` with the column of ` B ` that has the same ' id ' , that is row 1 of ` A ` with column 1 of ` B ` , row 2 of ` A ` with column 2 of ` B ` and so on : the multiplications that form the diagonal of ` C ` .
I want to ' unscramble ' ` A ` by reordering it in the dimensions ` {x0 ,..., xn} ` according to a corresponding array of indices ` ind ` , where ` ind.shape = ( n , A.size )` .
You essentially have the rank of each item in the form of n indices , ie a = ( 0 , 0 ) , b = ( 0 , 1 ) , c = ( 0 , 2 ) , d = ( 1 , 0 ) ... and so on .
Storing complete log in / Users / amormachine / .pip / pip.log
Both free and both have one-click installers that will install a clean version of python with most of the scientific stack outside of your system python .
u , s , vt = svd ( a , 0 )
File " C :\ Python27\lib\ site-packages \numpy\linalg\ linalg.py " , line 1278 , in svd
To follow up @USER ' s answer ; the following function produces an argand plot that is centred around 0 , 0 and scaled to the maximum absolute value in the set of complex numbers .
If you pad with zeros , or trim your data to the nearest power of 2 , everything should run much , much faster .
Can you get a stack trace ?
By looking at a stack trace it should be possible to see if the program is stuck waiting for something , stuck in a loop somewhere or just doing lots of work .
Being used to read stack traces is of course a plus .
The process does take a lot of CPU when it's stuck on that like , but there is no stack trace -- it doesn't throw any error .
If you pad with zeros , or trim your data to the nearest power of 2 , everything should run much , much faster .
and this is what I get as output just saving in a simple log file : #CODE
Looking at your input you have a lot of zeros ... keep in mind that the print out doesn't show all of it .
You may just be seeing all the " zeros " from your input .
So transpose your ` data_array ` : #CODE
A similar trick can be used to compute the sine , either from theta or from cos ( theta ) ( or vice versa ) .
Unfortunately there's no way good way to do a running sum and bail early if either of the two addends is greater than the right hand side of the comparison .
I want the y values of all the bins to sum up to one .
probability density , the integral of the histogram should be 1 ; you
Just a note : if your example is using array indices , then the coordinates of * s are ` ( 2 , 0 ) , ( 2 , 2 )` and @ is ` ( 0 , 2 )` .
Create one of these for each important point , then ` np.sqrt ( sum ( dist_arrays ))` to get the combined distance from all important points .
IndexError : too many indices - tried to use your adaptor idea but to no avail , the error remains .
try to get better view what is your AI / ML-Model Overfit / Generalisation ` [ C , gamma ]` landscape
it uses default ` [ C , gamma ]` values and thus have no relevance to behaviour of your Model / ProblemDOMAIN .
As you can see , while my original ` x ` array contains ` [ 9 , 4 , 7 ]` , no way I've attempted to stack the arrays and then index by `' x '` returns the original ` x ` array .
I guess the scipy correlate function pads the small image into zeros ?
You take its absolute difference .
Have you tried setting your ` x ` vector equal to the indices for which you have data , and your ` y ` vector equal to the values at those indices ?
if you check the max and min values , you get : #CODE
It needed imports for json and sys , and .tolist() to deal with getting json to encode the matrix result ( json won't encode matrix as-is because it isn't a simple array ) .
You can use the permutation matrices ( that's the numpiest way to partially reverse an array ) .
If you don't like the off by one indices #CODE
Column stack and row stack with H5py to existing datasets
I am trying to use Python to column stack and row stack data I have in an HDF5 file with additional data .
Therefore , I would like to be able to make one dataset in a new file and stack together all of the arrays from each image file into the single file .
My question is how can I column stack the data from the HDF5 file with the second array ( arr2 ) such that arr2 is saved to the file ?
Here is my new script structure that allows me to stack arrays in the HDF5 file : #CODE
I've seen there is a ` scipy.signal.deconvolve ` function that works for one-dimensional arrays , and ` scipy.signal.fftconvolve ` to convolve multi-dimensional arrays .
Randomness aside , working with indices is more efficient than searching the entire list for ` x ` all the time .
The poly can be of any order , which will probably give better results for most of the cases ...
can you post the full stack trace ?
Assuming ` sequence.apply ` applies the lambda to each element in the sequence , ` sequence.apply ( lambda x : x 0 )` yields a sequence of Boolean values , and ` sequence.apply ( lambda x : x 0 ) .apply ( lambda x : sum ( x ))` attempts to sum each Boolean value , resulting in a `' bool ' object is not iterable ` -kinda error .
Hmm very bizarre , sum ( True ) actually works for me ( not sum ([ True ]) , however both work on my pc ) .
I used another IDE , sum ( True ) now returns an error .
The fit compared to the histogram looks ok , but not very good .
The parameter estimates are a bit higher than the ones you mention are from R and matlab .
It is easy to verify which result is the true MLE , just need a simple function to calculate log likelihood : #CODE
The result from ` fit ` method of ` exponweib ` and R ` fitdistr ` ( @USER ) is better and has higher log likelihood .
BTW1 , MLE fit may not appears to fit the distribution histogram tightly .
It doesn't need to visually fit the histogram well , that will be something minimizing mean square error .
that at least gets rid of the zip statement , But not sure if the zip statement takes down your time by one min , or by nearly two hours ( i know zip is slow , compared to numpy vstack , but no clue if that would give you two hours time gain )
This redefines the builtin function ' sum ' which I'd like to access .
And how come numpy.sum would overwrite the builtin sum ?
When I open ipython and import numpy manually , I need to call numpy.sum for the numpy sum and sum for the built in sum .
That would overwrite the built in sum .
I can currently achieve this by cropping the window to centre the smaller windows with the bigger ones ( so for example if I have a 256x256 image , I crop it by 6px from every direction , getting a 244x244 image ) .
I couldn't reshape one_point_2d as 6x2
Of course you can't reshape one point to 6x2 .
If you've got the wrong shape , just transpose it by sticking ` .T ` on the end ( e.g. , ` last_points_2d = np.array ([ last_points.real , last_points.imag ]) .T `) .
Python - creating a histogram
I would like to plot a histogram of a particular form , which I will use an example to describe .
I want a histogram with :
How do I create such a histogram using matplotlib ?
histogram usage is e.g. here :
I'd create this special data structure you want beforehand , then feed it into the histogram : #CODE
However it is not an histogram as you refer in your question .
I actually do not understand why you are talking about a " histogram " .
I was wondering if there is a way to join an numpy array .
I need to join these array , but the Numpy documentation says if the records are not unique , the functions will fail or return unknown results .
Does anyone have any sample to do a ` 1 : M ` join instead of a ` 1:1 ` join on numpy arrays ?
What exactly do you mean by " join " .
Think of each array as a table , and I need to join Column 1 in array 1 with Column2 in array 2 .
Example [ 1 , c , d , alpha , beta , gamma ] and [ 1 , c , d , t , y , u ] .
These libraries come in many different versions , including the highly portable ones available from netlib , and other which are optimised for particular systems .
divide by arrays containing zeros , python
triangular matrix plus the diagonal elements .
reshape function in ` numpy ` but that is not achieving this result .
I want to get the sum for a column from that file and also get ` sm1 ` ( which gives me growth rate over time t ( in days ) .
While the last one gives sqrt (( VVm-VVs ) ^2+ ( HHm-HHs ) ^2 ) , while the others give ( VVm-VVs ) ^2+ ( HHm-HHs ) ^2 , This is not really important , since otherwise further on in my code i take the minimum of R [ i , :] for each i , and sqrt doesnt influence the minimum value anyways , ( and if i am interested in the distance , i just take sqrt ( value ) , instead of doing the sqrt over the entire array , so there is really no timing difference due to that .
I think it does sqrt (( VVm-VVs ) ^2+ ( HHm-HHs ) ^2 ) ^2 ( the only reason i can think why it would be slower than ( VVm-VVs ) ^2+ ( HHm-HHs ) ^2 - I know its a stupid reason , anyone got a more logical one ? )
I just took the lines which are of the form a+ =b or *= , and looked up how they would be in C , and put them in the blitz statement , but i guess if i put lines with the statements with flatten and newaxis in C as well , that it should go faster too , but i dont know how i can do that ( someone who knows C maybe explain ? ) .
Well , i was more like thinking at something like numpy.linalg.norm or so , which does the squaring etc in the internal C , but the problem is that that takes the norm as if it were an entire matrix , so results in 1 number , instead of in the thing i want , which is a matrix ,
Whenever you have multiplications and sums , try to use one of the dot product functions or ` np.einsum ` .
Since you are preallocating your arrays , rather than having different arrays for horizontal and vertical coordinates , stack them both together : #CODE
Your fastest way still is slower than mine , but i think it is because in my case , i just do ( x-x ') **2+ ( y-y ') **2 , ( later on , i get the minimal value from it , but taking the sqrt of it wont change the place of the minimal value , so i dont do that in my calculation , while the cdist does that too ( i assume ) , ( the one [ 15 ]: times out at 15.7 sec , while mine times out at 11.8 sec , but i assume its due to the sqrt being taken in the cdist routine , but i really think this way , if theres some routine like that without the sqrt in it , it would end up much faster
the one where you just use R= spdist.cdist ( precomputed_flat.T , measured_flat.T ) .T times out at 15.8sec , while the one with sqeuclidean times out at 17.2sec , which makes me think that in the implementation of sqeuclidean they take sqrt ( a**2+b**2 ) **2 , instead of just a**2+b**2 , because otherwise why would doing a calculation less actually result in needing more time
` inner1d ` will be written in C , and is probably just broadcasting plus calls to the dot product functions of the BLAS library .
You would have to do something like ` numpy.tensordot ( deltas , deltas , (( 2 , ) , ( 2 , )))` , but the return would be a rank 4 tensor , and your answer would be buried into some diagonal plane of it .
You can skip the diagonal indices completely .
Another way to create a symmetric matrix from just the upper or lower triangular is just add the transpose of the half that you have .
numpy histogram with 3 variables
This returns a list of tuples of slices enclosing all of your objects , in 50% more time thn it takes to find the indices of one single object .
You would still need to figure out the collisions , but you could get about a 2x speed-up by computing all the indices simultaneously using a ` np.unique ` based approach : #CODE
I defined one new matrix ( filled with zeros ) and three labels , each referring to the same matrix .
> diff in location b-a -125520
> diff in location c-a -173376
> diff in location B-A 72
> diff in location C-A 144
The main " symptom " of that is that , if you perform , say , an average or a sum on an array containing NaN , even a single one , you get NaN as a result ...
I usually read / translate NaN as " missing " .
I never tried dealing with the raw pointers , SWIG knows enough about std :: vector , you should be able to wrap myfun ( std :: vector & , std :: vector ) without a hassle .
I can reshape the rows and display the images with matplotlib.pyplot .
Background : I want to translate the following snippet from Python to Matlab #CODE
However , it'll likely work fine though you won't be able to specify a gradient via ` optimset ` because Nelder-Mead doesn't use them .
If you need gradients , try ` fminunc ` -the large-scale algorithm is even [ based on conjugate gradient methods ] ( #URL ) .
Python buggy histogram ?
maybe the actual range of values in x exceeds that of xout , can you try to do ` np.histogram ( x , bins-100 )` to see whether the sum of y comes out right ?
and then call the histogram function : #CODE
And of course if you want to skip ` new_im ` , you can do it in one line , and still only ` flatten ` is doing any copying .
After this , you can flatten your array , ravel is a clear function for this purpose .
As an aside note , ` np.rollaxis ` or ` np.transpose ` return a view of the original data , but when calling flatten on that view , a copy is likely to be triggered .
If you want to output this in a 4x2 shape instead of 1x8 , you need to reshape the array first : #CODE
If you want to write it in 1 row of 8 columns , first you need to reshape it into something with 1 row of 8 columns instead of 8 rows .
This result should be very similar both for numpy-MKL and plain numpy because MKL is a fast LAPACK implementation and here you're not calling any function that uses it ( MKL definitely helps when solving linear systems , computing dot products ... ) .
And then just go through ` A ` and find corresponding indices : #CODE
If some of the items in ` A ` are not in ` B ` ( which you could check with ` np.all ( np.in1d ( A , B ))`) then the return indices for those values will be crap , and you may even get an ` IndexError ` from the last line ( if the largest value in ` A ` is missing from ` B `) .
Is B always the unique array of A ?
B is always the unique of A .
But I want it to give all indices : ` 0 , 3 , 5 ` .
As documentation of ` np.argmax ` says : " In case of multiple occurrences of the maximum values , the indices corresponding to the first occurrence are returned .
the output I need : ` S = [ 2 , 5 , 8 , 11 , 14 ]` I thought something like : ` S1 = np.array ( L [: ] [ 1 , 0 ])` should work but whatever I try I have the error like : ` TypeError : list indices must be integers , not tuple ` .
I have a python script that reads two tiff images and finds unique combinations , counts the observations and saves the count to a txt file .
You can ` reshape ` and then average over an axis : #CODE
If not provide , the average of the flatten array will be calculated .
Split numpy array by unique values in column
I've been trying to find a way to use numpy's vsplit method using a list I generated of unique ID values ( ` id_list = list ( set ( a [ ' id ']))`) , however I get the erorr : ValueError : vsplit only works on arrays of 2 or more dimensions .
I understand why attempting to broadcast a ( 2 , 1 ) array into a ( 2 , N , N ) array fails , and that I have to reshape the ( 2 , 1 ) array into a ( 2 , 1 , 1 ) array before this broadcasting goes through .
My question is : is there any way to tell Python to automatically pad the dimensionality of an array when it attempts to broadcast , without me having to specifically tell it the necessary dimension ?
I don't want to explicitly couple the ( 2 , 1 ) vector with the multidimensional array it's going to be broadcast against --- otherwise I could do something stupid and absurdly ugly like ` mult_v_A = lambda v , A : v.reshape ([ v.size ] + [ 1 ] * ( A.ndim-1 )) * A ` .
Do you want to reshape ` v ` to have shape ` ( 2 , 1 , 1 )` or shape ` ( 1 , 2 , 1 )` ?
Forcing an explicit reshape is a better general procedure , and leave it to the user to write a special function to perform the reshaping automatically if the user has a fixed convention .
This ambiguity is readily solved by specifying that the first non-singleton dimension will be used in the broadcast .
Assuming I didn't want to reorganize the code ( and my users ) to expect row arrays , I am debating whether a function that swaps dimensions to do a broadcast multiply is really any less complected than a reshape operation that intertwines ` A ` and ` v ` in my original post .
The last transpose returns everything to the right order .
Knowing that transpose reverses all dimensions makes this much neater .
Im using this list for testing purposes , but I have one condition , I need to append to the list the 1st number every time it generates .
It is interesting that concatenate and hstack have such a large difference in timings , do you have any idea why that would be ?
If you want to create new matrix of some other shape , you'll have to reshape the resulting 1d array .
The basic thing , though , is that if you want to return an array , you can't use vectorized numpy functions like ` cos ` in some cells , while putting literal scalars ( like 0 ) in other cells .
Thank you for your answer , I saw that the correct way to transpose it would be ` arr.transpose ( 2 , 0 , 1 )` .
Perhaps ive asked this question in the wrong forum and should repost on stats stack overflow to ask how to measure the normal distribution of real values rather than to test if a set of random numbers are normally distributed .
Which means we don't have to have a multiple copy of each data item to do the sum as you indicated in your question .
Just so you know , some of us , in Python , do iterate over indices and not modified objects when it is helpful .
What I actually need is to know the syntax how to modify array mnX by modifying it column by column in place without using column indices .
In fact , it's precisely " modify [ ing ] array mnX by modifying it column by column in place without using column indices " .
I am very new to programming in python , and im still trying to figure everything out , but I have a problem trying to gaussian smooth or convolve an image .
I should have a histogram with 9 bins .
You can also do a 3D histogram : #CODE
3D Bar histogram : #CODE
Any chance you know how I can display this as a normal 3D histogram with bars .
` concat ` , ` merge ` , ` join `
So far I've only been able to combine the values , ( ie . add the elements together , append the series to each other , or merge based on values ) .
Can numpy einsum() perform outer addition ?
In numpy , we can perform " outer addition " between two vectors ` a ` and ` b ` like this : #CODE
I can't think of a faster way to do it than the plain sum using broadcasting that you wrote , is it too slow ?
Should I be using a histogram or empirical CDF or something completely different
I have a ` for-loop ` that repeatedly calls ` roll ` and I want to invert the order of the created arrays .
rolling everything all the way round ( but one ) and also printing before the first roll ( so you get ` c ` as ` range ( 5 )` for the first line ) .
transpose : #CODE
Polynomial fitting problems in numpy
I suppose I could iterate over them and stack them together in the right way , but my actual data files are pretty large and would rather not have too many while loops if possible .
Use reshape .
A brute force way is to check every pairs ( which are symmetrical given the main diagonal of the matrix ) and see if an edge is missing between the two .
Another option I've considered is to truncate all the numbers that end in " .0 " with sed , which will work , but is more of a hack than a real solution .
Why you don't want to read them as floats , and truncate after the fact ?
I am trying to create a function that will take a sigma sum of values in ` x ` and store them in ` y ` .
For instance , ` y [ 0 ]` should be the sum of ` x [ 0 ] *x [ 0 ] + x [ 0 ] *x [ 1 ] + x [ 0 ] *x [ 2 ] + ...
Similarly , ` y [ 1 ]` should be the sum of ` x [ 1 ] *x [ 0 ] + x [ 1 ] *x [ 1 ] + x [ 1 ] *x [ 2 ] + ...
Similarly , in the sum for ` y [ 1 ]` , ` x [ 1 ] *x [ 1 ]` must be zero .
This is my code , but it always gives me some sort of error regarding indices : #CODE
( For one thing , you're missing an end parentheses in the ` sum ` line ) .
Now it returns y as a list of zeros , but that is not supposed to be the outcome .
By "` x [ i ] *x [ i ]` must be zero " , do you mean that ` x ` must consist entirely of zeros , or are you trying to say that you want to exclude ` x [ i ] *x [ i ]` from the computation of ` y [ i ]` ?
First , get the sum for x [ n ]: #CODE
Your function could be shorter and simpler if you computed the sum of ` x ` first , then used that to compute the elements of ` y ` .
Replace ` sum ` and the list comprehension with numpy operations if you're using numpy : #CODE
Here is a way to do it with numpy , by using the outer product : #CODE
The problem is that when I select the first column of matrix1 and put it in vector1 , it gets converted to a row vector , so when I try to concatenate with matrix2 , I get a dimension error .
But this looks too ugly for me to do every time I have to concatenate a matrix and a vector .
Non-trivial sums of outer products without temporaries in numpy
The actual problem I wish to solve is , given a set of N unit vectors and another set of M vectors calculate for each of the unit vectors the average of the absolute value of the dot product of it with every one of the M vectors .
Essentially this is calculating the outer product of the two matrices and summing and averaging with an absolute value stuck in-between .
The main difficulty I have is due to the presence of the absolute value .
As noted above , if the absolute value were not required then we could proceed as follows , now using ` einsum() ` #CODE
Can you do it with [ ` dot `] ( #URL ) and some axis fiddling ?
I think you can use ` dot ` or ` tensordot ` to generate the temporary passed to ` abs ` , saving an order of magnitude or two of memory since you don't have to ` sum ` .
All I really want to do is insert an absolute value in the right place in that calculation !
edit 3 : Ah , it's the absolute value thing . hmm #CODE
It can also improve the indexation of unique values such as vals [ i ] ( not slices ) .
For if instead of the mean of the absolute values you could use the root mean square , you would still be somehow averaging magnitudes of inner products , but you could get it in a single shot as : #CODE
If you decide to go down this road , see how well ` np.einsum ` performs for large ` N ` and ` M ` : it has a tendency to bog down when passed too many parameters and indices .
That order seems a little odd to me , but IIUC , all you need to do is to transpose the result of multiplying ` na_values ` by ` array ( ls_alloc )` : #CODE
numpy doesn't have a svd method - you may have to seem if there is the same by another name or in another module .
@USER : The import in that example can be ( and probably should be ) changed to two lines : ` from numpy import eye , asarray , dot , sum ` and ` from numpy.linalg import svd ` .
I believe this should be " gamma = 1.0 " in the argument list .
Otherwise , the " gamma / p " when calculating the SVD input will be treated as integer division and will result in 0 if p > 1 ( which it typically is ) .
And we want to do something with that or simply append it to a larger array .
I generate a first trash read , either empty or with zeros and just append over it , and then erase it .
Do you want to append the individual values of foo to data , or the entire array of foo ?
` resize ` changes the original array in place , though .
If ` resize ` acts as ` realloc ` , then it might have to copy the array depending on the " fragmentation " of memory ( and in that case it's * almost * the same as calling ` append `) .
I'm not sure how to translate this to myself .
Return the indices of unmasked elements that are both zero and not zero
Now I can use the nonzero function from ma library , but my arrays also contains zero values .
In short , how to get the index of all unmasked elements and not only the nonzero values ?
Unless you need the actual indices for something else , this is serious overkill : all you need is the boolean array , i.e. ` values = x [ m == 1 ]` will return the exact same data and runs much faster .
If I simply append multiple return values , I have to modify the other code and unpack the first return value .
Not sure if you want unique values for column A ( If you do , use groupby on the result below ) #CODE
Does anybody have any tips on how to pack a header and then append it to some chunk of data ?
For example , if I have a bit-packed header of 16 bytes , and then I want to append it to a raw RGB image of about 2MB , what is the most efficient way of doing this ?
To concatenate your header with the numpy array you'll need to have the array in binary form .
` vms_list = vms [ np.logical_and ( vms [ ' date '] == log [ 0 ] [ ' date '] , vms [ ' code '] == log [ 0 ] [ ' code '] )]`
Is ` log ` how you would expect it to be ?
With NumPy fancy indexing , when ` Y ` , ` X ` , and ` angle ` all have the same shape ( or broadcast to the same shape ) , #CODE
If you have a dataframe for each cluster with columns [ ' A ' , ' B ' , ' C '] combine them into a new dataframe with columns [ ' ID ' , ' A ' , ' B ' , ' C '] where ID is a unique identifier per cluster .
Sorry , but I cannot replicate your problem : you should publish your full stack trace with version information ( numpy , kernel , glibc ) and hope that some can one replicate the bug .
Consider using subplots instead , and / or using ` savefig() ` to dump the plot to a file .
Can you strip your program down to the minimum amount of code that runs and triggers the problem , then post that ?
exp ( - 1.0 *z ) should solve your problem , ( numpy.exp is the exponential function , not the euler-number )
So it will still be a sin function but stretched as x -> inf .
Well i want to concatenate two of this descriptor arrays . for e.g a =[[ 128 values ] ... [ 128 values ]] , len ( a )= 300 and b =[[ 128 values ] ... [ 128 values ]] len ( b )= 1000 result should be c =[[ 128 values ] ... [ 128 values ]] len ( c )= 1300
Use vstack to stack row wise .
Dstack is really a depth stack : #CODE
As can be noticed the sign of each element in the Jama decomposed matrices ( u VT ) is opposite to the ones in the original example .
is not unique up to a sign change of the columns of U and V .
Different implementations of the svd will give slightly different results : to check for a correct svd you have to compute norm ( A-U *S*V^T ) / norm ( A ) and verify that it is a small number .
The alignment angles may be unique , a discrete set , or a continuum as below .
U = ( ( 0 , exp [ i p ]) , ( -exp [ i t ] , 0 ) )
S = sqrt ( 2 ) ( ( 2 , 0 ) , ( 0 , 1 ) )
V * = ( 1 / sqrt ( 2 ) ) ( ( exp [ i t ] , exp [ i t ]) , ( exp [ i p ] , -exp [ i p ]) )
This works , but I'm concerned about the efficiency ( list comp , ` flatten ` , ` reshape `) of the pertinent line when dealing with large ` ndarrays ` : #CODE
Edit : If you want to return all elements in ` data ` which pass this criterion , you could loop over the unique ` queryid ` values in ` data ` and update a set of boolean indices specifying which elements pass the test : #CODE
Maybe you mention numpy.unique to get the unique qid .
It took a little above 6 min to scan through a 4gig database .
Of course , your script should be a little slower than 6 min as it skipped ` bitscores ` that were not properly sorted , but even it shouldn't reach 12 hours !
@USER : I am using ` concat ` to concat columns of two dataframes together and I want to avoid concatenating together two dataframes that are not indexed that's all . indexed I mean dataframes where the user explicitly assigned a unique column identifying each row to the dataframe with ` set_index ` . maybe this is impossible to tell ?
Numpy , dot products on multidimensional arrays
Is there some subtility to use dot on multidimensional arrays ?
The issue is that ` np.dot ( a , b )` for multidimensional arrays makes the dot product of the last dimension of ` a ` with the second dimension of ` b ` : #CODE
Using ` np.tensordot() ` allows you to control in which ` axes ` from each input you want to perform the dot product .
I worked out how to get numpy to quickly do the inner loop , but not the outer loop : #CODE
Actually the code does look like a one to one mapping , except that it is [ ` reshape ` d ] ( #URL ) .
Give the full stack trace .
All of the propertys of the top-level card object are the fields , so my intention is to create another masked array that holds the properties of " flip " cards , so all the flip cards can be categorized as such ( and properties analyzed , etc . ) I am able to create a masked array of these properties , and append it as a field to the larger array using : #CODE
Think of it in terms of database organization ; assuming the ` MultiverseId ` attribute is unique to each card or card+flip combo , you can use that as the primary ID , so to speak , in your flip array .
Alternatively , you could assign the flip cards their own unique IDs and store them in the same record array as the regular cards , as the properties seem to have the same names , and then have a ` flip_id ` field that would be some set value such as ` 0 ` or ` None ` for cards without flip aspects and then the ID of the flip card for those cards that do have a flip .
As you can see , the inner loop is updating variables x and y while the outer loop starts with a different value of x each time .
In particular , I don't know what you mean by " starts with a different value of x each time " , and " first " or " second " loop is rather unclear compared to " inner " or " outer " .
By _starts with a different value of x each time_ , I mean that the first iteration of the outer loop starts with x = 1.0 .
Therefore , in the second iteration of the outer loop , x is another value .
However , the algebra part doesn't apply to the real loop I'm trying to optimize ( it involves gamma functions )
I don't think it's going to add up to any important speed up , but you can save some function calls if you generate all your gamma and normally distributed random values at once .
Gamma functions have a scaling property , so that if you draw a value ` x ` from a gamma ( k , 1 ) distribution , then ` c*x ` will be a value drawn from a gamma ( k , c ) distribution .
I have the following example where I am trying to concatenate the date and time string columns from a csv file .
Did you mean to concatenate the date and time strings together like this : `' 2012-04-01 00:10 '` ?
2 ) For z1 = arr1 [ ' Date '] + arr1 [ ' Time '] I wanted to concatenate the strings from the column and return a numpy array column that I can use in plotting a time series graph .
` numpy.take ` in `' wrap '` mode will use your indices modulo the length of the array .
Note that you can also access ` take ` as an array method , i.e. ` A.take ( indices , mode= ' wrap ')` .
I tried the most obvious method ( pass a list of tuples as the ` indices ` argument ) but that doesn't work as hoped .
To apply the neighbor function to every 5 you need to be careful about your start and end indices ( in the range ( ... ) command ) and the relative slice you take from A .
I am trying to concatenate the first two string columns ( date + time ) into a date object , and I have found an example for this on stackoverflow .
Why do you need the sparse matrix , instead of just using the indices ?
If you choose the sparse ` coo_matrix ` you can create it passing the indices like : #CODE
Now I am at the cell phone ... you can try it passing one more array of indices and defining shape with one more integer in the tuple ...
You can significantly improve your performance using ` np.where ` to get the indices where your conditions happen : #CODE
you will see that ` ind ` is a tuple with two elements , the first is the indices of the first axis and the second of the second axis of your ` flow_direction_np ` array .
You can work out with this indices to apply the shifts : ` i-1 ` , ` j-1 ` and so on ...
When you do ` ind [ 0 ]` you get the array of indices of the first axis , then ` ind [ 0 ] -1 ` will subtract one from this array , which is equivalent to ` i-1 ` .
I am finally getting to grips with indices and axis's !
In the vectorized version of the code , ` 0 ` is returned both when the indices are negative and when they're beyond the bounds of the grid .
Notice that negative indices up to the size of the sequence are valid and return the " opposite end " of the sequence .
ps2 : stack overflow is somewhat crazy ...
You can flatten put all flatten argument in an array and call your function again in __call__ ( self , * arg ) .
The code takes the value from -10 to 10 and it will append it to a list , according to its probability .
The problem is that the histogram will generate an accurate plot , but certain values will break the trend .
But if it's just the histogram that's doing something wrong , then it doesn't matter , I don't really need it .
ali_m's answer below is clever , but then it raises the question of why you're making this plot in the first place , e.g. plotting pos itself might be better than a histogram
I think the underlying issue is that your bin size is uniform , whereas the differences between the unique values in ` pos ` scale exponentially .
Because of that you'll always end up either with weird ' spikes ' where two nearby unique values fall within the same bin , or lots of empty bins ( especially if you just increase the bin count to get rid of the ' spikes ') .
You could try setting your bins according to the actual unique values in ` pos ` , so that their widths are non-uniform : #CODE
Do you actually need to return tuples or do you want to reshape the array ?
You can't fit all the old ones in there .
My question is unique because I'm looking for a way to perform multilateration with weights .
See other questions hee on SO for exact figures and tricks to push it to the max .
I believe this happens because ` genfromtxt() ` does not decode the text file correctly .
The question is if there any way to normalize every column along axis=0 ( i.e. by sum ) after every product operation , without cycles ?
If each list is unique you can try : #CODE
unique is a must for me before concatenate , thank you , works great as well , have to compare speedups for different configurations
If you know the elements within each array are unique , use the argument ` assume_unique=True ` : #CODE
invalid value encountered in sqrt
To avoid the ` invalid value ` warning / error , the argument to numpy's ` sqrt ` function must be complex : #CODE
You need to use the sqrt from the cmath module #CODE
Others have probably suggested more desirable methods , but just to add to the conversation , you could always multiply any number less than 0 ( the value you want the sqrt of , -1 in this case ) by -1 , then take the sqrt of that .
I just discovered the convenience function ` numpy.lib.scimath.sqrt ` explained in the sqrt documentation .
Dealing with zeros in numpy array normalization
Can I somehow use ` nonzero ` to apply the division only to ` x [ i ]` such that ` nonzero [ i ]` is ` True ` ?
Efficiently recalculating the gradient of a numpy array with unknown dimensionality
I have a second array , ` G ` that stores the gradient of ` S ` , as calculated by numpy's ` gradient() ` function .
The function returns a vector indicating the gradient of ` S ` at ` coords ` in each dimension .
It calculates this without calculating the gradient of ` S ` at every point , but the problem is that it does not seem to be very efficient .
For each of these slices , I calculate the gradient and place the appropriate value from that gradient into the correct place in the returned vector ` grad ` .
In fact , just taking the gradient of the whole array ` S ` seems to run faster ( for ` nBins = 25 ` and ` nDim = 4 `) .
` gradient ` returns an array if the input is 1-dimensional and a list of arrays if the input has a number of dimensions other than 1 .
Uhmmm , I could work better if I had an example , but what about just creating a secondary array , S2 ( by the way , I'd choose longer and more meaningful names for your variables ) and recalculate the gradient for it , G2 , and then introduce it back into G ?
What you are uncomfortable with is to recalculate the gradient of the whole array , S .
If you change S [ ii , jj ] ( assuming two dimensions ) , the only values of the gradient that would change would be those whitin [ #URL and [ #URL i.e. a square 3x3 centered in [ ii , jj ] .
So , what I propose is to " cut " a slice of S around [ ii , jj ] so that we recalculate the gradient and then insert it into G .
You may want to extend the limits to ` j-2 ` and ` j+3 ` so that the gradient is calculated using central differences whenever possible , but it will be slower .
To make it compatible with gradient ( see the comment made by user2357112 above ) the assignment required a little bit more than what you suggested .
gradient descent using python and numpy
Calculate the gradient = X ' * loss / m
Update the parameters theta = theta - alpha * gradient
You need to take care about the intuition of the regression using gradient descent .
In this case , this is the average of the sum over the gradients , thus the division by ` m ` .
Using ` loss ` for the absolute difference isn't a very good idea as " loss " is usually a synonym of " cost " .
I want to loop over the columns in the original array , and if the sum of the rows in any one column is about a certain value I want to remove the whole column .
Each coordinate in the list ( ` pi ` , ` pj `) has a unique ` dt ` , ` i_frac ` , ` j_frac ` , ` ip1_frac ` and ` jp1_frac ` associated with it .
This problem is described briefly with an example in the Tentative Numpy Tutorial under fancy indexing with arrays of indices ( see the last three examples before boolean indexing ) .
I think you can reuse all of the unique coordinate finding above for the other updates , so the following would work : #CODE
` view ` is basically taking your two coordinates as a single variable that can be used to find the unique coordinates .
Deep copy will create a new array t0 that has unique memory locations in which the values are copied from temp .
` det ( A - diag ( 2 ) *ev1 )` is close to 0 , perhaps the algorithm cannot converge in such circumstances .
How to get the highest element in absolute value in a numpy matrix ?
@USER that's a good point , I got confused by the indices .
I was looking for a way to get the signed values of the maximum absolute values of an N-dimensional array along a specified axis , which none of these answers handle .
Note this will return the minimum in the case where ` x.min() ` and ` x.max() ` have the same absolute value e.g. -5 and 5 .
If you have a preference just order the inputs to ` max ` accordingly .
To find where these indices happen , we plug this into ` np.where ` and get the first value of the tuple .
Now , I don't have a fast way to do this with multiple indices yet ( although I have a feeling there is one ) .
I don't see how to use ` np.in1d ` to find indices though , so I would go the ` np.searchsorted ` route : #CODE
As for the np.in1d this would be to generate a mask instead of the indices
I know need the sum of the values for each label and column .
If I try sum with where also wont give me right value : #CODE
But that requires me to first reshape the matrix to a 1D array .
If your array only has a few unique values , ie 2 and 127 , it might be worth reducing the array using unique before calling bincount ie : #CODE
Read each line from a file and proceed to take the log of the values to use in a fitting function
I am trying to read each line individually , and then take the log of each line
I then use the log values as an y value to a power law fit .
I am however unable to do read the lines and take their log individually with a loop .
As you can see I haven't figured out how to read and take the log of each line .
firstly , sorry , I didn't paste the whole copy of my code , I have done now-x = [ 3.6 , 4.5 , 5.8 , 8.0 ] y809 =13 , 14 , 15 , 20 and so on y values logx = log10 ( x ) logy = log10 ( y809 ) logyerr = 0.05 fitfunc = lambda p , x : p [ 0 ] + p [ 1 ] * x errfunc = lambda p , x , y , err : ( y - fitfunc ( p , x )) / err pinit = [ 1.0 , - 1.0 ] out = optimize.leastsq ( errfunc , pinit , args =( logx , logy , logyerr ) , full_output=1 ) pfinal = out [ 0 ] covar = out [ 1 ] print pfinal print covar index = pfinal [ 1 ] amp = 10.0 **pfinal [ 0 ] indexErr = sqrt ( covar [ 0 ] [ 0 ] ) ampErr = sqrt ( covar [ 1 ] [ 1 ] ) * amp
Hi Frodon , I assume the value of each argument is taken the log of separately .
gives the log of each value , with a separate array for each line : #CODE
I know that exponentials are just specific forms of a more general distribution ( gamma ) but why include the uneeded information ?
Even gamma doesn't have a location parameter .
What about for gamma : #CODE
It has been suggested to remove this , and make distributions like ` gamma ` ` loc ` -less to follow their canonical formulations .
However , it turns out that some people do actually use " shifted gamma " distributions with nonzero ` loc ` parameters to model the sizes of sunspots , if I remember correctly , and the current behavior of ` scipy.stats ` was perfect for them .
For a gamma distribution , which has one , it will return ` ( shape , loc , scale )` .
I want to sort all the values contained within the array from the highest value to the lowest value but maintain the indices and shape of the array .
I want to sort all the values contained within the array from the highest value to the lowest value but maintain the indices and shape of the array .
I want to sort all the values contained within the array from the highest value to the lowest value but maintain the indices and shape of the array .
numpy tofile() with very large arrays saves all zeros
When I try and save a very large ( 20000 x 20000 element ) array , I get all zeros back : #CODE
In the IDL code , ` ind2 ` is an array , where the mod operation has been applied element-wise .
The Python code you give for performing the same action is correct for applying ` mod ` to a NumPy ` ndarray ` .
Just thinking aloud here , but since the SciPy Sparse formats are really just an array of data and two arrays of indices , could we somehow just pupulate the ` SparseDataFrame ` with that ?
Suppose I create a histogram using scipy / numpy , so I have two arrays : one for the bin counts , and one for the bin edges .
If I use the histogram to represent a probability distribution function , how can I efficiently generate random numbers from that distribution ?
Do you want a certain number of random numbers per histogram interval or do you want random numbers based off a weight function that is based off a polynomial interpolation of the histogram values ?
Uses the count of the histogram as a weight and chooses values of indices based on this weight .
@USER I've edited my answer with a 2D histogram .
I've also added to your code ( as a new answer ) an example how to generate random numbers from the kde ( kernel density estimation ) of the histogram , which captures better the " generator mechanism " of the histogram .
@USER solution is great , but you should consider using the kde ( kernel density estimation ) of the histogram .
A great explanation why it's problematic to do statistics over histogram , and why you should use kde instead can be found here
It looks almost the same , but captures better the histogram generator .
I have a dataframe , with several columns , one of these columns contains ' product codes ' and these are unique strings .
` fdesign.lowpass ( ... )` returns a struct that describes the filter ; the actual filter design is invoked by something like ` H =d esign ( d ,... )` where the arguments to this call might affect how one would translate it into python .
If k is an numpy array of an arbitrary shape , so ` k.shape = ( s1 , s2 , s3 , ..., sn )` , and I want to reshape it so that ` k.shape ` becomes ` ( s1 , s2 , ..., sn , 1 )` , is this the best way to do it in one line ?
You need to convert your indices to a single index .
I'm trying to find the minimum array indices along one dimension of a very large 2D numpy array .
Is there any way to speed it up to comparable to min ?
Maybe ` min ` is smart enough to do its job sequentially over the array ( hence with cache locality ) , and ` argmin ` is jumping around the array ( causing a lot of cache misses ) ?
assuming mydict and exp both have the exact same keys ?
+1 If I had any money I'd bet it on some time soon there being a ` np.count_unique ` function that calls ` np.bincount ` on the indices returned by ` np.unique ` with ` return_inverse=True ` , it's a construct I find myself typing over and over again .
I just figured out that it is much faster for large datasets , and only slightly slower for really small ones , to do : ` rep_mask = np.in1d ( a , ua [ count < thresh ])` .
I think the approach is the right one , and it can be made to work with minor changes : with the OP's test data it has trouble with ` np.in1d ` as I suggested , it probably worked with your earlier version , but comparing indices instead of the actual objects works just fine : ` rep_mask = np.in1d ( uind , np.where ( count < thresh ))` .
An error function represents the error ( sum of squares of differences ) between the desired and the actual t-test probabilities .
What I need is to insert all the missing years between min ( year ) and max ( year ) and to make sure that the order is preserved .
Moreover , if any sublist contains only a single item then the same process should be applied to it so that the original value preserves its supposed order and rest of the min to max ( year , value ) items are inserted properly .
Alternatively , you can use ` reshape ` : #CODE
` reshape ` works for this #CODE
note also that ` reshape ` doesn't copy the data unless it needs to for the new shape ( which it doesn't need to do here ): #CODE
Call function with internal sum over 2D array
Faster way to calculate sum of squared difference between an image ( M , N ) and a template ( 3 , 3 ) for template matching ?
If you expand your ` sum (( a-b ) **2 )` , you can turn it into ` sum ( a**2 ) + sum ( b**2 ) - 2*sum ( a*b )` , and this multiply-then-reduce-with-a-sum operations you can perform with linear algebra operators , with a substantial improvement in both performance and memory use : #CODE
Each iteration of this pad , roll , roll combo essentially does what you are looking for : #CODE
If we take numpy , for example , " new " arrays created by indexing existing ones are only views of the original .
Even the ones encapsulated on another object .
You first get an index that sorts the whole array , and then convert that flat index into pairs of indices with ` np.unravel_index ` .
what is an efficient way to get the most variable rows from a ( numeric ) pandas DataFrame ? by most variable rows , I mean the rows that are most variable with respect to column values - rows with the highest standard deviation , but since each row might be on a different scale , can't just take the largest absolute standard deviation of each row across column .
One way to define this is to compute the absolute coefficient of variation : #CODE
the rows that are most variable with respect to column values - i.e. have the highest standard deviation , but since each row might be on a different scale , i can't just take the largest std
I would like to do a where clause which will return the indices of the elements in ' a ' where the sum of the values for this element is equal to 1 .
Something like : ` where ( sum ( a ) == 1 )`
So the sum of the 2nd row ( index == 1 ) is 1.0 .
Without specifying an explicit axis , numpy takes the sum of all elements of the array .
Note , there is a difference between the python builtin ` sum ` and ` np.sum ` .
@USER : ` argsort ` returns the indices of the sorted array .
The index of the sorted indices is the rank .
Returns the indices that would sort an array .
Returns the indices that would sort an array .
It simply sorts the elements of the array in the ascending order and returns their corresponding indices .
I used gradient to try to calculate group velocity ( group velocity of a wave packet is the derivative of frequencies respect to wavenumbers , not a group of velocities ) .
I need to calculate gradient and I did expect a 2d vector , being gradient definition #CODE
But i got 2 arrays with 3 colums each , i.e. 2 3d vectors ; at first i thought that the sum of the two would give me the vector i were searchin for but the z component doesn't vanish .
What i mean is : I want to calculate gradient of an array of values : #CODE
Even if i make my data as a grid that would have lots of zeroes outside of the polygon of my original data , that would add really high vectors to my gradient affecting ( negatively ) the precision of calculation .
Question is what do gradient do ?
Do gradient actually compute really a gradient ?
the problem is that you are giving gradient the wrong input .
It's implicit that your matrix of ` i**2+j**2 ` values correspond to the ` xy ` plane , and the optional scalar arguments of ` gradient ` account for step size assumptions , i.e. if your ` x ` points are not ` 1 ` away from eachother , and the same for your ` y ` points .
You need to give ` gradient ` a matrix that describes your angular frequency values for your ` ( x , y )` points .
Here is how to interpret your gradient :
Since my data was generated from ` f ( x , y ) = sin ( x+y )` gy looks the same .
Here is a more obvious example using ` f ( x , y ) = sin ( x )` ...
And what are our gradient components at that point ?
` dz / dx = cos ( x )` and ...
You'll notice they aren't exactly correct , that is because my Z data isn't continuous , there is a step size of ` 0.05 ` and ` gradient ` can only approximate the rate of change .
And again : i don't know why should i have 2 3d vectors when for a 2 variables function gradient is a 2d vector ...
I already tried to do what you did in your example but gradient still give me 2 3-d vectors ...
Maybe gradient simply don't calculate gradient ?
How can i obtain the real gradient from such output ?
I think your confusion comes from your expectation that gradient returns a vector of functions , when it cannot . numpy doesn't do algebra like mathematica or maple .
You give ` gradient ` a matrix of your Z values and it computes step-wise the slope between each ` X , X+1 ` and ` Y , Y+1 ` , giving you for every point the rate of change of ` x ` and ` y ` at that point .
What should i do to get gradient ?
All i need is gradient module on each point , but i don't know how to work it out from numpy.gradient output ...
I did those checks before , maybe the problem is how i give data to gradient ...
It doesn't try to dump packages on top of a system Python and hope that things work .
Running pk_fit seems to work at first , but then dumps many error messages related to the ODE solver .
EDIT So the code first computes the variation from every point to the nex ( ` gradient `) .
If you do ` np.diff (( gradient 0 )` the resulting boolean array is ` True ` where there is a change from growing ( ` 0 `) to not growing ( ` = 0 `) .
If all the data is 64 bits , the total size in bytes of all your 105 arrays should be ` sum ([ 24*60 / j*2* ( 20+6 ) *8 for j in ( 5 , 15 , 60 , 240 , 24*60 )])` which comes out to ` 172640 ` , well under 0.2 Mb of memory .
The second first calls the first , then uses its result to basically cross indices stored in a numpy array with the numbers in the lists of arrays to form queries ( new numbers ) on a ( pybloomfiltermmap ) bloom filter .
What I want to do is obtain all combinations and all unique permutations of each combination .
It sounds like you're thinking of a sort of " permutations with replacement " , where an input of `' AB '` with a permutation size of 2 would give the outputs #CODE
Goal is to find arrays of indices to the data that yield the closed loops .
Here , the indices of the ordered data follow a periodic pattern , but that wouldn't be true in the actual data .
Hi @USER ... its not one term , each value in the f-list is a multi-dimensional vector ( where each term in a document is a vector , and the sum of the vectors ( terms ) in a document is just one value in the f-list ) .
If you want to use the cosine similarity as a metric , each of the element of the f-list should be a vector itself , and not the sum of its elements .
The fact is , each value in the f-list is multidimensional vector ( I added up the vectors for each unique term in a document to form a weight vector ( multidimensional )) .
fast way to invert or dot kxnxn matrix
So I have a tensor with indices a [ n , i , j ] of dimensions ( N , M , M ) and I want to invert the M*M square matrix part for each n in N .
However , you can think of your matrix as block diagonal or as banded .
Of course you would also have to build the banded matrix , there will still be a lot of zeros in the bands , etc .
How do I inner join two array in numpy ?
Now I need to inner join these two array by the first column ( date ) , the result should looks like this : #CODE
Will both arrays have the same indices present , once and only once ?
I need inner join , not just combine them together :D .
It will do several kinds of SQL like joins , including inner join .
Note that this looks like it requires a structured array to work , so you may have to cast into a recarray , rather than just say " join on column 0 " #CODE
Conditional numpy array modification by the indices
Then I should compare x and y ( i.e. I need values , not indices ) for the resulting nodes , and modify X or Y depending on the result of comparison .
set value , which will be dependent from indices ?
where now the parenthesis are for the extra digits beyond the 4 significant ones we expected .
exp ( 1j*theta ) = cos ( theta ) + 1j*sin ( theta ) and cos ( theta+2*pi*n ) = cos ( theta ) , same for sin() , for n an integer .
I would roll your own least-squares fitter ( you can do it with just some matrix math ) and omit the constant term .
One simple solution is to read the file with ` csv.reader() ` from python's csv module into a list and then dump it into a numpy array if you like .
If I just use ` norm ` function to calculate the distance one by one it seems to be slow .
I don't think ` dot ` vectorizes like that ; it computes matrix products for 2-d inputs .
You can squeeze out a tiny bit more from the latter by avoiding ` pow ` : #CODE
Note that the ` x-locations ` associated with ` deriv ` have been shifted
Numpy reshape yields a different size error
I would love if someone is able to explain to me why this error occurs as to my understanding the numpy reshape function takes the size of the input array for the actual resize .
This has the effect that certain coordinates are ever so slightly shifted a litle bit ( ie 1398.5152 ... vs 1398.5151 ) causing the reshape to fail .
I find the backtrace odd : it jumps from ` ax.set_ylabel ` to ` return _wrapit ( a , ' reshape ' , newshape , order =o rder )` , without any intermediate call inside matplotlib ( eg , I would expect a call to ` fromnumeric.reshape ` from within ` set_ylabel ` .
The only occurence of ` reshape ` in your code is #CODE
But I also have other things ( like std ) I want to calculate with the same precision
In that way , you would have an integer division , which truncates everything after the dot , resulting in another integer value .
Is there a more efficient way ( or at least pythonic ) to stack n copies of a subarray in order to create a new array ?
You can add ` x ` ( whose shape is ` ( 4 , )`) with ` y ` ( whose shape is ` ( 3 , 4 )`) , and NumPy will automatically " broadcast " ` x ` to shape ( 3 , 4 ): #CODE
As you can see , there is no need to ` tile ` ` x ` first .
If you run the code posted above you see a strip ?
If you change the dimensions given to np.zeros does the size of the strip change ?
yes i have used your code above , with np . zeros .
I have copied your code in another computer with different display and still i see this white strip , and if i change the size of the numpy array the size of the strip doesn't change .
You can notice that there is gray strip on top and on the left ...
I am trying to make a contour plot with defined levels and log norm .
note that the spacing between the ticks are indeed in log scale ...
With my made up exmaple I can run ` a [ 0 ] .split ( ' - ')` , which should be equivalent to ` data [ ' field1 '] [ 0 ] .split ( sep= ' - ')` , so reversing the order of your indices .
Note that the size of ` object_array ` may be quite large ( up to about 1000x1000 ) so I'm concerned that this is sub-optimal in that I'm making at least one copy of the results when I call ` reshape ` .
With larger arrays the absolute differences become larger , but at least on my machine are still smaller for the larger datatype : #CODE
Python sum lat / lon points within geographic radius and sum to grid
also add the log stored if that's different for some reason
yeah you were trying to link non existent libraries imho , I was just wondering if it said that more explicitly in the log
@USER you were missing a dot . please include a small subset of the data which demonstrates the problem ( it seems hard to believe : s )
Sorry the data is 27 cols long , kind of unwieldy even to post a clip .
Finding indices of repeated values in numpy array
I'm trying to find the indices that correspond to certain receiver values that show up .
For my code below , I was trying to find all indices that had a value of 6 .
My problem is that for the output ( print ) I'm only getting [ ] , as if there are no indices that correspond to values for receiver 6 .
numpy.nonzero returns the index of nonzero elements inside of a container
Python : taking the dot product of vector with numpy
I have taken the dot product of vectors in ` Python ` many of times , but for some reason , one such ` np.dot() ` command isn't working .
However , whenever I attempt to take the dot product of these two vectors , I receive #CODE
I am not a ` Python ` expert , but why isn't the dot product working in case 1 and why isn't specifying the vector component working in case 2 ?
In my example , I have used both the dot product vector component specification previously with no such issues .
You need to flatten it before passing it to ` np.dot ` , which you can do as follows : #CODE
Numpy arrays support any number of dimensions ( even 0 ! ); ` dot ` is only dot product for 1-dimensional arrays .
Numpy sum of operator results without allocating an unnecessary array
It doesn't support the notion that True is 1 and False is 0 , so I have to use ` ne.evaluate ( " sum ( where ( a == b , 1 , 0 ))")` , which takes about twice as long .
I currently have an array of indices of the minimum values in an array .
I'm trying to create a histogram plot in python , normalizing with some custom values the y-axis values .
Try loading the data as a numpy array , and selecting the range of elements yourself before passing to the histogram function .
My data file has only one column , out of which I want to make that histogram .
hist [ 1 ] contains the limits in which you have made the histogram .
I want the values to be variable in size which means i want to put in a max worth for x , y and z axes .
Remember , an array has no side length , so I don't understand how you wish to translate that aspect of your rubikscube model to a NumPy array .
Interestingly , to transpose the long list first , convert it into numpy array , and then transpose back would be much faster ( 20x on my laptop ) .
It is indeed ~30% faster than the flatten generator method , though as the cost of requiring additional package .
However , if you change the data type to flatten the individual arrays , you can control the formatting of each element .
In feature hashing , h represents the indices of the new vector I am hashing x to , i.e the index 0 of the hashed vector should have 4 and 6 summed up , index 1 should have 4 , 0 and 1 summed up , etc .
One way of doing this is of course by looping through the hash indices , i.e : #CODE
Getting indices from numpy array and applying to 2nd lower dimensional array to create new array
I have an array ` Gamma ` , ` Gamma.shape =( 20,7,90,144 )` , that represents the dimensions ( t , z , y , x ) and whose respective indices I denote by ` [ l , k , j , i ]` .
The lists that I would like to join are two dates lists on which has one more value than the other .
I think this can be done by numpy append : #CODE
I get the new list of dates joined in sequence however this changes the dimension which I want to keep the same and I don't know which axis to join them on .
Note that the OP is using different indices for ` ssa ` and ` check ` .
you're essentialy trying to create a new list of two items Y_a and I_a from just one value - which is the result of applying min function to diff_a list .
The problem is that you are return a single value ` min ( diff_a )` to a list ` [ Y_a , I_a ]` .
` min ( diff_a )` finds the smallest value in the iterable , in this case ` diff_a ` .
When you do ` [ C , I ] = min ( ... )` in Matlab , it means that the minimum will be stored in ` C ` and the index of the minimum in ` I ` .
and set centroids in these image regions ( with max and min numbers of black pixels ) and compute graph edge distance between the nodes lying in these centers .
As a side the note the reason I didn't just simply compute the RMS after centering is because the ` std ` method calls ` bottleneck ` for faster computation of that expression in that special case where you want to compute the standard deviation and not the more general RMS .
Good eye , thanks so much
To avoid duplicate pairs nested loop should go upwards from the index of the outer loop , i.e. : #CODE
If you want a flattened array with only the unique , off-diagonal values , you can use ` scipy.spatial.distance.squareform ` : #CODE
exact histogram of an array
Could be useful when number of unique elements is known to be small .
Using iterator to sum arrays in python
Now i want to add up the points so that it becomes the sum of the vectors using an iterator .
How do i use an iterator to sum up all the rows ?
lets say a = [ x1 , y1 , z1 ] and b = [ x2 , y2 , z2 ] and the sum is a+b but i want to use an iterator so i can process all rows .
This sounds like a job for something built around ` np.sum ` or the ` sum ` method of an ` ndarray ` .
im not trying to sum the points accross the columns like x1+y1+z1 im trying to sum up all the x1 points and all the y1 points ...
It takes an optional ` axis ` argument representing which axis to sum over .
First , we initialize a blank ` sum ` vector equal to the width of the data matrix .
Then , we iterate over the actual data vectors and add them to sum .
Make special diagonal matrix in Numpy
So this involves updating the main diagonal and the two diagonals above it .
You can use ` np.indices ` to get the indices of your array and then assign the values where you want .
` i , j ` are the line and column indices , respectively .
I wish numpy's diag function can let me specify which super / sub diagonal I want to update and then return a view of the diagonal .
Using my answer to this question : changing the values of the diagonal of a matrix in numpy , you can do some tricky slicing to get a view of each diagonal , then do the assignment .
Is there a function , such as ` np.where ` , that can be used to return all row indices where ` [ 5 ]` is the row value ?
One way is to initialize another array that takes the row indices where ` [ ? ]` is not present .
You can use the following list comprehension to get the indices where 5 appears : #CODE
Or the following list comprehension to get the indices where the list is exactly ` [ 5 ]` : #CODE
@USER is ` mx ` a matrix containing indices or values ?
Hi Saullo , indices follow by values .
The only difference between this and other past successes is that before I loaded images and did processing on them before rendering , but this stack is from matplotlib.pyplot with a switched backend to FigureCanvasAgg ; however , when I save the image prior to its NumPy conversion , the image looks perfectly fine .
Currently I've been grouping the data by the columns indices to take the mean of the groups and calculate the 95% confidence intervals like this : #CODE
std in numpy will give me the standard deviation ignoring NaN values but I need to divide this by the square root of the group size ignoring NaNs in order to get the standard error .
If you have a subset of data for which you know already which ones you would like to be similar to each other i would suggest trying some different kernels and plotting the resultant similarity matrix over these samples ( if you had 100 test samples you would get a 100x100 similarity matrix that you could plot simply as an heat map using the imshow method in matplotlib.pyplot ) .
You don't need to use any ` numpy ` , you can use ` sum ` : #CODE
I got similar results with other Numpy functions like max , power .
For ` max ` , numpy vs plain python : #CODE
It might give you an idea , that the intensity of the line color in the cython log has to be assessed in context .
Here's a simple example that takes the sum of a 1D array of boolean values ( the same as the ` sum() ` method would for a boolean NumPy array ) #CODE
The spectrum of a real signal is symmetric and ` numpy.fft.rfft ` returns only the unique components of this spectrum .
Check-out the library , this is not a simple DFT ( though it's called ndfft , which means n-dimensional fft )
Still it is strange that I get a complex result when applying the reverse fft .
I've calculated a discrete sin wave ( 10 points ) , and have put the sin wave through the transform you describe above , though I've replaced the random function with a random array that doesn't change from one iteration to the next .
One of the first things I tried was a forward fft of a sin wave , immediately followed by a backward fft of the result , and in both numpy and nfftd , the results were properly normalized .
` numpy.packbits ` pads the axis you're packing along with zeros to get to a multiple of 8 , so make sure you keep track of how many zeros you'll need to chop off the end when you unpack the array .
Unless the size of the array is guaranteed to be a multiple of 8 , he'll need to save to disk its length ( or its length modulo 8) , to chop off the last few zeros that ` np.packbits ` is going to add when packing it .
That is to say , given a ` [ 10 , 10 , 2 ]` array , the first 2 elements are indices ` [ 0 , 0 , 0 ]` and ` [ 0 , 0 , 1 ]` .
In Matlab they are indices ` [ 0 , 0 , 0 ]` and ` [ 0 , 1 , 0 ]` .
Using PyTables manually ( deal-breaker : I want this for constantly changing research code , so it's really convenient to be able to dump dictionaries to files by calling a single function )
As far as I know , there is nothing which can automatically dump dictionaries etc . to hdf5 .
Pickling inflates the size , numpy.save stores the binary data as is , while PyTables can compress low entropy data .
You could even store a file as an argument to a key , allowing you to compress your numpy array using numpy.save() .
I'm going to keep my eye out for something like this .
The conclusion for you could be to use possibly not so space efficient solution and compress it via zip or another well known algorithm .
Using the known algorithm helps when you need to debug ( to unzip and then look at the text file by eye ) .
If you want matrix multiplication you can use ` dot ` #CODE
You can do the operation you are after with a single matrix multiplication , if you first stack all your vectors together into a single array : #CODE
( What ` zip ( ix , iy )` does is : make a list of tuples ` [( ix [ 0 ] , iy [ 0 ]) , ( ix [ 1 ] , iy [ 1 ]) , ... ]` , i.e. the x and y indices for each element we need . )
Use ` np.where ` to get the indices where a given condition is ` True ` .
Also is ` x ` unique ?
x values are not unique .
First idea ; prevent multiple calls to ` np.arange ` and ` concatenate ` should be much faster then ` hstack ` : #CODE
Really demonstrates how slow ` concatenate ` is .
` map ( bool , map ( sum , zip ( *X )))` where ` X = [ a , b , c ,... ]` .
I don't see the blue curve , but I'm guessing you're looking for the distribution of the the sum of the two independent gaussians
It seems that the ` loadtxt ` method expects a ` str ` and not a ` unicode ` since it tries to decode it . glob in your case is returning unicode strings so try encoding your filenames : #CODE
Monte Carlo Simulation with Python : building a histogram on the fly
I have a conceptual question on building a histogram on the fly with Python .
As a result I would like to build a tabular histogram showing the distribution of the returned values , which can be plotted later .
Keep collecting the returned values in a numpy vector , then use existing histogram functions once the MonteCarlo analysis is completed .
Setting up a histogram with a range and an appropriate bin size is an unknown .
I edited the post a bit at the end related to performace issues , however it is lower priority compared to the histogram problem I have .
Numpy : Difference between dot ( a , b ) and ( a*b ) .sum()
I think the part about " numerically precise " was referring to subtracting out the average from the values , rather than using ` dot ` .
Numpy dot is one of the routines that calls the BLAS library that you link on compile ( or builds its own ) .
The importance of this is the BLAS library can make use of Multiply accumulate operations ( usually Fused-Multiply Add ) which limit the number of roundings that the computation performs .
Numpy : array of indices based on array shape ?
Normally you use ` np.indices ` to get an index grid , then you can use that grid , or zip or flatten it as appropriate .
Extract non-main diagonal from scipy sparse matrix ?
How can I extract a diagonal other than than the main diagonal ?
How would I get back the vector of ones without converting to a numpy array ?
The output above is printing the offset followed by the diagonal at that offset .
I know I can use ` scipy.sparse.coo_matrix ` to represent documents as sparse vectors and take dot product to find cosine similarity , but how do I convert the entire corpus to a large but sparse term document matrix ( so that I can also extract it's rows as ` scipy.sparse.coo_matrix ` row vectors ) ?
@USER , Sure , ` .T ` gets you the transpose matrix and ` .A ` converts from sparse to normal dense representation .
Python - Find K max values in each row of one matrix and compare to binary matrix
I would like to compare the indices returned from the last result and , if ` b ` has ones in those positions , return the count .
In other words , in the first row of ` a ` , the top-2 values correspond to only one of the ones in ` b ` , etc .
I read that I can't pass the fmin function a list of arrays , so I had to concatenate all data into x_data_lin , keeping track of the different sets with the size parameter .
Since this data is on a log scale , the errors in the fit near the high y values are very costly , while errors near the low y values are insignificant .
To get this scalar , you can simply square and sum the result ( like I have written in this post under ` err_func2 `) Squaring the data is very important , otherwise negative errors take over and drive the resulting scalar extremely negative .
I have the following dataframe in pandas where there's a unique index ( ` employee `) for each row and also a group label ` type ` : #CODE
I define a function which is a sum of Gaussians in which every Gaussian is centered at one of the Gausspoints .
what you want is : evalutate sum of gaussians with given mean and variance for each component and each component centered in one of points in XYZ , and your means are what you are calling ` Gausspoints ` , is that right ?
You can roll out your own implementation using broadcasting by doing something like : #CODE
Assign to variables or list elements , or append the input to a list .
I have a function y = f ( x ) defined as a sum of two trigonometric functions ( with different pulsations ) .
define a reasonably small increment ` i ` and precompute all pairs ` ( f ( n*i ) , n*i )` for ` n = 0 .. ceil ( T / i )` , store the result in an array indexed by possible values of ` n ` . when calling , use ` floor ( y / i )` as an index into this array .
Instead of offsetting the y-values of the sine-wave , it would be better to specify the argument ` extent =[ min ( x ) , max ( x ) , min ( y ) , max ( y )` when calling ` imshow ` , as this offsets the image rather than the plotted line .
Beat me to it nordev that was my next question : how to offset the image rather than the sin .
So first I create an array of zeros of the size I want to fill : #CODE
so that will give me an array of zeros that will be 5X5 .
First , you lack some parenthesis with zeros , the first argument should be a tuple : #CODE
Then , the corresponding indices for your table are i / 2 and j / 2 : #CODE
You can also use ` np.savetxt ` with ` reshape ` to get a flattened array : #CODE
I have a numpy fft for a large number of samples .
This is correct only if you assume he meant the distance to be the L-infinity norm .
If you scroll down that page : " Christoph Gohlke provides [ pre-built Windows installers ] ( #URL ) for many Python packages , including all of the core Scipy stack .
You can assemble the Scipy stack from individual packages .
Christoph Gohlke provides pre-built Windows installers for many Python packages , including all of the core Scipy stack .
Try that with max = 1 , inc = 0.3
Hint : Try the ones from the original question .
@USER , ` transpose ` of a 1d array doesn't do anything .
Unlike np.array , reshape is a light weight operation which does not copy the data in the array .
kernel - more complicated the kernel , slower the process ( rbf is the most complex from the predefined ones )
And didn ' know that rbf is most complicated kernel - but it is true , when I change kernel on ' poly ' it gave result in 2 hours .
If you can flatten the whole list into a numpy array , then use argsort , the first row of argsort will tell you which array contains the minimum value : #CODE
using fft to find the center of mass under periodic boundary conditions
My idea for an solution would now be do a ( mass weigted ) histogram of these points and then perform an fft on that and use the phase of the first fourier coefficient to determine where in the box the maximum would be .
so now I can use fft since it expacts a periodic function anyways .
that way i get a sin wave with its maximum exactly where the maximum of the histogram is .
Now I'd like to extract the position of the maximum not by taking the max function on the sine vector , but somehow it should be retrevable from the first ( not the 0th ) fourier coefficinet , since that should somehow contain the paseshift of the sine to have its maximum exactly at the maximum of the histogram .
Instead , you can simply compute the dot product of your data with
the dot product ` y.dot ( w )` is basically a projection of ` y ` onto
` cos ( 2*pi*arange ( N ) / N )` ( the real part of ` F1 `) and ` -sin ( 2*pi*arange ( N ) / N )`
the maximum , it is based on the functions cos ( ... ) and sin ( ... ) .
my problem is I don't correctly understand what is hull.simplices , to sum up I want to find the index of point which is on the facet of convexhull so I can use this index to get the point from x and y
In the 2-D case , the ` simplices ` attribute of the ` ConvexHull ` object holds the pairs of indices of the points that make up the line segments of the convex hull .
One way to get just the indices is to get the unique elements of the flattened ` simplices ` array .
( In scipy 0.13.0 and later , you can use the ` vertices ` attribute to get the indices ; see below . )
I think your for loop is wrong , and for this case ` dot ` seems to be enough : #CODE
If you want to sum over the last dimension to get the ` IJK ` array : #CODE
I may find time to roll this into a full answer later today , but the above should get you on your way .
The challenge would be to join your code and mine into a vectorized interpolation routine , but that is [ left as exercise ] ( #URL ) .
Could you post the exception stack trace ?
The indices where ` b ` appears in ` a ` can be obtained using : #CODE
See that the first three indices are not your answer , since the pattern is incomplete .
You just have to define a rule to check if the pattern is complete , i.e. to check where you have ` b.shape [ 0 ]` times a number of ` b.shape [ 1 ]` indices belonging to the same line .
I want to convolve an n-dimensional image which is conceptually periodic .
and I want to convolve it with this kernel : #CODE
Note that I am not interested in creating the kernel , but mainly the periodicity of the convolution , i.e. the three leftmost ones in the resulting image ( if that makes sense ) .
I think you'll have to roll your own code , pretty easy using FFTs and the [ convolution theorem ] ( #URL ) .
You would need to roll the result to get what you are after : #CODE
I assume that the numbers before a Timestamp are the ones belong to it :
The groups are the unique columns with more than one ` 1 ` .
I can't seem to manage to stack these arrays to create a result with those dimensions - I've tried ` np.dstack ` as well as various other techniques , but nothing seems to work .
You need to concatenate the arrays across a new axis ( the third dimension - axis 2 ) .
numpy : log with -inf not nans
EDIT : At the moment I am using ` clip ` to substitute negative numbers for ` 0 ` , it works but is it efficient ?
Given for instance a base ` 10 ` log where ` log ( x )` is the inverse of ` 10**x=100 ` , it is mathematically impossible to achieve ` 10** ( -inf )= =-1 ` .
For numpy arrays you can calculate the log and then apply a simple mask .
Would it be possible to have levels of the colorbar in log scale like in the image below ?
@USER - I believe the OP is asking how to set the tick locator and formatter on the colorbar to display labels at regular log intervals .
If you wanted to automatically generate levels like the ones I have used , you can consider this piece of code : #CODE
As an example of a partitioning of the column indices , we can choose the following : #CODE
I would like to find the groups of sets of column indices that have columns with identical values .
Also , I assume that a ` list ` of the column indices is desired in ` group ` .
But you can also use many other containers for storing the column indices .
but I don't know how to translate this line ` d = diff ([ 0 c ( n )]); `
I was thinking of taking the gradient of the gradient , but wouldn't that introduce artifacts , even in evenly sampled data ?
eg , given the data ` 0 , 0 , 0 , 4 , 8 , 8 , 8 ` and 1d laplace operator ` 1 , -2 , 1 ` , the laplacian would be ` 0 , 0 , 4 , 0 , -4 , 0 , 0 ` , but using the gradient of the gradient would yield ` 0 , 1 , 2 , 0 , -2 , -1 , 0 `
Find unique columns and column membership
Find unique rows in numpy.array
Pandas : unique dataframe
and they all discuss several methods for computing the matrix with unique rows and columns .
However , the solutions look a bit convoluted , at least to the untrained eye .
Either way , the above solution only returns the matrix of unique rows .
which returns , not only the list of unique entries , but also the membership of each item to each unique entry found , but how can I do this for columns ?
Where the different values in ` u ` represent the set of unique columns in the original array : #CODE
First lets get the unique indices , to do so we need to start by transposing your array : #CODE
Using a modified version of the above to get unique indices .
You already have the unique array in ` ua ` , all you need to do is ` .view ` it properly , no need to do any data copying .
Essentially , you want np.unique to return the indexes of the unique columns , and the indices of where they're used ?
If you want it to be the unique columns , then you could use the following instead : #CODE
How can I concatenate these arrays efficiently in a final 2D array along the column axis ?
Will definitely think again before asking something that I can answer myself in 10 min ...
Almost , but this will concatenate them in an arbitrary order , not in the order implied by the keys .
The ` concatenate ` function is more general than ` hstack ` , in that you can stack on a dynamically-chosen axis ; I think ` hstack ` is more readable when you're specifically thinking in terms of " columnwise " instead of " axis 1 " , but really , that's a minor quibble either way .
If it helps to do it faster , I guess you can assume the floor is flat and use that to calculate how many points to sample , rather than using the actual x-values .
The ultimate goal is to be able to take the gradient and not have too many theta-dependent effects creep in .
The importance of ` ravel ` over ` flatten ` is ` ravel ` only copies data if necessary and usually returns a view , while ` flatten ` will always return a copy of the data .
To use reshape to flatten the array : #CODE
` np.atleast_1d ` , etc . actually use the reshape method , probably to better support some weird subclasses such as matrix .
The ` np.vstack ` is used to correctly position the ` x ` and ` y ` indices .
First look at the sum and derivative of a constant function .
remove the zeros from an array with a function Python
Here's the deal , i have an array of multiples elements with about half of it being zeros .
I want to remove these zeros by using a function instead of the traditional ` x=x [ x ! =0 ]` .
An array with multiple zeros .
It is transpose .
I've also tried using ` reshape ` on the data , but the order gets all messed up
You need to transpose the numpy array before passing it to ` matplotlib ` : #CODE
I am trying to convert Matlab code into Python , but I'm receiving an error when I append zeros in my array .
You want to join the list with the array , so try #CODE
You can also use ` zeros (8 , dtype=int )` in place of the list of zeros ( see ` numpy.zeros() ` ) .
Unlike in Matlab , something like ` [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , bits ]` in Python creates a list with the initial zeros follows by an embedded list .
ValueError : operands could not be broadcast together with shapes ( 8 ) ( 100 )
To concatenate , use ` numpy.concatenate (( array1 , array2 , ... ))` .
It gives me a type error : TypeError : can only concatenate tuple ( not " int ") to tuple .
I understand how to join two lists , but I'm not sure about inserting data based on missing values from a sequence , and having it apply to the whole array .
For example , with the data above , I'd like all arrays to have year and month columns extending from 1966 to 1981 so I can correlate the different months together .
Eventually , I would also like to alter the rainfall data to be a percentile of all the rainfall data at that site , and then correlate all the rainfall values in percentile form with the " site of interest "
If I import the data with a [ nan , nan , nan , nan ] row above the first data block ( without any month 12 in the dataset ) I get the following error : " IndexError : too many indices "
I want to plot 2d histogram using matplotlib.pyplot.hist2d .
Any idea how I can get round this and still get a normalised 2d histogram ?
` cmin=1 ` : this is applied * after * the normalisation , and there will be no elements in the histogram , as all bin values will be lower than 1 ( those values are actually set to NaN ) .
If you do need to filter you can consider using numpy's 2d histogram function and masking the output afterwards .
I will have a go at the numpy 2d histogram in the next days .
So I defined ` quantized ` as zeros of length equal to ` data_length ` .
* " So I defined ` quantized ` as zeros of length equal to ` data_length ` .
I think you don't want the line ` b = diff [ a ]` , you want something more like : #CODE
When I try the following code I get an error : ` ValueError : operands could not be broadcast together with shapes ( 3 , 2 ) ( 2 , 4 )` #CODE
In this class , the " * " operator is interpreted as the usual matrix dot product .
The guts of ` melt ` suggest that both ` id_vars ` and ` value ` are copied since ` id_vars ` creation uses ` tile ` and ` value ` creation uses ` df.values.ravel ( ' F ')` which I believe makes a copy if your data are not in Fortran order .
Particularly the flatten / reshape is a bit clumsy .
Not a considerable improvement , but to avoid the flatten and reshape you can use ` np.put ` : #CODE
If you find yourself generating long lists of indices into an array , there is a good chance that it can be solved in more elegant way using boolean matrices .
Please see the log file for port cctools for details :
I am a beginner to PyBrain ( and fairly new to ANN's ) , so to familiarize with using PyBrain I have tried to train on a sin function .
My input is an integer in the range ( 0,100 0 ) and my output / targets are simply the sin of that integer multiplied by some constant ( so it evaluates between zero and 2pi ) .
IndexError : too many indices
when to reshape numpy array like ( 3 , )
If you do ` reshape ( 3 , 1 )` you get a two dimensional array with one column and 3 rows .
reshape ( n , m ) is used to change the dimension of existing multi-dimensional array .
And to get more on reshape ( n , m ) go to the official documentation of numpy module .
Assigning identical array indices at once in Python / Numpy
I want to find a fast way ( without for loop ) in Python to assign reoccuring indices of an array .
When I try to add ` x ` to ` a ` at the indices ` Px , Py ` I obviously do not get the same result ( 3.3 vs . 3.1 ): #CODE
I like the way you used the np.ravel_multi_index function to get the flat indices and then use the np.bincount with the optional weights parameter .
Arrays should be constructed using ` array ` , ` zeros ` or ` empty ` ( refer to the
I realize this usage pattern circumvents most of the performance benefits of the numpy / Pandas stack .
I've much to learn about the numpy / scipy / Pandas stack , but it seems that for truly arbitrary logic , you may sometimes need to just use a slow pure Python architecture like the one above .
As for the second part of the question : row wise operations , even optimised ones , using pandas ` apply ` , are not the fastest solution there is .
Some operation could be converted to column oriented ones ( one in my example could be easily converted to just ` df [ ' a '] + df [ ' b ']`) , but others cannot .
so this gives me all the values including the ones for missing months as 0 , but I am not sure how to generate the corresponding mm-yyyy element for them .
I think a simpler solution is to just iterate through the months and years in the range , and add the ones that are missing .
Note that clip is not required with ` mode= ' nearest '` .
numpy.ndenumerate to return indices in Fortran order ?
When using ` numpy.ndenumerate ` the indices are returned following for a ` C-contiguous ` array order , for example : #CODE
Just taking a transpose would give you what you want : #CODE
" AttributeError : sqrt " when calculating a simple standard deviation
This is because `' same '` effectively causes ` y ` to be padded with zeros at both ends so that there are enough ` y ` values to use when computing the convolution .
@USER yes , that's because the convolution is effectively padding your input signal with zeros when the `' same '` argument is used .
( You maybe didn't notice because you preassigned a zeros array of the same size . Your for loop will always produce an array with a bunch of zeros at the end . )
If you try to multiply them element by element ( which is what numpy tries to do if you do ` a * b ` because every basic operation except the ` dot ` operation is element wise ) , it must broadcast the arrays so that they match in all their dimensions .
If there is only one element in any dimension , use it for all indices in that dimension .
How can I correlate the first array of i ` [ 0.88005044 , 0.60633474 ]` with the first array of j ` [ 0.52745975 , 0.07748147 ]` ?
This will scale with ` n * m ` where ` n == df1.shape [ 1 ]` and ` m == df2.shape [ 1 ]` so you should transpose the axes of either ( you said above that you have the option to do this ) such that the smallest axis is the columns axis .
So , to recap , ` correlate ` ( aside from giving a different result because it's a different function ) fails completely on objects with ` nan ` values .
Why not concatenate them all into one big frame and then you can call ` df.corr() ` on that .
Can I select which columns do I want to concatenate ?
I don't want to correlate columns of the same dataframe .
A little bit cleaner version would be ` map ( correlate , i , j )` .
The idea is to convert those to uint8 , use unpackbits , concatenate the result .
Create array of outer products in numpy
I want to take the outer product of each vector with itself , then concatenate them into an array of square matrices of shape ( n , m , m ) .
Bonus question : since the outer products are symmetric , I don't need to m x m multiplication operations to calculate them .
Changed the output from list ==> dict , since the indices are non-sequential .
It's giving me an invalidindex error for the line simrec [ n , i ] = nonzero ( datasim [ i ] [: , 1 ]= =no [ n ]) [ 0 ]
I did some testing with numpy pre allocated zeros arrays vs appending to a simply list and found the appending was faster .
There's bound to be some way to use numpy to calculate the indices too , but I don't know which is more costly of parsing manually like this and loading the full table ( well , the interesting columns ) using loadtxt .
Yes , after loading up the list , it is converted to a numpy array using np.sort() and a number of steps are performed after that including producing a histogram .
You decode an RLE encoded vector , then make a histogram from that ?
Sounds rather roundabout , as the histogram is binning of the data in the RLE list , and sorting it is a lot cheaper in that form as well .
I load it into memory to sort it to let me easily calculate the median and confidence levels for the histogram .
My desired output is a table in the form x , y , min ( z ) , count ( min ( z )) .
There's only about 600 unique coordinates , so the output table will be 600x4 .
When you strip down code to post it , strip it down to something that actually runs and actually demonstrates the problem when you run it .
AFAIK there's not a very good way to do this with ` numpy ` or the ` scipy.sparse ` module -- the sparse matrices in ` scipy.sparse ` are designed to be 2D matrices , and to create one in the first place you'd basically need to use the code you've already written in your first loop ( i.e. , to set all of the nonzero locations in a sparse matrix ) , with the additional complexity of always having to specify two index values .
Seems like this should be a simple join or matching concatenation , but can't find the method ( and / or keywords ) to do it .
Well , yes , but then you'd need to decode the text file to Unicode , which is probably not appropriate here .
** If you knew enough , you could have easily deduced that , because `' \x01 '` in almost any charset will decode to ` u ' \u0001 '` .
As in subject , I'm using python / numpy / scipy to do some data analysis , and I'd like to create an object of class LTI for a discrete system , specifying ( num , den , dt ) or ( zeros , poles , gain , dt ) , or even ( A , B , C , D , dt ) , but the documentation never mentions how to do that .
Many of the procedures for analog system that are under scipy.signal.ltisys work well for these type of systems but is not the case for the ones you find in flat scipy .
Take the log of all your likelihoods , and add or subtract logarithms instead of multiplying and dividing .
need to find subarrays having min and max number of pixels
Use sum function ( #URL ) on each submatrix ( row or column in this case ) and then find min and max value from results .
Numpy array : concatenate arrays and integers
In my Python program I concatenate several integers and an array .
For max : #CODE
replace ` argmax ` with ` argmin ` for min : have you considered there may be several array meet you criteria ?
print areasplit [ argmax ( sum ( areasplit == 0 , axis=0 )) , :]
TypeError : list indices must be integers , not tuple Thank you
I am trying to use ARGWHERE like : meanr = areasplit [ argwhere ( sum ( areasplit == 0 , axis = 0 ))] However : meanr = areasplit [ argwhere ( sum ( areasplit == 0 , axis = 0 ))]
pandas HDFStore.append data_columns after append operation
In my append operations , I am passing a list of data columns via the data_columns parameter so that I can perform out-of-core filtering via HDFStore.select passing conditions to the where parameter .
I am guessing that each time an append is made to HDFStore with new data , the index in data columns is recalculated .
Determined the max string length of each column to be passed to ` min_itemsize ` .
However , part of the MCP class attributes necessary for this are the start and end indices .
Add a single row of zeros at the top and the bottom of your array , and specify the start and end points on those .
Now I have the start and end indices for my mcp process .
Basically , it is part of a larger code that I use to modify this csv file and encode it differently .
Well if you have Series that are indexed the same , then its clear you should use a DataFrame , see @USER answer ; however if they are NOT indexed the same , then you could present the user with different Series ( one float and one boolean ); IMHO , this is much better off done with a DataFrame , must simpler ; you can effectively have more indices if you need ( either multi-level or use additional columns ) .
As @USER said , the best way is going to be to append a ` Series ` with ` object ` ` dtype `
And if you had that , this would be a one-liner ; you could need to transpose the right-hand array into the second axis , then use the ` == ` operator .
Of course you can also apply ` nonzero ` , grab the first axis , whatever you want to do , you can vectorize it as long as you have a vector in the first place , instead of a big collection of separate values that are only related by the meta-information in the variable names you happen to have bound them to .
Numpy TypeError : list indices must be integers , not tuple , how to change the code ?
Is there any way to select subarrays with min and max number of white pixels ?
i had to transform the result with : numpy.power ( scipy.stats.norm.pdf ( numpy.r_ [ - 100:100 ]) *sqrt ( 2.0 * numpy.pi ) , 1 /( 25**2 )) . and even then it simply jumps to zero at + / - 39 indices from the middle ( index 61 and 139 ) . compared to calling gauss ( numpy.r_ [ - 100:100 ] , 25.0 ) . what i want is simply the gaussian function - #URL but thanks for the answer anyway :-)
You can find the min over a stack of images in a fast vectorized fashion without looping over pixels .
You can ` reshape ( 11 , 25 )` and then call ` mean ` only once ( faster ): #CODE
If you do not mind switching row / column indices you can drop the final ` swapaxes ( 0 , 1 )` .
To count the number of zeros in the subarrays you could do this : #CODE
The leading theory is from @USER comment that ` np.einsum ` can make use of SSE2 , but numpy's ufuncs will not until numpy 1.8 ( see the change log ) .
It's been covered before ( particularly with regards to ` sum `) , but I'm surprised that ` einsum ` is consistently ~2x faster than ` outer ` , ` inner ` , ` kron ` , etc .
I was trying to point out my timings , in numpy 1.7.1 are very different then these , on my machine your last example is 540us ( einsum ) and 1.18ms ( sum ) .
Because sum is naive ( might not be on 1.8 + not sure ) , while einsum is specifically written to use of SIMD instructions , most of the ufuncs do not .
In addition to what @USER already said , ` sum ` uses a more appropriate accumulator for arrays
For example , ` sum ` is more careful about checking the type of the input and using an appropriate accumulator .
Note that the ` sum ` is correct : #CODE
Do you have a good link for how ` sum ` picks the accumulator ?
Interestingly with your ` x ` array extend to ` 1E8 ` elements ` np.einsum ( ' i -> ' , x , dtype= np.uint64 )` is only about 10% faster ( 15ms ) then ` sum ` .
@USER - The documentation for ` sum ` has some details .
You can specify it with the ` dtype ` kwarg to ` sum ` .
Also , ` sum ` is implemented through ` np.add.reduce ` , so have a look at the source for reduction ` ufunc ` s here , if you're interested in the details : #URL
Efficient way of OpenEXR to 8b it format with gamma encoding by OpenEXR and numpy ?
I'm looking for more efficient solution of converting .exr to 8b it JPEG with gamma encoding .
Each experiment outputs a file which has various info like mean / std / throughput / latency / nth percentile etc .
See docs ( #URL ) ` np.std ` returns the std of the flattened array , as does ` sum ` .
Well , v [ 0 ]= =v [ 2 ] , thus in the kl function p-q is 0 , then the sum is 0 .
According to [ wikipedia ] ( #URL ): " The K L divergence is only defined if P and Q both sum to 1 and if Q ( i )= 0 implies P ( i )= 0 .
Note that you multiply ( p-q ) by the log result .
I even answered a question on JS divergence on stack overflow .
This means basically that what you pass to the function should be two array-likes , the elements of each of which sum to 1 .
Kullback-Leibler divergence or Kullback-Leibler distance ) S = sum ( pk *
log ( pk / qk ) , axis=0 ) .
The bonus of this function as well is that it will normalize the vectors you pass it if they do not sum to 1 ( though this means you have to be careful with the arrays you pass - ie , how they are constructed from data ) .
The row variable Konskund_MEAB contains strings ( a few hundred different ) , ProdID is numerical and have 4 unique values .
` scipy.signal ` has both ` convolve ` and ` fftconvolve ` methods for you to play around .
It looks like FFTW has acceleration : " Sizes with small prime factors are best , but FFTW uses O ( N log N ) algorithms even for prime sizes .
matplotlib : How to conditionally plot a histogram from a 2d array
I have a 2D array , where I am trying to plot a histogram of all the rows in one column , given a condition in another column .
( Most likely "` IndexError : too many indices `" . )
If your data is really a surface in 3D space , the natural way of storing would be to have two vectors of unique values of your two independent variables , ` a ` and ` b ` , and a 2D array with the values for the calls to ` f() ` with the Cartesian product of your two vectors .
What's happening is that the min and max of the data are changing .
Initially , you plot an array of all zeros , so ` imshow ` scales the image between 0 and 0 ( plus a tiny fudge factor ) .
You'll need to either a ) set the ` vmin ` and ` vmax ` of the data to the maximum range when you initially plot the image , or b ) update the color limits with the min and max of the data each time you update the image ( use ` im.set_clim `) .
I then use those indices on the data array to calculate the difference .
Test if numpy array contains only zeros
We initialize a numpy array with zeros as bellow : #CODE
Also , it is faster than ` numpy.count_nonzero ( a )` because it can return immediately when the first nonzero element has been found .
If the row does not contain a zero , you attempt to use np.vstack to append the row to an array called b .
I had to resort to manipulating the transpose instead .
To append data to a current array : #CODE
For example the array ` [ 1 , 1 , 1 , 2 , 2 , 3 ]` would be encoded in this histogram dictionary as :
Moreover your new data structure only compress the input data , but does not change the processing's speed .
If your data is univariate , then you can use an fft version of kernel density estimation which is much faster .
If you already have a histogram as your data , then you can use histogram smoothing .
The fft version in statsmodels is binning the data first .
The second step is similar to a convolution of the histogram .
It might be possible to use something similar for histogram data .
You can do Gaussian KDE yourself : you just first need to create the simple histogram with small enough step size .
Then convolve the result with the Gaussian using fftconvolve
I am trying to concatenate arrays in python similar to matlab #CODE
` axis=2 ` doesn't work , indices start from 0 in python .
` concatenate (( a , b ) , 1 )` or
1 ) the cumulative sum step generates and increasing sequence , so sorting is unnecessary ( 2 ) the scaling is because you have used a linear search algorithm while the numpy function most probably uses a O ( logn ) binary search .
a= numpy.cumsum ( a ) generates an ascending sequence as it is a cumulative sum .
As a side note , why are you looping over the column indices instead of just looping over the columns in the first place ?
and , if possible , append the name of the processed file before that row of output ( like : ` imagename 7897987 90890898 ... `) .
( I assume you're using a 2.x version for access to a full scientific stack for medical image processing , but please mention it if you're not . )
is it possible to stack a sparse and a dense numpy array in python ?
You can first obtain the indices and then use ` np.take ` to form the new array : #CODE
Something to note your function is taking square frames and placing the std dev in the bottom right index , not sampling around the index .
As you can see this duplicates a lot of indices drastically increasing your memory footprint likely ` arr_size*N*N ` .
Cool trick : you can compute the standard deviation given just the sum of squared values and the sum of values in the window .
It computes the quantity ` sqrt ( mean ( x^2 ) - mean ( x ) ^2 )` for each window .
We can derive this quantity from the standard deviation ` sqrt ( mean (( x - mean ( x )) ^2 ))` as follows :
So if you create an accumulated sum array over both dimensions from your array , you can get the sum over a window with a couple of sums and a subtraction .
@USER If you strip all unnecessary intermediate arrays from the code above , using ` np.add ` and ` np.subtract ` with ` out ` keywords , it runs about 20-30 % slower than ` uniform_filter ` , with similar performance for things like independence of window size .
I'd like to make a histogram where the data live in periodic space but one bin intersects a boundary , that is , the leftmost and rightmost bins should be one and the same .
` np.bincount ` than returns the number of times each bin is hit , which is the histogram you want .
You can also do this with ` histogram ` and some slicing tricks #CODE
The n+1 value for each current value for indices 5 .
Curiously I wouldn't get this error for the n-1 value which would mean indices smaller than 0 .
This is what I would like to achieve , I flatten a matrix to an numpy array on which I want to calculate the mean of the 6x6 neighborhood of each cell : #CODE
I think you don't need to flatten you 2D-array , that causes confusion .
It excludes ` ( i , j )` because of the ` - arr ` bit , which removes the original value from the window sum .
Is it an array of indices such as that from a numpy.where ?
If you want to just ignore your ` nan ` s when plotting , then strip them out #CODE
Suppose I have an array : ` adata=array ([ 0.5 , 1 ., 2 ., 3 ., 6 ., 10 . ])` and I want to calculate log likelihood of Weibull distribution of this array , given the parameters ` [ 5 ., 1.5 ]` and ` [ 5.1 , 1.6 ]` .
On my computer this is a max of 20 columns and 60 rows .
This is the way to look at it , the diagonal cell , i.e. , ` ( 0 , 0 )` cell , is the correlation of your 1st vector in X to it self , so it is 1 .
@USER because the distribution is not well sampled I am looking for something that calculates the center of the peak rather than the maximum position in the image ( the indices should be float values )
While this may occasionally make matrix operations a bit less concise ( especially in the case of ` dot ` for matrix multiplication ) , it produces more generally applicable code for when your data is naturally 1-dimensional or 3- , 4- , or n-dimensional .
By giving one number as one of the indices ( like 0 in 3rd row ) , you reduce the dimensionality by one .
By giving a range as one of the indices ( like 0:1 in 4th row ) , you don't reduce the dimensionality .
Or , you could initialize an array of all zeros if you know the size of the array ahead of time .
append values ( for filling a vector ) #CODE
or append lists ( for filling a matrix row or column ): #CODE
And what if you actually need the locations / indices for future operations ?
I think if ` UNIQ_IDS ` really has the unique entries of ` obj_data ` precalculated , you can call ` np.digitize ( obj_data , UNIQ_IDS ) - 1 ` to get the same result as your ` uind ` in about half the time .
Is it possible to create a sparse matrix using these vectors in python such that I have zeros in place of elements for the vectors which are smaller than the maximum size ?
It could be rewritten to generate the indices of a CSR matrix directly , but that would be much less clear .
Both ` 2**a ` and ` log a ` are single floating-point instructions on modern processors .
Mathematically we can write ` x**y = exp ( y*log ( x ))` .
It seems you could apply this technique generically by splitting any integer into a sum of the powers of two , computing each power of two as above , and summing : #CODE
( On the other hand , if you do have a good reason to use ` numpy ` here , you may want to read the lines into a ` pandas ` table and apply the random indices to that . )
The Y axis should be a log axis .
I.E. log ( y ) .
OP asked was unclear whether wanting log axis on y or wanting y-coordinates to be log ( y_data ) .
Use ` plt.yscale ( ' log ')` for pyplot interactive mode .
In both cases you are dropping the ` if ` statement , in case you have to keep it , you can use ` np.any ` which will return ` True ` if any element in ` Check ` has a ` True ` value , avoiding the sum .
And it's better to reshape it in-place .
You can use reshape and change the order parameter to FORTRAN ( column-major ) order : #CODE
As proposed by @USER , you can use a reshape passing ` order= ' F '` , and " if possible " the returned array will be only a view of the original one , without data being copied , for example : #CODE
` map ( max , abs ( a [ i ] -a ) )` is very slow .
Use the vectorized max #CODE
@USER [ docs ] ( #URL ) If the index arrays do not have the same shape , there is an attempt to broadcast them to the same shape .
If they cannot be broadcast to the same shape , an exception is raised :
You need to add those extra dimensions of length 1 at the end of the first indexing arrays , for the broadcast to work properly .
In case myarray is a masked arrary , I get : TypeError : tuple indices must be integers , not tuple .
To answer your question about why it doesn't work : when you use lists / arrays as indices , Numpy uses a different set of indexing semantics than it does if you use slices .
how does ` y ` correlate with ` x ` , in a general equation of the form : ` y = ax + b ` ?
In the example below I create a CSR matrix from three arrays : the data , the indices and the index pointers .
Thus far I've been using the trick of storing and loading the data , indices and indptr , which seems messy
I was thinking of finding the correct ` id ` of the starting and ending positions using binary search and then access ` self.f ` using these ids , but is there a way to implement above syntax ?
Once the missing dates and their corresponding values ( 0 ) are in , I want to sum up the values for each date , so that no date repeats - for each sublist .
Now , What I am trying to goes as follows : I am breaking up A's dates and values separately ( in lists named u and v ) and converting each sublist into a pandas Series , and allocating their respective indices to them .
I need to find the indices of all the elements in a that are not present in b .
I am genuinely interested in the indices only .
But I'm trying to use SciPy stack .
How to get the cumulative sum of numpy array in-place
OBS : your array is actually a 2-D array , so you can use ` axis=0 ` to sum along the rows and ` axis=1 ` to sum along the columns .
Currently I am looping through the arrays and using numpy.dstack to stack the 1000 arrays into a rather large 3d array ... and then will calculate the mean across the 3rd ( ? ) dimension .
There's no point trying to remove an outer for-loop .
For all intents and purposes , an outer for loop is always free -- It's nested for loops that you need to worry about .
Maybe you could calculate ` mean ` and ` std ` in a cumulative way something like this ( untested ): #CODE
The calculation of ` std ` in this way might suffer from numerical problems , since you might be subtracting two large numbers .
If that is a problem you have to loop over the files twice , first to calculate the mean and then accumulate ` ( image - mean_image ) **2 ` in the second pass .
I thought it might be due to Pandas somehow sharing metadata about Index objects and perhaps being allowed to truncate when inserting data if the data comes from an object that has the same index .
So the takeaway is that if you happen to use the same indices for the two DataFrames and one is a truncated version of the other ( even if you didn't intend for them to be semantically seen as the " same " index ) then it will accidentally work .
There are a ton of uses for indices besides aligning , so it seems overly specific to me that Pandas will assume row indices are always intended for aligning .
In my case , the DataFrames do have a relationship , and the " proper " way to do this is to first calculate the column that I want in the second DataFrame ( the argument of ` np.where `) , then perform a ` merge ` or ` join ` based on other ( non-index ) columns .
Your problem is a bit ' unique ' in that sometimes you don't want to align .
It seems really undesirable that I'd have to do this extra work instead of just getting that back directly from the broadcast performed by ` np.where ` .
numpy gradient function and numerical derivatives
Only the gradient of y ( x1 ) returns the correct result .
not those ones ; I'm stuck on the distance transform so far , which in my algorithm comes before those function .
How to plot a 2d histogram ?
Before when i did it for a single histogram i did it like this ...
Heres the code i have for the 2d histogram .
And then you can do your manipulation with the histogram ` H ` .
If you take a look at your code , you have a line : ` pdf_t , bins_t = np.histogram ( ... )` which means that the ` histogram ` function returns two values which you store in ` pdf_t ` and ` bins_t ` variables .
` C ` is initially filled with zeros .
` A ` gives indices of ` C ` elements which should be changed ( they may repeat ) , and ` B ` gives values which should be added to initial zeros of ` C ` .
In this case I can hardly believe is LAPACK to blame since they only use the dot product .
And in Matlab for sum2 I am using a dot product because I do not know any trick like the one you told me for Numpy .
Could I optimize it even more redesigning it so all the operations ( or at least most of them ) were between fortran contiguous matrices or between C contiguous matrices and then apply the most convenient dot product operator or the improvemnet would be negligible ?
He needs a way to reshape it to ` ( 25 , 1 )` .
First transpose the square matrix , multiply , then transpose the answer .
Better to transpose the row to a column matrix then you don't have to re-transpose the answer .
You could also transpose twice : #CODE
You could also use matrix multiplication ( aka dot product ): #CODE
` dot ` is really overkill here .
And ` np.einsum ( ' ji , j- ji ' , A , b )` does , in effect , the double transpose .
[ I adapted this from pv for a faster ` dot() `] ( #URL ) when calling from Cython
But It likes the histogram operation .
possible duplicate of [ Assigning identical array indices at once in Python / Numpy ] ( #URL )
` mat = vec2mat ( vec , matcol )` .
However , vec2mat adds zeros to the last row if there are not enough values in your vector to completely fill the last row .
Therefore four zeros are added to the bottom row .
How would I go about adding the extra zeros in python to create a 2D array ?
Except that ` resize ` simply adds zeros to the contiguous memory block .
@USER are you aware that it won't work if you need to resize on any axis except the most minor axis ?
The solution below is if you need to expand or shrink an array to arbitrary size , padding with zeros as necessary .
Edit : The problem with ` resize ` as a solution is that it expands the array in memory only and then effectively reshapes that new memory block to your requested size .
My understand is that you require an array with the first column as ` [[ 1 ] , [ 2 ] , [ 3 ]]` and the second column as all zeros .
Take a look at the stack : #CODE
Might want to round it though : ` round ( sum ( x ) / float ( len ( x )) , 2 )`
To do it more efficiently without duplicating calculations and only calculate unique pairs : #CODE
Ideally , I would create a last value that is the mean of the remaining ones whilst keeping the code compact .
Just for fun , you _could_ use ` sorted ` to sort by indices .
You just need to use ` enumerate ` to decorate the list with its indices , sort , then undecorating by stripping the indices : ` [ item [ 1 ] for item in sorted ( enumerate ( list_1 ) , reverse=True )]` .
I believe this is a case of outdated documentation rather than something you're not supposed to rely on , since similar routines such as ` sum ` document the same feature .
You can reshape and then perform the average : #CODE
And with this matrix I can easily sum the grouped elements to form a new element in the 3x3 matrix .
I use lexsort to generate the list with indices .
However when I create an array using bult-in functions like rand everything is fine #CODE
You can you ` vstack ` combined with transpose : #CODE
Numpy AttributeError : ' float ' object has no attribute ' exp '
And this line gives me error " AttributeError : ' float ' object has no attribute ' exp '" .
Efficient way to find the index of the max upper triangular entry in a numpy array ?
More specifically , I have a list of rows / columns that need to be ignored when choosing the max entry .
In other words , when choosing the max upper triangular entry , certain indices need to skipped .
In that case , what is the most efficient way to find the location of the max upper triangular entry ?
I need to find the index of the min element among all elements in the upper triangle except for the entries ` a [ 0 , 1 ]` , ` a [ 0 , 2 ]` , and ` a [ 1 , 2 ]` .
So , in the example , are you taking the max of just [ 1 , 4 , 6 ] ?
Do you include the diagonal or not ?
Yup , just [ 1 , 4 , 6 ] and the diagonal is not included .
I did a naive implementation traversing all the elements in the upper triangle and checking if the indices are in the indices_to_skip list .
Something interesting that I ran across , a faster way to take upper triu indices : #CODE
This looks good but what about finding the index of the max element , not just the max element itself ?
I get ` Numpy Operands could not be broadcast together with shape ( 200 , 1 , 25 , 25 ) ( 200 , 1 )` error by the division of two array with the following dimention #CODE
Numpy matches up axes for broadcasting starting from the right ; ` ( 25 , 25 )` gets matched up with ` ( 200 , 1 )` in the first example and fails to broadcast , but ` ( 4 , 4 )` matches up with ` ( 4 , 1 )` and broadcasts successfully .
Question : how to concatenate , vstack or hstack of titled numpy array ?
If you want to concatenate the datatypes , then you have to create a new datatype .
The upshot is that to join individual blocks of memory , you actually have to modify the ` dtype ` of the array .
I want the new array to have titles inherited from the join ...
As Bhajun said , curve_fit expects a 1D array , so you need to flatten the result : #CODE
You will need to reshape your array ` x ` before saving .
If not , reshape away !
Expand numpy array of indices into a matix
How to return all the minimum indices in numpy
Return the indices of the minimum values along an axis .
will return an array of all indices : which will be ` [ 3 , 4 , 5 , 7 ]`
In case of multiple occurrences of the maximum values , the indices corresponding to the first occurrence are returned .
The phrasing of the documentation ( " indices " instead of " index ") refers to the multidimensional case when ` axis ` is provided .
It returns the first index of each minimum value , not all indices of a single minimum value .
To get all indices of the minimum value , you could do #CODE
sum parts of numpy.array
I want to sum the first two values of the first row : ` 1+2 = 3 ` , then next two values : ` 3+4 = 7 ` , and then ` 5+6 = 11 ` , and so on for every row .
You can do ` np.sum ([ a [: , i :: n ] for i in xrange ( n )] , 0 )` if you might need to sum ` n ` consecutive columns , for example .
The behaviour shall be the same as x= np.range ( 0 , 10 , 1 ) and y = np.sin ( x ) where y is then a list conrtaining the values of the sin function .
I need to sum up the columns 1-4 over all * .columns files and plot the result against the first column of any of that ( all equal ) .
I guess you probably need to sum up columns 1 to 4 inside the loop : ` accum += data [: , 1 :] ` and then plot that directly after the loop : ` plt.plot ( data [: , 0 ] , accum )`
I think you are trying to sum columns in an imported array .
should give the sum of the columns .
I need to sum up all the elements on a certain position over all files in that folder and put that sum into an array element on the same position .
Can you post a ` Series ` object that reproduces the stack overflow ?
One of the methods a class can have is the ` __getitem__ ` method , this is called whenever you append ` [ something , something ... something ]` to the name of the instance .
The cosmetic difference is that you use square brackets instead of curved ones , so you are not doing a function call , but you are actually indexing the object .
You have to rewrite the entire file , you can't just append to each line .
Premultiply both sides by the transpose of ` A ` and solve using a matrix solver .
If that wasn't appropriate , you could take the natural log of both sides and it's a simple linear equation .
convolve unevenly spaced vectors in scipy
Now I need to convolve this spectrum with a Gaussian function of predefined FWHM .
Each time , numpy has to take the cumulative sum of the p list , put that into a new vector , and then iterate over it .
You're effectively doing preprocessing by knowing that there are only three variables , and that the sum of the first and third is .5 .
I suspect the generality of ` np.random.choice ` is slowing it down , more so for small samples than large ones .
Are you building the outer product using numpy commands ?
This is a 3D array where the unique combinations are in the last two axes .
Seems to be reasonably fast , 1M unique combinations takes about 1 / 6 of a second .
First , you have a binomial response : having or not having a particular behavior .
K-means is designed to minimize within-cluster variance (= sum of squares , WCSS ) .
numpy.array indices via argparse - How to do it properly ?
I then have a set of boolean indices to select a certain part of this slice : #CODE
If I want to set these indices to a different value I can do it easily : #CODE
However , if I create another set of boolean indices to select a specific part of the indexed array : #CODE
and then try and set the value of the array at these indices : #CODE
I thought maybe I needed to AND the two index arrays together , but they are different shape : ` indices ` has a shape of ` ( 5 , 5 )` and ` high_indices ` has a shape of ` ( 12 , )` .
So when you indexed it first time with boolean index in ` a [ slice_ ] [ indices ] [ high_indices ]` , you got back a copy , and the value 42 is assigned to a copy and not to the array ` a ` .
Finally take the max along the last axis .
That last reshape after swapping the axes triggers a copy of the full array , which can be costly for large arrays .
Actually , I rephrased the max as an array operation ( because that's what seemed most natural ); I just tried it as an explicit loop , which is more like OP's code anyway , and it's doing much better ( about 4x as much time as Ophion's , instead of 125x ) .
Unfortunately , you do seem to have to write out the max-finding operation yourself ; you can't call ` max ( arr [ ii , jj ] for ii in ... for jj in ... )` because generator expressions aren't supported , and doing it with a list comprehension gives a 6x slowdown .
Why is sin ( 180 ) not zero when using python and numpy ?
Pi cannot be represented exactly as a floating point number , so ` sin ( pi )` isn't going to be exactly zero .
And ` sin ( 3.1415926535897931 )` is in fact something like ` 1.22e-16 ` .
You have to work out , or at least guess at , appropriate absolute and / or relative error bounds , and then instead of ` x == y ` , you write : #CODE
If you really have no clue , you can use the defaults , which are ` 1e-5 ` relative and ` 1e-8 ` absolute .
If sin ( double ( pi )) returns zero then your sin function is broken .
The bug I point out in the article is that sin ( double ( pi )) on some machines returns the * wrong * non-zero number .
Cos works because the error is below the threshold of accuracy for 1.0 , which is related to calculus ( slope of cos ( pi ) is zero , slope of sin ( pi ) is one ) and the magnitude of the answer .
res [ sl ] = _fitpack._bspleval ( xx , xj , cvals [ sl ] , k , deriv ) IndexError : too many indices
I just edited the original answer to include a solution for non-overlapping ones .
I assume the number produced in exp is too big to fit in a ` float64 ` .
what is the range of the exp function ?
Maybe you should consider turning all the huge variables into a global variables.That worked for me when i got stack overflow in visual studio ( c++ ) .
The OP is getting a * numeric * overflow , not a * stack * overflow .
Is there a particular reason you do not want to flatten the array ?
Im not sure if this defeats your " cannot flatten the array " , but without reshaping the array in some way all of these methods will be vastly slower then the following : #CODE
The " best " solution will depend on the reason why you don't want to flatten the array in the first place .
I have some normalised histogram data in array of shape ( 12 , 1 ): #CODE
The ` leastsq ` algorithm stops when the sum of the squares of the differences ( the so-called " residuals ") between the fitted model and the supplied data is less than the ` ftol ` , or if the relative error between two iterations is less than ` xtol ` .
Taking the log of both sides , you get #CODE
will give you ` slope2 == -k ` and ` offset2 == log ( Vs - Vi )` , which should be solvable for ` Vi ` since you already know ` Vs ` .
You might have to limit the second fit to small values of ` t ` , otherwise you might be taking the log of negative numbers .
` findiff = scipy.misc.imresize ( diff , 30 ., interp= ' bilinear ' , mode=None )
diff is a 699x699 array .
Instead , it took the diff array and plotted it multiple times in the basemap window .
Instead , it took the diff array and plotted it multiple times in the basemap window .
The calculations all perform correctly but the array stays as just zeros .
And after this i taken histogram , where frequency for every item about 5 , not 0.25 % .
You could check the histogram result by assigning ` plt.hist ` as follows : #CODE
I have made a distance detector using a webcam and laser to estimate the distance of the laser dot .
I am setting a threshold with the numpy arrays , just looking for the reddest pixels using xy_vals = [ img_array_rgb ] > 236 . this gives me an array of true elements where the red dot is .
What do you do with the indices of the red dot after finding them ?
I need to set the upper y axis limit to 1 . leaving the lower limit to whatever the min value in ` m2 ` is ( ie : let ` python ` decide the lower limit ) .
In this particular case I could just use ` plt.ylim ( min ( m2 ) , 1.0 )` but my actual code is far more complicated with lots of things being plotted so doing this is not really an option .
You could combine this with ` itertools.chain ` to flatten the pairs from enumerate : #CODE
Oversampling / gridding a 2D histogram
I've got a numpy 2D histogram that bins data points with two spacial coordinates ( x_array and y_array ) and averages the values of a third , data_array component within each bin .
e.g. max of a , and min of b for example #CODE
Hi low diff is just #CODE
Surely the index on datetime fulfils the need of the rolling_max and min to be able to go back / forward 15 records ?
Array A contains indices for rows in X that need replacement , with the value -1 .
Note : my example parses a grayscale image - modify the count argument and reshape function accordingly for a color image
Do I need to reshape a row or a column of an array into a matrix for every matrix multiplication , or are there other ways to do matrix multiplication ?
You can also do this with sum : #CODE
What does -1 mean in numpy reshape ?
A numpy matrix can be reshaped into a vector using reshape function with parameter -1 .
and now I would like to create a numpy 1D array consisting of 5 elements that are randomly drawn from array1 AND with the condition that the sum is equal to 1 .
You should pass ` len ( indices )` instead .
To get 5 elements that the sum is equal to 1 ,
substract the sum of 4 numbers from 1 -> x
Do you want each of the unique permutations of the result sequences to be equally likely ?
If so , a solution like ` ( 2 , 2 , 2 , 2 , 2 )` that has only a single unique permutation is going to be much rarer than the five unique permutations of ` ( 0 , 0 , 0 , 0 , 10 )` or the 20 unique permutations of ` ( 0 , 0 , 0 , 1 , 9 )` .
Alternatively , writing additional lines of code to manually replace each value I want to switch ( from ones or zeros for example , one of the standard matrix functions ) would be quite a lot of code writing to make my matrix ... and I have a sneaking suspicion that there's an easier way .
You should always use the outer brackets .
You forgot the outer brackets .
I would try to avoid changing the ` xticklabels ` if possible , otherwise it can get very confusing if you for example overplot your histogram with additional data .
I also added the option " aspect= ' auto '" in imshow() so that I can ' stretch and squeeze ' the seismic display .
You should turn the zeros to ` float ( ' nan ')` s , that should do the trick .
The simplest thing would be to turn ` nan ` s into zeros via ` nan_to_num ` .
If I turn those values into zeros , then they will be used in the convolution , modifying the final result ( i.e. lowering the velocity in the regions near the pixels with zero ) .
On the other the log transformation can do all sorts of unwanted stuff , so performing nonlinear fits on the original data without performing a log tranform may be the best solution .
I would like to create a 3-dimensional array and populate each cell with the result of a function call - i.e. the function would be called many times with different indices and return different values .
I could create it with zeros ( or empty ) , and then overwrite every value with a for loop , but it seems cleaner to populate it directly from the function .
What is the correct way to populate an array based on multiple calls to a function of the indices ?
fromfunction is being called once per cell - what do you mean by " multiple calls to a function of the indices " ?
Where ` indices ` is fairly equivalent to ` meshgrid ` where each variable is ` np.arange ( x )` .
Since ` fromfunction ` works on array indices for input ,
The documentation does not make this clear , but you can see that the function is being called on arrays of indices in the source code ( from ` numeric.py `) : #CODE
( My simple example , derived from the examples in the manual , may have been misleading , because ` + ` can operate on arrays as well as indices . This ambiguity is another reason why the documentation is unclear . I want to ultimately use a function that isn't array based , but is cell-based - e.g. each value might be fetched from a URL or database based on the indices , or even input from the user . )
You could also do it as an iterator , but the iterator still needs to track its own indices .
Use fancy indexing to take simple indices .
Note that the all indices must be of the same shape and the shape of each index will be what is returned .
Just as you can multiply that scalar ` number ` by that ` ones ` array to multiply all of its ` 1 ` values by ` number ` , you can pass that scalar ` number ` to ` allclose ` to compare all of the original array's values to ` number ` .
As a side note , if you really do need an array of all 2s , there's an easier way to do it than multiplying 2 by ` ones ` : #CODE
I'm not sure this solves your actual problem , because it seems like you want to know which ones are close and not close , rather than just whether or not they all are .
By the way , the ` close ` function you've defined is almost identical to the existing ` np.isclose ` except that the ` np.isclose ` will automatically broadcast the array shapes .
If you have up-to-date numpy ( 1.7 ) , then the best way is to use ` np.isclose ` which will broadcast the shapes together automatically : #CODE
Examples of things which could slow you down include allocation of arrays on the heap instead of the stack ( with expensive calls to ` malloc `) , although I would expect such effects to become less significant for larger array .
Also you can avoid the calculation of your indices by simply inverting of the loop : #CODE
I needed to add a step of -1 to make the loops work for inverted indices .
If the sizes are not powers of two , a small remainder loop has to take care of the dangling indices .
Also , relative times with respect to ` numpy ` could be deceiving , as the absolute time with numpy could be very bad .
ps : I am limiting the size of p to max 100 ( univariate ) , 100x100 ( bivariate ) and 100x100x100 ( trivariate ) .
Evaluating the code for small x ( x << 1 ) works great , however for x > = 1 I get an error norm of 1e40 .
What concerns me about this is that by the Perron-Frobenius theorem , all of the components of the second eigenvector should be positive ( since , according to Wikipedia , " a real square matrix with positive entries has a unique largest real eigenvalue and that the corresponding eigenvector has strictly positive components ") .
You are just misinterpreting ` eig `' s return .
As @USER points out in the comments , choice ( without replacement ) is actually sugar for permutation so it's no suprise it's constant time and slower for smaller samples ...
@USER What I find really surprising is it seems choice is no faster than permutation !
You don't need to import string , and you don't need to loop through all the lines and append text or count the characters .
I have a rather basic question for pandas , but I've tried merge and join to no success
So I want to just have one product code column with the unique product codes and their other data .
I'm a little confused by your question , specifically where " unique product codes " enter in ... are we condensing the data ?
Now , ` join ` requires no extra parameters ; it gives you exactly what ( I think ) you want .
I'm currently using matchTemplate using a singular black dot which finds all the filled in dots ( image is first converted to grey scale ) .
You may also want to enhance image by using histogram equalization , denoising and other techniques .
So , to sum up :
You can also use ` sum ` : #CODE
Applying the sum over the mask counts the number of ` True ` values .
You only need to have ` slice ` objects for the first ` axis+1 ` indices , the trailing ` : ` are added by default if missing , i.e. ` idx = ( slice ( None ) , ) * axis + ( slice ( start , end ) , )` will do the same as your ` ndim ` long list .
So ` temp ` should be a tuple or list of arrays to join ; if you mean to append to ` MasterArray ` , then your definition of ` temp ` will need to look something like ` temp = ( MasterArray , newrow )` .
This is not unique to numpy , you'll see similar usage patterns in matlab and c programs .
I am appending the data to temp and then I wanted to stack it into another array so all the data is in one matrix .
I've tried ` np.insert ` to add the second array to a test matrix I had in a python shell , my problem was to transpose the first array into a single column matrix .
Instead , I would like to generate K disparate subsets of S , such that the K chosen subsets minimize the sum of the size of the all pairwise intersections between the K subsets .
And then I sum the size of all of those sets together .
Later : note that the sum of the size of the intersections of all distinct pairs of returned subsets can be computed easily from the ` index2count ` vector : it's #CODE
Instead ` hi ` is the set of indices with the larger of the two values , and ` lo ` the rest of the indices ( ` lo ` is empty if all the ( conceptual ! not computed ) values are the same ) .
By construction , every prefix of the generated sequence minimizes the sum of the sizes of the intersections of all pairs of sets in the prefix .
Finally , a variation that more obviously just repeatedly cycles thru all the indices .
Every time I try , it fails with the error : ` gcc : internal compiler error : Killed ( program cc1 )` , and then further down the line I get a bunch of python errors , with easy_install I get : ` ImportError : No module named numpy.distutils ` , and with pip I get : ` UnicodeDecodeError : ' ascii ' codec can't decode byte 0xe2 in position 72 : ordinal not in range ( 12 8) ` .
It goes through and if ` b ` is 1 continues to add to the cummulative sum , otherwise it resets it .
How to flatten only some dimensions of a numpy array
Is there a quick way to " sub-flatten " or flatten only some of the first dimensions in a numpy array ?
e.g. to flatten all but the last dimension : #CODE
If there are five consecutive zeros , I do not want to take the average and simply say the average is zero .
But this doesn't work as then I need to check that the new section does not contain zeros and it would detect the original zeros .
I could check the extension for zeros , repeating the process , but this seems like a very long winded way to do this .
but I want the output ( for any i in first dimension ) to be the transpose of this .
Because I don't see how you can assign a 1D array to a 3D slice , and when I try it , as expected , ` x [: , : , : , 0 , 0 ] = y ` raises ` ValueError : operands could not be broadcast together with shapes ( 20 , 11 , 11 ) ( 10 )` .
If you want the output to be the transpose , just do : #CODE
I actually want the output to look like the transpose of this ( don't want the actual transpose )
So , just reshape it on the fly : #CODE
I presume that you think ` log ( y )` is some polynomial function of ` log ( x )` , and you want to find that polynomial ?
` poly ` is now a polynomial in ` log ( x )` that returns ` log ( y )` .
So here we took a 3rd degree polynomial in log x that fits with log y . and then calculated new values of y taking exponential of f ( logx ) .
I plotted using ax.plot so not on log scale .
You fitted a polynomial to ( log ( x ) , log ( y )) data and then plotted x vs .
If log ( y ) = P ( log ( x )) , where P is some polynomial , then y = exp ( P ( log ( x )) = yfit ( x ) , by definition of yfit .
Starting from an array indicating bin indices , I want summed up values for those bins .
i have a 3d array of zeros and i want to fill it with a 1d array : #CODE
You could reshape ` s ` from ( 5 , ) to ( 5 , 1 ): #CODE
You can do the reshape more compactly as ` C [: ] = s [: , np.newaxis ]` .
Python and integer representation ( leading zeros issue )
Note that you can use the Polynomial class directly to do the fitting and return a Polynomial instance .
the problem is optimizing ` f ( x )` where ` x =[ a , b , c ... n ]` . the constraints are that values of a , b etc should be between 0 and 1 , and ` sum ( x )= =1 ` .
Creating an ndarray using permutation is too long .
Now , the other part , ` sum ( x )= =1 ` .
There may be more elegant ways to do it , but consider this : instead of minimizing ` f ( x )` , you minimize ` h=lambda x : f ( x ) +g ( x )` , a new function essential ` f ( x ) +g ( x )` where ` g ( x )` is a function reaches it minimum when ` sum ( x )= 1 ` .
Such as ` g=lambda x : ( sum ( x ) -1 ) **2 ` .
` sum ( k for k in x )` is also known as ` sum ( x )` .
I am wondering if there is way to sum up the same index in each array in the list giving me only 1 array ?
Now that you have a single array you can sum on the axis you need using ` np.sum ` : #CODE
@USER It is just taking the sum along a different dimension .
On the other hand , in the context in which one would use ` -- pylab ` the numpy version of things like ` sum ` and ` max ` are the ones you want ( IMO ) .
Passing a C++ std :: Vector to numpy array in Python
By the way you can use C++ std :: vector in the call to ` PyArray_SimpleNewFromData ` .
If your std :: vector is ` my_vector ` , replace ` fArray ` with ` my_vector [ 0 ]` .
Now to convert a c++ std :: vector to a numpy array is extremely simple .
However I want a unique plot that combines both features .
This is a standard numpy feature , since you can call ` np.sin ` as ` sin ( array ([ 4 , 5 , 6 ]))` , or as ` sin ([ 4 , 5 , 6 ])` or as ` sin ( 4 )` as well .
I tried , ` strip ` , ` set ` , ` uniq ` but that is not working ; it gives the error :
If you want it to be unique based on the first value , just convert it to dictionary and back .
In the case of your data you can get the max as : #CODE
If you want to show also the times , keep them in your DataFrame as a column and pass a function that will determine the max close value and return that row : #CODE
@USER ` max ` is doing completely different thing - it returns the maximal value .
One cool thing about using ` head ` and ` tail ` ... you get a reverse roll with negative ` n ` , e.g. #CODE
I was thinking of something similar with ` append ` , but why bother now :)
Should be all ones right ?
I'll note that while my matrix R is sparse , the sparsity pattern is such that every row has at least one entry , and every column has at least one entry , such that the result of the unique calls is the full range of m and n .
In short , if you just need the CI for each parameters , you just need the diagonal element of ` vcov ` matrix .
The off diagonal elements are co-variance .
Also you can think it this way : the diagonal elements are the covariance of each parameter to itself , which is the variance .
Taking ` sqrt ` of those gives you the standard errors .
You can count the number of - 1's in the ` diff ` ed array .
Note , you'd also need to check the last element -- if it's True , there wouldn't be a -1 in the ` diff ` ed array to indicate that .
Better yet , you can append ` False ` to the array before ` diff ` ing .
To append ` False ` at the end : #CODE
A trick to avoid the ` append ` operation ( and make your code more obscure ) is by doing : ` ( d 0 ) .sum() + a [ -1 ] 2 ` ( i.e. , if a [ -1 ] is True , count it as a block ) .
@USER , I'd really be surprised if diff was faster than !
= because you need to copy a bool array to do a diff .
= operation on the diff result .
So diff can be faster in some situations , as shown above , but that doesn't include the second pass you'd need to do .
Since you want them to still be in the original order , you can select from indices like in @USER ' s answer , but then sort them and grab from the original list : #CODE
You can generate a random sample without replacement by shuffling the indices of the elements in the source array : #CODE
Sorry , my question more specifically was how I can get the indices of the True booleans in the array ?
@USER You are getting those , I've labled the indices in gray , and note that the numbers underneath the ` True ` s are the numbers returned by ` where `
Scikit Learn Gaussian HMM : ValueError : startprob must sum to 1.0
ValueError : startprob must sum to 1.0
I fixed the issue by setting the params attribute to a set of all unique values in my training set .
I have a text file which contains a log like this : #CODE
extract indices from multi dimensional array using condition , max
I want to be able to extract the indices of the first value in each row of a certain ( 2d ) slice that meets a condition , then use those indices to extract the value of the corresponding indices in another slice .
Now , I want to create an array of indices of the first element in each row of sliceB that meets a condition ( eg . > =35 ) , ie these values : #CODE
then use that to create an array of values in sliceA with the corresponding indices , ie : #CODE
Now I want to run gradient descent to get minimum value of theta .
I have converted my costFunction in python using numpy library , and looking for the fminunc or any other gradient descent algorithm implementation in numpy .
I would later loop the elements and append to each of them .
In working with some text data , I'm trying to join an np array ( from a pandas series ) to a csr matrix .
When must call transpose , call it like ` .transpose() ` or ` .T ` otherwise you are actually passing the function object to create the array giving the verified object type ...
You are using ` transpose() ` , not ` transpose ` as in the question .
An ndarray does not have a transpose attribute , but it does have a transpose() method ; and as far as I can tell from the docs , the sparse.csr_matrix() method does not take a method as an argument .
However , feel free to post a link which describes ndarray's transpose attribute , or a link describing how csr_matrix() takes a method from another class as an argument .
It was definitely the transpose .
EventIDs are unique .
PictureIDs are not unique , although PictureID + Date are distinct .
So that I can then stack ( ? ) this new data frame into something that provides period counts for all unique PictureIDs : #CODE
I.e. because rows 2 and 3 are both for Picture A in 2010-02 , then adding the sum of the previous 6 periods would just count 1 .
Divide all samples in window by max amplitude sample value ( sensitive to noise and outliers ) before FFT
As far as normalization goes , I'd recommend scaling the microphone signal's spectrum so that its energy matches that of the WAV file's spectrum ( where " energy " is the sum of the squared magnitude of FFT coefficients ) .
to your 2nd point : by scale you mean divide the spectrum periodogram by the sum of the square of the abs value of the coeffs ?
Since I'm going to use signal energy as a measure of " size " I want to find a constant coefficient such that sum ( abs ( c*A ) **2 ) = sum ( abs ( B ) **2 ) , where c is the constant coefficient .
You can calculate the sum of square absolute values of the audio samples or you can calculate the sum of square absolute values of the FFT coefficients .
` nonzero ` returns a tuple ( one item for each axis ) of indices of the ` True ` rows : #CODE
Also , I do not want to reuse elements - I want a unique bijection between each value A [ n ] [ 1 ] to B [ n ] [ 1 ] within the interval A [ n ] [ 0 ] + / - 0.5 .
But each grayscale value must be translate to the -1 ..
But is there a similar / easy way to translate this values using the function I mentioned above ?
All of the operations inside of your ` translate ` function can be applied directly to your array :
Just stating to use numpy so maybe it's something unique to numpy arrays ?
This was my first stack overflow question .
Can someone tell me how to join two unequal numpy arrays ( one sparse and one dense ) .
This is why you get the AttributeError : a normal Python float doesn't have the ` exp ` method , which is a numpy ufunc method .
You have it reversed , which is why you are having to transpose the ` z ` values to make it work .
To avoid requiring the transpose , create your array as : #CODE
Yes , but that's exactly my question : len ( x ) is the _first_ index , so the reshape must obey that , and so the correct answer is reshape ( n_x , n_y ) , and not the other way around .
You didn't get an error with the ` reshape ` call because numpy will gladly reshape your array to any shape that keeps the number of elements the same .
numpy diff
numpy nonzero
I'm trying to map a color histogram where each pixel also as another ( float ) property , alpha , from a similar size array .
I want eventually to have a dictionary of ( color ) -> ( count , sum ) where count is actually the histogram count for that color , and sum is the sum of alpha values that correspond to a certain color .
Which is a function that sums up vector according to another vector of indices .
The radial ( r ) and tangential ( t ) derivatives are the components of the gradient in polar coordinates :
The centerpoint is related to indices of the data . the r is dependent on x- x0 and y - y0 , where x and y are the array indices and x0 and y0 are the coordinates of the centerpoint .
I am trying to fit a gamma distribution to my data points , and I can do that using code below .
I want to reconstruct a larger distribution using many such small gamma distributions ( the larger distribution is irrelevant for the question , only justifying why I am trying to fit a cdf as opposed to a pdf ) .
could you just construct an emprical cdf from your data and fit it to the gamma cdf using eg ` curve_fit ` , #URL ?
I think your ` ss.gamma.fit ( dataPoints , floc=0 )` doesn't produce any meaningful results , because your dataPoints are not a sample from a gamma distribution .
Or , fit sample points to a truncated version of the gamma distribution .
And secondly the cdf for gamma is slightly non-trivial ( but of-course possible ) .
@USER I have used this only as an example , I am not really trying to fit something that is deterministically not gamma to gamma .
And what do you mean by fit to the shape of cdf , and truncated version of gamma distribution ?
I understand that you are trying to piecewise reconstruct your cdf with several small gamma distributions each with a different scale and shape parameter capturing the ' local ' regions of your distribution .
Don't know if you have specific reasons behind specifically fitting several gamma distributions , but in case your goal is to try to fit a distribution which is relatively smooth and captures your empirical cdf well perhaps you can take a look at Kernel Density Estimation .
This is a bug which was fixed in Numpy 1.6.2 ( Change log here ) .
( Change log here )
However , it contains a dead link to #URL So , I then did a search for " numpy bug 1675 " which then helped me find the change log on sourceforge.net .
Numpy histogram on multi-dimensional array
given an np.array of shape ` ( n_days , n_lat , n_lon )` , I'd like to compute a histogram with fixed bins for each lat-lon cell ( ie the distribution of daily values ) .
I looked into ` np.searchsorted ` to get the bin indices for each value in ` B ` and then use fancy indexing to update ` H ` :: #CODE
I don't want to do a 2d histogram .
If you had a function to return the number of decimal places in a single float , you could do ` max ( estimate_decimal_places ( f ) for f in x )` .
However , the matrix multiplication ` A.dot ( B )` unnecessarily computes all of the off-diagonal entries in the matrix product , when only the diagonal elements are used in the trace .
You can improve on @USER ' s solution by reducing intermediate storage to the diagonal elements only : #CODE
One would think that ` np.einsum ` would have a slight edge , because it doesn't have to store all the diagonal elements before adding them , it keeps a running sum .
if you have such a " clarity " algorithm looping over array indices and want to have it run fast , have a look into fortran and f2py ( which is part of the scipy / numpy universe ) .
This finds the locations of the zeros .
Basically I just want to know how many zeros there are in the data ?
Just do check for the zero value and call sum on the resulting boolean array : #CODE
This is a more general solution that can be used for finding any value not just zero and nonzero numbers in the array .
A point on the sphere is given by ( px+rcos ? sin ?, py+rsin ? sin ?, pz+rcos ? ) .
= ( px+rcos ? sin ? ) * ( py+rsin ? sin ? ) .
But my main question is , how to mask arrays based on the value of the indices ?
In your special case of wanting to mask the diagonals , you can use the ` np.identity() ` function which returns ones along the diagonal .
` tile ` is basically a wrapper on ` repeat ` .
@USER Cool , I'd never considered that along dimensions of length 1 , ` tile ` and ` repeat ` are equivalent .
If you do not explicitly pass the axis keyword you get a 1D array out and will have to reshape it manually .
In general , to access the value of the indices , you can use ` np.meshgrid ` :
As @USER points out , ` meshgrid ` supports a ` sparse ` option , which doesn't do so much duplication , but requires a bit more care in some cases because they don't broadcast .
IndexError : too many indices when working with pandas data
wrote a function that finds the peaks and troughs a signal and outputs to two ndarray objects from the numpy library called mintab and maxtab whose indices are timestamps and whose values are the value of the peak .
But i keep getting an error that says : ` IndexError : too many indices ` and points to the line ` xMin = mintab [: , 0 ]`
Because you'll get that error if you try to give two-dimensional indices to a 1d array .
Now i just need to figure out how to set xMin and xMax as a list of the indices of mintab and maxtab .
Value of column A is a unique id , corresponding to one single entity ( could be seen as a cross section data points on one SINGLE user , so it MUST go in one unique sample of train , test , or cv ) , and there are many such entities , so a grouping by entity id is required .
Finding all unique values of A's - denoting this as my sample I now distribute the sample among-st train , intermediate cv test -> then putting the rest of the rows for this value of " A " in each of these files .
To sum up - you should really deeply analyze whether your approach is reasonable from the machine learning point of view .
I need to find the indices of the first less than or equal occurrence of elements of one array in another array .
indices has the value [ 0 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 2 , 3 ] , which is what I need .
This may be a special case , but you should be able to use numpy digitize .
As I mainly wanted to focus on the speed of ` searchsorted ` vs ` digitize ` I pared down Jamie's code a bit .
I have plotted the logarithmic ratio of ` timing_digitize / timing_searchsorted ` so values greater then zero ` searchsorted ` is faster and values less then zero ` digitize ` is faster .
For example is shows that in the top right ( a = 1E6 , b=1E6 ) ` digitize ` is ~300 times slower then ` searchsorted ` while for smaller sizes ` digitize ` can be up to 10x faster .
Looks like for raw speed ` searchsorted ` is almost always faster for large cases , but the simple syntax of ` digitize ` is nearly as good if the number of bins is small .
+1 - I think this is correct for all cases specified by the question : " I need the indices of the * first less than or equal occurrence * of elements of one array in another array .
I left ` np.take ` in as its scaling is likely ` N ` while ` searchsorted ` would be the dominant ` N log ( N )` operation .
I would think a density map ( 2D histogram ) would be more informative .
In addition , you can fill between a moving min and moving max , and plot the moving mean over the top .
The min and max often aren't the best statistics to use .
You could do all kinds of other things , including plotting two filled intervals , say one for the 25th and 75th quantiles , and then the min and max behind that .
I ended up with calculating the sum for each 10 minutes and filtering out a lot of unnecessary data .
Group values into bands on each day and use a 3d histogram of count , value band , day .
It seems to change the fill_value after applying the sum operation , which is confusing if you intend to use the filled result .
I guess I need to do all my work within the apply , and flatten it back out .
` -2*log (( 2.51 /( 331428*sqrt ( x )) ) + ( 0.0002 /( 3.71 * 0.26 )) ) = 1 / sqrt ( x )`
so it dies here : log ( x , 10 ) ..
eigenvectors are not unique : #URL but I think both MATLAB and NumPy rely on the same LAPACK routines to compute them , so so you will likely get similar results .
So you can get the matlab eigenvectors by multiplying the numpy ones by ` - 0.13-0.99j ` , i.e. they are colinear and therefore the same as far as eigenvectors are concerned .
` .T ` gives us the transpose .
The ones that I have used will randomize starting points in any fashion you wish .
@USER thank you ... there was an error in the final reshape command , now it seems to be right
If you have [ 2 4 4 4 0 ] , should 4 be considered as the max ?
but I think , finding the local max can be simplified to : #CODE
It gives you the indices of the relative maxima of a 1d array .
Now I'd like to store ` A ` as sparse matrix since it is sparse and only contains a very limited number of nonzero entries which results in a lot of unnecessary multiplications .
Unfortunately , the above solution won't work since the numpy dot doesn't work with sparse matrices .
The problem is that we need to have the sparse matrix be the first argument , so that we can call its ` .dot ` method , which means that the return is transposed , which in turns means that after transposing , the last reshape is going to trigger a copy of the whole array .
You can convolve with a 3-D Gaussian .
this surface seems to be described using finite elements , so you should calculate the area for each one and sum them up ...
And you can get a vector , perpendicular to the other two , with a norm equal to the area of the parallelogram described by them , taking their cross product : #CODE
It is tempting to simply go ahead and integrate A , but it is its norm you want to integrate , not the vector itself .
With set do you just want to get the unique rows in the array ?
Someone can probably replace this with a more thorough answer , but it appears that this effect is unique to the MacOSX backend , since it does not appear when saving the figures as png .
You can't conveniently take the max of a 2D data set represented as a list of lists ; calling ` max ` on it will compare the lists lexicographically and return a list .
As a bonus , this algorithm will also run much faster because it never calls the log function .
The constant ` __logBase10of2 ` has far more precision than a float , and the python interpreter will truncate most of that .
Is it possible to get indices of all values of a where values of b are equal ?
( I dont want indices where a = b , I want indices for all ' a ' where ' b ' are equal )
if you want to get only the indices where value of a is equal to value of b , _for 2-24 , it should just return me 6_ would not return anything , right ?
@USER I dont want indices where a = b , I want indices for all ' a ' where ' b ' are equal
Do you want a list of indices returned or a single value ( like the last line ) ?
@USER I want indices of all
you sum up from 0 to N-1 on j in the first case ( inner loop ) and from 0 to len ( IO ) -1 on j in the second case
The sum of the VA in the nested loop would be 49.6894189183 , while the first suggestion of Veedrac would result in 49.7519189183 .
Can you compute the max absolute difference between VA using the two methods ?
The sum of the VA in the nested loop would be 49.6894189183 , while your first suggestion would result in 49.7519189183 .
If you were sorting by one column , you could use ` np.argsort ` to find the indices of that column , and apply those indices .
But just a live comment : the sum of each line of my matrix is 1 ( because it is a transition matrix of a markov chain ) .
The implementation is based on creating a simple histogram from the samples and then convolving this with a gaussian .
Can I avoid the .T for the histogram and the fftshift for KDE1 ?
Can I avoid the .T for the histogram and the fftshift for KDE1 ?
I could be misreading your code , but I think you just have the transpose because you're going from point coordinates to index coordinates ( i.e. from ` x , y ` to ` y , x `) .
Quoting from [ that same link ] ( #URL ): " I agree that the general recommendation is if cond : and if not cond : , and this is sufficient for 95% of the use cases .
I was conflating PEP8 with the specific message E712 , which gives both ` if cond is True : ` and ` if cond : ` as equally preferable to ` if cond == True : ` .
Below is an example of one of the sums I am doing and for the example's sake , I sum over the entries of an array filled with random integers .
I first calculate the product of two matrix entries and then sum over half of the columns .
This I do twice and then sum over half of the rows in steps of two .
You are absolutely right , I misread the indices , it's only working because his data is real ...
I interface it like in the dot example : #CODE
Suprisingly , when ` dtype= np.int8 ` , zeros are interpreted as 127 ( or 12 8) !
My intuition tells that 0 is a black pixel ( no light ) , 255 is a white pixel ( max light ) .
In PIL or Matplotlib , for example , zeros of ` uint8 ` , ` int8 ` and ` float32 ` will all be interpreted as black .
Edit : You need to transpose your data .
If your data are ordinary lists you can transpose them with ` list_A = zip ( *list_A )` ; if they are numpy arrays you can transpose them with ` list_A = list_A.T ` .
You need to transpose it so you have lists of XY pairs , not separate lists of X and Y coordinates .
If you're looking for min dist for each point in a , then replace the last line with #CODE
@USER He wants the minimum distance found , so I have to do the sqrt at some point and it's only called once .
Try a more simple case : make another image with median filtering ( for example , by a pattern 3x3 ) and compute absolute value of differense between your image and filtered image .
You can concatenate the two time series and sort by index .
Pay attention to indices that show up both in ts and in data
If you weren't doing the " append 1 " thing , you could do it all with no loops as : #CODE
With @USER ' s edit , ` VV.shape ` is now 2116 , so doesn't broadcast in your solution ( since ` Wg.shape ` is 16309 )
No need for the tile call , as the ` np.multiply ` broadcasts .
There is a high repetition of ` ( row , col )` pairs , and in my final sparse matrix ` M ` I would like the value of ` M [ row , col ]` to be the sum of all the corresponding elements in ` data ` .
Below is a sample strace log with comments , from one iteration ( i.e. one input image set ) .
Yes , your code works , but when you already have a NumPy array , using NumPy slicing will be much much faster than using a ` for-loop ` with ` append ` ( which creates a Python list instead of a NumPy array ) .
After that I need to concatenate those lists and using the last value of the list compute next element .
I know that using ` np.zeros (( 4 , 4 ))` outputs a 4x4 array with all zeros .
` reshape ` works on any array as long as the number of values stack nicely .
Construct two dimensional numpy array from indices and values of a one dimensional array
but then couldn't figure out how to populate / fill in the ones from the list of indices
After I make a search in this matrix , I return some indices where the rows must be permuted .
This works by applying the roll to a copy of ` a ` only at indices ` w ` and then setting those in the original ` a ` with the rolled values :
Someone had an answer ( I think @USER , but it's now deleted ) that showed that for two columns rolling is equivalent to reversing , you do not actually need to ` roll ` , and can use the reversing by index trick as so : #CODE
For shorter arrays , the ` roll ` solution is slower .
Yes , I am working here with a simple permutation , not with complicated data structures , so 2 columns are enough to encode the cycle .
Get elements of a permutation
I have an 1D-array whose elements are a permutation of ` 0 : N ` , and I need to take the first K elements of this permutation
For example , in the case the permutation is #CODE
I need a faster way to extract the elements from permutation .
In this example displayed there the permutation was the identity permutation .
For other permutation it does not work .
I need a code with the same logic as my code , but to work faster , and to work for every permutation .
With numpy , you can sometimes skip the recursion , e.g. using a ufunc's [ ` accumulate `] ( #URL ) method .
My rec . does not always loop from 1 to n-1 , because the stopping cond may be accomp . before .
Neither field is unique , but rows are unique by combination ( no ID has more than one record on the same date ) .
If you wanted ones and zeroes instead of ` True and False ` , you could use ` ( df [ " ID "] == df [ " ID "] .shift ( -1 )) *1 )` or ` ( df [ " ID "] == df [ " ID "] .shift ( -1 )) .astype ( int )` to convert them .
If you want to stack both arrays use ` numpy.vstack ` : #CODE
I can't entirely ensure that the elements are unique though , the number ` [ a , b ]` may occur in both ( for example ) the datai and dataj text files - however it is very unlikely , but I can't rule it out entirely .
Define the length of the strings you want , let's say it's 6 ( two places before the dot , three after , and the dot itself ) .
Resize / stretch 2d histogram in matplotlib with x-y dependent number of bins ?
I am making a 2d histogram as follows : #CODE
The error mentioned above ( which is different from some other ones I got before trying pip ) seems to be related to a 32 / 64 bit incompatibility maybe ?
Be cautious though , because of course this doesn't give you the full gradient but I believe the product of all directions .
Clip x_new_indices so that they are within the range of ` x ` indices and at least 1 .
It expects a list of columns , each with uniform type , not of rows , hence the transpose : #CODE
Nope , list can support any ` type ` , also different ones in the same list .
I know the error will plot in the form of ` exp ( -c * dx )` where ` c ` is an arbitrary constant .
I know this because the pseudo spectral method has error of ` exp ( -c / dx )` but I will be plotting against ` 1 / dx ` .
For different ` dx ` will give different sized arrays , which cannot stack .
I know the error will plot in the form of ` exp ( -c * dx )` where c is an arbitrary constant .
I know this because the pseudo spectral method has error of ` exp ( -c / dx )` but I will be plotting against ` 1 / dx ` .
Obtain a subset of 2-D indices using numpy
In the 2-D case , it accepts a ( number of points ) x 2 array of indices and a ( number of points ) x 1 array of corresponding values to interpolate from .
So , I need to get a subset of the indices and values : those that are " good .
So , I need to get a subset of the indices and values : those that are " good .
If you've created a " mask " of the bad indices , you can take the negation of that mask ` ~ ` and then find the indices from the mask using ` np.where ` .
It's easy to get lost in novel techniques and miss the obvious ones !
Use a larger ` square ` ( or any shape from skimage.morphology ) if you have big gaps of zeros .
do ` in_data = in_data.T ` to transpose the python array
Changing dtype of structured array zeros out string data
So just write your ` myprint ` function so that it'd check for " log " before , and your prints should be the same .
A more pythonic way might be to pass a log function into the module instead of a flag .
Firstly , use the full iterator interface instead of the ` yield ` shortcut because it allows better sharing of state between the outer and inner iterables without making a new ` subsection ` generator each iteration .
Having this done , feeding the new ones to your algorithm hopefully gives you desired results as shown in the worked examples below .
matlab find() for nonzero element in python
I have a sparse matrix ( numpy.array ) and I would like to have the index of the nonzero elements in it .
I have tried numpy.nonzero ( but I don't know how to take the indices from that ) and flatnonzero ( but it's not convenient for me , I need both the row and column index ) .
Assuming that by " sparse matrix " you don't actually mean a ` scipy.sparse ` matrix , but merely a ` numpy.ndarray ` with relatively few nonzero entries , then I think ` nonzero ` is exactly what you're looking for .
` nonzero ` returns the indices ( here x and y ) where the nonzero entries live : #CODE
We can even modify ` a ` using these indices : #CODE
If you want a combined array from the indices , you can do that too : #CODE
If you want the indices to access some other array there is some very simple sytax : #CODE
Set rows of scipy.sparse matrix that meet certain condition to zeros
I wonder what is the best way to replaces rows that do not satisfy a certain condition with zeros for sparse matrices .
I want to replace every row whose sum is greater than 10 with a row of zeros #CODE
I want to replace a [ 2 ] and a [ 4 ] with zeros , so my output should look like this : #CODE
This relies on the fact that if we set 2nd and 4th elements of the diagonal in the identity matrix to zero , then rows of the pre-multiplied matrix are set to zero .
@USER - Just subtract the min , divide by the range , multiply by 255 , and then cast to uint8 .
Also , I don't think you need to tile the 2D array to 3D RGBA .
Behind the scenes , numpy ought to translate that to a pretty speedy operation .
I am using numpy histogram2d to compute the values for the visual representation of a 2d histogram of two variables : #CODE
The reason this is necessary is because the ` np.matrix ` maintains its two-dimensionality even after slicing , so that the column of matrix has shape ` ( N , 1 )` , not ` ( N , )` as the histogram functions expect .
More precisely , I want to construct ( declare and then initialize ) an m by n matrix , call it A , in which each entry [ i , j ] is a 1-dimensional array of doubles ( of length ` min ( i , j )` , filled with zeros ) of the form #CODE
Element [ i , j ] has in it an array of zeros of size min ( i , j ) .
If there is only a linear variation in lengths ( so the max length is a reasonable factor [ I'd say up to about 10 ] of the mean length ) then this should have acceptable overhead .
I can get the example graph to stack correctly ( always a , b , c , d from bottom to top ) #CODE
If ` a ` , ` b ` , ` c ` and ` d ` are numpy arrays you can also do ` sum ([ a , b , c ])` : #CODE
The only tricky part is ` sm.add_constant ( X )` which adds a columns of ones to ` X ` in order to get an intercept term .
The names do not always translate directly from ` matlab ` to ` scipy ` , sometime even miss leading .
As my application has several million of these collections , all of which need ordering , performance is something of a concern ( which is why I have not blindly tried to roll my own tolerant np.lexsort without first seeing if there is a better way to do it ) .
The ` idx ` parameter permits one to control what indices are considered ( allowing for elements to be crudely masked ) .
Or it only contains unique values ?
They are all unique and in increasing order
Yes the max value of zfit in each row is always greater then max of samplez
In ` samplez ` there are 1 million unique values but in ` zfit ` , each row can only have 500 unique values at most .
The entire ` zfit ` can have as much as 50 million unique values .
And you say the underlying mathematics might allow for more simplifications . samplez contains unique values from .08 to 1.1 , and each row of zfit contains unique values from 0 to 2 in increasing order .
How can I concatenate these 2 matrix by column , so that it will become : #CODE
These can be also used to remove the column ( concatenate two subarrays ) - this can be used to remove many columns .
To remove i'th column you can take subarrays to this column , and from the next one , and concatenate them .
and deleting multiple ones can be performed using #CODE
Basically , to do what you want , you want to do some form of dot product of df1 with df2 #CODE
but since they have matching dimensions you need to transpose one of the DataFrames #CODE
And if you understand how matrix dot products work you'll understand you only want the array data from the diagonal of the resulting matrix .
ie : You only want ( AAPL price of day X * AAPL shares of day Y , where X == Y ) Therefore , the values in the matrix that are relevant to you are located at ( 0 , 0 ) , ( 1 , 1 ) , ( 2 , 2 ) , etc ie : the diagonal .
Just use a list comprehension and the ` min ` function #CODE
pylab histogram get rid of nan
I have a problem with making a histogram when some of my data contains " not a number " values .
I can get rid of the error by using ` nan_to_num ` from numpy , but than i get a lot of zero values which mess up the histogram as well .
So the idea would be to make another array in which all the nan values are gone , or to just mask them in the histogram in some way ( preferrably with some builtin method ) .
Remove ` np.nan ` values from your array using ` A [ ~ np.isnan ( A )]` , this will select all entries in ` A ` which values are not ` nan ` , so they will be excluded when calculating histogram .
that works , thanks ( i can only accept your answer in 4 min ) Not entirely sure where you found that ~ statement in the documentation , but it works
In your setup , your function is ` y = 2*log ( 2 ) - 2*x - 4 * log ( pi ) - log ( kappa )` and you're trying to fit the parameter ` kappa ` ?
which got me looking into generalizations of ` dot ` such as ` np.inner ` , ` np.tensordot ` and ` np.einsum ` , but I can't figure out how they would solve my problem .
@USER : I'm aware of that ; Euclidean distance is a dot product plus the sum of two of the above operations , and the layout is already optimized for the dot product .
Of course your code should be safe for both binary / mono images as well as multi-channel ones , but the principal dimensions of the image always come first in the numpy array's shape .
Or to use boolean indexing , use broadcasting to create an ` outer equals ` , and ` sum ` with ` any ` and ` all ` #CODE
The curve is by nature parametric , i.e. for each x there isn't necessary a unique y and vice versa .
For fminsearch , you could specify the min / max bound for t to be ` 1 ` and ` len ( x_data )`
I think another trick is to set the min time step as well .
Firstly , eigs can only return < rank ( A ) -1 eigenvalues , so det cannot be computed .
It's very simple , you just multiply the diagonal entries on both the L and the U matrices .
The usual technique for computing the log-determinant term in the likelihood expression relies on Cholesky factorization of the matrix , i.e. ? =LLT , ( L is the lower triangular Cholesky factor ) and then using the diagonal entries of the factor to compute log ( det ( ? ))= 2 ?
The main trick lies within the observation that we can write log ( det ( ? )) as trace ( log ( ? )) , where log ( ? ) is the matrix-logarithm .
You should try to use some method which doesn't require gradient / derivative , such as Powell s method ` fmin_powell ` or Nelder-Mead ` fmin ` .
Your interpretation is correct , ` model101() ` returns the flatten 1D residue array for your 2D data .
Is it possible to iterate implicitly on an array with two indices ?
There is an implicit loop ( the sum ) and an explicit one ( ` for i in range ( 3 )`) ...
you can do it like this ` y = np.array ([ sum ( x - k ) for k in x ])` if it counts as `' implicit '`
I then just sum across the rows using the argument ` axis=1 ` in ` np.sum ` .
You could avoid the ` - ` ve if you sum along ` axis=1 ` .
I tried unubtu's answer and it works well for smooth curves but seems to break for not so smooth ones :
In other words , you want ` dx = step / sqrt ( 1 + ( y ') **2 )`
You'll want to build your interpolation as a function ` y ( x )` and then call it on some array ` x ` which is not linearly spaced , but spaced with each step in x ( dx ) being your desired step ( say ds ) divided by the the sqrt of ( 1 + the slope squared ) .
This is because the arclength of a segment of the curve has length ds = sqrt ( 1 + slope^2 ) *dx ( from pythagorean theorem ds^2 = dx^2 + dy^2 )
I need to select k points in this set so that the sum of their pairwise distances is maximal .
In other , slightly more mathematical words , I want p1 , ..., pk in S such that sum ( i , j k ) dist ( pi , pj ) is maximal .
Virtual bonus point #1 for a solution which works for any function that gives a score out of the four points ( one of which could be the square root of the sum of the squared distances ) .
until you reach a solution of size k , add to the solution the point for which the sum of distances from it to all the points already in the solution is the greatest .
This algorithm takes 2 points on the outer circle that belong to the same diameter , then adds a point at 90 degrees on the circle , which leads to a right triangle that is quite far from the equilateral triangle that is the optimum shape .
I had in mind an error in the distance sum of ~ 0.1-1 % from the optimal solution .
This works because , by definition , dilation ( x , y , E , img ) = { max of img within E centered at pixel ( x , y ) } , and therefore dilation ( x , y , E , img ) = img ( x , y ) whenever ( x , y ) is the location of a local maximum at the scale of E .
An efficient way to do this is to first detect all the local maxima as above , sort them descending by height , then go down the sorted list and keep them if their value in the image has not changed and , if they are kept , set to zero all the pixels in a ( 2d+1 ) x ( 2d+1 ) neighborhood of them , where d is the min distance between nearby maxima that you are willing to tolerate .
to get the indices you use the " where " function #CODE
How do I select components from a particular list of indices of a Python vector ?
And here is the list of x1Vals indices that I want to select #CODE
numpy unique strange behaviour
It all boils down to not using a stable sort ( mergesort ) when the return indices are needed .
That is , don't reshape ` a ` , leave it regular 1d , and then use the outer function instead of regular multiplication .
) You cannot broadcast an operation like ` ** 2.00 ` across a standard python array .
You will achieve a better performance using ` numpy `' s ` sqrt ` and ` exp ` , which will operate elementwise : #CODE
Numpy - group data into sum values
and three ' sum ' values : #CODE
Note the sum of ` b ` , ` c ` and ` d ` should remain the same (= =26 ) .
How can plot ` sum ( c * p ( x ) for n in range ( 1 , 51 ))` where ` p ( x )` is the Legendre polynomial or order n ?
You do the sum and plot them .
The problem I am having is we don't have an n to sum over so how can I sum ?
I don't really understand " we don't have an n to sum over .
I've modified the answer to show how to get this sum in numpy .
the python builtin ` min ` accepts a ` key ` function : #CODE
Brief explanation , if you slice an array by ` [[ list1 ] , [ list2 ]]` , both lists are supposed to be lists of indices .
When you've got a hammer in your hand everything looks like a nail :) I agree that as part of the SciPy stack , Pandas should be available and has a friendlier interface for this sort of work .
I tried all the reshape functions ... but cannot figure out how to do this .
What is ` var_vec ` and where did ` sum ` and ` multiply ` come from ?
The chisq cdf is the incomplete gamma function so you could use scipy.special.gammainc() directly .
The call goes directly to a routine written in C that computes the incomplete gamma function , and all time is spent there .
MKL et al . are no help --- they do not have incomplete gamma function .
Could you provide the code you used to test the calculation using the incomplete gamma just out of curiosity ?
@USER The chi2.cdf is just gamma ( k / 2 , chisq / 2 ) so in this case replace stats.chi2.cdf ( chisqr , [ 5 ]) with scipy.special.gammainc ( 2.5 , chisqr / 2 ) .
because the claim is that gamma is a tad faster .
It's even more from to do ` from X import * ` when ` X ` is a dynamically-generated bindings library like the ones that gobj and pyobjc create ; ` from Cocoa import * ` scans the entire Objective-C runtime few a few dozen seconds before giving you a few thousand names
basically you will get a unitary matrix , a diagonal matrix and another unitary matrix .
The diagonal matrix has number of nonzero elements equal to rank ( A ) .
But keep in mind there's not a unique way of such decomposition .
For sparse , Scipy support dot products and others linear algebra basic operations .
diagonal matrix of a matrix with numpy and scipy
I have a matrix ( n*1 ) and I want to make a diagonal matrix with it .
And do you want a dense or a sparse diagonal matrix ?
What do you mean with making a diagonal matrix with it ?
Diagonalizing it ( by searching for eigenvalues ) or just taking out the diagonal part of the matrix and creating a matrix with it which is otherwise zero ?
just taking out the diagonal part of the matrix and creating a matrix with it which is otherwise zero
and I use ` numpy.unique ` to trim them down to just the unique lists #CODE
When you apply ` np.unique ` to this array , each of the 6 items is considered a separate value , and the unique ones are returned .
When you apply ` np.unique ` to ` q ` , it is finding the unique values among the 6 ints .
When you apply ` np.unique ` to ` q2 ` , it is finding the unique values among the 2 lists .
Should the unique values be ` [ 1 , 2 , 3 , 4 ]` or ` [[ 1 , 2 , 3 ] , [ 1 , 2 , 4 ]]` ?
" The same operation for columns " is just the transpose of the above , i.e. skip the ` reshape ` operation : #CODE
The advantage is that ` gradient ` runs in three dimensions .
One of the things that I would like to do with these distributions is to find a convolution of two of them ( to have a distribution of a sum of two random properties ) .
Filter numpy structured array based based on unique elements in one dimension
I would like to create a new array with only unique elements from ` name ` , I guess first occurrence is fine .
to get unique indices you can use np.unique #CODE
then you know the unique indices in the name dimension that you need to access .
Then you should be able to do just select those indices #CODE
Thank you , I knew there must have been something built-in somewhere to get the indices but I somehow missed it .
Or get the max or min : #CODE
Now try to get the daily sum as follows :
df.resample ( ' D ' , how= ' sum ')
Now try to get the daily sum as follows :
df.resample ( ' D ' , how= ' sum ')
If you go back to the ` sum ` function in each you can see the disparity there is well , and it's essentially 0 .
Interesting , in R ` Reduce ( ' + ' , s )` yields the same sum as in python .
Dunno anything about the mean , but in python , ` std ` is ` sqrt ( variance / n )` , possibly R gives ` sqrt ( variance /( n-1 ))` ?
For the ` std ` , which is clearly off by some substantial amount , in ` numpy ` , ` std ` returns ` sqrt ( sum (( x-x.mean() ) **2 )) / ( n-ddof )` where ` ddof=0 ` by default .
If you sum these numbers you find that the sum , just like the mean , is very close to 0 .
Each of the functions you reference needs to first sum the numbers .
In R ` Reduce ( ' + ' , s )` yields the same sum as the python function ` sum ` .
However , the ` mean ` and ` sum ` functions in R use more accurate methods to do their math . when you do all of the math within R in the same way it's being done in numpy then it's identical .
The ` sum ` in R handles the scaling properly as both sums are the same .
However , both R and python can't deal with it using a sequential sum method .
I won't provide the code but ` sum ` in R consistently gives the same value while both ` Reduce ` in R , and ` sum ` in python give different values depending on the orders
The mean error that derives from the sum just blows up when you start then doing variance .
If you roll a mean and sd like I've shown you sum with ` Reduce ` then the results will be the same .
Of course , if you can find python functions to replace the numpy ones and make your python code more accurate that would be even better .
The difference in Python is that ` sum() ` adds " left to right " suffering a rounding error after each addition , while ` math.fsum() ` conceptually computes an infinite-precision sum , with a grand total of one rounding at the end to replace the infinite-precision sum with the closest representable double precision number .
That would explain why @USER reports that R returns the same mean regardless of the order of the numbers in ` s ` ( an infinite-precision sum is wholly insensitive to the order of the summands ) .
R is probably using a better numerical method for computing std dev too - " better " in the sense of smaller numeric error , but probably " worse " in the sense of taking more time to compute .
About std dev
That will truncate the file .
Consider each not NaN data point ` X [ i , j ]` as a Gaussian centered in ` [ i , j ]` , with variance=1 , scaled so its ` pdf ( [ i , j ] ) = X [ i , j ]` , so ` f_ij ( [ a , b ] ) = X [ i , j ] * exp ( - || [ a , b ] - [ i , j ] || ^2 / 2 )` .
For each NaN data point ` X [ a , b ]` set ` X [ a , b ] = sum ( f_ij ( [ a , b ] ) )` where sumation is performed over all ` [ i , j ]` indices of not NaN data points
So the code would be just a one loop over all NaNs , and for each of them you loop through all not NaNs and sum the gaussians values .
Your ` values ` array can be attained by using the ` flatten ` method of a 2D array ( matrix ) .
Then it's just a matter of constructing the ` x ` and ` y ` arrays to contain the proper indices .
Looking at ` coo_matrix ` I see that ` nonzero() ` is used to get the indices : #CODE
If there could be zeros in ` x ` , you'd have use a work around like ` np.nonzero ( np.ones ( x.T.shape ))` .
To create a 1D array , all zeros but for a stretch of 1.0 values - a rectangular impulse : #CODE
The I-index should be the inner loop , the K-index shoould be the outer loop , and the J-index the loop in between .
There is no deprecation warning , and there may never be , but the docs [ are clear ] ( #URL ) that the way to go for new code is the Polynomial package , not the older poly1d .
I will try the Polynomial package .
Is there an easier way to get the sum of all values ( assuming they are all numbers ) in an ndarray : #CODE
Yes , just use the ` sum ` method : #CODE
By default this sums over all elements in the array - if you want to sum over a particular axis , you should pass the ` axis ` argument as well , e.g. ` matrix.sum ( 0 )` to sum over the first axis .
Python histogram with points and error bars
I want to plot a histogram with points and error bars .
I'm not sure what you mean by ' normalized ' , but it would be easy to , for example , divide the counts by the total number of values so that the histogram sums to 1 .
The bigger question for me is what the errorbars would actually mean in the context of a histogram , where you're dealing with absolute counts for each bin .
This will print out all the indices of e and whether it is equal to [ 1 , 2 ] .
If you wanted to return the indices , instead of printing them , you could add ` ( num , num2 )` to another list , and that would give you all the locations of ` [ 1 , 2 ]` .
They do provide some classes to transform / encode such features ( like Onehotencoder , DictVectorizer ) but I cannot find a way to use these on my data .
I know there are quite a number of steps involved here before I fully encode them to numbers but I am just wondering if anybody knows a simpler and efficient ( since there are too many such features ) way that can be understood with an example .
One reason the memory is so bad is likely b / c the dot product routine expects square-ish matrices and actually pads out with zeros for optimization .
There are a few work-arounds , for example by computing a truncated version of the covariance matrix by imposing a penalty on its L1 norm ( e.g. [ here ] ( #URL )) .
In looking at NumPy's ` append ` , it calls NumPy's ` concatenate ` which at the very least does extra checking for masked arrays .
The problem is that using ` numpy.append ` makes the algorithm ` O ( n^2 )` , while python ` list ` s have ` O ( n )` performances when doing sequences of ` append ` s .
This means that the algorithm suddenly became ` O ( n^2 )` , while it is ` O ( n )` using python ` list ` s since they do not copy the whole list for every ` append ` .
The right way to do this operation ( rolling means ) in ` numpy ` is to vectorize it , for example using the convolve function ( thanks to @USER for the suggestion ) .
That's a really cool but complicated way to do a rolling mean , it only has to be so fancy to be able to do a function like ` std ` .
@USER : Excellent idea : I've changed my solution to use convolve
I timed it , and it is tiny bit faster than ` convolve ` method on my machine .
can numpy interpret column of indices like matlab does
Is there something similar in python or numpy that does what the last line does basically , ie you can enter in a column of indices and it will automatically populate the nan matrix with the corresponding rows instead of iterating through it and doing this row by row manually .
Also notices cd can have more rows than mat and mat would be able to expand accordingly on its own , at least matlab does .
I am trying to get a table that lists each unique political boundary area ( array values represent a unique id ) as rows and the total number of pixels within each boundary for each landcover class as columns .
You might want to use ` np.bincount ` , which is like a special histogram where each bin has spacing and width of exactly one .
You can do this more simply with histogram , though , if you are careful with the bin edges : #CODE
You can get a list of the unique values in your zones map with #CODE
( i.e. I would get a sum of gaussians for each location )
The ** sum ** of the radial basis functions has to equal 1 at each of the locations specified by ` points ` .
Only the sum must equal 1 .
The problem is there is no sum of RBFs which can equal 1 at all those points .
Above , where I said " sum of the radial basis functions " , I should have said " sum of the * weighted * radial basis functions " .
The X , Y , Z values that I have ( the ones I included in the OP ) are meant to represent locations where an event happened , and I want to compute the probability on a 2D field of that event happening ( the probability should fade as I move away from those observations ) .
with x belonging to R^n , and x^T the transpose operation on vector x .
The problem is that if i use the transpose function #CODE
Two points : ( 1 ) Originally you said that ` numpy ` wasn't changing the shape of a ` ( 3L , 5L )` array , which was very surprising , but isn't not surprising at all that ` ( 5L , )` stays as ` ( 5L , )` ; it's a 1-D object , and its transpose is itself .
This is either the inner or outer product of the two vectors , depending on the orientation you assign to them .
Go and take a look at [ ` scipy.fftconvolve `] ( #URL ) and observe that the algorithm has none of your strange fft shifts or scalings .
When you convolve two signals , the edges of the result depend on what values you assume outside the edges of the inputs .
Normalized histogram of float : unexpected behaviour in numpy / matplotlib
I thought that a normed histogram would do what I want , but I get some puzzling behaviour .
Basically , the normalised histogram should return what I want ( I would have thought ) but it throws me values that are outside [ 0 , 1 ] .
You get the same behaviour with ` numpy `' s ` histogram ` function and ` matplotlib `' s ` hist ` .
I realised that it's pretty easy just to set ` normed = hist / float ( sum ( hist ))` where ` hist ` is the un-normed histogram ( which works fine ) .
Well , you won't be able to vertically stack the data if each potential row has a different length .
This would be the way you translate the ` R ` code to ` Python ` code .
Using ` int16 ` probably will make the ` dot ` product operation possible on a decent desktop PC nowadays .
Retrieve [ s ] the start , stop , and step indices from the slice object slice assuming a sequence of length length
Is there an easier way to do this than defining a skewed distribution and sampling from it to get array indices ?
If the form of the distribution doesn't really matter , you could do something like a poisson distribution of indices : #CODE
The stack and unstack operations make a little more sense to me now in this context .
Does Numpy dot and outer function consistent with Linear Algebra ?
It seems that dot and outer operations don't behave like what I have learn in Linear Algebra class .
(( 41 , 1 ) outer ( 11 , 1 ))
Again , I think the dot is strange .
According to the documentation #URL , the ` outer ` function of two row vectors ` A ( 1xn )` and ` B ( 1xm )` is a matrix ` M ( nxm )` - and the transpose will be of dimension ` mxn ` .
Thus , the dot product of a vector and a matrix is again described in the documentation : #URL - where it is essentially described as the matrix multiplication of the row vector ( first argument ) with the transpose of the second argument ( matrix ) .
Instead of saying " you must transpose your first vector in order to take a proper outer product " , the fact that you are explicitly asking for the outer product " is enough " for Numpy to figure out what you intended .
After much tinkering , I have finally come up with a piece of code that takes the differences between each of the integers in the same list , and dumps them into a new list .
Or in that case , try to use [ sum ] ( #URL )
Likewise , if you want to find the 10 nearest pairs of coordinates , I can assure you that the 10 with the smallest squared distances will be exactly the same ones .
In your case it looks like the weight arrays will have the same dimension as ' A ' , so you reshape them accordingly and multiply dx and dy by their individual weight vectors .
I don't understand why you added a ` .sum() ` function to sum all the distances .
To compute the euclidean norm , you take the square root of the sum of squared differences across dimensions , e.g. in the 2D case _sqrt (( x0-x1 ) ^2 + ( y0-y1 ) ^2 ) _ .
Is the spectrum array just a vertical gradient sampled along the streaks ?
For the ** colorize ** array , coordinate values give the wavelength , or color , of the image , which results in the rainbow gradient .
Numpy matrix multiplication with custom dot product
I am trying to use a custom formula instead of the dot product to get #CODE
There are other ways of adding dimensions to arrays ( reshape , repeat , etc ) , but the effect is the same .
Expand ` a ` and ` b ` to the same shape to do element by element multiplying ( or == ) , and then sum on the correct axis .
+1 for masking an array , in the case of a list I would go with slice and concatenate over using compress .
the log format is like that
As you vary a set of fitting variables , you will evaluate the deviation at each point and finally look at the sum of deviations .
Now I'm reduce the sample rate from 44100 to 11025 , maybe join your ideal that is using smaller data type.Thank you .
I have this code ( need to subtract sum of previous elements from current ): #CODE
I think you can get the sum to be done more efficiently at least : #CODE
I assume that your real data ` arr ` is not all zeros -- Otherwise , the sum will be an array of zeros which you then subtract from an array of zeros leaving you with ... and array of zeros ( which isn't very interesting ) .
Python picks the object up from the outer scope .
Python cannot find sample_array in side function namespace , it will find the outer namespace .
I calculate the distance between a number of points first ( no problem ) , but after that , i need to get the mean of the values all the points in one list that are closer than ( in this case 20m ) , and if that 20 is small , this piece of code is fast , but otherwise , it is very slow , since i need the indices etc-
Personally i think the problem is the fact that i first use a dummy dictionary to get the points close enough , and than need to cast to a list to use .index , to get the indices where it is close enough .
Perhaps you will still need to iterate over one of the dimension ( i.e. you might still need the outer ` for ` loop from your example ) .
You could squeeze down another line by combining line 2 and 3 .
Which repeats in an infinite loop until you run out of stack space .
` But no ` max ` .
Out : A matrix C where C [ i , j ] = max ( A [ i , j ] , B [ i , j ])
This answer doesn't work for getting the element-wise max of two matrices , and even for the max of one matrix it's incorrect : what if the max of ` .data ` is negative ?
It's based on finding the set of unique indices for the nonzero elements in the two arrays using code from Jaime's excellent answer here .
Here is an example plot , my points are scattered in three diff . colours .
Why dont you use a 2d histogram to show your data ?
I have settled for a 2d histogram for now , but I am very keen to nut this one out !
The elements are of different length and their number varies , so I thought it's best to use a list as the outer structure .
If you had a series of indices you may use ` numpy.sum ` for fast masking and addition on multi-dimensional arrays .
@USER : I have the series of indices but I need to add and subtract multiple elements .
@USER add example / sample data with the series of indices to your question and we can show you how to get the result quickly with NumPy .
If you simply want to retrieve the values in an array from a list of indices , and sum them , you can do : #CODE
Note that I set indices ` ind ` as a list of indices pairs , and thus ` zip ` is necessary .
` a [ ... ]` just needs to get a list or tuple ( not numpy array ) of indices grouped by axis , i.e. ` a [( 0 , 2 , 1 ) , ( 1 , 2 , 1 )]` .
One last point : this method is useful for a set of indices of arbitrary size ( i.e. which would be a parameter of your code ) .
If by eye you can see it peaks at ` x~35 ` then tell it through the ` p0 ` .
you have two choices , either call the elements of the array ` ax ` like you have suggested ( but you will need to use two indices or flatten it ): #CODE
With this line I can check for zeros rapidly : #CODE
but I also have very small elements like 1.e-22 which I would also like to treat as zeros ( otherwise I get a Divide by zero warning further down the road )
There's no reason to loop in Python ; just broadcast the abs and the and use ` np.any ` : #CODE
However , it may not like Unicode files , so you may have to " transcode " it to ASCII , which means basically putting _two_ wrappers around it -- one to decode the UTF-16 , the other to encode the result to ASCII .
Or encode them to something different , if you prefer : #CODE
Using ` decode ` before ` split ` should work better , though now I want to test whether ` readlines ` works correctly or not .
To fix this , just decode the data : #CODE
You can also pre generate all indices : #CODE
Lets assume that you have a ` rating ` matrix , and a list of weights vectors ` weights ' , then you can simply do ( assuming , that these " empty " fields are zeros - this is some border case you have to think of , because you can encounter dividing by 0 either way , when all of the users " neighbours " also did not give any rating to some item ): #CODE
If you want it as matrix so you can access both column wise and rows wise you might want to load id to two different data structures or to load it to one , calculate and then transpose it .
Index of max element from two lists
If I have two lists and find the max element between those two lists .
I would like to have a system , where I have a " Data class " that has some arrays and a " Selection class " that has the same array names whose data are just a mere view on a subset of the arrays from the Data class . which ones should be determined by the instance of the selection class .
Those look like regular column numbers , except the first ones step by 4 .
Again my H1 contain near about 20,000 list i.e the outer for loop runs for 20,000 times .
Also , any global functions or methods you use ( e.g. ` min ` , ` np.dot `) can be converted to locals in the function signature using default arguments .
This would be similar to the optimization you already make with the list ` append ` methods .
Its principle is to play on indices instead of the elements of lists , and to use the peculiarity of a list defined as a default value of a function #CODE
The dot product and np.min can operate on whichever axes you want .
operands could not be broadcast together with shapes ( 780,108 0 ) ( 780,108 0 , 3 )
I was proud of calculating the ` ma ` ( moving average ) and the ` std ` ( rolling standard deviation ) for the whole frame instead of doing a loop over the symbols , so what I want to do know is ( pseudo-code ): #CODE
How to pad with zeros a tensor along some axis ( Python )
Sample from weighted histogram
For example maybe data not optimally stored on disk or readed from disk , not properly chached , and also dot product use only one core.Maybe I should use something like PyTables ?
Oh , and while this is better in newer versions of numpy , while the first transpose does not copy the data , dot may expect C-contiguous arrays also for the input , so that you should probably not transpose it ( or safe in fortran order , and then transpose to c order ) .
It's likely less efficient because : 1 ) it requires the array of ones , 2 ) it does ` xi*xj ` multiplications by 1 , and 3 ) it does a bunch of concats .
Basically , my function in time-space is given by ` X ( t ) = SUM_n sqrt ( a_n ) + cos ( w_n t + phi_n )` , where ` a_n ` is the value of the ` PSD ` at a given ` w_n ` and ` phi ` is some random phase .
To get a realistic timeseries , i have to sum up ` 2^25 ` modes , and my ` t ` of course is sized ` 2^25 ` as well .
where dataCOS is sqrt a_n , w = w and random.uniform represents the random phase shift phi
You can use the ` outer ` functions to calculate the angles and then sum along one axis to obtain your signal in a vectorized way : #CODE
In this case , using numpy outer operations allow you to compute the multiplications and sums at the ` C ` loop speed .
okay , maybe my question was not clear enough I use alreads arrys ( t is an array ) bit i have to sum of some billions of mode , each consiting of of a timesample of cosines !
One problem : As soon as w or t_full overcomes 2**12 , i get the error " array is too big " -> But i have to sum over 2**23 modes ... and ideas on that :/
Why won't numpy calculate std deviation on one 5-element list and not another ?
I need to change ` yerr =[ std ( f10 ) ...
` on line 61 to ` yerr =[ std ( solf10 ) ...
Can you strip this down to remove all the irrelevant stuff and make it easier for someone to debug it ?
I'd like to fill a 3D numpy array with these data ( it's a regular grid ) , something like ` p ( i , j , k )= p_ijk ` so I can use the gradient and other operators from the numpy toolbox .
If I understand your situation correctly , you can just ` reshape ` it .
If you have ` z ` variance , too , simply reshape to three dimensions : #CODE
The ` p ` you get from your ` vtk_to_numpy ` is already an array , and that's the one I think you need to reshape .
Python multiprocessing of a sum
My sum goes from 0 up to 2**16 , so speeding this up is essential .
My problem is , that i first don't know how call my function correct and how i can sum all my replies up .
Python - create mask of unique values in array
This will return an 256*256 image based on that equation ` ( sqr ( x ) +sqr ( y )) mod 256 ` .
I just need to know how to add the square of the rows to the column , mod it by 256 and return a new array .
In numpy-speak , the arrays ` x ` and ` y ` of shapes ` ( 256 , 1 )` and ` ( 256 )` broadcast to a common shape ` ( 256 , 256 )` .
You can automatically broadcast the vector against the outermost axis of an array .
So , you can transpose the array to swap the axis you want to the outside , multiply , then transpose it back : #CODE
I would probably let ` swap_axes ` or ` roll_axis ` figure out the whole permutation of the axis , but either one ends up calling ` transpose ` anyway .
IMHO , this is more readable than ` transpose ` , ` einsum ` and maybe even ` v [: , None ]` , but pick the one that suits your style .
There might be some ideas here [ Faster way to calculate sum of squared difference between an image ( M , N ) and a template ( 3 , 3 ) for template matching ? ] ( #URL )
I just saw your other comment , and I'm not sure if you'll be able to use this solution , since the standard ` np.convolve ` only works on 1d arrays , not 2d , so this answer uses the ` scipy.ndimage `' s ` convolve ` .
It will install the full SciPy stack ( which includes NumPy and MatPlotLib ) as well as many other useful packages ( including IPython , which is awesome ! ) .
This is a pure drag and drop installer ( like the Firefox or Chrome ones ) .
I would reshape this to have shape ` ( 24 , 365 , 2 )` so that even if it's not an array of tuples as in the question , at least the pairs are still there , which I assume is what @USER is probably after .
The mean of the sample will have the same mean as the lambda , but the standard deviation will be equal to the sqrt of the variance , which in a Poisson distribution is equal to the mean .
Your starting image looks like it already has noise , so I would start out by modeling the star response through your aperture to obtain the noiseless image say using some point spread function like an airy disk , sinc function or similar as the input to the numpy.random.poisson function .
mask is init to an array of zeros
Running this on a opteron core with intel mkl blas I get a odd result as I would expect the ` dot ` call to take the majority of the time .
In your code , ` numpy.any ( 0 , axis=0 )` tests whether any value in "` 0 `" is nonzero , so it will always evaluate ` False ` .
Instead you want to look for columns in ` test ` where there are not any zeros in the row values : #CODE
Or equivalently , where all row values are nonzero using ` np.all() ` : #CODE
Here'a version that uses dot matrix multiplication .
But for larger ones like ` K , M , N = 300,100,150 ` it's a bit slower .
The red dot in the on far right axis shows the next " predicted " point .
Since I can't easily construct a sample version with your exact column labels , there might be slight additional tinkering with getting rid of any index columns that are included in the ` diff ` and moved with the transposition .
In 3D you would need a bilinear interpolation algorithm at the least , and coding it up ( and keeping indices in order ) is non-trivial .
reshape the resulting array to be 2D
for this to work I think you need a transpose
Thanks @USER it does need a transpose .
@USER If you want legends with color blocks , then try @USER ' s link ; otherwise if you want the annotations like the ones in your example plot , jus use ` annotate ` or ` text ` .
Can probably squeeze some more speed in a few places .
The image is stored as an array of ( 1000 , 1000 ) integers between 0 and 255 , or indices into the pallet array .
I would like a new array , created from this , where the non zero elements are converted to their value in zeros and zero elements are converted to a single number equal to the number of consecutive zeros i.e : #CODE
This should do the trick , it roughly works by 1 ) finding all the consecutive zeros and counting them , 2 ) computing the size of the output array and initializing it with zeros , 3 ) placing the counts from part 1 in the correct places .
how to translate matlab function max ( y ( x > 3 ) > 2 ) to numpy in single line
i want to convert matlab functions like all , any , max min to numpy .
for ex . max ( y ( x > 3 ) > 2 ) #CODE
how can i write the above max function in numpy so that it accepts the diffrent expressions .
In Matlab ` max ( x > 3 )` will return ` 1 ` or ` true ` .
Plus equal is giving a different answer than assigning to the explicit sum ( which is the answer one expects ) , when slicing is involved .
What I need is simply to iterate row by row , and then for every row I want retrieve the list of indices and values .
Shift and zero-pad sublists , causes operad / broadcast runtime error that I do not understand
I want to create a new list of which each sublist has its entries moved to one side ( forward or backward , doesnt matter ) , then end clipped and the begin padded with zeros so it still has 64 entries .
Related ; [ Numpy Operands could not be broadcast together with shape ] ( #URL )
We now have all of the values and we need to tile .
The reason you are getting this error is the ` + ` operator will not append a list and a numpy array together and instead tries to broadcast them against each other .
Or if you want to sort by the sum of the participation key and focus key #CODE
If you have to use Numpy , then you'll need to create a Numpy array of indices for sorting , then use ` argsort ` to get indices that will sort the data , then apply this back to ` data ` .
It's easy to reshape and discard that zero value : #CODE
I recall that for some operations OpenCV requires a contiguous array , so you may have to add a ` .copy() ` to the end of that expression to really get rid of the column of zeros , not simply ignore it , although this would of course trigger the copy of data that the code above had avoided .
i use ` np.eye ( 5 )` to get diagonal 1 #CODE
So you want to create a diagonal array out of a vector , right ?
Outer join in Python for thousands of large tables
So , I have some 4,000 CSV files and I need to outer join all of them .
Each file has two columns ( a string and a float ) and between 10,000 - 1,000,000 rows and I want to join by the first column ( i.e. , the string variable ) .
What other outer join implementations exist for Python ?
Would it be more efficient if I simply had Python call , say , sqlite3 , and then have sqlite3 do the join ?
Do you have the same memory footprint if you dump the dataframe in a persistent store ( hdf5 for insistance ) , and then load it back into a fresh python instance ?
However , it looks like you need to flatten as well into your vector space .
You're going to need to flatten with numpy too .
Keep an eye in JyNI which is at alpha.2 version , as of March-2014 .
Develop a script to calculate the average and sum of some variables .
Edit : if you're using a version of numpy > = 1.8.0 , then ` np.linalg.eigvals ` operates over the last two dimensions of whatever array you hand it , so if you reshape your input to an ` ( n_subarrays , nrows , ncols )` array you'll only have to call ` eigvals ` once : #CODE
The answer to this question depends on what the ` reshape ` function does .
In a simple list comprehension ( like the ones described in this answer ) the expression after the ` in ` is an iterable object , meaning that you can loop through it .
We can of course have more than one variable inside a list that we chose to append .
One way is to create an array of the indices you want to index your array with : #CODE
That makes it hard to vectorize the outer loop , I don't think you can do it unless there is some structure to your data that can be exploited .
it was doing this transpose in micro seconds , ( much smaller than the time required to copy the ( 112x1024x1024 ) array i was having .
It doesn't really seem to speed up with transpose .
I think the reason why ` sum ` etc . along the final axis is recommended ( I'd like to see the source for that , btw . ) is that when an array is C-ordered , walking along the final axis preserves locality of reference .
That won't be the case after you transpose , since the transposed array will be Fortran-ordered .
So is it correct to say that , to improve speed along an axis , there is no point in transposing it first and then applying the sum ?
no matter how you transpose .
I would like to break the inner loop when xy - 0.006 and continue with the outer loop [ i+1 ] .
Move the ` bb={} ; ss={} ` inside the outer loop .
numpy concatenate of empty with non empty arrays yields in float
is initialized with ` dtype=float ` , and concatenate casts the second ` int ` array to ` float ` .
Now , it's worth asking if it ever happens that you use concatenate on empty arrays .
it is true that ` a [: 0 ]` is empty , but it still retains the ` dtype=int ` and the result of concatenate is an array of integer anyway , as you expected .
Using reshape in Python to reshape an array
How can I use reshape to divide it into 4 chucks , such that it looks like #CODE
I tried different integer combinations of m , n , l in reshape ( m , n , l ) , but none works .
Basically numpy uses a strided layout , which means you can for example slice / transpose arbitrarily without copying .
Here , you are right , since it is a reshape + transpose + reshape , you need a copy .
To get the 3-d one , you would have to reshape once more .
And that extra reshape will copy the data .
dtype =o bject )] with block values [ operands could not be broadcast
Note : the error is a direct copy and past from my actual code , so the description and the shape in the error would not correlate directly to my example DataFrame .
yes .... the issue is the Series vector ( e.g. `` df.D > 1 ``) * looks * like it should work , but its ambiguous how it should broadcast , e.g. should that Series named D apply to all of the other columns ( in which case what should it do ? ) , or should it effectively have no name which means it SHOULD broadcast .
You problem could also be solved by using `` df > 1.0 `` because I think that is what you intend ( e.g. that it DOES broadcast )
Python random sample of two arrays , but matching indices
Is there an easy way to use the lovely , compact random.sample ( population , k ) on both x and y to select the same corresponding indices ?
Your matrices are not hermitian , so you cannot take advantage of those special routines , but the general case ones will work with no problems .
minimize : the sum of { w ( a , b ) * ( r_hat ( a , b ) - ( r ( a , b ) ^2 )) }
Another way to think of it is that I am trying to minimize the weighted sum of squares ( the least of squares ) , but I am only searching for one coefficient , r_hat , as w and r change .
Sure in that case you can use set function to get the unique string names ( hurricanes ) .
So you would do s1 = set ( names ) to get the unique hurricane names then loop on that .
How can I create a histogram that shows the probability distribution given an array of numbers x ranging from 0-1 ?
I expect each bar to be = 1 and that if I sum the y values of every bar they should add up to 1 .
which gives me a histogram with bars that go above 1 .
which also gives me bars whose y values sum to greater than 1 .
I think you are mistaking a sum for an integral .
A proper PRF ( probability distribution function ) integrates to unity ; if you simply take the sum you may be missing out on the size of the rectangle .
They do not need to sum up to one , they need to integrate to one .
When a point is outside of stddev ( for ex here , the first occurence of 4 ) , make sure the next point is also outside of stddev ( first occurence of 5 ) , and if so , append a new index with the first deviant point ( 4 here ) .
I'm aware I could take ` slope_down_begin_points ` , check the closest ` slope_down_begin_points ` and see if the sum of points ' duration between the two is > minimum time .
correct , I've updated the values ( and min window size ) :) I typed those a bit too quickly .
Basically I want to reshape tensors represented by numpy.ndarray .
Using ` reshape ` is never ambiguous .
Because I'm using einsum to build these tensors , the default option for memory is K ( " try to keep the memory structure unchanged ") , which means indices in front may have smaller strides ( according to my understanding ) .
If you do repeated computations in floating point ( ` arange() ` here is adding 0.1 to 1 and then adding 0.1 to that sum another 29 times ) and if the numbers you are dealing with are not exactly representable in floating point , you won't get an exact answer at the end of the computation .
Im trying to encode each line into an integer
At first glance it may appear that extra zeros have been added .
It's not that extra zeros are being inserted , it's that the ` uint16 ` representation of ` 2 ` is equivalent to two ` uint8 ` s , one with a value of ` 2 ` and one with a value of ` 0 ` .
The first one creates a 1D ` numpy.array ` of zeros : #CODE
The second creates a 2D ` numpy.array ` of 1 row and 3 columns , filled with zeros : #CODE
` np.zeros (( 1 , 3 ))` creates a 2D array of 1 row and 3 columns , filled with zeros .
For instance sum : #CODE
They also have different behavior with matrix / tensor operations like ` dot ` , and also operations like ` hstack ` and ` vstack ` .
You may have wanted ` from numpy import sqrt , exp ` , but that's just a guess .
For scale=sqrt ( 20 ) the " 3-sigma " range for x in the log normal distribution spans more than 11 orders of magnitude !
[ That is , exp [ 6*scale ] ~10^{ 11.6 } . ] Thus I would expect to need more than 10^{11 } values to properly sample the distribution .
TypeError : list indices must be integers , not tuple .
Hi @USER , I tried that but kept getting the following error code : ValueError : could not broadcast input array from shape ( 120 ) into shape ( 0 , 11 )
I could not do any math between two matrixes of ( 120 ) and ( 0 , 11 ) without getting the following error : ' ValueError : operands could not be broadcast together with shapes ( 120 ) ( 0 , 11 ) ' If I made that second matrix ( 1 , 11 ) rather than ( 0 , 11 ) I was able to perform operations between them .
I tried multiplying / adding ' [ 0 , 0 , 0 , 0 ] with np.ones (( 4 , 11 ))' but I get ' operands could not be broadcast together ' So I tried switching ' cluster_results = np.ones (( 4 , 11 ))' to ' cluster_results = np.ones (( 11 , 4 ))' and switched [: n-1 ] with [ n-1 :] .
You could keep adding new ones ( and filtering out the values that are too large ) until the size is what you want , but it would be simpler to make it sufficiently large in the first place , then use only ` size ` of them : #CODE
The sqrt function in my example shouldn't rely on BLAS .
Probably want to google and check the numpy doc or the repo's commit log .
Ok , It compiles , I believed I could stack path after -I ....
It need to be noted too that one of the bug I get in this SWIG process was the bad encoding of my .i files , they need to be carefully encode in utf-8 .
I already have an array of these ( actually a ` std :: vector `) , and I want to make a copy -- or use the same memory if possible .
I need to concatenate values+indexes of ndx1+ndx2 and sort by value ( take k nearest vectors from 2k vectors ) .
I can't stack data1 and data2 because then it doesn't fit in RAM .
If ` data1 ` and ` data2 ` have at least one of the dimensions equal you can stack vertically or horizontally ` d1 ` and ` d2 ` and then ` argsort ` the stacked array .
I can't stack data1 and data2 because then it doesn't fit in RAM .
I can't stack data1 and data2 because then it doesn't fit in RAM .
Exception stack trace would help a lot
tile operation in python
could anyone explain what is the function of tile operation in numpy ?
As your example show , tile ( like most numpy functions ) returns an array and not list , even if the input was a list .
` reshape ` doesn't modify ` A ` so the end example would still give a broadcast error .
Check your feature extraction layer and try to extract float values ( that can optionally encode boolean feature with values 0 . and 1 . when needed ) .
You can easily solve for ` a^2 ` , ` b^2 ` , ` a b cos ( c )` , and ` a b sin ( c )` .
We're going to translate 4 observations of ( the code we'll write will take any number of observations , but let's keep it concrete at the moment ): #CODE
In python , this would translate to : #CODE
In a sense , you make an initial guess of the model parameters ( all ones , by default in ` scipy.optimize `) and follow the slope of ` observed - predicted ` in your parameter space downhill to the bottom .
Otherwise ` scipy.optimize `' s " hill climbing " algorithms ( like LM ) won't accurately calculate the estimate the local gradient , and will give wildly inaccurate results .
possible duplicate of [ Getting the index of the returned max or min item using max() / min() on a list ] ( #URL )
No , it doesn't , since you are comparing to the index of the min , which is 0 .
EDITED , use min instead of argmin , sorry .
Moreover , you are overwriting a built-in function ` min ` .
@USER you missed my edit . regular min works fine as you pointed out .
Looking at its source code , it first seems to flatten the two arrays , after which it does some clever sorting tricks .
` transpose ` returns a view to the original array .
max of Corresponding Elements in a numpy.ndarray
' log lines , can you upload it somewhere ?
I have a 5x5 array of arrays and I'm trying to matrix multiply the transpose of one row with another row .
and so ` np.dot ` treats it like a real dot product rather than doing matrix multiplication .
Am I missing a simple way of getting the transpose without having to reshape it ?
In my program I current create a numpy array full of zeros and then for loop through each element replacing it with the desired value .
To convert NaNs to zeros , use : #CODE
Will keep an eye on it in case it changes in future versions .
How does numpy scale values , when you convert an array from a float dtype to an integer dtype , if you have an array with a max value higher than what the integer type can hold ?
How do I split the array into two new ones ?
Also , if you're working with the Python data analysis stack , I highly - ** highly ** - recommend getting aquatinted with [ iPython ] ( #URL ) and [ pandas ] ( #URL ) .
Writing an infinite sum ( of a function that has an integral ) in Python
If n is chosen to be quite high ( instead of infinity ) it can be shown how the sum tends to 1
Move ` quad ` inside loop , and evaluate sum of squares ( instead of currently evaluated sum ( c_i )) this way : #CODE
By the way , how can you SHOW that limit of this sum equals 1 ?
Note that for larger and larger ` n ` , the numeric ` sin ( n* np.pi *x )` is a worse and worse approximation .
If the intention is to prove that the sum is equal to one then it should be done analytically .
Once run the array ` S ` will contain the desired sum as a function of n .
dst = norm ( b - a );
You can use the ` zeros ` function to create a 2-dim array full of zeros , and then just populate the required entries .
If i would sum them with excel or calculator , output would be : #CODE
But if I would sum this numbers with numpy array : #CODE
Binary random array with a specific proportion of ones ?
What is the efficient ( probably vectorized with Matlab terminology ) way to generate random number of zeros and ones with a specific proportion ?
Round / floor / trunc ?
@USER : A binomial or other random distribution functon won't give you a 2 / 3 chance of 3 and a 1 / 3 chance of 4 ; it'll give you a high chance of 3 , a lower chance of 4 , an even lower chance of 2 , an even lower chance of 5 , etc .
@USER , a binomial sample with N=2 and p=ration will generate whatever I want I believe !
E.g. suppose ` frac ` is the proportion of ones : #CODE
A simple way to do this would be to first generate an ` ndarray ` with the proportion of zeros and ones you want : #CODE
Note that this approach will give you the exact proportion of zeros / ones you request , unlike say the binomial approach .
If you don't need the exact proportion , then the binomial approach will work just fine .
or ` left outer join ` with ` where ` clause : #CODE
Where I get an error cause I cannot flatten only once to make the matrices multipliable !
In my special case the matrix is in the shape ` ( N , 2 )` and with the second column as all ones if that would help .
You can simply reshape the array .
The difference between ravel and flatten primarily comes from the fact that flatten will always return a copy and ravel will return a view if possible and a copy if not .
I have two arrays of integer values representing particle IDs say A B ( IDs are unique to each particle ) .
Check your B array , there are just 35590 unique indices there .
Check your B array , there are just 35590 unique indices there .
So you want this - 2 dot products , one for each row of ` a ` and ` b ` #CODE
the ` dot ` product can be expressed with ` einsum ` ( matrix index notation ) as #CODE
For example if ` A ` were ` ( 2 , 3 , 4 )` and ` B ` ` ( 2 , 4 )` , it would be more obvious the dot sum has to be on the last dimension .
Also , the moments do not specify a unique distribution .
The generalized gamma function has non-zero skew and kurtosis , but you'll have a little work to do to figure out what parameters to use to specify the distribution to get a particular mean , variance , skew and kurtosis .
This displays a histogram of a 10,000 element sample from a normal distribution with mean 100 and variance 25 , and prints the distribution's statistics :
Replacing the normal distribution with the generalized gamma distribution , #CODE
For instance , if I had the array [ 4 9 13 25 ] and I was allowed to add 1 more number in order to minimize the maximum difference between elements I would insert a 19 between 13 and 25 ( max difference is now 6 rather than 12 ) .
Your method adds ( 3+12 ) / 2 = 7 , resulting [ 0 , 3 , 7 , 12 ] , then 5 and 9 [ 0 , 3 , 5 , 7 , 9 , 12 ] to achieve max gap <= 3 , making total of 3 new points .
For example , we have ` [ 2 , 10 ]` and we want to add two elements : ` -> [ 2 , 6 , 10 ] -> [ 2 , 4 , 6 , 10 ]` and the max difference is 4 .
I honestly see no need to join the answers .
For the larger outer ones , it is simpler as they are relative well separated in subplots 5 and 6 .
Note , this ' bug ' is also seen in the fact that the min , max , and mean are not the same ...
and the Scypi stack is installed in ` site-packages ` where it is maintained by pip .
From the number of questions that go by , the scientific software stack on mac is very fragile ( and it's apple's fault ) .
It used to work find and I'm not sure what I did in the meantime ( nothing I can recall ) * maybe * and update of some part of the stack using pip .
I didn't know that function , but it seems to replace nan with zeros .
Not all elements of the output array are zeros .
I have two arrays , polycoeffs and cov , that were generated by the numpy function polyfit .
I've managed to pull out the leading diagonal from the cov array and I'm trying to match these values with the appropriate coefficients in a list called uncert_coeffs with the uncertainties function " ufloat " .
I've ammended the line to cov_diag.append ( element ) but am still getting the error i didn't mention in my last comment : AttributeError : ' numpy.float64 ' object has no attribute ' strip ' .
You're getting the ` AttributeError ` because you're trying to use the ` strip ` method on a numerical value .
Strings have a ` strip ` method to remove whitespace .
Floats don't ( it wouldn't make any sense ) , so you're getting the error when you try to strip a float .
sum =( sum + freq [ i ]) / i
During the first iteration you append an item , giving value to ` freq [ 0 ]` .
but that one complains about out-of-bounds indices .
The variable ` i ` is normally used for indices or complex numbers .
Create a diagonal matrix ` W ` from the elementwise square-roots of ` w ` .
If you product X and y with sqrt ( weight ) you can calculate weighted least squares .
Vector fields can be constructed out of scalar fields using the gradient operator .
PANDAS : Incorrect shape of indices passed , not sure how to fix it
ValueError : Shape of passed values is ( 3 , 27 ) , indices imply ( 4 , 27 ) # pandas DataFrame
I have two arrays ` A ` and ` B ` of unknown dimensions that I want to concatenate along the ` N ` th dimension .
What I have done is to extend the shape with 1's up until the ` N ` th dimension and then concatenate : #CODE
With this code I should be able to concatenate a ( 2 , 2 , 1 , 3 ) array with a ( 2 , 2 ) array along axis 3 , for instance .
The core of ` expand_dims ` is a reshape : ` a.reshape ( shape [: axis ] + ( 1 , ) + shape [ axis :] `
all my data lie within an arbitrary XY array ( surface spatial data ) -I need to extract lines availlable ; for example I want to extract only the uncommented blue star line - and then move the next ones ( currently commented in the code
It'll basically look like ` A = R D R^t ` , where ` R ` is a [ 2D rotation matrix ] ( #URL ) , ` D ` is a diagonal matrix with elements something like ` [ 1 / width^2 , 1 / height^2 ]` and your values to threshold are given by ` XYarray^t A XYarray `
The actual RMS would be ` norm ( x ) / sqrt ( x.size )` , but for minimization the constant multiplier doesn't make any difference .
Your gradient function doesn't make any sense to me , but it's optional regardless .
This problem doesn't have a unique solution .
Kingston : Thanks , but do you think I am searching only one min point ? or N ?
which both throw an error ` _pickle.UnpicklingError : unpickling stack underflow ` .
Unsuccessful append to an empty NumPy array
I am trying to fill an empty ( not np.empty ! ) array with values using append but I am gettin error :
` np.append ` is just a ` concatenate ` : ` return concatenate (( arr , values ) , axis=axis )`
` numpy.append ` is more like concatenate , it makes a new array and fills it with the values from the old array and the new value ( s ) to be appended .
The error you're seeing has nothing to do with types , it has to do with the shape of the numpy arrays you're trying to concatenate .
If you append an ( 2 , n ) and ( n , ) you'll get a ( 3 , n ) array .
Your code is trying to append a ( 1 , 0 ) to a ( 2 , ) .
Cause I want to dynamically append numbers to my array !
Unfortunately numpy arrays are not dynamic arrays , but you can append to them by making a copy .
If you append what you want without forcing it to be equal to result [ 0 ] there is no any issue : #CODE
The ` append ` produces a ` ( 2 , )` array .
What exactly do you want ` result ` to look like after the ` append ` operation ?
Now you can use this array to append rows of similar shape to it .
In short , YES , of _course_ there is a way to concatenate onto an empty array !
It uses triangulation to split the polyhedron into multiple sub-tetrahedrons ( simplex ) and calculate the volume independently , then sum up all sub-volume values .
Or are there different ways to solve the minimization and finding zeros problem ?
If it's an underdetermined problem , adding the a-priori constraint that ` x ` must have a norm of ` 1 ` can allow you to solve it .
Or you could mask the x value as well , so the indices were consistent between x and y #CODE
And then sum up the columns and make the plot .
I checked and the min and max values are effectively integers ...
You can also use the ` numpy.transpose ( a )` function ( which is also exposed with ` a.T `) to transpose ` a ` making the columns become rows .
I had been trying to use the a [ 0 , :] form , but I forgot that the 1-dimensional object I was trying to set it to was defined as a row -- your mention of transpose turned the light on for me .
I am trying to decompress the array , and then I need the dot product of this array with an array of weights to get a total weight .
For comparison , if you wanted to restrict your values with a minimum as well as a maximum , without ` clip ` you would have to do this twice , with something like #CODE
You can just append a value to the end of an array / list using ` append ` or ` numpy.append ` : #CODE
is it correct to use ` w=w ` in polyfit for these cases or should I use ` w = sqrt ( w )` of what should I use ?
If you have normally distributed measurements , then your uncertainty in each value would be proportional to ` 1 / sqrt ( n )` where ` n ` is the number of measurements .
The diagonals of your ` cov ` matrix are the individual variances on each parameter , and of course the off-diagonals are the covariances .
So given a row and column index ` ( i , j )` , ` mat [ i , j ]` is a ` QxQ ` matrix .
I want to reshape this array to shape ` ( N*Q , N*Q )` such that #CODE
You can see that ` mat [ 0 , 0 ]` goes to ` new_mat [ 0:2 , 0:2 ]` .
Currently ` mat.reshape ( N*Q , N*Q )` takes ` mat [ 0 , 0 ]` to ` new_mat [ 0:4 , 0 ]` ( which is what I do not want ) .
How can I use reshape or rollaxis or something similar to reshape this array ?
cond = multiprocessing.Condition()
One subprocess releases the cond then anothe can acquire .
I tried it , it was ok but needed a small change . results.append ( r ) does not append the result into results .
Detailed call stack of the error :::
I don't understand what you're trying to do , but I've written something close to what you have above in a clearer form using concatenate and reshape .
Commute numpy sparse matrix dot product
but actually , this doesn't solve the issue of copying the data -- it will require me to pass the indices returned by argsort which will result in a copy .
What is best way of doing : given a 1-D array of discrete variables size N ( here N=4 ) and X is the number of unique elements , I am trying to create a multidimensional array of size ( N*X ) where elements are 1 or 0 depending on the occurrence of elements in the 1-D array , e.g. Following array_1D ( N=4 and X=3 ) will result in array_ND of size 3*4 : #CODE
which is the transpose of the one you expect .
What's happening here is that numpy uses the elements of ` array_1D ` as row indices of ` eye ` .
There may be a faster way to create the indices then ` np.where ` .
sequentially puts the input array into the output array in locations given by indices and would work like @USER ' s answer .
Thats interesting that ` copyto ` will broadcast in that way .
` np.copyto ` is new and very unique in the fact that it is the only one of these capable of broadcasting .
How to filter a numpy array based on indices ?
I retrieved the array indices by using numpy.indices but could not mange to find an efficient way to construct the filter .
Now I want to extract the values of those points whose indices are at a distance of say , 1 from the central point i.e. ( 2 , 2 ) .
Another way , ` mask = np.hypot ( * ( indices - pt [: , None , None ])) <= distance ` , which isn't quite as simple as I thought it would be :P
While this does not return indices at the very least it should give you a starting point that you can add the center to .
In case you want the indices : #CODE
If you want both the indices and the values , you can simply use [ ` ndenumerate `] ( #URL ) instead .
If you want to store a list of strings , then I suggest using a ` std :: vector std :: string ` : #CODE
There are a number of python-based trajectory readers for many of the major ones .
We want to strip the parens , then split on either whitespace or comma followed by whitespace .
Another way to say that is : strip the parens , then split on whitespace , then strip the optional trailing commas .
Meanwhile , what bugs have you ever caught with ` strip ( ' ( )')` that you wouldn't also have caught with ` [ 1 : -1 ]` ?
try ` print max ( signal1 )`
If ` signal1 ` is an iterable of numbers ( or anything sortable ) , then ` max ( signal1 )` will give you maximum value .
If you need to convert those items to numbers , then use the ` key ` argument : ` max ( signal1 , key=abs )` .
Yes , it is the largest considering + and - values , and in absolute value , it's 6929 , doing : ` abs ( signal1 ) .max() ` , Am I right ?
Im trying to concatenate several hundred arrays size totaling almost 25GB of data .
I think alloc a large array and fill in it is much faster than concatenate , because concatenate will copy the whole array every time .
Depending on how often the files on disk change , it might be much more efficient to concatenate them with the OS tools to create one huge file and then load that ( or use numpy.memmap )
If so you might want to not reshape Z for simplifying the output process ( that is my Z1 ) #CODE
I believe it still does reshape the array , as you are providing X.shape as second part [ may becomes a tuple , I dont know much python notations ] .
Each element in the matrix , except in the principal diagonal , ( if I am not wrong ) simplifies to E ( x_{i } * x_{j } ) - mean ( i ) *mean ( j ) where i and j are the row number and column number of the covariance matrix .
possible duplicate of [ numpy cov ( covariance ) function , what exactly does it compute ? ] ( #URL )
` cov ` in ` \ site-packages \numpy\lib\ function_base.py ` #CODE
What would be the most efficient way to concatenate sparse matrices in Python using SciPy / Numpy ?
I also get a ValueError of shape missmatch : Objexts cannot be broadcast to a single shape when the select function is executed .
So in your line ` hsv [ ..., 1 ] = np.where ( maxc == 0 , 0 , dif / maxc )` , ` dif / maxc ` is computed even for elements where ` maxc == 0 ` , but then only the ones where ` maxc !
where ` 0 ` stands for a matrix of zeros with the relevant dimensions .
Judging by the block matrix you're looking to create , you need a 3x2 matrix of zeros : #CODE
Find max since condition in pandas timeseries dataframe
I'm trying to find the max since condition was true in a pandas dataframe .
However , I'd like some sort of display where I can move the cursor around the image , and see the indices of the current pixel , and its gray ( or RGB ) values , for example : ( 213 , 47 : 178 ) or ( 213 , 47 : 122,10,205 ) - these values of course constantly changing as the cursor moves .
If you strip away all the numpy , this is just a standard sequence unpacking error .
Really starting from the ground floor
If you want them to be 255 , ` clip ` them first : #CODE
@USER Yeah , ` clip ` is definitely easier , but kudos for solving it and posting your own solution :)
The easiest and most likely the fastest method would be using fft from SciPy .
The error is contained in ` pconv ` , where the diagonal contains the variance for each one estimated coefficients , which in this case are ` a ` and ` b ` .
Stating that , if you consider your 2k x 2k matrix being a 2 x 2 matrix , then you operate in a tensor product of vector spaces , and have to use ` tensordot ` instead of ` dot ` .
is one possible ( generalized ) dot product , ( 2 , 4 ) ( first and last dim of H ) #CODE
from the documentation ` svd ` requires ` 2-D arrays ` as inputs .
I am not sure which reduced SVD algorithm numpy uses ( I think it might be the Compact SVD , or thin ): a brief description of 3 widely used ones is on wikipedia : #URL in the Reduced SVDs section .
One can always reduce the size of the U matrix to M x min ( M , N ) .
I received this code to group data into a histogram type data .
I now want to apply some aggregate functions to the records in each of my bin groups ( An aggregate funcitn is something like sum , mean or count ) .
Now I can't think of an existing function that counts the number of ones in a numpy array ( it has to work on a numpy array ) .
Because python treats True as 1 if I sum the values of my boolean array I'll get a count of the values that == 1 .
It becomes very clear if you think of list indices as pointing between
That mostly shows endcases , which are the ones any sane reader would worry about ;-)
Yup , it's much cleaner this way :-) It becomes very clear if you think of list indices as pointing * between * elements , with 0 " to the left " of the leftmost element , and ` len ( hugelist )` " to the right " of the rightmost element .
My instinct is to define ` a , b=f ( x , y )` but python tells me that ` these operators could not be broadcast with shapes ( 3 ) ( 2 )` .
scipy.optimize() Value Error : Shape mismatch for sum
The result is captured by ` theta ` and the optimized ` x ` from the attribute ` theta.x ` , which you will see is also flattened , needing the same reshape .
You could pad the elements with zeros or use a sparse matrix , or simply not convert to NumPy .
To get you started here's how you can set up a masked array from a jagged array and compute the sum along an axis .
Now what I need additionally is to get the sum over the third dimension , i.e. a ` MxN ` matrix which the element ` i , j ` corresponds to ` np.array ( A [ i ] [ j ])` if my array of depth free would be ` A ` .
Unless your data is large enough to cause memory issues I would pad the last axis with zeros and use a masked array .
You can [ sum over an axis without looping ] ( #URL ) .
The important point is that the summation I want is : ` A.B ` , i.e. if ` A ` was the first array and B the second which will be multiplied to it , I want the element wise dot product in third dimension , not exactly the sum !
( Summation of ` np.multiply ( A.B )` in third dimension will be equivalent to dot product in third dimension )
I'm having trouble relating your original description ( with update ) to the last bit about ' element wise dot product in the third dimension ' .
ones .
Now lets look at your ' sum on the last dimension ' problem .
Notice first that ` sum ` ( or ` add.reduce `) has not been extended to work with this type of array .
The speed advantage of the ` ndarray sum ` isn't as great .
` sum ` can be sped up by coding it as a ` dot ` product ( with ` np.dot ` or ` einsum `) : #CODE
` tile ` actually converts ` reps ` to a tuple .
You are passing a 2D array ( ` vectors `) with shape ( 1 , 14 ) as the second argument of ` tile ` .
You are using ` tile ( A , reps )` wrong .
The ` type ` array contains different strings , but I only want to use the ones that are ` Ia ` , ` Iabg ` , and ` Iat ` .
there is likely little to no difference ... you could run ` dis.dis ` on each and look at the instruction stack
So , what you're doing is a dot product , so use ` numpy.dot ` , which is built with the multiplying and summing all together from an external library ( LAPACK ? ) ( For convenience I'll use the syntax of ` test1 ` , despite @USER ' s answer , because no extra argument needs to be passed . ) #CODE
Each of the ` _mask ` variables are boolean arrays , so when you sum them you count the number of elements that are ` True ` .
However , when I run the for loop , my sum is only for ` htype = 10 ` .
Efficient way of sampling from indices of a Numpy array ?
I'd like to sample from indices of a 2D Numpy array , considering that each index is weighted by the number inside of that array .
@USER , but that means to make an extra array to store the indices , right ?
I'd save the output of ` np.indices ( A )` , flatten the result ( ing tuples ) as well as your array of weights , use the linked method and your result is then given by ` flattened_indices_x [ idx ] , flattened_indices [ idx ]` .
I think keeping indices in a separate array would make computations cheaper , right ?
The search is quite cheep , especially compared to ` cusum ` or ` sum ` .
I don't see how indices in a separate array would help here , but maybe I'm missing something .
@USER , ` cs ` is sorted and ` searchsorted() ` exploits that to do a binary search - only ` O ( log ( len ( weights )))` comparisons are needed .
If the indices you want are not contiguous you can do something like this : #CODE
I then get the error SyntaxError : ( unicode error ) ' rawunicodeescape ' codec can't decode bytes in position 2-3 : truncated \uXXXX
Aha , I didn't think to transpose B .
Because it will broadcast to match ` B ` ( this is what ` np.arange ( 2 ) [: , None ]` gives ) .
Just the transpose , no ?
Note that if in your implementation ` A ` is a * one-dimensional * array ( e.g. ` A.shape == ( 3 , )`) , then the transpose doesn't actually do anything .
In numpy , the transpose operation simply reverses the dimensions of the array .
If the array has only one dimension , the transpose is a " no op " -- it doesn't do anything .
@USER Hmm in fact ` A ( 2:3 )` is 2x1 so for datatype ` array ` the transpose would indeed not make sense as described here : #URL Perhaps using datatype ` matrix ` in numpy would make for a better translation .
I think that the transpose is unnecessary in Python , because array ([ 1 , 2 ]) is the same as its transpose .
use ` a.conj() .transpose() ` for Matlab's transpose ( ` a '`)
and ` a.transpose() ` for Matlab's non-conjugate transpose ( ` a . '` or ` transpose ( a )`)
A ( 3:2 ) is a single value , so I didn't think that transpose made sense .
Third , the transpose is then ` transpose ([ A ( 2 ) A ( 3 )])` ( i.e. a row vector becomes a column vector etc )
It's just to return the transpose of the matrix .
What is most efficient way of setting row to zeros for a sparse scipy matrix ?
possible duplicate of [ scipy.sparse : Set row to zeros ] ( #URL )
But I want to convert the image from RGB color model to HSI color model and then plot a histogram for the saturation and intesity parameters of the HSI image .
here is the histogram i have got by taking bin size 100 but I'm not sure what should be the appropriate size for my data
I intend to separate the saturation component and then plot its histogram
@USER I have modified the code and tried to plot H S and I paramateres separately but is there a way I can verify the correctness of my histogram
and my data for saturation ranges from 0 to 1 so how do I modify the plt.hist ( lu , 256 ) for the histogram to display correctly
The value for H S and I paramters are in the range 0-1 but the bin size here is 256 how do I modify that so that my histogram displays the data correctly using the right number of bins
Including ` np.ceil ` to make ` num ` an integer is sensible , but if you don't use ` max ( ..., 2 )` I don't see how your solution has any relation to mine .
I think I should make it like ` max ( 1 , np.ceil ( alpha_beta_dist / float ( precision ))` .
Many of the most common array statistics ( like sum and mean ) are DataFrame methods ,
The reason this works in your example is because np.sqrt is a ufunc , i.e. if you give it an array , it will broadcast the sqrt function onto each element of the array .
The square of this permutation is #CODE
How can I truncate them ?
Well , why do you want to truncate ?
Overwrite char ( or std :: string ) array positions with SWIG ?
It's possible to do this with ` char* ` , ` char** ` and / or ` std :: string* / std :: string** ` ?
Use std :: vector : #CODE
%template ( IntVector ) std :: vector ;
%template ( DoubleVector ) std :: vector ;
%template ( StringVector ) std :: vector ;
%template ( ConstCharVector ) std :: vector ;
At y=3 ( roughly ) , not only are there not 3 x-values but none of the ones listed seems to get even close to the plot .
your code does not show the part of copying to the list so you concatenate an array and numpy array .
just ` append ` the data to your list
no this does not get me what i want , if i use append i got a list of lists , but i want one lists with all elements in it .
You can use ` append ` method if its a single item #CODE
Then , to sort the list with such indices , you can do : #CODE
You're overwriting the ` arr ` with the indices which would sort it .
I think you mean ` indices = np.argsort ( arr ); arr [ indices ]` .
True , sorry , I meant your first sentence , " np.argsort doesn't sort the list in place , it returns a sorted list " It returns the indices which would sort a list .
There are two issues here ; one is that ` np.argsort ` returns an array of the indices which would sort the original array , the second is that it doesn't modify the original array , just gives you another .
You might not need this array of indices , and would find it easier to just use ` np.sort ` : #CODE
Cheers for that , it does work the only problum is which i should have probly mentioned earlyer the file im dealing with is huge its about 180000 arrays like the ones above .
A simple way to read this data back is to just read line by line and accumulate the values into an array , let's call it " row " .
Then when we read an empty line we just append the row to the result array and clear the current " row " .
This works by putting a one at the central location of each atom , in a stack of grids .
Then , for each atom type , those ones are all added .
You only has to reshape your final result to obtain what you want : #CODE
@USER Or don't create ` z ` at all , since any of the solutions can just broadcast with simply ` z = 99 ` .
` line is unnecessary , since ` where ` will broadcast : ` y = np.where ( x ! =0 , x , 99 )`
` svd ` decomposition function first to get the nullspace .
As you can see from the matlab code for null.m , they also call svd to get the null space .
In fact they also call svd to get the rank ( I didn't copy that here , but feel free to read the code it's about 4 lines ) .
How to filter numpy array by list of indices ?
I would like to find standard deviation of the z values for the neighbors returned by query_ball_point , which returns a list of indices for the point and its neighbors .
I can append the values to a list and calculate std dev from that , but I think it would be easier to use numpy to calculate std dev on a single axis .
NumPy : financial irr method returns ' nan ' .
When I calculate the Internal Rate of Return ( irr ) using the numpy method ` irr ` , I receive ` nan ` as return .
When I calculate the irr with another program , I get -8% returned .
At rate = zero , the value of NPV is the sum of the cash flows .
I would like to transpose the last two axes , i.e. #CODE
You can considerably improve your code speed creating an 2-D array with ones along the diagonal and then extract the right rows based on the input array : #CODE
An even faster solution would be to create the output array with zeros and then populate it according to the indices from ` a ` : #CODE
M = [ sum ( x ) / len ( x ) for x in [ y for y in A if y ! = -999 ]'
I went down this road a few years ago , when I was getting frustrated with Matlab's ` shading ( ' interp ')` as well : It works by simply interpolating the 4 corner colors on each quadrilateral , which means that the direction of the color gradient can be different on neighboring quadrilaterals .
You can use the ` diff ` method : #CODE
@USER After you upgrade , you may consider switching to use [ ` from numpy.polynomial import polynomial as poly `] ( #URL ) and [ ` poly.polyfit ( x , y , 3 , ... )`] ( #URL )
If ` Y ` follows a function , perhaps you can make the ` equation ` a 2d problem , and solve for a line of zeros .
What's left is an equation of the form ` t log ( 1 + t ) = z ` , with ` t ` and ` z ` being related to your ` x ` and ` y ` .
Now tabulate the values of ` z = t log ( 1 + t )` over the range you need , interpolate ` t ` vs ` z ` , and you have solutions for your equation .
Depending on the range of the values of ` x ` and ` y ` you need , you might or might not want to augment the tabulation with the explicit functional behavior at the boundaries [ e.g. ` t log ( 1 + t )` approaches ` t^2 ` as ` t- 0 ` etc ] .
Then not only do you make your code more compact , but the same function can take single indices , slices or even lists of indices : #CODE
This uses ` numpy.c_ ` , which is a convenience function to concatenate various arrays along their second dimension .
Functions like ` hstack ` ( see Jaime's answer ) , ` vstack ` , ` concatenate ` , ` row_stack ` and ` column_stack ` are probably the ' official ' functions you should use .
By my current methods , I encounter errors or failure to transpose .
I hope this will help you perform your transpose and column-wise operations
I have the objective function as frobenius norm of ` |q0_T* P-q1_T | ` squared , where ` q0_T ` is the transpose of a ` nX1 ` vector and ` P ` is ` nXn ` matrix and ` q1_T ` is a transpose of a ` nX1 ` vector .
2 ) all the rows in ` P ` must sum to 1
So there's an issue in that it only accepts a 1d array as the independent variable , one hackish solution would be to pass it as 1d , and reshape in the function .
` my_array ` has 18 values , that means that if I specify that I want two rows , then ` reshape ` function knows that I want 9 columns , basically ` reshaped = my_array.reshape ( 2 , -1 )` is the same as ` reshaped = my_array.reshape ( 2 , 9 )` .
No need to reshape anything : #CODE
I am trying to read an image into a multidimensional numpy array and compute the magnitude of the gradient , which should produce a single channel image .
The magnitude of the gradient will be computed at each pixel , hence your result is the same size as the input image .
My numpy array is 3D and I am expecting a single channel image ( scalars ) from the magnitude gradient of the image
This is how I am trying to compute the magnitude of the gradient , but it doesn't work
So , in your comment above , ` gradient ` gives the gradient at each pixel ( which itself is a vector , so that actually _adds_ to the dimensionality ) .
` sqrt ` and ` dot ` ( in this case ) each return an array of the same size as the input .
One simple example would be the ` norm ` of the array , as below , but this choice should depend on your use case .
So , in the above code , ` gradient ` gives the gradient at each pixel #CODE
Since the gradient itself is a vector , it actually adds to the dimensionality , so a 2d image becomes a 3d array .
` sqrt ` and ` dot ` ( in this case ) each return an array of the same size as the input .
If you use ` dot ` on two arrays , it is doing the matrix version of ` a.T * b ` Here , ` a ` and ` b ` are both the same shape .
The output of the inner or dot product of a 1d or 2d array has the same width as the right hand side and same height as the left hand side , hence two square matrices give a square matrix .
I thought the magnitude of the gradient would work .
i am trying to plot the function sin ( x ) / x and a taylor approximation of it .
Can you not create 30 random numbers for all columns and then set back to NaNs the ones you want blank ?
Don't overthink this one : just make the random series of the length you want and concatenate them together .
Since Ix.max is 255 and min is 0 .
Furthermore , I have to compute the magnitude of the image gradient
I thought the magnitude of the gradient would work .
And , would I still need to take the gradient magnitude ?
Here's the full stack trace : #CODE
Do you have a full stack trace for the error ?
I'm curious if he gets the same result for a 46Kx46K array , which would put the converted data size at just under ` 2**31 ` ( max size for 32-bit signed int ) .
the line ` max = max ( ...
` is going to give you an issue after the first time through the loop as well , since you've just re-assigned the built-in function ` max `
Also a bad idea to over-write the builtin ` max ` , especially since you're actually trying to use the ` max ` function ...
This means you use circle brackets , not square ones .
It's the ` numpy ` -specific ones which can deal with non-scalar inputs : #CODE
For each subsequent values , clip the previous value inserted in the new array between the min and the max from the sliding window .
We find for min and max : #CODE
And for the next value , I clip 3 ( the last value inserted ) between 4 and 5 ( the min and max found at index 1 ) .
For the next value I clip 4 between 3 and 5 .
You can use ` numpy.clip ` to perform the clipping operation in a vectorized way , but computing the min and max over a moving window is going to entail some Python loops and a deque or stack structure like you've already implemented .
Find the min number in all contiguous subarrays of size l of a array of size n
OK , first the windowing an min / max calculations can be done much faster : #CODE
Probably not ideal , but slightly faster solution will be create numpy array with zeros and fill with list values .
numpy concatenate dimension mismatch
I've been running into an issue with numpy's concatenate that I've been unable to make sense of , and was hoping that someone had encountered and resolved the same problem.I ' m attempting to join together two arrays created by SciKit-Learn ' s TfidfVectorizer and labelencoder , but getting an error message that " arrays must have the same number of dimensions " , despite the fact that the inputs are a ( 77946 , 12157 ) array and ( 77946 , 1000 ) array , respectively .
Even though a transpose in numpy simply returns a different " view " on the underlying data and doesn't actually copy or move any data it still seems a bit redundant - just take the mean over ` axis=0 `
If the absolute values of both alpha and beta are smaller than or equal to 1 , the intersection point is inside both line segments .
If one absolute value is =1 but the other is > 1 , the intersection point i is inside only one of the two line segments ( say , s2 ) .
If both absolute values are > 1 , simply find the intersection point by multiplying s1 with ( alpha / beta ) , then adding s1 [ 0 ] to that .
I have a problem using multi-dimensional vectors as indices for multi-dimensional vectors .
As Jaime says , every * row * conincides with one set of indices .
C [ tuple ( INCIDES.T ) then gives IndexError : arrays used as indices must be of integer ( or boolean ) type
On the other hand , tuple ( x ) gives me N " subtuples " of 2 indices , which is correct so far ( I guess ) .
However , C [ tuple ( x )] will give me IndexError : too many indices .
" set of indices " is ambiguous -- does each row contain the full coordinates for a specific element , or all of the 0-coordinates ?
3 ) When ' global ' version of numpy ( i.e. the one installed in / usr / local / lib / python2.7 / dist-packages ) is removed , virtualenv is deactivated and I append ' / home / me / gae / project / lib ' to sys.path in python shell , it is possible to import numpy or any other part of it .
Here's the log from Django : #URL
` ValueError : shape mismatch : objects cannot be broadcast to a single shape `
Maybe this alternative error message helps you : ` IndexError : shape mismatch : indexing arrays could not be broadcast together with shapes ( 7 , ) ( 7 , 10 ) ( 10 , )` for the ValueError you see .
Really , only the ` B ` in axis ` 1 ` matters , the others can be any range that will broadcast to ` B.shape ` and are limited by ` A.shape [ 0 ]` ( for axis ` 1 `) and ` A.shape [ 2 ]` ( for axis ` 2 `) , for a ridiculous example : #CODE
My problem is that I am getting modulus values between 0 and 1 , and when I take the log of those twice I get nan .
But since I'm taking a double log they really need to be between 1 and 2 ( 1 , 2 ] .
Actually , since I don't want any of the final values to be obscenely negative , the escape radius needs to be the base of whatever log I am taking , and the value of the points in the set need to be that base .
So I switch to log2 and set all of the points in the set at the end of the iterations equal to sqrt ( 2 ) + sqrt ( 2 ) j .
How do I get an array which is the sum of a number and the number below it : #CODE
Note that ` np.in1d ` flattens both arrays , so if your ` original_array ` is multidimensional you will have to reshape it back , e.g. : #CODE
Based on val ' s solution , I would suggest to avoid using " x+=step " because depending on the limits and the step , rounding errors will accumulate and you may leave the last value undefined or even worse , try to define the N+1 value , causing a panic .
I am able to generate a histogram for the H S and I parameters of an image using the code mentioned .
I intend to apply histogram equalization on the H S and I histograms and then convert it back to a resultant image so that I can compare the changes .
Can somebody help me with the necessary code for histogram equalization and converting back the equalized histogram to an image .
As far as I am aware , you need only apply the histogram equalisation to the value component of the HSV transform to equalise the image : #CODE
Hey i just did as you described but i want to use the same technique for the saturation parameter also as well as calculate the average value of the histogram how do I go about that ?
Actually I'm performing a special type of histogram equalization on coloured images thats called HE-VED or histogram equalization with variable enhancement degree in which u have a parameter a ( alpha ) for the saturation parameter
@USER : Do you want to take the unique along an axis ?
Although if there are two elements with equal counts this will simply take the first from the ` unique ` array .
I thought this would work automatically , as B would be broadcast to a 100x10 shape .
The problem is that ` tuple ( val )` can obviously be one of 2^24 different values so having an array for every possible value is not possible since it'd be gigantic and mostly zeros , so I need a more efficient data structure .
@USER pass the return_inverse flag and then just check the length of the indices list you get back for each unique value
You'll need to view the return of ` np.unique ` as 3 ` uint8 ` s and reshape it to ` ( -1 , 3 )` to make sense of the data , but it will be much faster , as nothing is done aside from viewing the exact same memory in a different way .
And now you have the unique pixels and their counts in those two arrays : #CODE
Dealing with a large amount of unique words for text processing / tf-idf etc
But as far as the number of unique words , I am not able to deal with the array / matrix , whether it is to get the size of the amount of unique words printed , or to dump the numpy array to a file ( using savetxt ) .
One thing I would like to do is , given an index , which would represent the column of the array , how could I extract those indices ?
I saw how one could set max features amount for the tfidf vectorizer , but that is for the words with the top tf count .
One idea I looked up is doing something like tf_idf_matrix.sum ( axis=0 ) , which would sum up the columns .
If I could use something like argsort to access the top K column sum values , that would be helpful .
I have already tested my backprop method using central differences and the resulting gradient is equal .
The network hast one hidden layer ( 100 neurons ) with tanh activation functions and a output layer with a linear activation function .
The training is done by simple gradient descent with a learning rate of 0.2 .
The problem arises from the gradient , which gets with every epoch larger , but I don't know why ?
Unfortunately the gradient " explodes " nevertheless , just slower .
If the steps would be to big , the gradient should oscilate ?
The gradient could diverge for this reason : when exceeding the position of the minima , the resulting point could not only be a bit further , but could even be at a greater distance than initially , but the other side .
Python / Scipy : Find " bounded " min / max of a matrix
Let ( n , m ) be the indices through NxM .
So , given that the previous array is a simple stack if [ 0 , 2 , 1 ] along the last observation , I would expect #CODE
Also , a [ ..., :: -1 ] [ indices ] , my second attempt , was not fruitful either .
indices = np.argmax ( b > = end_slice [ ..., None ] , axis=-1 )` ( semi-colons are where we should have newlines ... )
You now have the indices , to extract the actual values , you need to do some indexing magic : #CODE
then you can find appropriate indices with np.where : #CODE
and perform the mean on thoses indices : #CODE
To sum up , I want to either tell what data-type of input an encoder expects , or
The stack trace is as follows : #CODE
It's likely that a single set of zeros of the same shape of your array won't align with the dtype .
I think ` reshape ` is what you are looking for .
The only requirement when using reshape is that your total number of elements is unchanged ( i.e. the product of dimensions must be unchanged between the first and second shape )
To get multidimensional indices , it expects a tuple of ints , ` None ` or ` slice ` objects ( which are the objects created when using slice notations , e.g. ` A [ 1:2 ] = A.__getitem__ ( slice ( 1 , 2 ))`) .
` reshape ` returns a view of the original array , and doesn't modify the original array .
Here is my attempt to " translate " #CODE
The alternative in python is using numpy's fromfile and reshaping ( according to this thread here : MATLAB to Python fread ) using the built in reshape function .
@USER , that was my reasoning too , but why is it that we need to reshape ?
` reshape ` simply makes this change , without touching the data itself .
Without the reshape , you get : #CODE
I always forget , and wind up having lots of rather unreadable ` reshape ( ... ) .T `' s around .
I looked it up , and it had something to with my matrix being to densely populated with nonzero elements while presented as a sparse matrix .
You append training and test data into ` all_data ` and get a part of it as train .
You can strip ( to remove trailing comma ) and split ( to break into list of single characters ) each string in a list comprehension to get a list of rows #CODE
This will read CSV files in parallel and concatenate them .
It strips the types to basic Python ones .
Then you could read the pickle files in the main process and concatenate them .
Polynomial approximation would be enough , but numpy.polyfit needs a certain degree .
getting indices in numpy
( If so , we should probably figure out what the appropriate idiom to decode is . )
But that's not the best solution , because we should explicitly decode the bytes into strings and use free functions instead .
Otherwise we should probably call ` decode ` manually .
It even affects something as simple as matrix multiplications , which can no longer be handled with a simple call to NumPy's ` dot ` routine , requiring custom code instead .
The data shows a small linear regime with a shallow gradient , followed by a steep linear regime after a threshold value .
I attempted this using ` scipy.optimize ` ` curve_fit ` and trying a function which included the sum of a straight line and an exponential : #CODE
decode the ` bytes ` object into a string ( like ` .decode ( ' utf-8 ')` and compare with `" val1 "` .
First convert to an array , then squeeze to remove extra dimensions : #CODE
Look at is shape and ndim , transpose it , squeeze it , convert it to matrix and back .
Certain actions jut give a view , for example , ` reshape ` and slicing ( ` a [: 10 ]` is a view of the first ten items ) , and ` np.asarray ` doesn't copy the data if possible .
The best I achieved using concatenate is :
` concatenate (( a , b ) , axis=2 )`
3 ) I replace the values = 5 with zeros : #CODE
4 ) I want to replace now the values > 5 with ones : #CODE
( Not to plug my own answer , but it's relevant : #URL ) You can either roll your own reader and use ` fromiter ` or , these days , just use pandas .
where ` known_roots [: , None , None ]` is the same as ` known_roots.reshape ( len ( known_roots ) , 1 , 1 )` and causes it to broadcast with ` rootsmatrix `
Use Numpy's ` exp ` instead of ` math `' s : #CODE
I have a large image in numpy array form ( opencv returns it as a 2d array of 3 uint8 values ) and want to compute a sum of gaussian kernels for each pixel , i.e. ( there's still no LaTeX support in SO is there ? ):
for N different kernels with a specified weight w , mean and diagonal covariance matrix .
Kernels is a ` namedtuple ( ' Kernel ' , ' mean covariance weight ')` , mean is a vector , covariance is a ` 3x3 ` matrix with everything but the diagonal being zero and weight is a float ` 0 weight 1 ` .
Build your kernels and then convolve them with the array using the convolve method .
Added a bounty in the hope that someone wants to provide an example on how exactly I'd use convolve and co to implement the function .
Can't use dot , though , because doing that will add extra dimensions .
For example an array M x N x K dotted with an array M x K x N would produce result M x N x M x N , dot behaves differently on one-dimensional and multi-dimensional data .
So we just do an element-wise multiply and then sum along the last dimension .
Actually the " sum of gaussian kernels " bit in the question had me confused .
The question is a bit confusing , are you trying to calculate a bunch of images convolved with different gaussians , or a single image convolved with a sum of gaussians ?
Played around with convolve but don't see how I'd use it to implement the function .
[ There is currently ( 2013-11-20 ) a mistake in the code in your question and @USER I's answer - the ` | | ` surrounding ` \Sigma ` in the above equation actually denotes the determinant rather than the vector norm - see e.g. here . In the case of a diagonal covariance the determinant is just the product of the diagonal elements .
the spherical ( i.e. diagonal ) nature of the covariance matrices in your problem : #CODE
amplitude of numpy's fft results is to be multiplied by sampling period ?
I try to validate my understanding of Numpy's FFT with an example : the Fourier transform of ` exp ( -pi*t^2 )` should be ` exp ( -pi*f^2 )` when no scaling is applied on the direct transform .
By the way , do you know why the factor is not taken into account in standard fft routines ?
My guess would be that most people use fft , then do something , then use ifft and so the factor is dropped to spare computational time .
It seems you will only need to replace the current check with one based on the sum of the product of the strides with their corresponding dimensions .
More specifically I want a dictionary that has keys that correspond to the row , so key 1 should be the sum of row 1 .
s1 is my array and I know how to get the sum of the row but doing ` numpy.sum ( s1 [ i ])` , where i is the row .
I was thinking of creating a loop where I can compute the sum of the row and then add it to a dictionary but I am new to programming so I am not sure how to do this or if it is possible .
First , we can use ` enumerate ` , and second , we can call the ` sum ` method of ` ndarray ` s .
Running the code and just returning arrays of zeros works fine .
The way I've done it , there's a lot of overhead for spawning each process , just to work on one pair of indices from the data .
The multiprocessing machinery uses pipes under the covers to give them work to do , but does * not * spawn yet another new process for each pair of indices .
Vector sum of multidimensional arrays in numpy
If I have a an N^3 array of triplets in a numpy array , how do I do a vector sum on all of the triplets in the array ?
For some reason I just can't wrap my brain around the summation indices .
I would expect that as N gets large , if the sum is working correctly I should converge to 0 , but I just keep getting bigger .
The sum gives me a vector that is the correct shape ( 3x1 ) , but obviously I must be doing something wrong .
A set of N random numbers over ( - 0.5 , 0.5 ) doesn't sum to 0 as N -> inf ?
As the comments note , there is no reason why the squared sum should approach zero .
The average sum goes to zero as your intuition expects .
So now each of the three items of ` s ` is the sum of 125 numbers , each drawn from a standard normal distribution .
It is a well established fact that adding up two normal distributions gives you another normal distribution with mean the sum of the means , and variance the sum of the variances .
So each of the three values in ` s ` will be distributed as a random sample from a normal distribution with mean 0 and standard deviation sqrt ( 125 ) = 11.18 .
Furthermore you then go and compute the norm of those three values .
The former is easier to deal with , and it predicts that the average value of the square of the norm of your three values will be 3 * 125 .
Unfortunately , I am still not 100% sure I am doing my sum correctly .
The actual purpose of my code is not to sum random numbers , that was only a ( poorly conceived ) unit test .
Is my a.sum (( 0 , 1 , 2 )) the correct way to get the vector sum of my triplets ...
I would think that since I didn't tell cython to store in on the heap it would be stored on the stack , but after doing the following experiment it seems that it is stored on the heap , or somehow efficiently memory managed .
It's almost certainly heap -- consider that size of array is not known at declaration time , numpy usually works on large arrays and stack is limited .
Although stack optimization is technically possible , ` ndarray ` s can be views , thus data reference can escape current scope .
It'll probably be easier to read in all the rows and then keep only the ones without missing data .
However , the raw data is stored in a TIFF image as a single stack of ( x , y , channels ) , with z * time-steps frames .
Is it possible to reshape the array once it's been written into HDF5 , or to write 1D data in a way that it automatically fills in an array ( i.e. writes in with x varying fastest , y second-fastest , etc . ) Update : Something like numpy.ndarray.flat would be ideal .
Numpy and Matlab difference in sum ?
I have a code that I am trying to translate from Matlab to Python however there is a problem with summation : #CODE
Also note that you can sum over multiple axes at once : ` a.sum (( 0 , 2 ))` .
If you then try to sum that along axis=2 , you'll get the error you see because a 2d array only has axes 0 and 1 .
` sum ( a )` in matlab is equivalent to ` a.sum ( axis=0 )` in NumPy .
How to stack a ' numpy masked array ' and a ' numpy array '
I want to stack only Positive values and I don't know how ?
" stack only Positive values " : What does that mean ?
Assuming you want to stack ` a ` and ` b ` for every position where ` b =0 ` : #CODE
If you only need to stack 1-dimensional arrays and the removal of negative numbers from ` B ` will make its shape the same as ` A ` , this should work : #CODE
The question concerns the min .
To see both the normal distribution and your actual data you should plot your data as a histogram , then draw the probability density function over this .
You can use ` matplotlib ` to plot the histogram and the PDF ( as in the link in @USER ' s answer ) .
Last question : where did ` norm ` come from ?
Assuming you're getting ` norm ` from ` scipy.stats ` , you probably just need to sort your list : #CODE
You can plot a smooth ( looking ) probability density function evaluated on a dense set of points , you can plot the probability density function evaluated on your input data ( as in this answer ) and you can plot a histogram ( as I wrote in your previous question ) .
how can I get the position of the nonzero values in a numpy array ?
Cant broadcast ...
so it will take 16384 motifs to go through 12000 lines for 2 mins , it will take ( 16384*2 == 32768 min -> 546 hours -> 22days ... )
then you could run a loop over the length 2 substrings and record which ones are present in a ` dict ` : #CODE
Now you can sum along the columns , mask non-values or whatever manipulation you need to do , for example : #CODE
Finally to find how many occurrences a given motif has , you must find the index of the corresponding motif / chunk and then evaluate in the previous sum : #CODE
What I expect is how many AAAAAAA are in the list of sequences with down-regulation and not in the list and also for up-regulated ones .
Let x denote an array of N rows and 1 column , which is initialized to be a vector of zeros .
I'm not sure if it matters or not , but the sets indx_j will have non-trivial intersection ( e.g. , indx_1 and indx_2 may have indices [ 1 , 2 , 3 ] and [ 3 , 4 , 5 ] for example ) .
Finally , I really cannot say anything about how this will handle writing to overlapping indices at the same time .
To get the output you specified in the question , i.e. to flatten the indices and the values , you can do : #CODE
I test the indices , but for large arrays indices don't work well .
But your approach ( stack columns ) has been useful .
How to efficiently concatenate many arange calls in numpy ?
I'd like to vectorize calls like ` numpy.arange ( 0 , cnt_i )` over a vector of ` cnt ` values and concatenate the results like this snippet : #CODE
I'm trying to implement a function that would sum ( or eventually average )
To achieve exactly what you are asking for I would apply a ` [ 3x3 ]` box-filter on the image and than I would resize the matrix using nearest neighbor interpolation .
It works , but I think the correct sum is block_view ( A ) .sum ( axis =( 2 , 3 )) .
The simplest numpy only approach , which does much less work thatn convolution and will therefore be likely faster than fileter based methods , is to resize your original array to one with extra dimensions , then reduce it back to normal by summing over the new dimensions : #CODE
Scipy - how to further optimize sparse matrix code for stochastic gradient descent
I'm working on implementing the stochastic gradient descent algorithm for recommender systems using sparse matrices with Scipy .
I am facing an issue when using MATLAB eig function to compute the eigenvalues and eigenvectors of a symmetric matrix .
all diagonal elements = 0.45
When using [ vec , val ] = eig ( D ) some of the resulting eigenvectors contain complex numbers ( i.e 0.3384 + 0.0052i ) .
Note : if I try 4x4 matrix with all diagonal elements = 0.375 and all off-diagonal elements = - 0.125 then MATLAB eig ( D ) gave all real eigenvalues and eigenvectors .
Due to the same issue of limited numerical precision I guess there could eventually be very small complex parts , too , but actually Matlab's ` eig ` should detect symmetry and produce only real-valued eigenvalues .
P = eye ( 10 ) - 1 / 10 ;
delta = 1 - eye ( 10 );
However , then I decided to experiment with two other " signals " ( i.e. a randomly generated array of numbers and an array of zeros ) with 1,000,000 samples and resample these ones .
My final experiment was extracting the zeros from my original signal ( there are about 50,000 samples that are zero-valued ) and resampling them .
I was totally surprised to see that resampling only 50,000 zeros takes about a minute .
Previously , I resampled an array of zeros that had 1,000,000 samples in a few milliseconds and now I have to wait about a minute for an array of 50,000 samples .
I really don't see any reason for this behavior ; especially the zeros ( 1,000,000 and just a few milliseconds vs 50,000 and a minute ) surprise me a lot .
scipy.signal.resample use fft , which is very slow if the length of the data is a prime number , can you show the length of your data ?
Yes , the size is a prime number : 595117 is the total length , 58897 is the number of zeros .
fft is most efficient with arrays whose length is a power of two , sometimes adding zeros to reach the next power of 2 is faster even if there are more samples
Where the x-coord is ` mod ` ed by ` w ` ( the image width in pixels ) .
` mod ` ing by ( w*3 ) should correct the problem .
Now , when I plot sin ( PHI ) , matplotlib fills in the contour between phi_1 and phi_2 , between phi_2 and phi_3 and so on , even though it has ** no ** information about what happens in the middle .
From the NumPy docs for ceil , the ` numpy.ceil ` function takes two arguments , the second being ` out ` .
this results in ` 5.0 ` while OP's ceil will return ` 6.0 `
Lexsort don't sort array , it returns indices .
However , if I have function given as array ` ampl ` on a time grid ` tgrid ` , I found ( through trial and error ) that I have to do ` T = tgrid [ -1 ] - tgrid [ 0 ]; deriv = scipy.fftpack.diff ( ampl ) * ( 2.0 *pi / T )` .
I was thinking that the same effect could probably be achieved by passing the correct ` period ` to the ` diff ` function , but I couldn't figure out the correct value .
An intermediate solution might be a C++ binary to do the parsing and then dump it in a binary format .
which could turn out quite expensive because of the repeated call to exp .
I'm using the ` compress ` command to extract the data of each frame .
One simple thing I would do to increase the speed of the first function is to use different notation for the accessing of the list indices as detailed here .
DSP : audio processing : squart or log to leverage fft ?
Apply fft on an audio array given by audiolab to get the different frequencies of the signal .
Now there are two technics : apparently one suggests square ( first link ) whereas the other a log , especially : 10ln10 ( abs ( 1.10 **-20 + value ))
You just have to transpose your arrays : #CODE
One way to accomplish what I want is to hstack ` vec ` to itself 3 times , yielding a 3x3 matrix where each column is ` vec ` and then subtract that from ` mat ` .
If we keep ` vec ` sparse we'd have to add a test for nonzero value at ` row ` .
Also this type of iteration only modifies the nonzero elements of ` mat ` .
If ` vec ` has lots of zeros , then it makes sense to iterate , modifying only those rows of ` mat ` where ` vec ` is nonzero .
Use outer function with FUN= " - " in Python
In R , you can use ` outer ` function to compute the outer product of the matrix , but can also use other operations , such as ` FUN= " - "` to operate subtractions .
In ` numpy.outer ` , you can compute the outer product by writing something like ` numpy.outer ( x , y )` as can in R code , but as far as I know , there are no other argument permitted to make other operations , such as subtraction .
here A is an m X n matrix , the first term in the minimization is the residual sum of squares , the second is the matrix frobenius ( L2 norm ) of a sparse n X n matrix W , and the third one is an L1 norm of the same matrix W .
In the function A is an m X n matrix , the first term in the minimization is the residual sum of squares , the second term is the matrix frobenius ( L2 norm ) of a sparse n X n matrix W , and the third one is an L1 norm of the same matrix W .
I would like so direction on how to achieve this as I have no idea how to take the frobenius norm or how to tune the parameters beta and lambda or whether the scipy.optimize will tune and return the parameters for me .
using fmin would likely be slower , since it does not take advantage of gradient / Hessian information .
The code in HYRY's answer also has the drawback that as far as fmin is concerned the diagonal W is a variable and fmin would try to move the W-diagonal values around until it realizes that they don't do anything ( since the objective function resets them to zero ) .
Here is the implementation in cvxopt of HYRY's code that explicitly enforces the zero-constraints and uses gradient info , WARNING : I couldn't derive the Hessian for your objective ... and you might double-check the gradient as well : #CODE
The overall goal ( which I've already obtained ... the answer to this question will just make the class better ) is to make dot products and other processes more unified and easier for numerical methods .
Second , if centroidsm is supposed to be an array of centroid matrices , you simply need to call ` centroidism.append ( centroids )` inside you for loop ( ` centroidsm [ k ] .append ` attempts to append to an array at index k - an array that doesn't exist ) .
If i just do centroidsm.append ( centroids ) , i can only call all the arrays of centroids not specific ones
Find indices of N concurrences in arrays using numpy
What I'm trying to do , using numpy , is to search some values , for example between ` 2.7 ` and ` 2.3 ` , and at the same time I'd also like to return the indices when they are found in the rows of the arrays .
I have read a lot , and I have found for example ` .argmin() ` , which does partially what I want ( but it only shows you where the zeros or values lower than zero are located , and just one concurrence ) .
What do you want to use the array indices for ?
You are probably better of simply using the result of ` ( v > 2.3 ) & ( v < 2.7 )` ( a boolean array ) instead of an array of indices .
What you need is ` numpy.where ` , which returns a tuple containing the indices of each dimension where some condition is ` True ` for the values of an ` numpy.ndarray ` .
@USER ` i , j ` are the indices returned by ` np.where ` ...
Both ` argwhere ` and ` where ` use ` nonzero ` .
I'm impressed you didn't get an error on ` array [( 1 , 2 , 3 )]` or the lack of ` zeros ` being imported ...
I believe you could combine the ` resize ` and ` reshape ` steps and just do ` a.resize ( 6 , 10 )` .
You're right , it repeats the array :P When testing , I saw that the shape matched and didn't look for the zeros .
Now the signal ` sin ( Y )` _would_ have a peak ...
If you receive a signal at each timestep , then ` data = [ 1 , 1 , 1 , 1 ]` is what your _signal_ should be for an ` fft `
` fft ` takes the _signal_ and you can you use ` fftfreq ` to get transform the timing points to get the frequency axis on your power spectrum plot .
If your signal is not approximately symmetric around 0 , then it's normal to have a high DC component ( index 0 of the fft ) .
999 the fft will be nearly the same .
You are taking the fft of a monotonically increasing ( step ) function and the fft will give a peak at the 0 Hz bin .
Even if you do the fft of 1000 , ...
yes , my input signal is from a physical source.A cos singal is easy to plot .
As others have hinted at your signals must have a large nonzero component .
This cosine function cos ( 0 ) *ps ( 0 ) indicates a measure of the average value of the signal .
In the end your data array should probably not be arange ( 1000 ) , but should be something like : data = zeros ( 1000 ); data [: : 10 ] =1 This will indicate that once per second ( assuming a sample rate of 10Hz - every 10th value a photon comes in ) a photon comes in .
If I have a ` np.array ` of values , ` Y ` , with a ` no.array ` of corresponding errors , ` Err ` , the error in the log scale will be #CODE
NumPy has the function ` log1p ( x )` that computes the log of 1+x .
I have tried multiple approaches using concatenate that look like these examples : #CODE
If you really want to use ` hstack ` , you need to make them multidimensional first ( or use ` atleast_2d ` or similar ) , or use ` vstack ` and transpose the result .
I'm attempting to translate some matlab code again and I've run into another pickle .
DSP : audio processing : squart or log to leverage fft ?
possible duplicate of [ DSP : audio processing : squart or log to leverage fft ? ] ( #URL )
No it is not : my other question was related to the best equation to take log or sqrt . this one is related to the whole extraction of magnitude as i understand it is two different things .
In order to get the amplitude you just need to take the absolute value : ` np.abs ( spectrum )` .
In order to get the power spectrum square the absolute value .
The output of the fft is complex .
To find the magnitude of a given frequency , you need to find the length or absolute value of the complex number , which is simply sqrt ( r^2 + i^2 ) .
You can see the indices for this with ` where ` : #CODE
local histogram equalization
I need to do both a global and local histogram equalization .
But I think it should be 256 because the cdf should be the cumulative sum of how many pixels have each brightness value , so binning by pixel value instead of number of pixels make sense ( i think ?! ) .
The normalization step , where you divide pairwise distances by the max value , seems non-standard , and may make it hard to find a ready-made function that will do exactly what you are after .
numpy append array to array
I'm trying to append one numpy array to another numpy array , like this : #CODE
You can use ` hstack ` and ` vstack ` to concatenate arrays : #CODE
An alternative is " horizontal stack " ( also creates a new array ): #CODE
Generate large number of unique random float32 numbers
I need to generate a binary file containing only unique random numbers , with single precision .
How can I get 370914252 ( this is my biggest test case ) unique float32 inside a binary file , even if they're not random , I think that a shuffled sequence would suffice ..
You can generate your ` set ` from the integer values rather than the floating point ones , then do the conversion on output .
Select a submatrix based on diagonal value
I want to select a submatrix of a numpy matrix based on whether the diagonal is less than some cutoff value .
I want to select the rows and columns where the diagonal value is less than , say , 6 .
In this example , the diagonal values are sorted , so that I could just take Test [: 3 , : 3 ] , but in the general problem I want to solve this isn't the case .
However , this is fugly code , with all kinds of dangerous things like initializing the ii / jj indices to -1 , which won't cause an error if somehow I get into the loop and take M [ -1 , -1 ] .
At first , it may be tempting to just try ` Test [ d , d ]` but that will only extract elements from the diagonal of the array : #CODE
The reason ` Test [ d , d.flat ]` works is because numpy's general broadcasting rules cause the last dimension of ` d ` ( which is 1 ) to be broadcast to the last ( and only ) dimension of ` d.flat ` ( which is 3 ) .
Similarly , ` d.flat ` is broadcast to match the first dimension of ` d ` .
Index 2D numpy array by a 2D array of indices without loops
I am looking for a vectorized way to index a ` numpy.array ` by ` numpy.array ` of indices .
When using arrays of indices to index another array , the shape of each index array should match the shape of the output array .
You want the column indices to match ` inds ` , and you want the row indices to match the row of the output , something like : #CODE
The index ` np.arange ( a.shape [ 0 ])` simply indexes the rows to which the array of column indices ` inds ` applies .
I want to compute a dot product of them .
Currently , I'm simply creating an array of zeros , then walking through every entry in the tuple and putting it in place in the NumPy array .
The difficult part about this is that you only want to take the sum of the elements with ` j = i ` .
That way the upper half of ` F ` ( offset by 1 ) is zero and will not contribute to the sum .
One common heuristic for vectorizing code when it's a little troublesome to do the whole thing is to vectorize over the long axes and loop over the short ones , which gets you almost all the benefit for a fraction of the headache .
and thus you can sum it like this using the ` np.newaxis ` -construct : #CODE
I'm trying to find an efficient way to find the biggest clump of zeros in the array ( by returning the number of zeros , as well as a rough idea of where the center is )
If you have a list instead of an array , you just do ` x [ x.index ( min ( x )): ]` similar to the example you showed .
However , the heap won't work for a max value at the same time it does for a minimum value - the treap and red-black tree will .
There's also something called a min-max heap that's O ( 1 ) time for both min and max , but I've not seen one implemented in Python .
We can see that the transpose of V from Armadillo matches V from NumPy .
Technically the SVD decomposition is not unique ( not sure about that change of sign tho ) .
Beside that , probably one of the two is returning V * instead of V , hence the transpose .
The rows of the ` v ` you got from numpy are the eigenvectors of ` M.dot ( M.T )` ( the transpose would be a conjugate transpose in the complex case ) .
Actually , you can only multiply the rows of ` v ` by -1 in the real domain , but in the complex case you can multiply them by any complex number of absolute value 1 : #CODE
You can always access the underlying numpy N-dimensional arrays with the ` .values ` attributes if the indices are meaningless , while still getting all the other advantages of pandas .
Which works fairly well , except it's somewhat cumbersome to do interactive work since you constantly have to remember which array dimensions correspond to which axes , and which parameteres correspond to which indices along that certain axis , etc .
May give ideas on how to relate Matlab structures to numpy ones .
The problem however is not relating MATLAB structures to numpy ones , but finding a data format ( at least 3D ) suitable for intuitive interactive work .
Looks like the outer product .
For N dimensions it is a sum product over the last axis of a and the second-to-last of b
it is a sum product over the last axis of a and the second-to-last of
So it does the sum product of every row of ` a ` with ` b ` , as your test readily shows .
I want to change the array of tuples to a sparse matrix where the tuples appear on the main diagonal and diagonal just beside to it as the following example shows it .
The [ wikipedia page on sparse matrices ] ( #URL ) is a great starting point on what those three arrays ( ` data ` , ` indices ` and ` indptr `) are .
Get indices of numpy.argmax elements over an axis
I'm however having trouble finding the original indices of the final elements ( which contain the information about the values taken by the other parameters ) .
I though about using ` numpy.argmax ` in the same way as ` numpy.max ` but I can't obtain back the original indices .
But how to get their indices ?
Use ` np.indices ` to generate the indices for the other axis .
What is the ones for ?
You similarly don't need to expand the angle into an array the isze of ` x ` and ` y ` : multiplying by ` sin ( angle )` should do it .
Also , why create an array of two zeros and then replace each zero , instead of just creating an array of the two things you want ?
So , if you want three arrays of length two , filled with zeros of dtype object , you can do this : #CODE
Your original code can be speeded up with memorizing intermediate ` sin ` product , i.e. #CODE
gives the error " non-broadcastable output operand with shape ( ) doesn't match the broadcast shape ( 2 )" .
After the if statement , have it append to a new list .
Later on , ` centroids [: , 0 ]` will also result in ` IndexError : too many indices ` error , as ` centroids ` is a 1-d array .
also I need to append string to file , savetxt seems can't do this .
I could have sworn there was an equivalent function that took an index instead of a mask , but unfortunately it seems the similar functions ( ` np.put ` , e.g . ) that take indices modify the array in place , instead of returning a new one functionally .
Namely , you can filter ones in each step array ( ` k ` loop ) , propagate each value to its neiboughours , i.e. roll dice 4 times number of ones , and evaluate next step array .
Each operation can be done with a numpy one liner ( using ` where ` , ` reshape ` , ` + ` and ` * ` for matrices ) , so there will be no inner loops .
this expression and absolute and relative error : #CODE
First question : This is a quite high absolute and relative error , this is just catastrophic cancellation , isn't it ?
I suspect that this may just be the price you pay for generality - ` np.dot ` calls BLAS subroutines ( ` dgemm ` etc . ) that are very highly optimised for the special case of dot products between two matrices , whereas ` np.einsum ` deals with all sorts of scenarios potentially involving multiple input matrices .
The sum dimension , ` m ` is only 20 .
It does give some warnings : RuntimeWarning : overflow encountered in exp , RuntimeWarning : overflow encountered in multiply - is it safe to ignore these ?
Also , unrelated to your question but , ` gamma / m*g ` is the same as ` gamma*g / m ` since multiplication and division are just done left to right in python .
You want ` gamma /( m*g )` according to your equation at top .
Your equation at top says that ` g ` is in the bottom of the square root inside the ` log ( cosh() )` , but you've got it on the top because ` a / b*c == a*c / b ` in python , not ` a /( b*c )` #CODE
I presume the first array is the value of gamma , and the second one is the error bar on it ?
The first is the best fit value for ` gamma ` , but the second is the variance ( not really the error ) .
I believe one estimate of error would be ` sqrt ( variance / t.size )` .
The smaller I make gamma0 , the smaller the fitted gamma value is .
I know gamma we are trying to find should be around 0.025
In fact , the fake data I was using to test this code was ` y = cos ( w*t ) + noise ` and I was fitting to ` w ` , and if I used ` w0 ` far from the original value , it would fit to almost exactly ` w / 2 ` .
For instance , I set gamma0 = .0000000000000000000000000000000000000001 ( yes , there 40 zeros ) , and what was returned was gamma_best = 1.00000000e-40
If you know that ` gamma > 0 ` but nothing about its magnitude , an appropriate guess would be ` gamma0 = 1.0 ` .
If you actually expect ` gamma ` to be much smaller than one , you should rewrite your equation to normalize it better , since these types of applications work best for numbers near 1 .
Also been looking under indexing in the numpy cookbook - indexing and histogram digitize in the example list but I just can't seem to find a function that could do this .
The ` bincount ` solution would work if you just flatten the array .
You'd have to incorporate a sufficiently high zoom factor about the centre of the image into the transform ; for a square image in a square window a factor of sqrt ( 2 ) would do it .
I use here ` T.inv =T^-1 ` as pseudo-python notation to mean the inverse of a matrix and ` * ` to mean the dot product .
For a pure rotation ` T=R =[[ cos , -sin ] , [ sin , cos ]]` , and in this special case ` matrix= T.inv = T.T ` , which is the reason why @USER had to apply the transposition still ( alternatively one could just use the negative angle ) .
If the image is to be first rotated , then stretched , the order of the dot product needs to be reversed : #CODE
and the user can enter several parameters : goal is , to change the values inside the matrix into the parameter-value which is the next highest ( and if a matrix-value is higher than max ( paramter ) than change it to nan ) for example , when the user entered a " 4 " and a " 7 " , then the matrix-value " 5 " shall change to " 7 " (= next highest of the entered values ) .
IndexError : too many indices
So generally , use a list comprehension , but if you surely know that you will run this only on small ( max . 1-2 thousand ) arrays , you have a better shot .
This looks like a correlate problem .
Depending on array size and BLAS dot can be faster , your milage will vary greatly : #CODE
His comment also eliminated the use of ` np.convolve ( a , b [: : -1 ] , mode=valid )` to the simpler ` correlate ` syntax .
It's actually more of a [ ` correlate `] ( #URL ) problem , but yes , +1 .
@USER Oh neat , I never noticed that function- its good to know you can do this sort of thing without convolve trickery .
Another way is to create the right pairs in the array ` a = np.array ([ a , b , c , d , e ,... ])` , reshape according to the size of array ` b = np.array ([ x , y , ... ])` and then take advantage of ` numpy ` broadcasting rules : #CODE
I am choosing gradient boosting trees specifically because the data is of mixed data types .
Since gradient boosting is based on decision trees , and decision trees work based on feature splits rather than distances , the " 0 , 1 , 2 , etc .
Reverse diagonal on numpy python
to get [ 1 , 1 ] which is 5 its diagonal is zero ; according to numpy , ` a.diagonal ( 0 )= [ 0 , 5 , 10 ]` .
How do I get the reverse or the right to left diagonal [ 2 , 5 , 8 ] for [ 1 , 1 ] ?
In my program , I dont get to know the " 1 " inside the diagonal function ?
@USER , You can use ` max ( a.shape [ 1 ] - a.shape [ 0 ] , 0 )` instead of ` 1 ` .
@USER is closest as you need to rotate or filip and transpose ( equivilant operations ) .
numpy IndexError : too many indices for array when indexing matrix with another
Get norm of numpy sparse matrix rows
So I want to get the norm of each or the row* row.T .
However , numpy will apparently not transpose an array with less than one dimension so that will just square the vector .
But numpy.transpose ( numpy.atleast_2d ( tfidf [ 0 ])) still would not transpose the row .
I moved on to trying to get the norm of the row ( that approach is probably better anyways ) .
So I tried to calculate the norm manually .
Getting the norm would be best .
Now I just need to figure out how to not make each row's norm equal 1 .
I figured out a way to get the norm with the sparse form using ` tfidf.multiply ( tfidf ) .sum ( 1 )` .
To get the norm of each row from the sparse , you can use : #CODE
If you have enough memory for the dense version , you can get the norm of each row with : #CODE
plot the histogram of a 1D array in Numpy
I want to plot the histogram , and I don't know how ?
I'm using this simple function to determine the gradient on a function using numerical methods .
TypeError : can only concatenate tuple ( not " float ") to tuple
OpenCV Error : Assertion failed ( src.dims = 2 esz = ( size_t ) 32 ) in transpose , file / build / buildd / opencv-2.4.2 +dfsg / modules / core / src / matrix.cpp , line 1877
#URL error : ( -215 ) src.dims = 2 esz = ( size_t ) 32 in function transpose
Numpy array modifies its strides to achieve many operations like flip , transpose etc .
You can check it as follows with following transpose function : #CODE
Now try with Numpy transpose #CODE
Now try with OpenCV transpose #CODE
In earlier version of OpenCV , python wrappers couldn't translate a negative strided array to corresponding Mat structure .
All evidence seems to point to me using append incorrectly , but like I've said , I type it just the same in a shell and it runs fine .
2 suggestions : give us an example with just the append , without the f_prime stuff .
I also periodically flush the matrix to ensure that if the computer crashes or something I don't lose all the data .
However , I have noticed that the " deeper " I get into the matrix , so to speak , the time it takes to flush the data increases .
I would like to sum the amount column by person and itemCode and transform my results to a numpy array .
Any thoughts on how to efficiently sum these records would be greatly appreciated !
How can I read part of an array without specifying element by element but giving the indices of the array I want as a pack ?
Then I need to partly sum up .
Marginalizing sounds like a job for ` sum ` with the optional ` axis ` parameter .
It can take a tuple of axes to sum over .
If you want to select specific indices on specific axes , where both indices and axes are determined at runtime , I'm not sure what the best way to go about that is , but you can construct a tuple of ` slice ` objects and pass that to the indexing operator easily enough .
One is to roll the axis you want to get all of to the end : #CODE
I'm trying to take a list of transactional data and sum it to a 2d numpy array .
In order to sum on the DataFrame , I've had to call the groupby function , which returns a series , not a dataframe .
Unless you are running on a mainframe or a cluster I would strongly suggest restructuring to either sum as you read or stage 1 being read into a database .
Note that you don't have to read the entire file into memory at once , you can use the ` chunksize ` parameter ( see the docs here ) and accumulate your table piece by piece .
I am looking at options to accelerate the log-sum-exp ( using the " max trick ") operation from Python code .
Edit : wrt initial notebook , added max trick in all methods to make comparison less trivial , and nearer to my actual need .
But anyway , it * does * " the max trick " in recent SciPy and is therefore more stable than your other approaches .
" max trick " : indeed , I had seen that , but that's only if you specify the b argument , otherwise it won't do it , cf [ first example in doc ] ( #URL ) .
Finally , the ' max trick ' seems to be implemented regardless of the value of ` b ` : #URL
re max trick : thanks for proof-by-source-code :-) since it's not comparable to the other implementations , I'll remove it from the comparison .
@USER , I totally rewrote my answer and actually compared it to the ones you posted ( except numba ) .
If you're using the newest version of numpy ( 1.8 ) , there is a new function ` np.nansum ` to get the sum of all non-nan values : ` np.nansum ( a )` --> ` 10 `
And how should I get the indices .
you will get ` [ 0 , 2 , 3 , 4 , 1 ]` but all the ones are the same !
What I want is an efficient way to shuffle indices of identical values .
Any idea how to do that without a while loop with two indices on the sorted vector ?
If I comment the gamma function part , it works like a charm , so the problem is there , but I have no clue on how to solve it .
There is no need for optimization here if you have the data ( not just a histogram ) .
As the following code shows , ` empty ` array replaces my int values with float ones .
Using this reshape approach , ` np.polyfit ` can compute 2nd order fit coefficients for the entire ndarray ( vectorized ): #CODE
But when I open the csv file and sum columns it gives zero !
Output file shows exactly what I expect . but as I said they just look like numbers , but you cannot sum , average ... they should be numbers so that I can perform statistical analysis . the one under is copied from csv cells and pasted here .
Using ` bi ` to flatten your tuple of two lists should be fine , if overcomplicated .
So , I took matters into my own hands and came up with this beast , which runs at about half the speed of sorted on small arrays and runs nearly as quickly on larger ones ( e.g. ` len ` around 10,000,000 ): #CODE
Running ` cProfile ` on this tells me that a lot of time is being spent on the ` append ` method for lists , which makes me think that this block : #CODE
I'm currently using ` itertools.chain.from_iterable ` to flatten the ` buckets ` , but if anyone has a faster suggestion I'm sure it would help as well .
Any other comments on the implementation or ways to squeeze out more performance are also appreciated .
That's because your ` num byte_check ` takes time proportional to ` log ( num )` - it generally has to create an integer about as big as ` num ` .
Another way is to convert all the fields to the same type , and reshape it .
I need to fill a numpy array of three elements with random integers such that the sum total of the array is three ( e.g. ` [ 0 , 1 , 2 ]`) .
This will , of course , slow down proportionally to the desired sum , but would be ok for small values .
This problem can be solved in the generic case , where the number of elements and their sum are both configurable .
The idea is to pick random numbers sequentially , each of which is less than the required sum .
The required sum is reduced every time you pick a number : #CODE
Do not combine the indices together with a ` * ` , but use the boolean and : ` ` .
Do you mean that assigning outside the bounds of the list ( which negative indices won't necessarily do - they start from the end and work backwards ) should create new entries ?
Then add zeros : #CODE
If you reshape the ` N ` to be a column vector , then ` B1 ` will return a 2d array : #CODE
And then , if I just use ` exp ` instead of ` math.exp ` , the graph results a constant .
@USER Marnach got the programming bit right , you need to use Numpy ` exp ` .
Efficient way of getting a subarray of an array that the indices are mapped
I need to get a sub-matrix of it , which basically the indices of it are coming from a mapping on the indices of the main matrix ( This map is not necessarily 1-1 ) .
I have the following code to generate the sub-matrix and here the mapping is considered to be ` sum ` .
` np.ogrid ` can be a convenient way to make expressions based on the indices in an array .
I am currently using Cython but my code involved taking the norm of each row / column combination multiple times in a loop so even a 1000x1000 matrix is extremely slow .
The most expensive steps , it looks like , involved taking the dot product of all row / column combinations and looping over the upper triangle of the matrix .
For example , if you have mostly zeros in the matrices , you have what are called " sparse matrices " and there are many well established methods of dealing with these .
Maybe you can reduce the dimensionality first with svd .
Ah , I got lost in all the zeros .
This can clearly be done with traditional array operators such as ` append ` , but I need this to be as optimized and as fast as possible .
I have many millions of elements in my ` master_list ` , so is there a way to accomplish this without directly looping through ` master_list ` and using ` append ` to create two new arrays .
I look at histogram but it is way too slower and I think using it would defeat the advantages of using pypy .
` ( x [: , None ] == np.unique ( x )) .sum ( 0 )` is slower than ` bincount ` , but 3x faster than ` histogram ` .
First I reshape both the X and Y coordinate matrices into one-dimensional arrays , calculate all the values , and then reshape the result back into a matrix .
With your code , it should be enough to decorate ` func ` with ` np.vectorize ` and then you can probably just call it as ` func ( X , Y )` -- No ` ravel ` ing or ` reshape ` ing necessary : #CODE
Could this be generalized for the case that ` L2 ` has only one row , and we want to get ' row indices ' of matching rows in ` L1 ` , with the rows in ` L1 ` not necessarily being unique ?
The rest is just extracting the indices where this is ` True `
If the array we sort has length ` m ` and the other has length ` n ` , sorting takes time ` m * log ( m )` , and the binary searching that ` np.searchsorted ` does takes time ` n * log ( m )` , for a total of ` ( n + m ) * log ( m )` .
Assigning values to two dimensional array from two one dimensional ones
First , there's absolutely no reason to create the original ` zeros ` array that you stick in ` a ` , never reference , and replace with a completely different array with the same name .
Second , if you want to create an array the same shape and dtype as ` b ` but with all ones , use ` ones_like ` .
You could of course expand ` b ` to a 3x1-array instead of a 3-array , in which case you can use ` hstack ` instead of needing to ` vstack ` then transpose but I don't think that's any simpler : #CODE
@USER it must be , otherwise the error would be ` too many indices ` .
To force the broadcasting to the proper shape , you'll have to reshape your arrays to broadcast to a proper shape , something like this : #CODE
The ` [ 0 ]` are there becuase ` np.where ( a )` returns a tuple of length ` a.ndim ` with each element of the tuple being an array of indices that fit your condition .
After we get the array , we want to reshape it to match the shape that you want your output array to have .
of two dimensions with at least two columns , and at most 2 unique
` linmax=@ ( x ) max ( x , zeros ( size ( x )))`
By default iterating over a 2-D array means iterating over the lines , that's why you have to transpose or use ` unpack=True ` in this example .
Suppose a ` sin ( 2*pi*t )` curve as below , we should expect a sharp peak at frequency = 1 Hz , BUT the output deviates much .
You need around 10000 points to sample sin ( 2 pi t ) in range t=1 ...
So the sum of the log-likelihood was doing stupid things when the parameters started to go out of bounds .
It yields a pretty good estimate , but optimization clearly improves it ( as measured by plotting compared to a histogram / kde of the data ) for most of my datasets .
The optimizer used here ( ` fmin ` , or Nelder-Mead simplex algorithm ) does not use any information from gradient and usually works much slower than the optimizer that does .
If so , optimizers that utilize gradient / derivative will be better and more efficient choice ( such as ` fmin_bfgs `) .
And ` llh ` is the sum of a bunch of logs ... ghhhaahgh ?
` llh ` is the sum of log which I think is a good thing .
The derivative is also the sum of a bunch of things ( the derivative of PDF ) , each for one observation in ` x ` .
They're all cases that don't fit the distro well , so I'm not shocked -- but even starting with what look like very good parameter estimates ( PDF fits histogram of data ) doesn't seem to help ; it'll just go off the rails sometimes and I'm not quite sure how .
The matrix is actually ( in block form ) ` [ A C ; C^T 0 ]` , where ` A ` is ` N x N ` ( ` N ` large ) and ` C ` is ` N x 3 ` , and the ` 0 ` is ` 3 x 3 ` and ` C^T ` is the transpose of ` C ` .
I wrote ` mv ( v , C )` to go row by row construct ` A*v [ i ]` for ` i=0 , N ` , by computing sum ` f ( C [ i ] -C [ j ) *v [ j ]` ( actually , I do ` numpy.dot ( FC , v )` where ` FC [ j ] = f ( C [ i ] -C [ j ])` which works well ) .
You don't really need mv to accept two arguments , you could use a closure ( or even ` functools.partial `) instead : ` make_mv = lambda c : lambda v : sum ( f ( c [ i ] - c [ j ]) ...
S is diagonal so this is inexpensive .
Also , to access an attribute of your class you only need to use the dot syntax , I think your test should read something like this : #CODE
You might be interested in the reshape function that does this task for you .
What is the effect of using a negative value in reshape ?
Can't reshape numpy array
And reshape it to be ` n x 3 ` : #CODE
If you want each element to be shaped ` 3 x 1 ` , maybe you just want to transpose the array .
But yeah , my guess is the transpose is what he's after .
Yes , without a resize , ` ( n , 1 , 1 , 3 )` .
First find true indices #CODE
Numpy cumulative sum for csv rows gives type error
I'm using cumsum function to calculate cumulative sum of a csv column ( rows ) .
If you want to accumulate column 1 , then that's actually field `' f1 '` in a structured array , so use : #CODE
' average_ = np.array ( average_ , dtype= np.float ) , cum_sum = cumsum ( average_ [: , 1 ]) , com_arrays = append ( zip ( average_ ) , cum_sum )' .
AttributeError : ' QString ' object has no attribute ' strip '
You need to explore , either in PyQt documentation , or an interactive shell , what methods it has , and if you need need to do anything to convert it to a regular Python string . genfromtxt is attempting to split that string into lines , and then strip off the line ending characters , so it can then parse the line .
How to set items of a sparse matrix given the row and column indices ?
Suppose I have a list of lists where each sub list includes the column indices of the successive rows to be set into 1 .
My purpose is to set values of scipy sparse matrix conforming to given indices .
I'm trying to insert ` NaN ` values to specific indices of a numpy array .
However , I can append ` NaN ` values to the end of the array with no problem .
' means unicode of max length 4 , and results in the right array
by applying the permutation #CODE
I have a fairly large matrix and expect to get even larger ones , so I need a solution that does this quickly and in place if possible ( permutation matrices are a no-go )
First , you need to answer the following question : how many nonzero elements do you have in your matrix .
Just a tip - ` sum ` supports generator expressions .
So , you can just do this : ` sum ( 1 for i in column if i ! =0 )` .
Actually , if you only have integers , you can do this ` sum ( 1 for i in column if i )` , since ` 0 ` evaluates to ` False ` .
( I mean replace in your code the 4th line with b [: , indices ] .
and reshape result back to 2d #CODE
An onliner , proposed by @USER in comments , using ` np.ravel_multi_index ` helper function to flatten index : #CODE
Finally , reshape : #CODE
I was thinking I could transpose the data using numpy , so that columns can become rows , then i can use ` insert ` to put rows of ` None ` s in , then transpose the data back .
This makes sense since insert is O ( n ) but append is O ( 1 ) .
One hack you could use though for simple ones is replacing the whitespace with commas using ` re.sub ` : #CODE
Fixed it using strip to check for empty strings .
+1 However , although I see what you are trying to do , I think you should include a ` list ( filter ( lamda x : x is not None , ` around this to strip the ` None ` values .
Returning the values of an ' ndarray ' using an array of indices
I am trying to get the values of an ` ndarray ` using an array of indices .
Suppose ` a ` is the target array and ` b ` is the array of indices .
Making the numbers unique to make it easier to verify : #CODE
I am working to plot this and represent it in an histogram .
I couldn't find a way to parse dates and make an 1D histogram
Nice way to deal with overlap with the edges , but you should do it for both sides : ` array [ max ( 0 , i-5 ): min ( i+6 , array.shape [ 0 ]) , max ( 0 , j-5 ): min ( j+6 , array.shape [ 1 ])]` btw , even if ` i , j ` are far enough from edges your code would give ` ( 10 , 10 )` not ` ( 11 , 11 )` since the end of the slice is not included , which is why I've used ` i+6 ` instead of ` i+5 ` .
The only problem is the code is written in python with numpy ( not even using OpenCV ) , which makes it quite hard to re-written to C++ code using OpenCV ( In OpenCV , I can only find ` dft ` and there's no ` fftshift ` nor ` fft ` stuff , I'm not quite familiar with NumPy , and I'm not brave enough to simply ignore the missing methods ) .
Not only is there no fancy optimization , but for 1D vectors it is squaring and taking the square root to compute the absolute value .
grid is coming from sympy rather than pylab or pyplot , decimal isn't doing anything , ( probably ) , where are sin & cos coming from , etc ., the _problem_ with from ... import * is that you loose your namespaces so you can't see if you are mixing models .
-One iterates over all elements and applies the formula ` field [ y , x ] = ( 1-alpha ) *field [ y , x ] + ( field [ max ( y-1 , 0 ) , x ] + field [ min ( y+1 , field.shape [ 0 ] -1 ) , x ] + field [ y , max ( x-1 , 0 )] + field [ y , min ( x+1 , field.shape [ 1 ] -1 )]) * alpha / 4 ` in place .
I want to save some histogram data in a csv file .
Why isn't the inverse gamma function producing the original x value ?
In ` scipy.stats ` , ` gamma ` is the gamma distribution and ` invgamma ` is the inverse gamma distribution .
These are two different probability distributions -- see the wikipedia article for the relation of the inverse gamma to the gamma distribution .
` maxiter ` controls the number of restart cycles , not dot products .
The bound for dot products is ` restart*maxiter ` , where ` restart ` has the default value of 20 .
I want to get the indices of a where ` a [ x ] b [ x ]` , i.e. all x's
Furthermore , I want to get the indices of a where ` ( a [ x-1 ] b [ x-1 ]) ( a [ x ] b [ x ])` .
@USER , It depends on what you want to do with these indices .
Your question just asked for which indices match the condition , not how to do stuff with them .
If you want to be using ` a ` and ` b ` a lot with only the parts showing that match the condition , it might be a good use for masked arrays , but right now you can just access the matching indices by saving the index array ` m = np.where ( ... )` and then using it to index the arrays : ` a [ m ]` will be the values in ` a ` where your condition holds , and ` b [ m ]` for those in ` b ` .
Numpy : change max in each row to 1 , all other numbers to 0
I'm trying to implement a numpy function that replaces the max in each row of a 2D array with 1 , and all other numbers with zero : #CODE
Method #2 , using ` max ` instead of ` argmax ` to handle the case where multiple elements reach the maximum value : #CODE
Quick question on Method #1 : why wouldn't my ` : ` slice syntax on the rows do the same thing as providing an array with the row indices themselves ?
Here you append only a REFERENCE to your only one existing ` energy ` array .
Here , you create an explicit new zeros array in each append , so you will get 16 arrays stored in memory which you can modify separately .
How could I change the zeros to be the preceding value ?
I know I can easily change the zeros to a constant by : #CODE
In answer to @USER ' s question , the first value will never be a zero , but there may be a string of contiguous zeros .
Will there ever be a group of contiguous zeros ?
I would like to perform dot ( A , A.T ) where certain indices are omitted : #CODE
Note that I would like to keep the original dimension , i.e if A is square the dot product will have the same dimension .
Second , I modified your ` normalize ` lambda to be a proper function so that you can pre-calculate the min of the array .
I am trying to do matching using the networkx max weight matching function :
Basically , I am looking for a way to append those 55 extra features per sample from the second matrix to the features in the first matrix .
That is , they need not be normalized in any way , and the formula you are using to compute the Cartesian coordinates from the barycentric ones is only valid when they add up to one .
Hi guys i need help on my script below , i want to create a histogram of my differences column against time , is there anyone who knows how to do this ?
The plot of log of Radiation count vs Angle of Diffraction is plot .
No , as I said in the original question , nans aren't unique objects .
Look at the above transcript : the two ` nan ` s have different ids and ` nans [ 0 ] is nans [ 1 ]` is False .
In the simple case where your array's size is divisible by the downsampling factor ( ` R `) , you can ` reshape ` your array , and take the mean along the new axis : #CODE
numpy sum does not agree
I ran a sum over a matrix with small numbers ( here : #URL )
but the sum doesn't agree if I sum in different directions : #CODE
But if I sum small matrix , it agrees : #CODE
And which sum is correct for the first matrix ?
See also [ How best to sum up lots of floating point numbers ? ] ( #URL )
And when you sum the elements in a different order , you therefore can get different answers .
Which sum is correct for the first matrix ?
as the mean absolute values of the numbers is ~ 0.023 , I would personally conclude that the sum is indistinguishable from 0 ( given the finite machine-precision )
@USER : So Basically , you could stack all the images along the time domain and have a 3D Image Volume .
@USER I don't understand how that would work since ` b ` aren't indices .
This question is closely related to How to split an array according to a condition in numpy ? but I am looking for a more general way to split an array given an unknown number of indices : #CODE
I have a 3D stack of masked arrays .
I'd like to sample all arrays in the stack at the same fixed locations .
The following is very efficient , but only works for integer indices : #CODE
I'm trying to implement the scipy.ndimage.map_coordinates interpolated sampling for float indices , but I can't seem to figure out how to format the coordinates properly .
Most examples use map_coordinates to sample a single array , and the following works for a single array from the stack : #CODE
I can loop through each array in the stack , but I know there is a simple indexing trick that will sample the entire stack in a single call .
There must be a way to get it to broadcast automatically ... in the meantime , you can force the broadcasting with ` np.arange ( ... )` to get one point from each 2d array in the stack : #CODE
Fill half of fft ( x ) array with conjugate before ifft
I do lots of modifications tests by trial and error , so it's easier to make the modifications only in the first half , and replicate the second half of ` fft ` at the end .
But I don't like to use them : I like the fact that ` fft ` of an N-array is a N-array , ( and not a 2N ? 2N-1 ? N / 2 ? -array with realfft ) , etc .
It is simpler in my mind to deal always with standard ` fft `
I like the fact that an N-array produces an N-array with both ` fft ` and ` ifft ` .
can you just reshape ?
@USER reshape only change the shape , not the order
I have check the doc of reshape , it says ' order : { ' C ' , ' F ' , ' A ' } , optional
Is there nothing better than creating a ` ( 3 , 2 )` array and then use ` transpose ` ?
Here's an example from another stack : #CODE
The minimum number of unmasked values for each index is 6 , max is 45 .
So I reshape stack.ma_stack and run polyfit : #CODE
There might be a way to do this by carefully compressing x and y and keeping track of unmasked indices .
@USER , for the stack I described above , yes , the time slices are daily and there are no missing values in time or space .
However , most of my stacks are irregularly sampled in time and space , with the same stack containing intervals of 2 years , 2 days and 2 hours .
for the y with shape ( 50 , 2000 , 2000 ) , you can reshape it to ( 50 , 2000*2000 ) .
Optimizing a nested for-loop which uses the indices of an array for function
works , but I'm not sure how to avoid the creation of that big , useless array of zeros .
where my 2-d arrays will be an element-by-element sum of the arrays with same value in the ' foo ' columns .
If not possible , is it possible to pick the first occurrence of the ' mat ' element in the list of arrays with same ' foo ' ?
Have a look at multi-indexes , see : #URL or putting the ' mat ' data in a separate data frame .
To sum by elements I've used numpy sum over ` axis=2 ` ( converted lists into ` np.array ` beforehand ) .
I think you should be able to use ` sum ( axis=0 )` instead of ` .T .sum ( axis=2 ) .T ` .
I am able to build the histogram I need .
I did add some input data and the output has to be clear , that is , non-overlapping bars in the histogram .
I suggest you use precompiled installers , such as the ones from #URL .
It seems that we got a floor operation on the truncated value to 4 significant digits .
@USER I don't think ` y=1 ` creates an array with a 1000 ones ;) But yeah , matlab is terser because you don't need to type ` np .
You can reshape your original 2D array to a 4D array where all cells which would fall in a single 50*50 grid are put into two unique dimensions .
You can then reshape the original ` grid ` array to 4 dimensions and apply the ` mean ` on the second and fourth dimension .
Maybe some kind of function that can be applied on elements of the outer loop of a numpy array very quickly without the overhead of python stuff ...
I have been trying all sort of things with ` np.where ` but I only can get flatten results .
Count number of occurences along axis with sum : #CODE
Extracting the indices with ` where ` is unnecessary , you can use the boolean array to index the rows directly , i.e. ` a [( a > .7 ) .sum ( axis=1 ) > = 2 ]` should do the same , and boolean indexing tends to be the fastest .
You can sum over axis with ` a.sum ` .
And I want to roll each row of ` A ` independently , according to roll values in another array : #CODE
` roll ` effectively constructs ` column_indices ` with ` np.array ([ concatenate (( arange ( n - shift , n ) , arange ( n - shift ))) for shift in r ])` ( after ` r ` is ' corrected for negative values ) .
The indices are the same ( with a possible ` %=3 ` correction ) .
I'm reading from a jpeg though so what should be ` [ 255 , 0 , 0 ]` becomes ` [ 230 , 12 , 11 ]` so what ` clean_key ` does is threshold the values to the cleaner ones .
Then I append the number of times this combination occurs to a dictionary .
You don't need to flatten the images to do this .
This not only tells you how many pixels match , but where they are ( you could plot the above line and see dot at the points where they match ) .
Im have N pairs of portfolio weights stored in a numpy array and would like to calculate portfolio risk which is ` w * E * w_T ` where ` w_T ` is weight transpose .
Is there a vectorized approach to this such that given a weight pair ( or if possible N weights that all sum to 1 ) I apply a single covariance matrix to each row to get the risk ( ie without loop ) ?
So far I have an iterative solution involving populating a list of indices , making a permuted copy of that list and then assigning values from the original matrix from the original index to the permuted index .
Is there a way to the get the sum of the product of the respective elements during the multiplication that I would get if I was doing matrix multiplication and the elements were either 1 or 0 ?
I tend to see zeros ;)
They recommend using ` array ` , ` zeros ` or ` empty ` to create a new array , rather than the low-level ` ndarray ` .
I was trying to concatenate 1-D two arrays in Python , using numpy .
I see other stackoverflow questions where it is said that it is possible to concatenate an empty array .
EDIT solution to create an empty array of the right size is to ` reshape ` it : #CODE
The red area on the plot is the integral of ` min ( f ( x ) , g ( x ))` , where ` f ` and ` g ` are your two functions , green and blue .
Finally , you have to add trailing new axes to your ` yx ` outer product , to match as many trailing indices you have in ` F ` .
( Except that Alex replaces ` len ( list ( group ))` with ` sum ( group )` , which is a bit neater .
As @USER and @USER Dickinson pointed out in comments the above code will not work in some cases , so you need to append ` False ` on both ends first : #CODE
It looks to me as though the correct solution along these lines is to pad with a ` False ` both at the beginning and the end before making the first ` diff ` .
` sum ( IX )` sums along the array and , in this case , returns an object of type ` numpy.int64 ` , which can't be assigned to ` IX ` .
I want to multiply them ( which produces a triangular matrix - however I dont get this since I don't limit the dot product to yield a triangular matrix ) .
do you multiply them as booleans , i.e result is of boolean type ? and how sparse is your data , what % of ones ?
You can check amount of ones with ` sum() ` and divide by total size ( 6.4 *10**9 in you case )
0.6 % of numbers in the boolean matrices rest are zeros
It doesn't use much memory since ( at least if I wrote it in C ) it probably uses linked lists , and thus will only use the memory required for the sum of the datapoints , plus some overhead .
Since a Python dict lookup is O ( 1 ) ( okay , not really , probably closer to log ( n )) , it's fast .
Do not mind the extra zeros , it's an artefact from python's printing internal rounding , if we look at the " exact " values , they're still different : #CODE
To do many matrix multiply by one call , we can't use ` dot ` , ` einsum() ` does the trick , but I think it may be slower than ` dot ` .
Getting rid of empty and zeros arrays in a list of arrays in Python
and arrays filled with zeros : #CODE
Finally I would like to remove the zeros as well keeping the array list format to get #CODE
@USER ` compress ` one took around 138us .
You can do the second part with ` compress ` #CODE
Maybe you want absolute value of mismatchs ( then maybe use RMS ) .
=b ` as ` dtype = np.double ` for the accumulate operation which takes slightly longer then when you simply sum ` ( a ! = b ) .sum() ` as ` np.int ` .
There is also romb , where you must supply the values of integrand directly and a dx interval , but that will be too imprecise for my complicated function ( the marcum Q function , couldn't find any implementation , that could be another way to dot it ) .
When using the following code , the integer numbers are exported properly , but the zeros are replaced by random numbers with giganormous exponents ( 1.98E-258 ) .
` OP ` really should be using ` np.zeros ` if he / she wants an array to fill with zeros .
As others have said , if you really just want an array of zeros , and to export this to csv , then np.zeros is a better approach .
I assume you have bigger plans , and setting the values was the actual error in your code , so I focused on that , but if all you want is zeros , np.zeros will be more efficient , idiomatic , etc .
So we need three indices : #CODE
But the multitude of indices works .
using indices with multiple values , how to get the smallest one
If you want the " smallest " value in ` values ` for the repeated indices , then it's ambiguous ( if say the two rows in ` values ` are ` [ 0 , 2 ]` and ` [ 1 , 1 ]`)
One crude approach I have currently implemented currently is to generate all the row , column and vectors along the diagonal path and then iterate over each of them to find a continious block of ' q ' using iteration .
Better optimization : You only need to check win on the row and colum of the move , possibly the diagonal .
Then for each row / column / diagonal in the array you can find out the count of 1 and 2 , and if it equals the length of row then it means a Player has already won .
Here using ` numpy.apply_along_axis ` I've applied a function to each row / column / diagonal in the array and it returns the count of 1 or 2 in that row divided by length of that row , so if for any case 1 is returned then it means that row / column / diagonal contains either all 1's or 2's .
Counting non zeros in only 1 column of a numpy array
the non zero count returns 18 values here , the code then goes into the ` bisect ` -like search using ` data [ time ] [ 1 ] [ 0 ] [ 0 ]` as min X-coord and ` data [ time ] [ 1 ] [( np.count_nonzero ( data )] [ 0 ]` as max x-coord which results in the array stopping at 9 instead of 10 .
( i.e. between ` data [ time ] [ 1 ] [ 0 ] [ 0 ]` and ` data [ time ] [ 1 ] [ max ] [ 0 ]` )
Maybe a better approach would be to filter the array using ` nonzero ` and iterate over the result : #CODE
To count zeros only from the second column : #CODE
` nonzero `' s purpose is to skip ` 0 ` elements - isn't this what you wanted ?
If I understand you correctly , to select elements from ` data [ time ] [ 1 ] [ 0 ] [ 0 ]` to ` data [ time ] [ 1 ] [ max ] [ 0 ]` : #CODE
` stat = sum ( data ! = 0 )` to count the non-zero entries .
note : ` for i , line in enumerate ( file )` and ` ncols = max ( ncols , len ( line.split ( ' , ')))` builtin functions that you could use here .
it should be columns = max ( columns , len ( row ))
I have a bunch of 2D histograms ( square 2D numpy arrays ) that I want to stack in 3D like so :
I'm not sure where this time comes from , as profiling says the 200,000 calls to min only take 0.1s - maybe memory access ?
When you run your code , the first item that gets looked at is ` i = 0 ` , ` j = 0 ` , so you get ` M [ 0 , 0 ] = M [ 0 , 0 ] + min ( M [ -1 , -1 ] , M [ -1 , 0 ] + P1 , M [ 0 , -1 ] + P1 )` .
It seems to me that your ` try ` was trying to catch indices out of bounds ( you should , by the way , make explicit what you are trying to catch , i.e. do ` except IndexError `) , but the ` -1 ` s in your indices are taken to mean " last element along that dimension " so nothing ever gets set to ` np.inf ` .
You can convert the data into a numpy array of ` uint8 ` , then add the 0 to each sample by using ` reshape ` and ` hstack ` ; #CODE
Make the zeros that have to be added .
In case of text classification should i map each of my word with a number ( id ) , by maintaining a hash of words in vocab and a unique id associated with it ?
I have few questions in the strategy : Let say the if I use OneVsAll / OneVsRestClassifier , wouldn't it be susceptible to the fact that permutation of labels will constitute a different class while it should be same .
While permutation does not matter .
Scikit-learn will take care of that for you , but I would recommend applying LabelBinarizer to your labels , which will create a unique representation .
But you may not be able to reshape the resulting array into a shape of ` ( -1 , 4 )` , it is a coincidence that the resulting array here contains 3*4 elements .
You always reshape ` data [ group ]` first , right ?
I need to append the arrays created in each loop so that I get a single ndarray at the end .
I tried by creating an empty array ` a = array ([ 0 ., 1 . ])` and then ` append ( a , array ( list_2 ))` but doesn't work .
I'm aware of ` hstack ` ` vstack ` etc , but cannot make use of them together with ` append ` in the loop .
Are you sure you don't want to stack these vertically instead of horizontally ?
@USER , ' stack vertically ' is what I want , and I think when each array has 4 columns , and I said one array with n*4 columns , I meant that , yes ?
The method by which to stack though would be to append your arrays to a _list_ through your loop , then call ` np.column_stack ` or ` np.hstack ` on your _list_ at the end of the loops , don't ` np.append ` to the arrays within the loops .
Then you just stack them all up using ` np.hstack ` .
I think what @USER wants is ` np.column_stack ` ( or equivalently in their case , ` np.hstack `) Your answer is correct in that it is impossible to have a uniform array given their inputs , but your example is the transpose of theirs .
But then I noticed OP's comment '' stack vertically ' is what I want ' .
haha , true . actually I've found the names of all the stack functions can be ambiguous if you don't already know what they do .
e.g. , ` vstack ` : stack in vertical direction , or stack vertical things ?
The resulting array will be ( 2942 , 16 ) and I think the last rows of the shorter columns will be filled with zeros ?
This is more efficient than trying to concatenate NumPy arrays in the loop .
This actually will work for everything that I need currently , but I was wondering for my own enrichment ( and in case I need to do something like this in the future ) , does ` itertools.product ` generalize if I need to iterate some of the indices differently ?
A dumb example would be , all even indices I want ` range ( 0 , 10 , 2 )` and all odd indices ` range ( 10 )` ?
It reaches some other value for reasons unknown , and this value is different depending on the gradient of the rode .
The first sharp change in velocity is when the cruise control is initiated , and the second is when the gradient of the rode changes
If there wasn't for those ` for ` loops creating the arrays , then it is vectorized - and in principle I don't even have to create those arrays - just wanted to see how they will look like as structured ones ..
If you're doing arithmetic , maybe padding with zeros is more natural than padding with NaN , but I don't know your application , so we can do it either way .
Padding with zeros : #CODE
Note , the resultant array is just an array of python objects , rather than a " proper " numpy array , so you'll probably need to roll your own rounding function , and maybe also some other stuff that won't work .
I am trying to find the period of a sin curve and can find the right periods for ` sin ( t )` .
However for ` sin ( k*t )` , the frequency shifts .
I can adjust the value of ` interd ` below to get the right signal only if I know the dataset is ` sin ( 0.6 *t )` .
Why can I get the right result for ` sin ( t )` ?
The figure below is the power spectral density of ` sin ( 0.6 *t )` .
If you're not sampling enough ( the frequency is higher than your sample rate , i.e. , ` 2*pi / interd 0.5 *k `) , then there's no way for ` fft ` to know how much data you're missing , so it assumes you're not missing any .
That works fine for ` sin ( 0.6 *t )` .
And you may get the wrong peak for ` sin ( 6*t )` .
For the high frequency , that's not a problem with the ` fft ` ... plot your signal first and you'll see that if you take ` np.sin ( 6*t )` where ` t = np.arange ( 1,200 1 )` that you're not seeing a signal with frequency ` 6 ` .
` fft ` can only tell you what you give it .
Here , your sampling frequency ` f = 1 Hz ` or ` w = 2 pi rad / sec ` , so the fastest frequency you can detect will be ` pi rad / sec ` which is lower than the input frequency for ` sin ( 6*t )` .
Or more specifically , if k is always an integer , then sin ( 6*k )= sin (( 6-2 *pi ) *k ) , and 6-2 *pi is around - 0.2832 , so this will be the frequency you see , that is , the sampled sequence is the one of -sin ( 0.2832 *k ) .
You might need to take the transpose first , like ` df.values.T ` .
That is , all N rows must multiply all N columns , and each of these dot products contains N multiplications => O ( N*N*N ) = O ( N^3 ) .
Find index in array to which the sum of all elements is smaller than a limit , quickly
This would be slower since it calculates the whole cumulative sum before testing the limit right ?
edit : Another idea that might help : rewrite compare_images so that it calculates two versions of smoothed- ` a ` -- one with sigma= ` floor ( k )` and one with ` ceil ( k )` ( i.e. round k to the next-lower / higher int ) .
Also , if you want to generate an empty array , there's ` numpy.zeros ` , ` ones ` , or ` empty ` .
The code there was missing something fundamental : it was still non-deterministic in the sense that if you give it the same values and probabilities array , but transformed by a permutation ( say values ` [ ' a ' , ' b ']` and probs ` [ 0.1 , 0.9 ]` and values ` [ ' b ' , ' a ']` and probabilities ` [ 0.9 , 0.1 ]`) and the seed is set and you will get the same random sample , say ` 0.3 ` , by the PRNG , but since the intervals for your probabilities are different , in one case you'll get a ` b ` and in one an ` a ` .
` UnicodeDecodeError : ' unicodeescape ' codec can't decode bytes in position 56-57 : malformed \N character escape ` .
Clearly this is not the right approach because I just want to decode the backslashed , not the rest of the string .
Given that output , no wonder that reshape doesn't work : #CODE
If your matrix is of size ` ( n , m )` , there is no easy way of vectorizing the computation of only the ` n * ( n - 1 ) / 2 ` unique values you are after , although it is often faster to do a vectorized computation of the ` n * n ` values in a full cartesian product , even with the duplicates .
Is this supposed to give a matMI filled with zeros ?
Not totally filled with zeros , but with zeros wherever there is no MI between the columns .
I can't suggest a faster calculation for the outer loop over the n* ( n-1 ) / 2
you really prefer bits , just divide ` mi ` by log ( 2 ) .
Some of these packages may not be required ( don't have time to check in detail ) but in a setup script of mine to install the NumPy stack I install the following Ubuntu packages first : #CODE
For the outer product specifically there is ` np.outer ` : #CODE
To apply a function with one parameter over the outer product you would do : #CODE
It seems you need transpose operator : #CODE
" TypeError : list indices must be integers , not tuple " is produced .
This takes elements from the outer array where the boolean inner array are ` True ` .
After conversion of numpy.float64 to Pandas timestamp I have 1 minute diff - why ?
Without a clear definition of what you want , i assume your heatmap is a simple 2D histogram .
I need only 5-20 items on axis Y , as I understand the imshow() requires to specify every point of histogram
I am given list of indices which subsets this np.array : #CODE
The things that are changing is ` label ` and the indices ` x `
Shorter version of your current code : ` sum ( label in words for words in trainY [ x ])` .
The sum should compute instantly .
If it is a bottleneck in your program ; it means you compute the sum many many times .
for every word , hold a sorted list of the indices it appeared in the tuples .
You can find the indices of columns where there are exactly zero ` 0 ` s by summing the number of zeros in each column , and finding which sums are zero : #CODE
scikit_learn-0.14.1 RuntimeWarning : invalid value encountered in sqrt freezes on test
#URL RuntimeWarning : invalid value encountered in sqrt
For sums , could I just replace ` prod ` with ` sum ` ?
The GAE log indicated that the failure was lined to file import #CODE
I have a plot with two data sets which produces a slight gradient , where a curved line of best fit may be overplotted .
hmm , generally by eye for example it is possible to see the curve of best fit from this link is an exponential .
You can take the log of all your data points , fit a linear line to the log data , and IF IT FITS WELL , the coefficient of the linear part can be considered as the degree of the polynomial to the original dataset .
or whatever permutation you like .
Use ` None ` to introduce an extra axis , so we can broadcast : #CODE
Do you mean with columns each containing unique content ?
Downvoted for absolute lack of effort .
I need to find the indexes of " outliers " , whose values depart by ` 3 ` or more from the previous " norm " .
I agree that this is unusual definition of " norm " ( I think I get what different defn you're hinting at , and I think a vectorised soln to that is analogous to this answer but doing the backwards first ) .
It's converting the list of indices back into pairs of observations .
Make a heap of max size 100 ( if it grows bigger , reduce it ) .
And once you have the smallest indices , you don't need a loop to extract the indices , do it in a single shot : #CODE
@USER The conversion is just so you can tell how many columns are unique .
Here's something even faster - transform each column of 9 integers into 1 , using ` dot ([ 1 , 10 , 100 ,... ] , column )` .
Still have to do ` unique ` or ` set ` column by column .
This works , zipping the indices of rows\cols is key so they always have the same length , hence preserving the squareness of the matrix .
It works by scanning the main diagonal , removing row+column which are all nan , and then doing the same for the secondary diagonal : #CODE
I recommend ` r.T ` for transpose , though .
There's a very slight difference for 1-d arrays , but you probably won't be using either ` T ` or ` transpose ` for 1-d arrays .
Since numpy.shuffle shuffles the rows , but taking the transpose of a mtrix , you effectively shuffle the columns .
Then you transpose back .
Transpose the matrix , shuffle the rows , then transpose back .
It does not create a new , shuffled array , so there's no need to transpose the result .
The transpose of the transpose of a matrix == that matrix , or , [ A^T ] ^T == A .
So , you'd need to do a second transpose after the shuffle ( because a transpose is not a shuffle ) in order for it to be in its proper shape again .
` transpose ` returns a view of the original array .
There is no need to transpose twice .
Should I first create a zeros array and then fill it ?
With numpy , it is usually a good practice to define a zeros array and fill it with content .
making the y-axis of a histogram probability , python
I have plotted a histogram in python , using matplotlib and I need the y-axis to be the probability , I cannot find how to do this .
A simple way to compute the histogram for a sample from a discrete distribution is ` np.bincount ` .
E : 3 , 6 : Module ' numpy ' has no ' zeros ' member ( no-member )
Namely , ` zeros ` is in fact ` numpy.core.multiarray.zeros ` , imported in numpy with statement #CODE
` from numpy import ceil ` results in ` E : 1 , 0 : No name ' ceil ' in module ' numpy ' ( no-name-in-module )` I checked the commit referenced above and it appears that those changes are in the version of astroid I have .
I am not enabling absolute imports in 2.7 .
Apparently this is still not fixed in pylint 1.4.2 , astroid 1.3.4 : ` Module ' numpy ' has no ' zeros ' member ( no-member )`
I have tried a few options such as nditer , but this will not let me select the specific indices I want .
This selects the 6th column of all rows in your list of indices and adds 1 to each .
( Note that if an index appears multiple times in your list of indices , this will still only add 1 to the corresponding cell . )
That looks very neat indeed , However it kept giving me an error " too many indices for array " .
Also multiple indices do exist and these need to be counted too .
Do yourself a massive favour and install the pre-compiled NumPy stack from here : #URL
I considered using reshape , but didn't quite know how .
Plot function with large binomial coefficients
I would like to plot a function which involves binomial coefficients .
If you reduce max to 40 say it works fine .
First change ` max ` to ` N ` ( since ` max ` is a builtin ) and break up your function into smaller , more manageable chunks : #CODE
which one ones you need ?
Those ones ?
Find global min values from sub-sub-lists of different lengths
You can reshape to vector and covector and compare : #CODE
If memory is the main concern , what you can do is move columns around within your array such that the unneeded column gets at the very end of your array , then use ndarray.resize , which modifies he array in-place , to shrink it down and discard the outer column .
If an example in my test set can be found in my train set append the corresponding labels to my new list , otherwise , append my predictions to my new list .
There may be faster implementations using Series from pandas or numpy with where and unique .
@USER .nouri , here is what is looks like if the x-axis has a log scale -- #URL
The motivation for fitting the learning curve is probably somewhat unique ( and perhaps for good reason ) .
You can use ` hstack ` to concatenate your arrays and ` savetxt ` to save to csv ( links to docs included )
Set up data and stack : #CODE
As to M4rtini , they are not all ones , some are zeros .
Fastest turnaround on stack ever .
Is it possible for get python to do the computation exactly and only translate to floating point at the last moment when plotting ?
The log can't be exact but I would like to keep everything else exact as long as possible .
While default ddof for numpy's std is 0 .
Is to sum up the observations , dividing by 2 would give us the observation number corresponding to the median .
Which gives us a running cumulative sum .
Each element is the sum of all previously elements and itself .
At a glance it may be something with the indices formula - instead of 0-130 by 10's , I'm getting 0 , 10 , 50 , 80 as the output ( after modifying it to ( i-1 ) *10 to start at 0 ) .
Side note : instead of ` zeros() ` followed by ` fill() ` , you should use NumPy 1.8 ' s ` full() ` , or if using an older version of NumPy , you should replace ` zeros() ` by ` empty() ` : not only is it faster , but it also signals to the readers of your code that you don't care about the array being filled with zeros ( which makes replacing the values by something else less of a surprise ) .
Now , my problem is it does not append values to Theta which should be a two dimensional array with every column contains the values of r .
If I will create an empty array with zero's for r , outside the loop and then append values , it will not also work for me .
Can I append instead of concatenate ?
You could also just concatenate the input lists , but it'd be best if you used ` itertools.chain ` for that : #CODE
I had to guess that for every row of ` gages ` , ` pestID ` is unqiue , and that for every row of ` data ` , ` Name ` is unique .
plt.scatter ( relevant_gages [ ' date '] , relevant_data [ ' Measured '] , relevant_gages [ ' date '] , relevant_data [ ' Modelled ']) Still got same error message " shape #URL cannot be broadcast to a single shape "
georeference / stack geotiffs of different sizes using python / gdal
stack these images / arrays to produce 3 different ( 1 for each ' element ') N-band stacks ( or a 3-dimensional array -- ( samples , lines , N )) for each scene
unfortunately , I need to preserve all of the values for all images in the stack , and mosaicking would only keep the last image's values where the images overlap ( which is the majority of the images ) .
In all honesty , I don't technically need a stack .
I am pretty new to python and want to plot a dataset using a histogram and a heatmap below .
Using the mask I can easily reduce the big dataset to the valid ones : #CODE
The goal is to reduce the big array to the valid ones , conserving their positions .
I'd like to reduce this , i.e. don't save the invalid ones at all .
For an extreme example , consider a sequence that consists of 9 zeros followed by the result of a coin toss , 9 zeros and another coin toss , etc .
To mark a path to be absolute just prefix the path with a ` / ` : #CODE
Use a ` log ( x )` instead of ` x ` will solve the representation problem in this case without needing to go to mpmath .
Well , It looks like you can feed it a generator , so just strip the first column in a generator : #CODE
How to use eig with the nobalance option as in MATLAB ?
When I run the NumPy version of eig , it does not produce the same result as the MATLAB result with nobalance turned on .
If you normalize each of your eigenvectors j by max ( abs ( v [: , j ])) then you get the correct answer .
The function eig in Octave does not perform balancing of matrix A .
Can solve by numpy.reshape or resize ?
where ` e ` is the all ones vector and ` d ` is a boolean vector that has ` 1 ` in component ` j ` if ` outdegree [ j ] 0 ` .
Fast Way to slice image into overlapping patches and merge patches to image I want to add an offset to the indices of the patchified array , i.e : #CODE
Is there any way I can offset ( without copying any data ) the indices of P to have perfect correspondence ?
Now I still do have the biggest peak at zero and many smaller ones around 200 : #URL
If you succeed in finding the plane , as you say , the best thing to do is to rotate and translate your coordinate system in order to look the points in their natural projection .
x '' = 0.5 * ( 2y ' +z ') and y '' = sqrt ( 3 ) / 2 * z ' , your data will be seen in its natural projection .
\n " ( i.e. add a small dot ) #CODE
Your first example is bad because of the dot .
Let's say I want to do an element-wise sum of a list of numpy arrays : #CODE
It looks like reduce even has a meaningful lead over the non-numpy sum ( note that these are for 1e6 runs rather than 1e4 for the above times ): #CODE
However , I'm only interested in doing a sum once , so turning the array into a numpy array would still incur a real performance penalty .
And I'd expect reduce to beat ` sum ` by about 1 / 11 since it skips the ` 0 + tosum [ 0 ]` that is implicit in ` sum ` .
I start with a bunch of separate arrays , so turning them into a numpy array first would incur the same performance penalty as having sum do it for me ( since I'm only doing the sum once ) .
If you replace ` sum +=a ` with ` sum = sum + a ` , it becomes a bit slower than ` reduce ` .
I'll preface by saying I'm a programming n00b by stack standards .
Can someone please give me a clear example of how to use mpi4py , spawn , and bcast to broadcast an array to a number of spawned worker scripts ?
Does the broadcast complete ?
Or , a nicer result ( sometimes ) is to use Bezier curves ( and now you can have have more than 4 points at the cost of increased complexity ) or even NURBS ones .
I have a square matrix A ( could be any size ) and I want to take the upper triangular part and place those values in an array without the values below the center diagonal ( k=0 ) .
You can use Numpy's upper triangular indices function to extract the upper triangular of ` A ` into a flat array : #CODE
You can for example encode your strings ( i.e. manually construct pointers ) with say ` hash ` function , and store theme elsewhere .
getting indices in numpy
I personally coded flags into binary and provided ` str <=> uint ` dictionary mappings to decode them later .
I don't know why anything else on the script would fail to work after dealing with my imports , and numpy and pandas were the only ones I was expecting to have trouble with after upgrading ...
Rather than appending z , let's append ( z , x ) instead .
multiply them by ones to get two 2D nx-by-ny arrays containing the x and y
I have a square matrix A ( could be any size ) and I want to take the upper triangular part and place those values in an array without the values below the center diagonal ( k=0 ) .
The problem / error / change in question is that I'm looking for a way to modify the python code so that any cells that are empty or contain ` NA ` are given a completely different hex color ( not one that is in the gradient scale ) , however to explain the problem from the start .
As you can see they've all been given 3 hex colors , which in excel if you were to apply the colors to each cell would translate to :
I'm looking for a way to modify the python code so that any cells that are empty or contain ` NA ` are given a completely different hex color not one that is in the gradient
In matrix of 100 x 100 in python , filling the off diagonal elements
I'm trying to fill the off diagonal elements on a 100x100 matrix , as shown below in the matlab code , so how to duplicate it in python .
So I know that the first terms of RHS will fill the diagonal of the matrix with the value ` 2*t0 ` ,
but I don't know how to do the 2nd and the 3rd terms , I know that they will fill the values above and below the diagonal elements with value of ` -t0 ` , not all the off diagonal values but only filling the upper and lower value of diagonal with -t0 , rest all are zeros , but I don't know how to write the python code for it .
Argsort " ... returns an array of indices of the same shape as a that index data along the given axis in sorted order .
So there doesn't seem to be anything in the stack that solves your problem .
@USER , the fact is that for my problem , all elements are unique and sorting is not a problem , since those values are indexes by themselves .
Note that you may have problems with this ( and other methods ) if the elements of ` row ` are not unique or if you have repeated elements in ` a ` #CODE
It always takes me longer to understand the reshape operations vs the explicit broadcasting .
@USER : I guess it's a matter of taste ; I'm used to the scikit-learn codebase , where ` reshape ( -1 , 1 )` is the common way to transpose a 1-d array .
a comparison between lomb-scargle and fft ( scipy.signal and numpy.fft )
fft #CODE
And you use this to guide your parameters in fft .
The power spectrum is the absolute value of the fourier transform squared .
I should use log too .
Your figure is very clean.I tried log ( y ) but the base noise still exists .
And the peaks are not the same between fft and lomb-scargle .
Now ` times ` is an array of increasing unique time values , and ` counts ` is the number of photons detected at that time .
We can use the same pattern used above on ` timepoints ` to find the unique values ( and frequency of their occurrence ) in ` dt ` : #CODE
Now that we have an estimate of the fundamental frequency , we can use it to guide the choice of the bin size in a histogram of ` timepoints ` to be used in an FFT calculation .
If we do not know 0.024Hz , how do we choose parameters of ` lomb-scargle ` and ` fft ` to get the right highest peak ?
You are very close , you just need a different way to select the relevant indices in ` data ` .
` ( data 4 ) ( data 1 )` finds the indices of the ` data ` that are BOTH ` 4 ` and ` 1 ` .
SciPy has three methods for doing 1D integrals over samples ( trapz , simps , and romb ) and one way to do a 2D integral over a function ( dblquad ) , but it doesn't seem to have methods for doing a 2D integral over samples -- even ones on a rectangular grid .
For my problem , I keep integrating over arrays of the same dimension , so it's faster for me to create a weight array for simpson's rule ( call the array simp ) and do sum ( simp*z ) -- since I only have to define simp once .
Its also worth noting that the other methods this function is calling use the indices they are passed to perform operations on the same array .
More often , traps log diagnostic information or substitute valid results .
... and then I can simply cast the boolean array ( ` k `) to int , and use that as selection indices in the " distinct values array " ( ` z `) : #CODE
the only caveat is that you need to redo the tagging until ` sum ( tag ) !
= 0 ` and ` sum ( tag ) !
if you wrote your own max function in pure python , and autojited that , it would probably be faster .
This works fine e.g. with ` np.sum ` and ` np.prod ` , but min and max are not listed in the code .
` svds ` is a different method from ` svd ` as it is sparse .
How to vectorize finding max value in numpy array with if statement ?
In working set selection procedure i want to find maximum value of gradient and its index for elements which met some condition , y [ i ] *alpha [ i ] 0 or y [ i ] *alpha [ i ] #CODE
Finally , we can vectorize away the entire loop by using ` max ` and ` argmax ` on ` -y * grad ` , filtered by ` y * alpha B [ y + 1 ]` : #CODE
In addition in the same loop I have to find min value , so I think this solution is the best one .
I want to compute the ndarray ` outer_array ` of shape ` ( nrow , ncols , 3 , 3 )` containing all outer products of the vectors of shape ` ( 3 )` at each index ` ( nrow , ncol )` .
Now , I'm not sure that this will work : despite the fact that ` outer_array ` has the expected shape , the elements of the matrices of outer products do not correspond to what I'm expecting .
I think this is due to the choice of labels in the ` einsum ` expression : the product is supposed to be summed over ` x ` and ` y ` because the indices are repeated , but since I'm reusing them in the output expression , the result of the sum is somehow broadcast .
numpy will compute all possible combinations of outer products for each pair ` ( x , y )` and ` ( u , v )` , resulting in an array of shape ` ( ncols , nrow , ncols , nrow , 3 , 3 )` , where the diagonals ` ( u , v ) = ( x , y )` will contain the desired output .
How do I choose the first two indices in the einsum notation in order to obtain an array where at each index ` x , y ` I get the outer product of vector ` v ` with itself without having to resort to the second expression ?
I still wonder how does my code manages to work , since the indices ` x ` and ` y ` are repeated in the terms to be summed over and appear in the output again .
Now for an outer product on this ` x ` : #CODE
Does is suppress the summation over those indices ?
First I thought to flatten the array , but that operation doesn't preserve the reference , as it makes a copy .
You could do this using list indices : #CODE
The example of a diagonal assignation , was just an example .
To get Matlab-like indexing on the flattened array you will need to flatten the transposed array : #CODE
If you want to make the comparison to zero explicit , just compare to a scalar , don't build a zero vector ; it'll get broadcast the same way .
And , if you ever do want to build a zero vector , just use the ` zeros ` function ; don't build a ` list ` of zeros in a complicated way and pass it to ` asarray ` .
Floating-point numbers accumulate rounding errors .
The 3 incremental filters are similar in speed , about half of the fast ones .
The key is to avoid O ( n ) operations , like ` sum ` , inside your loop .
I've tried some alternative incremental functions that don't use the ` sum ` in the loop , and don't get any speed up .
* Some solvers , e.g. conjugate gradient methods , take the Jacobian as an additional argument , and by and large these solvers are faster and more robust , but if you're feeling lazy and performance isn't all that critical then you can usually get away without providing the Jacobian , in which case it will use the finite differences method to estimate the gradients .
I'm certain that this has a simple ( -ish ) solution in numpy , but I've spent the last hour banging my head against variations of repeat , tile , and anything else I can think of .
[ Simplified from ` ( np.arange ( len ( A [ 0 ])) [: , None ] B ) .T ` -- if we expand ` B ` and not ` A ` , there's no need for the transpose . ]
I have created another 1d array that that goes from min to max velocity in even bin widths .
How would I sum the intensity values from my 2d array which correspond to my velocity bins in my 1d array .
I wish to first create velocity bins from Vmin to Vmax and then sum the intensities over each bin .
The overall shape in the histogram is to be expected , however I don't understand the spike in the curve at V = 0 .
I could use this method if I knew how to sum the intensities using the corresponding velocities .
This is what histogram does .
What I was looking for is the functionality provided by the R ` deriv ` function .
@USER not well acquinted with R , can you link to docs for ` deriv `
The nxn dimensions can be reduced to 1d with reshape or ` flatten() ` ( or ` ravel `) .
I think you can use members of the ` bincount ` / ` digitize ` / ` histogram ` trio to avoid the listcomp , but IMHO it's more trouble than it's worth .
The -1 in the reshape tells numpy to figure out what that value should be to make the reshape work .
return sum ( 100.0 * ( x [ 1 :] -x [: -1 ] ** 2.0 ) ** 2.0 + ( 1-x [: -1 ]) ** 2.0 )`
pytables wont support calculating dot products of huge matrixes with a quick convenience function .
But implementing the dot product algorithm manually should be fairly straight forward .
@USER : dot products with very sparse mmap'd arrays will probably be orders of magnitude slower than with ` scipy.sparse.cs {c , r}_matrix ` .
Dense matrix multiplication doesn't know a priori where the zeros are and will loop over all the elements .
One workaround that may or may not be helpful in your specific code : create new mmap objects periodically ( and get rid of old ones ) , at logical points in your workflow .
The R code basically simulates 250 random normals , and then calculated a geometric mean return of sorts and then a max drawdown , it does this 10000 times and then combines the results , as shown below .
and the multinomial NB fit gives no Unknown Label error .
I also checked the unique values in X_train , Y_train with numpy.unique and it doesn't seem like there are any weird or mistyped labels -- it's all ' fresh ' or ' rotten ' .
For example , I need to sum ` v [ i - 1 , j ] + v [ i + 1 , j ] + v [ i , j - 1 ] + v [ i , j + 1 ]` and I have to make a matrix for each of them ( my matrices are much larger , so I use a parameter instead of the ` 5 ` in my question ) according to your answer , and two times for the different rows .
Also , when running the interactive session , data_interp is much larger in value ( > 3e5 ) than the original data ( this is around 20 max ) .
NumPy resize method
Also , when running the interactive session , data_interp is much larger in value ( > 3e5 ) than the original data ( this is around 20 max ) .
By assuming that , if she specifies the right s , the interpolator will use a spline f ( u , v ) which exactly reproduces the function underlying the data , she can evaluate sum (( r ( i , j ) -s ( u ( i ) , v ( j ))) **2 ) to find a good estimate for this s .
As far as the transpose goes , it's because ` fromarrays ` expects a sequence of " columns " .
The fsolve solver is based on a search following the gradient of your function .
The gradient of f is just ( 2*x1 , 2*x2 )
The KKT conditions tell you that in a local extrema the gradient of f and the gradient of the constraints are aligned ( maybe you want to read again about Lagrangian multipliers ) .
So compute the gradient of your constraint function !
Also keep in mind that in the corners of your triangle the concept of a gradient doesn't really make sense .
To do it in a clean way , you will need to iterate over the contents manually and join them up yourself , just as you would for a normal Python list of lists that you wanted to print out NumPy-style .
That's exactly what Pandas is good for basically , a DataFrame can be used as an array with names for the indices .
Since the list's have unequal length you can't broadcast them .
In the fist case , your two list's will get broadcast into a 3x2 array where a median along a axis makes sense .
Now the two list's of unequal length can't be broadcast this way .
This passes to the interpolator all values we have , not just the ones next to the missing values ( which may be somewhat inefficient ) .
Now from linkage I want to extract my original data ( i.e RGB values ) on specified indices with indices id .
Means cluster 6 contains the indices of 6 and 11 leafs .
Now at this point I stuck in how to map these indices to get original data ( i.e rgb values ) . indices of each rgb values to each pixel in the image .
** 1** : Why reshape image into ( -2 , 4 ) , what's the mean of -2 and 4 ?
Which may be used to map with cluster indices but don't know whether it is correct way or not .
4 : I am not sure about the indices , by writing couple of code lines I just able to get cluster indices based on fclusterdata .
;) i.e. what you sum up
then next questions : 1 ) you sum distribution , that is integrate . did you normalize it previously ? in your example ` distribution.sum() ` yields - 45.256707490195311 `
Yeah , sure , in real life it is normalized , meaning all elements were divided by it's sum
And some math functions of numpy , such as ` sqrt mean `
Edit : Inspired by @USER ' s use of ` flatten ` , this is a more general solution : #CODE
I think using ` flatten ` on an ` array ` makes more sense because it converts 2D array into 1D , while this cannot be done for matrices .
If you want get a list from 3x1 , 1x3 , use ` flatten ` : #CODE
In other words , it counts overlapping ranges , somewhat like a histogram .
Then the entire shape changes from ( x , y ) to merely ( x , ) and I get ' too many indices ' errors when I try to use masks .
Change the ` dtype ` of ` mat ` to ` numpy.object ` .
For simplicity , let's assume that A has max rank , i.e. some of its proper minors is invertible .
If it's not true , you can reduce matrix A to a smaller A ' with the same LU decomosition as below , with A ' satisfying this property and being equivalent in term of equation ( last rows of ` u ` will be zeros , remove them and proceed ) .
As my matrix will typically not be square and will be chosen at random I need to be able to handle the non max rank case too .
As a general comment , you'll get better performance using ` mat [ i , j ]` rather than ` mat [ i ] [ j ]` , but that improvement will be small vs using numpy broadcasting and ufuncs .
Original Timing for reference : ( with ` mat [ i ] [ j ]` changed to ` mat [ i , j ]`) #CODE
If you change ` mat [ i ] [ j ] = ...
` to ` mat [ i , j ] = ...
` indices ` : numpy array ; ` shape : ( n , 1 )` ; For each value in ` test_array ` finds the index of the closest value in ` known_array `
It does not matter if your data is sorted ; the method posted by HYRY takes care of that scenario , and has linear rather than the quadratic memory performance of the diff method ; his answer should be marked as the correct one
And use ` argmin ` and fancy indexing along with ` np.diagonal ` to get desired indices and differences : #CODE
The search sorted algorithm works well for me but it fails if any value of ` test_array ` is larger than the max value of ` known_array ` .
Numpy transpose usage
Can't inderstand why numpy doesn't transpose matrix .
Fix its shape if you want to transpose it : #CODE
This is also the difference between ` np.array ([ 0 , 1 , 2 , 3 ])` ( which is its own transpose ) and ` np.array ([[ 0 , 1 , 2 , 3 ]])` ( which , as noted in the OP , transposes into a 4x1 matrix ) .
You can add more dimensions with ` reshape ` : #CODE
I want to replace " values of ` indices `" by " index of ` zones `"
In that case , indices responds to a series , but it could not be the case .
If I understand the question , you can get what you want from ` np.unique ` by returning the " inverse " along with the unique elements .
The inverse is flattened , so you have to reshape it back to the shape of ` indices ` .
The indices to reconstruct the ( flattened ) original array from the unique array .
Then I try and resample the ` DataFrame ` to daily using ` df.resample ( " 1D " , how= " sum ")` .
The only work around we have found is to instead we truncate the dates using numpy datatypes .
Because in your example all ` username ` and ` session ` values are unique ?
So being able to truncate them to one day , two days , etc .
You need to convert X and y into correct float type and resize the shape of X into the right dimension .
Numpy -- Split 2D array into sub-arrays based on indices
` np.split ` and the corresponding components ` vsplit ` , ` hsplit ` and ` dsplit ` and only work along a specified axis and an array of indices .
The rectangles are not all that obvious , since this kind of tiling isn't unique .
So to use this function , you need to transpose the dataframe or modify the function .
There's also a ` sum ` builtin .
The error log now says : #CODE
When I calculate fft I get an array with complex values so I have to cast the result to float before calculating chroma .
I have two arrays ` a ` and ` b ` of dimensions 3x1 and 3x3 and I want to sum them up like here : #CODE
Can we sum matrices of different dimensions ?
But in your case you only want to add to the first column , so you should avoid the broadcast operation , and add to the first column directly , like : #CODE
Note , that I corrected columns indices , as it seems that ones provided by You in the question are not correct .
` a [ indices ] += b ` , except that results are accumulated for elements that
It might not be the perfect solution because of defining the min and max values but it does the job .
The min and max values will always be overwritten provided you have at least 1 element in your list .
The issue you're having is that you are searching for a vector min / max and instead you should be searching by each of the verctor dimensions ( two in your case ) .
so you should look for the min for X and the min for Y and the same for the max .
Those are builtin methods that step through a vector ( or axis of an array ) , calculating a cumulative sum or product .
The first uses the fact that all three values are the same by having NumPy broadcast the 1 to the correct dimensions .
I'm currently trying to implement the following algorithm for finding max .
If I initialize it to None on line 4 , then it works fine for the " w * is undefined " comparison in line 9 , but the second comparison " |w| |w*| " , which compares the norm of w and w* , fails since you can't just run np.linalg.norm() on a None variable .
I've * solved * this problem before ; I should have remembered multinomial coefficients were the wrong answer .
( Previously , I posted an answer suggesting the use of a multinomial distribution . That is not the right distribution ; it gives too little weight to results with repeated elements . Sorry for the error . Since the ways to place k stars and n-1 bars are in direct correspondence with the multi-subsets of { 1 ... n} , this solution should produce a uniform distribution . )
EDIT : I printed deriv and this is what I got :
But after experimenting with other values to compute derivative , I figured out that the result is -1 , 0 or 1 because ` deriv ` is actually ` sign ( - 0.5 *sqrt ( -4*x + 1 ) + 0.5 )` .
Another thing to note , if you plan to enter values such that the square roots become complex , you'll need to input a numpy array with complex dtype , or else you will get a TypeError because numpy's sqrt refuses to work with negative real numbers .
SymPy does do this , but only if it can deduce that the argument to the absolute value is real , which it can't in this case ( even if ` x ` is real ) .
What I want is to calculate vector dot products so that my correlation looks like :
Over which variable is the sum ?
Then use correlate to calculate the per-component correlation : #CODE
When you write out your updated version of ` x ` in ` query ( x )` , you truncate it to 3 decimal places .
` x ` by finite differences , if the step size is sufficiently small it might incorrectly estimate the gradient as 0 so the optimization doesn't progress .
For example , for fitting problems you'd often use the sum of squared errors * l = sum (( y - yhat ) ^2 ) * .
What I now want to do is determine the first 1000 maximum values and store the corresponding indices in to a list named x and a list named y .
This is because I want to plot the maximum values and the indices actually correspond to real time x and y position of the value .
Isn't ` partition ` similar to ` unique ` ?
Probably you should write your own algorithm to find the solution in one pass : keep only 1000 greatest numbers ( or their indices ) somewhere while scanning your 2D array ( without modifying the source array ) .
So , although sorting is ` O ( n log n )` , it may be quicker than a Python-based one-pass ` O ( n )` solution .
` nlargest_indices ` finds the ` n ` largest values in ` arr ` and then returns the ` x ` and ` y ` indices corresponding to the locations of those values .
They also return indices in a different order , but I suppose that does not matter for your purpose of plotting .
With ` cov ` ?
Then you can generate all possible indices with : #CODE
You just have to join those lists on x3=x3 , x5=x5 ( I bet solution size will significantly grow with this join , so can't guess it's usefulness ) .
I'm following the documentation from scipy.org using examples for ` fftfreq ` and ` fft ` .
Because the data arrives at irregular intervals , I am using an approach to append data - similar to this thread .
I say largest in the sense that I want all the boxes except the ones that are fully contained by another box .
Is it possible to place a linear function over a 2D array and sum all the elements in the 2D array that coincide with the function ?
I now only want to sum the elements of the 2D array that overlap with the linear line .
Is there a fast way to sum only the elements of the 2D array that coincide with the linear line ?
The linear line is defined by ` x cos ( phi ) + y sin ( phi )` , here phi is in the order of pi / 3 .
Then you can use the resulting mask to do a sum : #CODE
The linear line is defined by ` x cos ( phi ) + y sin ( phi )`
` x cos ( phi ) + y sin ( phi )` is a 2-d function of x and y -- it's not a line .
Use ` eigh ` instead of ` eig ` .
If you use ` eig ` , the transpose of ` v1 ` is not necessarily equal to the inverse of ` v1 ` .
first , why not ` df [ ' diff '] = df [ ' time2 '] .diff() ` ?
`` diff `` is better hear ; I was trying to ' illustrate ' so didn't overwrite the columns
In MATLAB , I would initialize them all as 1-D arrays of zeros of length n , n bigger than the number of entries I would ever see , assign each individual element ` variable ( measurement_no ) = data_point ` in the logging loop , and trim off the extraneous zeros when the measurement was over .
EDIT : Assume I'm going to have a lot of vectors of the same length by the time this is over ; e.g. , my post-processing takes each log file , calculates a bunch of separate metrics ( > 50 ) , stores them , and repeats until the logs are all processed .
They all get initialized as part of a big ` deal ( zeros ( n , 1 ))` , and I prefer to not have to keep track of how many there are on the left side of the assignment .
After I determine and store the sum of the elements , I want to shift my y_list by one and determine the sum of the elements again .
Are you essentially trying to do a dot product ?
Extracting indices from numpy array python
I am trying to equalise the histogram of an image I want to perform further processing on .
Is there a more efficient way to do this instead of using append and the for loops ?
To get rid of the append , allocate two arrays of ints using np.zeros ( shape ) , then assign to the relevant index and you should see a performance boost .
Your first array , a , can be evalueated if a second demension added ( for example with reshape ): #CODE
if y has an remainder can is the best way to coerce it to work in the reshape function to do an if statement then ` w = z [: len ( y ) % p*-1 ] .reshape ( -1 , p )`
Did I calculate something wrong or is the code for adorio-research using a different method to calculate the initial seasonal indices ?
It will return you with parameters for alpha , beta and gamma #CODE
Plug the alpha , beta , and gamma into the python code and you get : #CODE
where all the indices can have values 0 , 1 and the sum over a , a ' and a '' is carried for all cases where a+a ' +a '' = 1 or a+a ' +a '' = 2 .
So it is like the reverse of the Einstein summation convention : I want to sum only when one of the three indices is different to the others .
Moreover , I want some flexibility with the number of indices that are not being summed : in the example the resulting tensor has 2 indices , and the sum is over products of elements of 3 tensors , one with one index , the other two with two indices .
These numbers of indices are going to vary , so in general I would like to be able to write something like this :
I want to point that the number of indices is not fixed , but it is controlled : I can know and specify how many indices every tensor has in each step .
I tried ` np.einsum() ` , but then apparently I am forced to sum over repeated indices in the standard Einstein convention , and I don't know how to implement the condition I exposed here .
And I cannot write everything with various for because , as I said , the number of indices of the tensors involved is not fixed .
The problem with this is that I need to specify all the indices involved , and I cannot do that in the general calculation for all the lattice .
Are you summing over a permutation group of some sort ?
The problem with this is that I need to specify all the indices involved , and I cannot do that in the general calculation for all the lattice .
A k-Tensor is a multilinear function that maps its input which will be k-vectors to the real numbers , so in the question when you said " and the sum is over products of elements of 3 tensors " , are you referring to a function that takes 3 vectors ?
The first call to ` np.einsum ` is adding everything up , regardless of what the indices add up to .
The second only adds up those where all three indices are the same .
The problem is that , since you are not specifying what you want to do with the ` b ` and ` g ` dimensions of your tensors , it tries to broadcast them together , and since they are different , it fails .
More generally , if reducing over ` d ` indices , ` C_idx ` and ` D_idx ` would look like : #CODE
and the calls to ` np.einsum ` would need to have ` d ` letters in the indexing , unique in the first call , repeating in the second .
These three arrays don't braodcast together , but ` np.einsum ` adds additional dimensions of size 1 , i.e. the " remapped " shapes of the error above , so the resulting arrays turn out to have extra trailing ones , and the shapes amtch as follows : #CODE
I have almost achieved my desired result , but still I'm getting more axes than there should be ( I'm contracting a 2 , 2 with a 2 , 2 , 2 , 2 and another 2 , 2 , 2 , 2 and should get a 2 , 2 , 2 , 2 , but am getting a 2 , 2 , 2 , 2 , 2 , 2 , with the two extra axes full of zeros ) .
But I wanted to contract two indices for each tensor , a , a ' and a '' and b , b ' and b '' , each group of 3 fulfilling the condition of summation 1 or 2 .
Can you spell out the indices you want to sum over in your example ?
When I wrote it like you just did , it worked , but of course I would like to write it as something like np.einsum ( ij ..., kl ..., mn ... -> , B , C , D ) - np.einsum ( ' ij ..., ij ..., ij ... -> --- ' , B , C , D ) in order to cope with the variable number of indices .
You will need a different call signature depending on how many indices you want to reduce over , let me edit my answer to include the 2 index case , hopefully this will make it clear how to expand it to 3 or more if you ever need it .
I tried the numpy.uniquify function but that didnt care about the fact that I had a 2x2 array and instead returned each unique number inside each sub-array in a flat list which I dont want , and it sorted and changed the order of my original arrays which I also dont want .
How can I proceed with my example and end up with unique sequential subarrays ?
Do you think the problem can be further simplified that in the final result you only have the unique rows ?
No , the pairs cannot be entirely unique unfortunately because the coordinate pairs are geographical polygon coordinates , which means it matters if the polygon later returns to one of its previous path points .
File " C :\ Python27\lib\ site-packages \matplotlib\backends\ backend_tkagg.py " , line 276 , in resize
And remembering that p.T is the transpose :
test for point in poly using regular Python , but for my purpose this
try to decimate the vertex of the polygons removing the ones that stay in the same pixel based on the current Zoom level and see if an how the times will be reduced
Find max non-infinity element in pytables CArray
I could loop through the CArray in windows that fit in memory , but it'd be surprising to me if there weren't a max / min reduction operation .
I believe pytables in-kernel computations is done with numexpr , and as far as i know , Numexpr only support sum and prod reduction .
This works because ` where() ` never reads in all the data at once and returns an iterator , which is handed off to map , which is handed off to max .
` sin ` is more Latin via Arabic via Sanskrit than English * per se* , but I'll let it slide this once !
@USER This approximation of pi is exact up to 16 significant positions , which is just the accuracy of a double-precision float aka double and I would presume that a good implementation of ` sin ` will have an error not large than one epsilon ( that is the relative float precision ) .
@USER The approximate nature of both ` sin ` and ` pi ` is due to the use of floating point arithmetic , though .
A symbolic math system could represent ` pi ` as just a symbol and use pattern matching to get an exact ` sin ( pi )` .
The result is the best approximation of ` sin ` one can get to be representable as a floating point number .
If you are computing the mean of ` groups ` over and over again ( with small changes to ` groups ` in between iterations ) then it would be smart to keep running totals of the sum of each item in ` groups ` .
The first three steps are identical for all your interpolations , so if you could store , for each new grid point , the indices of the vertices of the enclosing simplex and the weights for the interpolation , you would minimize the amount of computations by a lot .
You could try to remove the floating point error , eg , truncate to x number of significant figures ( where x is determined by your applications required precision ) and then compare .
If I create a homogeneous rectangular list of lists ( and skip the resize ) , the AA [ i , j ] have type float .
so first you generate a hamming window yourself ( i'm still using python but you can translate it to c / cpp ): #CODE
Trying to minimize code to find sum of sub_arrays of a numpy array
I want to sum all the sublists in the following form : #CODE
If so then ` np.array ( a )` is a 2d array , and you can sum over ` axis=1 ` .
Otherwise some sort of loop is required , with a list comprehension being the most compact : ` [ sum ( x ) for x in [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 3 , 4 , 2 ]]]` .
I am looking for a vectorize way to do this but the closest thing I found on stack shows the problem can't be resolved because 2 values update depend on the i term see : #URL
I'm trying to calculate statistics ( min , max , avg ... ) of streaks of consecutive higher values of a column .
The max streak of consecutive values has a value of 3 because of : 2 , 4 , 6 sequence .
AttributeError : ' int ' object has no attribute ' reshape '`
Then sum the squares in the last axis ( the one containing x , y , z ): #CODE
gives an array where the first axis is the point selected from ` gal ` and the second is that from ` rand `
@USER : Go for a middle ground of your solution ( use one entry of ` rand ` at once ) and my solution ( use all entries at a time ) , by taking say every 15 entries of ` rand ` ( using ` np.array_split `)
The problem is , now and then I have to resize the arrays , and I'd preferably do that inplace .
I've seen this workaround , but unfortunately it doesn't resize the array in place .
You would have to create your own " resize " function to resize the base and reload the mmap into the memmap , yes .
At the very least , the docstring for the ` np.memmap ` class should be updated to reflect the fact that it isn't currently possible to resize memory-mapped arrays in place .
my general advice is to create a hierarchical index on the columns and then simply use the ` stack ` method of the dataframe .
Then if you ` stack ` the dataframe , you get this : #CODE
Storing debug log for failure in / Users / MYUSERNAME / Library / Logs / pip.log `
Seems you need consecutive min alongaxis .
Why do you use first time min and second time amin ?
Why do you set ` zeros ` twice ?
By the way , performance of ` append ` depends on size of the list .
However , single element operations like the ones you are testing may well be worse than Python lists .
Then , I would like the numpy array methods ( ` max ` , ` cumsum ` , ... ) to work transparently , requiring only a ` chunksize ` keyword to specify how much of the array to load at each iteration .
I save ` emission ` as an Extensible array ( ` EArray `) , because I generate the data in chunks and I need to append each new chunk ( I know the final size though ) .
if most things are zero or almost zero , aint it better to just store the indices where its not zero , and their values , knowing that the rest is zero ?
Just do `` counts.astype ( int64 ) .iloc [: , 0:4 ] .sum ( 1 )`` to compute the sum and avoid `` uint64 `` ( or really just use `` int8 ``) .
On " simulated " data with ~50% of zeros elements blosc compression ( ` level=5 `) achieves 2.2x compression ratio .
Dask.array can perform chunked operations like ` max ` , ` cumsum ` , etc . on an on-disk array like PyTables or h5py .
In general , you could do this by constructing an open mesh grid corresponding to the row / column indices , apply your predicate to get a boolean mask , then index into your array using this mask : #CODE
Your specific example happens to be the upper triangle - you can get a copy of it using ` np.triu ` , or you can get the corresponding row / column indices using ` np.triu_indices ` .
Also , built in functions like ` sum ` also works on numpy array .
` sum ` works on iterables , such as list or numpy arrays .
Use numpy.in1d() on this intersection and the first columns of each array to find the row indices that are not shared in each array ( converting boolean to index using a modified version of the method found here : Python : intersection indices numpy array )
Finally using numpy.delete() with each of these indices and its respective array to remove rows with non-shared entries in the first column .
Your indices in your example are sorted and unique .
and in case the first column is not in sorted order ( but is still unique ): #CODE
This method as assumes the integers in question are unique , otherwise only the first instance of each integer will be returned by ` searchsorted ` .
I wouldn't be so sure . searchsorted has complexity O ( log ( n )) in its first argument and O ( n ) in its last argument .
This should be quite fast and this method does not assume the elements of the input arrays are unique or sorted .
` genfromtext ` is meant to handle ` csv ` files , although ( AFAIK ) not ones with " strings " in them .
But I have to generate an histogram for each pixel .
I.e , for a collection of n images , I have to count the values that the pixel 0 , 0 assumed and generate an histogram , the same for 0 , 1 , 0 , 2 and so on .
Where each position of a matrix store a dictionary representing an histogram .
But applying it to my code , where the data are a large number of images generated with this constructor ( after the values are calculated and not just 0 anymore ) , I just got as h an array of zeros [ 0 0 0 ] .
What I pass to the histogram method is an array of ImageData .
One option would be to learn how to write Cython extensions and write the histogram bit in Cython .
Actually , taking a histogram of pixel values is a very common task in computer vision and it has already been efficiently implemented in OpenCV ( which has python wrappers ) .
Standard numpy has a histogram function that can be used for this purpose .
If you have only few values per pixel , it will be relatively inefficient ; and it creates a dense histogram vector rather than the sparse one you produce here .
I've already used histogram from ` Matplotlib ` , but I really didn't know how to apply the ` numpy ` ` histogram ` ( however I knew that it exist ) for multiple images / pixel ( and don't think that it is something trivial ) , and not just the entire image .
That said , the arange initialization gives a somewhat funny histogram .
I need to perform a product then a sum on each element of a dataframa as follow
this calculation is called ` convolve ` : #CODE
I'm wondering the convolve calculation can be accelerated .
I'm making a sum between an empty array and a scalar .
How to append a vector to a matrix in python
I want to append a vector to a matrix in python .
I tried ` append ` or ` concatenate ` methods but I didn't get the answer .
( Think " column stack " and " row stack " ( which are also functions ) but with matlab-style range generations . )
( In the docs I just found ' # Purists could use " Py_ssize_t " which is the proper Python type for array indices . ') -> does that mean always when indexing NumPy / Cython-Array ( s ) / -views whatever ..?
I have a set of CSV data that is ` 4203x37 ` which I reshape to ` 50436x4 ` in order to find the Euclidean distance between 12 sets of 3D points , recorded at each time-step .
Efficient dot products of large memory-mapped arrays
I need to be able to perform efficient dot products using these arrays , for example ` C = A.dot ( B )` , where ` A ` is a huge ( ~1E4 x 3E5 float32 ) memory-mapped array , and ` B ` and ` C ` are smaller numpy arrays that are resident in core memory .
A naive dot product computed using 3x nested loops , like this : #CODE
where ` M ` is the maximum number of elements that will fit into core memory , the number of I / O operations is reduced to O ( n^3 / sqrt ( M )) .
Does calling ` np.dot ` perform an I / O-efficient blockwise dot product ?
If not , is there some pre-existing library function that performs I / O efficient dot products , or should I try and implement it myself ?
For computing the dot product of two arrays that are in core memory , ` np.dot ` will be faster than the equivalent call to ` np.einsum ` , since it can use more heavily optimized BLAS functions .
I don't think numpy optimizes dot product for memmap arrays , if you look at the code for matrix multiply , which I got here , you'll see that the function ` MatrixProduct2 ` ( as currently implemented ) computes the values of the result matrix in c memory order : #CODE
In the above code , ` op ` is the return matrix , ` dot ` is the 1d dot product function and ` it1 ` and ` it2 ` are iterators over the input matrices .
As @USER Rico predicted , the time taken to compute the dot product is beautifully O ( n ) with respect to the dimensions of ` A ` .
I conclude that whatever caching ` np.memmap ` arrays natively implement , it seems to be very suboptimal for computing dot products .
You should probably be able to reduce dot products of slices with a solution like [ Strassen algorithm ] ( #URL ) .
My guess is that for the actual dot product it would be very hard to beat the optimized BLAS functions in terms of speed .
Also read their presentations about compression , it sounds strange to me but seems that sequence " compress -> transfer -> uncompress " is faster then just transfer uncompressed .
And I don't know how numexpr ( pytables also seems have something like it ) can be used for matrix multiplication , but for example for calculating euclidean norm it's the fastest way ( comparing with numpy ) .
I also have to perform dot products on the transpose of ` A ` , and since they lack a transpose method , this makes my indexing a lot more awkward .
The biggest problem may be choosing an appropriate chunkshape , since I also have to perform operations on single rows / columns of ` A ` as well as dot products which are best performed on square blocks .
Find the index of the min value in a pdist condensed distance matrix
As a general remark , you are using ints as indices .
@USER : Hey thanks , using the among the ones you mentioned
The previous tool in the pipeline returns a list of list of tuples as such , each element in the outer list is sort of a document .
For example , the expression ` ( belongs_to == 1 ) .nonzero() [ 0 ]` returns the array of indices to array ` belongs_to ` where the value is ` 1 ` .
Why do you want to keep the indices in the tuples ?
The versions on github are the ones you want .
And finally I reduced the datafile size to 1k rows and I still recieved an error , again different to the previous ones : #CODE
Unfortunately I cannot supply the full code for confidentiallity reasons , however I hope either of the following snippets encode the shape of X_train and Y_train when passing it to model.fit ??
What I could do is adapting the function , cause sometimes , for instance the mean of this vector is calculated , sometimes the sum .
I'm working on a problem where I need to calculate log ( n ! ) for relatively large numbers - ie . to large to calculate the factorial first , so I've written the following function : #CODE
It's actually a good deal ( ? 10x ) slower because it requires a memory-copy ( ` shuffle ` is in place , so you need to use ` permutation ` instead ) .
If there are only a few columns , then the number of all possible permutation is much smaller than the number of rows in the array ( in this case , when there are only 3 columns , there are only 6 possible permutations ) .
A way to make it faster is to make all the permutations at once first and then rearrange each row by randomly picking one permutation from all possible permutations .
I noticed the problem when I used gradient checking ( with finite difference approximation ) to verify that the small modifications I made to switch from numpy to gnumpy didn't break anything .
As one may expect the gradient checking did not work with 32 bit precision ( gnumpy does not support float64 ) , but to my surprise the errors differed between CPU and GPU when using the same precision .
Whereas you are only looking at the absolute difference between the values in your two result arrays , ` np.allclose ` also considers their relative differences .
Suppose , for example , that the values in your input arrays were 1000x greater - then the absolute differences between the two results will also be 1000x greater , but that doesn't mean the two dot products were any less precise .
For example , if you're dealing with values 1e-8 , then an absolute difference of 1e-8 would be a total disaster !
From what I understand , you just want your array values to be padded with two leading zeros and printed to two decimal places .
To break this down , the ` 5 ` means that the total length of the formatted string is 5 characters ( including the decimal point ) , the ` 0 ` means that you want to pad with leading zeros , and the ` .2 ` means that you want two figures after the decimal point .
So if you wanted , say , 3 leading zeros and 4 decimal places you would do : #CODE
If you just wanted to pad with leading spaces rather than zeros , you can omit the ` 0 ` in front of the total length : #CODE
Assume we have a numpy.ndarray data , let say with the shape ( 100,200 ) , and you also have a list of indices which you want to exclude from the data .
Look where it constructs ` keep = ones ( N , dtype =b ool ); keep [ obj , ] = False ` .
You can use ` b = numpy.delete ( a , indices , axis=0 )`
For a numeric list of indices , ` np.delete ` uses the ` mask ` solution that you earlier rejected as taking up too much memory .
What is the advised way to [ x for x in samples if x not in samples [ indices ] [ 0:3 ]] ?
Alternatively , one can view the problem as putting the first element of a on the first diagonal , the second element on the second diagonal and so on .
I would like to have a norm 1 numpy array .
If this is really a concern , you should check for norm < epsilon , where epsilon is a small tolerance .
In addition , I wouldn't silently pass back a norm zero vector , I would ` raise ` an exception !
Note that the ' norm ' argument of the normalize function can be either ' l1 ' or ' l2 ' and the default is ' l2 ' .
If you want your vector's sum to be 1 ( e.g. a probability distribution ) you should use norm= ' l1 ' in the normalize function .
Because the Euclidian / pythagoran norm happens to be the most frequently used one ; wouldn't you agree ?
Why do numpy cov diagonal elements and var functions have different values ?
I believe that covariance matrix diagonal elements should be the variance of the series .
In numpy , ` cov ` defaults to a " delta degree of freedom " of 1 while ` var ` defaults to a ddof of 0 .
So , each row of the new array represents a person and inside each row is the scores for each of their tests sorted from the test ids smallest to greatest [ score for test 0 , score for test 1 , score for test 2 , .. ect . ]
By computing the sum of personId and normalized testId .
Creating norm of an numpy array
I want to get the norm of this array using numpy .
for every array inside , I need sqrt ( x2+y2+z2 ) , so my output wull be array of 4 values ( since there are 4 inside arrays )
Can you define " norm " ?
it means for every array inside , I need sqrt ( x2+y2+z2 ) , so my output wull be array of 4 values ( since there are 4 inside arrays )
Did you mean matrix norm ( s ) ?
thanks . but what if i need norm ( 2 ) which means for this input I i will get an array with 4 values .
[ norm ( x ) , norm ( y ) , norm ( z )]
So each point will append into ` neighbour_points ` and then it will make a ` distanceQuery ` .
@USER My goal is to get cluster using dbscan process but at the same time restrict those cluster with " max distance between any two points in that cluster " .
And since you just take the max value of that list , you can just as well use a generator expression for that .
( Tested with Python 2.7 using a list of 1000 random numbers instead of Points and ` dist ` measuring the absolute distance between those numbers . )
@USER If you dont do c= 2*something , and than d=6371*c , but just get max ( something ) , you get another ( marginal ) speedup .
On a sidenote : Personally i change lat , lon , alt coordinates to ECEF coordinates ( x , y , z ) and than take the sqrt (( x0-x1 ) **2+ ( y0-y1 ) **2+ ( z0-z1 ) **2 ) to get the distances , since ECEF takes the flattening of the earth into account , as well as the altitudes of both points .
But I can't figure out how to concatenate these ndarrays , numpy.concatenate or numpy.append always giving me back error messages even if I try to flatten them first .
My code trying to concatenate the arrays looks like that now ( after trying numpy.concatenate and numpy.append I'm now trying numpy.insert ) : #CODE
Use ` numpy.cos ` if you want to apply ` cos ` to an iterable .
I'm not sure what you mean by " best way " here : less cumbersome to write , faster to index , faster to create the indices ,...?
Because apart from the missing indices on the ` *_Boundary ` variables in the last line , this seems to work fine .
You want to transpose the array ( think matrices ) .
If the period is fixed , you can reshape your data into a two dimension array , and then the following process will be simple .
Note the outer parens makes this return a generator rather than processing the entire file before returning anything .
as followup on alko : just play with dot and see what shape you get when vBeta is 1-D versus a column array .
Since the values to discard are 0 , you can compute the mean manually by doing the sum along an axis and then dividing by the number of non zeros elements ( along the same axis ): #CODE
as you can see , the zeros are not considered in the mean .
Otherwise , you just compare the old dd with ( newly entered point - old max ) .
or a command to make the sum of this look like this : #CODE
How to change the displayed representation or how to perform a sum along an axis or what ?
You are computing the reciprocal of the sinc function .
The numpy sinc function is vectorized , so you don't need to write the loops .
Not that it matters here ; by using vectorized numpy functions instead , you move the loops over the array indices into C code anyway .
sqrt for element-wise sparse matrix
As far as I can tell most of the other ufunc operations ( sin , cos , ... ) do have sparse ufuncs except for sqrt , don't know the reason why .
How could this matrix / vector dot multiplication be possible in numpy.linalg module ?
I use ` reshape ` to change vectors to column / row ones as appropriate and store them in default configuration .
There are lots of algorithms built in , including sorting and search ones .
You can accumulate the selection in a single boolean selection mask using in-place operations .
First one defined by image , is storing the sum of a movement at the pixel ( i , j )
You could convolve with a 3x3 mask like this : #CODE
square root of sum of square of columns in multidimensional array
I need to find square root of sum of square of columns .
I think the inner sqrt should be a square
How does l_bfgs optimization method approximate the gradient
My guess is that at each point , for each dimension , gradient is approximated by forward difference .
Step size used when ` approx_grad ` is True , for numerically calculating the gradient
N function evaluations are made at each point to get the gradient , where N is the number of dimensions ( size of vector x )
I figured out a workaround with a ` reshape ` but it seems bizarre if it was necessary .
And suppose in B , all the values in row k are nonzero , and those are the only nonzero values .
Then both A and B are very sparse , but the product is fully populated with nonzero values .
Your problem is very likely being caused by an overflow of an index stored in an ` int32 ` , caused by the result of your dot product having more than 2^31 non-zero entries .
( This can be changed either by using ` rowvar=False ` or by just passing in the transpose of the data . )
Or with a ` pandas ` dataframe , just call the ` cov ` method : #CODE
So the function is to calculate sigmoid ( X ) and another to calculate its derivative ( gradient ) .
So I'd like to do certain operations on certain parts of the arrays where zeros are involved : #CODE
First find out indices where the items are not ` nan ` , and then pass the filtered versions of ` a ` and ` weights ` to ` numpy.average ` : #CODE
+1 , though I think ` indices = ~ np.isnan ( a )` looks nicer ( and for huge ` a ` might be faster ) .
finding the value of the min and max pixel
ok this seems to work , but I don't understand how to use it , and find each min and max value in each color
I'm trying to get each color's max and min value . and I can't seem to get how to use this
( Though the bands might be in a different order depending on the library you're using , etc . ) To get the minimum values , just use ` min ( axis=0 )` instead of ` max ` .
Furthermore , you should be aware that many common scientific operations ( e.g. finding the eigenvalues / eigenvectors of a matrix ) don't have a unique solution .
Just because something like ` eig ` gives a different answer on different systems doesn't mean that it's incorrect .
@USER , I know that the data files are the same because I replaced them by the original ones .
Find two pairs of pairs that sum to the same value
I would like to determine if the matrix has two pairs of pairs of rows which sum to the same row vector .
From your code , it looks like you're after ** pairs of pairs ** of rows which sum to equal the same row , rathat than " two pairs of rows which sum to the same value " .
What's the scalar sum ?
Your current code does not test for pairs of rows that sum to the same value .
This generates the indices of all rows that have equal sum .
you need A.sum ( axis=-1 ) , and ` nonzero() ` is " wasting " time , as it is basically looping over the whole array for each value in unique .
rows , columns ; whats the difference ;) . the print is for illustration purposes ; its not a given that the OP is interested in these indices in the first place . either way , if n=100 is indeed the challenge , I doubt something involving python lists will turn out to be faster . but indeed , this loop does not scale linearly in performance for n -> inf
I meant pairs of pairs of rows that sum to equal the same row vector .
Based on the code in your question , and on the assumption that you're actually looking for pairs of pairs of rows that sum to equal the same row vector , you could do something like this : #CODE
This basically stratifies the matrix into equivalence sets that sum to the same value after one column has been taken into account , then two columns , then three , and so on , until it either reaches the last column or there is no equivalence set left with more than one member ( i.e. there is no such pair of pairs ) .
Can this code be generalised to pairs of triples of rows that sum to the same row vector do you think ?
I was just adding .astype ( np.int8 ); now n=1000 is feasible too , using a max of ~ 1.5gb , even though it takes a few seconds .
note ; performance might be increased further substantially , by computing a not-necessarily unique but short hash for each rowsum , and first checking for collisions in that space .
Note that extension to even higher combinatorics should be trivial , along the lines presented ; but keep an eye on the n used in that case .
You can get the same as ` np.add.at ` with ` count = np.bincount ( inverse , minlength=len ( unique ))` .
If [ this PR ] ( #URL ) is accepted , unique counting will be easier ( and faster ) in 1.9 .
Loop over a unique list of your y-values ( I've changed the labeling to make things clearer ) .
I want to do a test where I increment ` OMP_NUM_THREADS ` from 1 to a max value , time a routine at each thread count , and then finally manipulate the aggregate timing for all thread counts .
In particular , I can't return the value ` dot_time ` from ` numpy_test ` up to my outer wrapper routine , so I can't analyze the results of my test in any automated fashion .
I kind of figured that I can take the square root of the length of the list and then the ceiling value which will yield the square matrix dimension of 2 ( ` np.ceil ( sqrt ( len ([ a , b , c ]))) == 2 `) .
Also , why do you ` sqrt ` the dim ?
Sorry am new to python , again in this elif -2 <= value < 0 : its showing IndentationError : unindent does not match any outer indentation level
You want to translate a single number to a label , right ?
Find two disjoint pairs of pairs that sum to the same vector
This is a follow-up to Find two pairs of pairs that sum to the same value .
I would like to determine if the matrix has two disjoint pairs of pairs of columns which sum to the same column vector .
In the previous problem (( 0 , 1 ) , ( 0 , 2 )) was acceptable as a pair of pairs of column indices but in this case it is not as 0 is in both pairs .
It isn't entirely clear though ; one may get an arbitrary number of row-pairs that sum to the same total ; there may exist unique subsets of rows within them that sum to the same value .
Given this set of row-pairs that sum to the same total #CODE
There exists a unique subset of these rows that may still be counted as valid ; but should it ?
In the case of a 2 by 4 array [[ 19 19 30 30 ] , [ 11 16 11 16 ]] columns 0 and 3 have the same sum as columns 1 and 2 .
Indeed , the sum over all indices in each of the 4 columns gives the same total .
We then do the vector sum for columns 0 and 1 and get the vector [ 39 , 27 ] in this case .
The tables I wrote down contains the indices into A .
It is presumed that A has only 0 and 1 values in this code ; so the vector sum can only be something like [ 2 , 1 , 0 ] , not [ 39 , 27 ] .
However , a 4 by 4 all zeros matrix should return True so I am not sure how to modify your code now .
For an all-zeros matrix , you get a 2x6 matrix denoting all possible sums of pairs that sum to the zero vector .
Again , the code notes that set of combinations is not unique ; but it does have a unique subset , namely [[ 2 3 ] , [ 0 1 ]] , which as you just revealed , you do consider a valid combination .
Note that [[ 1 2 ] , [ 0 0 ]] does not have a unique subset ; we cant remove anything from it so that we only have one zero , and still have a pair of pairs .
Great , we have converged :) I definitely do want to return True when there is a unique subset as you say .
One could simply perform a brute-force search over all possible combinations of columns of such a matrix , and see if they form a unique subset .
The indices appear sorted , first by the top row , then by the bottom row .
If the first and last column don't form a unique subset , I don't think any combination does .
I edited the code to include a check for unique pairs .
build Scipy matrix with diagonal overlays
You need to supply three arrays : the data , the row indices and the column indices as per the second example in the link .
In the end it is usually not too complicated , especially if you use [ ` mgrid `] ( #URL ) or similar to get the indices .
I am attempting to fit a function using Leastsq to fit to a few relevant points in an fft .
The fitting function has a number of manipulations to find the correct indices , which I will not reproduce here to save space , but it returns a list of complex numbers : #CODE
Also you'll note that I went to a range around the target to fit in the newer versions ( edited opening post ) , rather than just a few specific ones .
Size is how big the pattern is , period is the period of the sin wave , threshold is where on the sin wave to shift from 0 to 1 and center is where to put the phase shit .
A 2d plot of n-m looks relatively constant along the m == n diagonal ; even though it drops to zero pretty steadily for larger n , the odds change much more dramatically off-axis
So you want to pick 4 rows , all of them disjoint and the componentwise sum of two of them equals the respective sum of the other two ?
you don't care about the remaining entries of the 2d field and all entries are ones or zeros ?
Because of that , it's dead easy to generate all arrays that are unique under row permutation via ` combinations_with_replacement() ` .
That reduces the trip count on the outer loop from ` 2** ( n**2 )` to ` ( 2**n+ n-1 ) - choose-n )` .
A precomputed dict maps pairs of rows ( which means pairs of integers here ! ) to their vector sum as a tuple .
The inner loop picks row indices 4 at a time , instead of your pair of loops each picking two indices at a time .
EDIT - speeding the sum test
This is minor , but since this seems to be the best exact approach on the table so far , may as well squeeze some more out of it .
As noted before , since each sum is in ` range ( 3 )` , each tuple of sums can be replaced with an integer ( viewing the tuple as giving the digits of a base-3 integer ) .
Can it be written as a simple sum for example ?
Obtain indices corresponding to instances for each type of error in a confusion matrix in numpy
Instead of just obtaining a confusion matrix , I want to be able to get the indices ( or array mask ) of the instances that committed a specific type of error .
( to get the indices , I could use ` np.where() `) .
SciPy's ` sparse_coo ` matrices add together repeated indices , so the following also works : #CODE
Yeah , this is more or less the same as this #URL but I do need the array mask or indices corresponding to each entry in the confusion matrix .
You should be using one of the non-interactive ones if you're running things from a webserver .
You should be using one of the non-interactive ones if you're running things from a webserver .
strange numpy fft performance
If you pad array with zeros it will produce so called " edge effect "
I have found in this topic that fft performance depends on the array size prime factorization .
If I dump the frames into a file with a " \x0\x00\x00\x001 " separator . and I create a VideoCapture object from it . all works just perfect .
This syntax will raise ` TypeError : list indices must be integers , not tuple ` , so I'm sure the object was not a regular Python list .
However , still unclear on how this is arranged and functions , so not sure what would happen if some of those zeros were changed .
and it worked for smaller subsets of an image , but when using it on the entire image ( 3858 x 3743 ) the code terminates very quickly and all I get is an array of zeros .
but when I include ` print img_array ` I end up with just zeros .
Still returns zeros when I use uint8 ..
What looping order ? as far as i know , transpose only change the strides
If you first compute the necessary parameters for all possible values of ` i - j ` and add them together , you can reuse those values on the outer loop , e.g. : #CODE
numpy already allows the creation of arrays of all ones or all zeros very easily :
` ones ` and ` zeros ` , which create arrays full of ones and zeros respectively , take an optional ` dtype ` parameter : #CODE
The `'` operator is the transpose operator in MATLAB .
If your arrays are 2D arrays , which are of shape ( m , n ) when you print ` a.shape ` , you should transpose the second array using ` .T ` , otherwise you get ` ValueError : objects are not aligned ` : #CODE
you need ` transpose ` the first array : #CODE
IndexError : too many indices on Python
Trying to count the element in the array and sum up the elements in the array .
Here's the absolute path for the .csv file and the python scripts .
I think you do not read the file at all , but instead just split up the file name , resulting in a 1d array , for which those complex numpy indices make no sense .
About the new error : If you want that fixed , add the absolute paths of both your script and the ` csv ` file to your question .
Included the error as well as the absolute path for the .csv and .py files .
` os.path.join ( ' train.csv ')` to ` os.path ( ' train.csv ')` , i.e. , instead of calling the ` join ` function from the ` os.path ` module , you are ( trying to ) call the module itself .
You have to use the absolute path together with the filename : #CODE
memory error in numpy svd
I am performing numpy svd #CODE
Then how can I find svd for my matrix ?
Maybe the ` svd ` function needs to cache some data as well .
However , computing the ` svd ` is fairly memory-hungry regardless , so this won't help a ton .
What is the purpose to get the svd of 10000 by 10000 matrix ?
All in all , I try to filter some elements of a 2D array based on some conditions on it's values and indices .
for ` max ( x+y * value )` : #CODE
For example if I would like to have the max ( x+y * value )
It's a big dataset so I would rather not have to convert the NaN values into zeros or something .
Simple 3D barplot of a 2D histogram after PCA
Obviously a PCA is still possible with a unique answer but it gives the above error .
This will automatically find a compatible datatype ( ` int64 ` , in this case ) for the new array and " stack " the fields of the structured array into columns in a 2D array .
I suspect a mod will see it and move it ...
Create a masked array in which the zeros of ` x ` are masked .
For example , if you are using numpy 1.8 , the ` norm ` function accepts an ` axis ` argument .
how to do len = sum ( sqrt ( sum ( v . *v ))) / N ; with numpy 1.8 ?
The Matlab expression ` sqrt ( sum ( v . *v ))` gives the Euclidean norm of the columns of ` v ` , so it can be expressed with numpy as ` np.linalg.norm ( v , axis=0 )` .
The array A contains the ids / serial no . info .
If the arrays were the same data type , you could concatenate them , and ` savetxt ` would work just fine .
( to hold the ids ) , and whose remaining M fields are floating point .
where you're multiplying an array with a view of its transpose .
create a C-ordered copy of any F-ordered input arrays , not just ones that happen to be views onto the same block of
If for whatever reason you can't upgrade numpy , there is also a trick that allows you to perform fast , BLAS-based dot products without forcing a copy by calling the relevant BLAS function directly through ` scipy.linalg.blas ` ( shamelessly stolen from this answer ): #CODE
Otherwise , in the outer loop you overwrite ` aj ` without ever using it .
` * ` in numpy does element-wise multiplication , not a dot product .
Those sums of products have to be expressible as a call to ` dot ` or ` einsum ` ...
Since K-means takes a numpy array I have to strip out the username and just use the numerical variables .
When I run this as is it returns X as a 1D array of f ( r ) and cos ( phi ) is 1 ( ie . phi = 0 ) .
( You'll need to initialize X before the outer loop . )
When I run this as is it returns X as a 1D array of f ( r ) and cos ( phi ) is 1 ( ie . phi = 0 ) .
@USER putting it simply ,, would the number of arguments in the reshape function determine the dimension of the array ?
Just keep in mind that the new shape must be compatible with the total number of elements in the original array , as explained in the [ reshape documentation ] ( #URL ) .
I have a 2d array of integers and I want to sum up 2d sub arrays of it .
At first I thought you wanted ` np.sum ( array , axis=1 )` to sum rows , or something similar to sum columns , but I don't get how you can obtain that 2x2 matrix as result .
@USER [ 0 , 0 ] is the sum of the first 2x2 subarray [ or more general the first lxm array ) ( basically ` arr [ 0:2 , 0:2 ] .sum() `) and so on .
Once you have the " blockshaped " array , you can obtain the desired result by reshaping ( so the numbers in one block are strung out along a single axis ) and then calling the ` sum ` method on that axis .
Should be much faster as the second reshape would trigger a copy of the array .
Its not the case in image where curves are conjoined , thus the diagonal line coming straight down back to the origin .
Is it possible you're looking for an outer product type function with a scalar ?
If ` F() ` works with broadcast arguments , definitely use that , as others describe .
I know how to calculate std dev , mean and the rest with pandas but my statistic understanding is rather poor to allow me to proceed further .
Is there a way to do this or would I have to use a ` for loop ` and ` concatenate ` function ?
Yes , the first reshape should probably be removed , that is what confused me .
The first reshape is there merely to illustrate that the function works on more than just 1d arrays
I'd like to compute the discrete X and Y gradient arrays of a 2-d numpy image array according to the following masks : #CODE
How to logically combine integer indices in numpy ?
Does anyone know how to combine integer indices in numpy ?
The programs import numpy arrays from various sources throughout the execution , which can sum up to sometimes just 1GB but sometimes 4GB .
Is there a better way to kick them out than to get the indices where ` mask.any() ` is ` False ` in a for-loop ( ` for i in range ( len ( masks )): if not mask.any() : del_index.append ( i )` and then ` bin_centers = [ m for n , m in enumerate ( bin_centers ) if n not in del_index ]` ?
` min ( X )` and ` max ( X )` fall right on bin edges .
` np.digitize ` places ` min ( X )` in bin 1 , and ` max ( X )` in bin 10 .
Since ` min ( X )` is the smallest value of ` X ` nothing ever falls in bin 0 , and the one value in bin 10 , ` max ( X )` gets ignored when you loop over ` range ( 1 , len ( bins ))` .
It is basically the sum of the pixels binned by their distance from the central spot .
By extracting the peaks of the resulting plot , i can accurately find the radii of the outer rings , which is the end goal here .
I used bincount and histogram in this implementation : #URL
Each ring is one of the histogram bins .
The call to mean returns the unique values in R , and the mean of corresponding values in data over identical values in R .
So not quite the same as a histogram based solution ; you don't have to remap to a new grid , which is nice if you want to fit a radial profile , without loss of information .
Should you start from the ones matrix instead ?
I would suggest to first program it with ` np.nditer ` and then translate it into C .
beat me to it . the only trouble with this approach is that we need to form the full ixjxk matrix , whereas the operation in log space could ( although currently does not use ) a directly contracting operation .
Then I thought the extract function might take a while to process so first I converted my 2D list in a 1D std :: vector in python ( I didn't really find a way to create a 2D vector ) #CODE
Use Matrix element wise product ` * ` instead of ` dot ` product
numpy diff doesn't deal properly with timedelata ( well it works but it's pretty raw ); you should have at least numpy 1.7
I'm guessing it is because all the vector are passed as the first argument of dot ; isn't it ?
You're right about the ` dot ` only getting the first argument in the second example .
Either reshape or split up the arrays for x and fitted .
You can use ` np.where ( A [: , 0 ]= =0 ) [ 0 ]` to find the indices where ` x == 0 ` and use that in a loop to split apart A - though I think ` np.split ( A , np.where ( A [: , 0 ]= =0 ) [ 0 ])` does it for you in one line .
The log after run " python setup.py build " : #CODE
That's the whole point of using ` numpy ` if you can broadcast a computation over an array , you replace a Python loop with a C loop , and remove all the boxing / unboxing around each arithmetic computation , meaning your code typically gets anywhere from 4-400x faster .
You should also remove the double calculation of ` sum ( list3 )` : #CODE
Now , let's broadcast the ` A ` and ` B ` calculations : #CODE
Saulo : Clever improvement in your edit ( using ` max ( array3.sum() , 1e-06 )` , but it changes the semantics a bit .
( I estimated that quickly skipping the ` exp ` cuts the time to .79x , so doing one instead of two is probably about .9x . ) But I'll bet you could get a whole lot more improvement by moving it out a level ( broadcast ` exp ` over all the rows at once , then ` sum ` all the rows , instead of doing it once per row ) , so I think if the existing improvement isn't enough , broadcasting over ` array1 ` and ` array4 ` should be the first step , then look to optimizing the math within them .
As for the ` 1e-06 ` number , I'm just trying to avoid ` 0 ` values in ` sum_list3 ` because they cause troubles later on , so I just replace any instance where the sum adds up to zero to a small value .
Just for the record : I'm after * every drop of optimization * I can squeeze out of python .
I have denoted a list with the starting and stopping X values ( not the indices ) .
As the bkg_ranges list denotes starting and stopping points along the input array ( not indices but the sorted x values )
I would like to count how many m by n matrices whose elements are 1 or -1 have the property that all its ` floor ( m / 2 ) +1 by n ` submatrices have full rank .
It is rather directly related to your question concerning pairs of row-pairs that sum to the same value .
Shouldn't you be comparing the rank to ` min ( n , rowstochoose )` ?
The largest possible rank of a ` JxK ` matrix is ` min ( J , K )` , and I assume that's what you mean by " full rank " .
Filter the matrices and sum the orbit sizes of the ones that remain .
This is a bit tricky to implement , but it works for this question , your other one , and other ones besides that you may have in the future .
Algorithm 1 - memorizing small ones
When you are checking a new large matrix , divide it into small ones
The other possibility is to create larger matrices by adding pieces to the lesser ones , already found .
You should set exact algorithm , which sizes are derived from which ones .
If each time when we can create new size from two old ones , we derive from more square ones , we could also greatly spare place for matrices remembering : For " long " matrices as 7x2 we do need remember and check only the last line 1x2 .
This is somewhat equivalent to asking , " How many collections of m n-dimensional vectors of {-1 , 1 } have no linearly dependent sets of size min ( k , n ) ?
maxrank = min ( m // 2+1 , n )
Little tricks like not recalculating ` int ( min ( n , rowstochoose ))` and ` itertools.combinations ( range ( m ) , int ( rowstochoose ))` can save a few percent - but the real gain comes from memoization .
@USER Also note that we're not doing a single chi2 test on a contingency table , but one test per feature , where a feature value ` X [ i , j ]` is the count of event ` j ` in sample ` X [ i ]` ( and a sample is a drawn from a multinomial ) .
I am looking for best way to take a data file append it to an array , and use it in multiple python modules .
I use the triu method to get rid of upper triangle of the array ( where duplicates are ) including diagonal ( which is where the self-distances are ) , and i dont sort either , since if you plot it , it does not matter if i plot them in order or not .
The speed difference is something like ten to twenty percent , and i think it will not get much better than this , since i got rid of all the sorting and unique elements things , so the part that takes most of the time probably is the ` d = cdist ( X , X )` line
The function converting indices ` k ` in condensed distance array to equivalent ` i , j ` in square distance array is : #CODE
` pdist ` implementation was always slower than cython , and takes much more memory , because it explicitly creates a distance matrix , which is slow because of sqrt .
As you can see from the following ipdb log , extra zero dates are added to pre and post arrays .
Now I want to sum up all the arrays in the list , so that I end up with another 2d array .
But it gives me a list of the sums of the elements of the arrays in the original list , not the sum of the arrays themselves .
will doing a sum of the result of map give you the result you want ?
What I want is the sum of the 6 arrays in QS : ` np.shape ( QQ )= > ( 64 , 16 )`
But try something like this to sum an array of arrays ( list of lists ): #CODE
I have seen the post Difference between nonzero ( a ) , where ( a ) and argwhere ( a ) .
` np.where ` returns indices where a given condition is met .
In your case , you're asking for the indices where the value in ` Z ` is not ` 0 ` ( e.g. Python considers any non- ` 0 ` value as ` True `) .
Is this occurring because numpy uses C , and it stores arrays of a certain size on the C stack and arrays of another size on the C heap ?
Forget about the C stack , numpy objects can't use it .
On a 64 bit Ubuntu installation , I get a jump ( but much less drastic ) near ` zeros ( 2**17 , float )` .
it is almost the same time for the per loop in two different experiment , this is not for the stack / heap problem , because the memory location will in the same may , I think it maybe the memory problem . the memory of my computer is 16GB .
If you already have the Numpy code ready , just optimize the bottleneck , which in your case must be that dot product .
I would try using the dot function instead of einsum , i.e. instead of : #CODE
using a better algorithm that'll converge faster than plain old gradient descent ( which I assume is what you're using ): adagrad or a second-order optimization algorithm - lbfgs , say -
So I would recommend you to start by replacing that with call to dot as Patrick said .
I have see and hear many times that einsum is slower then call to dot in some cases .
If you just move the dot call to Theano , Theano won't be faster if Theano and NumPy are linked to the same BLAS library .
About Patrick answer , you don't need to use the symbolic gradient of Theano to benefit from Theano .
But if you want to use the symbolic gradient , Theano can only compute the symbolic gradient inside computation graph done in Theano .
You can move that manual implementation of the grad to Theano without using the symbolic gradient .
So it isn't sure that doing only the dot on the GPU will help .
This returns the transpose of ` dummyvar ( partitions )` .
But since you only are after the product of the matrix with its transpose , and scipy converts everything to CSR format before multiplying , it is probably slightly faster like this .
If three rows are a sample , you can reshape your array to reflect that , use fancy indexing to retrieve your samples , then undo the shape change : #CODE
Under the covers , NumPy's ` bool ` values are just ` 0 ` and ` 1 ` as ` uint8 ` ( just like Python's ` bool ` values are ` 0 ` and ` 1 ` , although of a unique integral type ) , so you can just use it as-is in any expression : #CODE
Both ` x ` and ` element 4 ` in array of zeros are assigned value ` 1 ` .
Therefore , we need to first create an array of zeros , and then assign element assign ` element 4 ` a value of ` 1 ` , and these two cannot be done in one line .
which is perplexing since broadcasting should apply here and ` b ` should broadcast onto ` a ` .
The generic fft and dft methods transform vectors of complex numbers into vectors of complex coefficients .
Seems like I should now be able to use b and c to translate from one array to the other .
( The ` reshape ` is needed because it flattens the data first , and the ` -1 ` because it starts its ranks at 1 . )
That won't work if there are repeated indices , e.g. ` [ 0 , 1 , 1 , 2 ]` .
edited : now it will also work with repeated indices
The equivalent function there would be ` zeros `
numpy , scipy , interp1d - TypeError : tuple indices must be integers , not str
If I try it interactively I get ` ValueError : could not broadcast input array from shape ( 5 , 5 ) into shape ( 5 )` .
Indeed , you are correct if ` sum ` is the built-in Python function .
The default behavior of ` numpy.sum ` ( in particular , with the default ` axis=None `) is to convert its argument to a numpy array and sum * all * the elements .
` x.reshape ( -1 , 1 )` has shape ( 3 , 1 ) , and ` n ` has shape ` ( 4 , )` , so the ( broadcast ) result of ` x.reshape ( -1 , 1 ) **n ` has shape ( 3 , 4 ); column k contains ` x**n [ k ]` .
The desired result is the sum of this array along ` axis=1 ` .
According to his Cython code , the ` sum ` needs an ` axis= ` keyword rgument .
Minor detail : reshape ( -1 , 1 ) is more cleanly and efficiently written as x [: , None ]
But I've always preferred ` reshape ` .
` reshape ` is pretty explicit , but admittedly it requires that you know how the -1 is handled .
@USER when I try your above method I am getting ValueError : shape mismatch : objects cannot be broadcast to a single shape
Get indices of matrix from upper triangle
And I need to find the indices ( row and column ) of the greatest value without considering the diagonal .
And then the index of the max value #CODE
Couldn't you do this by using ` np.triu ` to return a copy of your matrix with all but the upper triangle zeroed , then just use ` np.argmax ` and ` np.unravel_index ` to get the row / column indices ?
Right now I am using max ( function ) to find the maximum value .
why not get the ` max ` of a generator about the iterable with a gen-comp filter for ( 1 , 10 ) ?
Are you looking for 2nd max value in 2D array ?
In that case , just solve for f ' ( x ) = 0 and choose the second biggest arg max in that x .
You can convert your nodes into indices using ` np.unique ` : #CODE
Finally , you loop over all the lines in the file and strip off the return line character with line.strip() .
This is not what i want to do , i am defining my function dG , with 3 variables thetaf , psi , gamma , and i would like to find the root of this function , in case :
` gamma = linspace ( 0 , pi / 2 , nt )`
' def dG ( thetaf , psi , gamma ) :
` return 0.35 * ( cos ( psi )) **2* ( 2*sin ( 3*thetaf / 2+2*gamma ) + ( 1+4*sin ( gamma ) **2 ) *sin ( thetaf / 2 ) -sin ( 3*thetaf / 2 )) +sin ( psi ) **2*sin ( thetaf / 2 )`
` gamma = linspace ( 0 , pi / 2 , nt )`
` x = zeros (( nt , np ))`
` for i , theta in enumerate ( gamma ): `
` g = lambda thetaf : dG ( thetaf , psi , gamma )`
You get theta and phi out of gamma and psi respectively , but then you never use them .
gamma in enemurate ( gammas ) , psi in enumerate ( psis )
Not sure whether this is fast , but at least it's short : ` array ( sum ( map ( lambda x , a : [ x ] *a , x , arrivals )))`
What you could do is create the result array prior to looping ; that way you don't need to concatenate afterwards .
I need to stack all such 1-d arrays row wise to form a 2-d array .
` numpy.ma ` can not broadcast what I have in mind and I always get the error of size differences .
A simple solution ( if that is indeed your problem ) would be to expand the mask along the time dimension using ` concatenate ` .
but I get " ValueError : operands could not be broadcast together with shapes ( 1016 ) ( 1018 ) "
but I am getting ValueError : shape mismatch : objects cannot be broadcast to a single shape
You can use ` numpy.char.decode ` to decode the bytes literal : #CODE
If you don't know the size before-hand , then don't use a numpy array to accumulate the values .
Converting files to digital image failing with " tile cannot extend outside image "
You can reshape ` TT ` to have shape ( 2856L , 1 ) by doing ` TT.reshape ( -1 , 1 )` and that will at least get rid of the error , but I'm not sure if that gives the desired result .
Vectorized manipulation of an array , based on a function of indices
Is there a way to find the indices where exceptions are thrown using ` np.where ` ?
Additionally you can start a background thread which will resize your original images to the 150x150 size .
To see a little better what is going on and how to handle larger numbers of indices : #CODE
If your data is like the one you gave in the example , you already have a mesh ( you have a value of z for each pair ( x , y )) and you only need to reshape the arrays : #CODE
My install log from pip : #CODE
In other words , I want to divide each element of the array by the norm of the row it belongs to .
Need sqrt not sum , sqrt doesn't seem to have a convenient axis or keepdims command .
If you are still curious what modules are the offending ones , ` ModuleFinder ` is not much helpful .
If you want them interpreter as single-digit integers , there's no way to do that directly , but you can do that in two steps as well ( e.g. , read it as an array of 2 strings , treat each string as a sequence of characters , broadcast ` np.vectorize ( int )` over it ) .
He says he's trying to produce the exact same shape he started with , so it's not even ` transpose ` , it's just ` copy ` .
In the outer comprehension , ` for j in X [ i ]` has the same problem , but is has an even bigger problem : there is no ` i ` value .
but that will of course transpose the array , which isn't what you wanted to do .
Just as the right way to transpose an array is : #CODE
Iterate over the elements , not the indices : #CODE
I don't care about the position on the image of features with the frequency f ( for instance ); I'd just like to have a graphic which tells me how much of every frequency I have ( the amplitude for a frequency band could be represented by the sum of contrasts with that frequency ) .
As it turns out I only get distinctly larger values for ` frequencies [: 30 , : 30 ]` , and of these the absolute highest value is ` frequencies [ 0 , 0 ]` .
I understand what a fft does in principle , I just don't really get the ` numpy.fft.fft2 ` output , I would have expected a 1D array with no " null " frequency band .
For example , you can get a better distribution of values by taking the ` log ` of ` freq ` .
The output , analogously to fft , contains the term for zero frequency
how come I don't get a second peak at 200 , a third at 400 etc ( as the log ( freq ) plot would indicate ) ?
Also , why does hist ( log ( freq )) stop at 16 on the x-axis ?
` plt.hist ` is making a histogram of the values .
The ` x-axis ` represents values of ` log ( freq )` and the ` y-axis ` represents a count of how freqently those values occur .
The upper value of ` 16 ` means that the largest value in ` log ( freq )` is near 16 .
Additionally , I suggest you to do use Python list to append data and then convert to numpy array : #CODE
I'm not familiar with ` pygame ` , but I guess that you have illegal indices in ` name.get_at (( x , y ))` .
All setrecursionlimit does really is remove the warnings that would prevent you from doing stupid things ; it does not magically create the transistors in your CPU to hold a 1500000000 call stack .
Implementing a floodfill on the stack is generally speaking a bad idea .
Use a stack datastructure in a while loop instead .
But for following case his max stack space needed is 250*100 which will is less than 1MB space should not be problem for stackoverflow ?
Regardless , there is no reason to be using all that memory for a simple floodfill , and especially not your limited stack memory .
@USER I am only saying that inbuilt stack based floodfill is not that bad as compared to stack data structure based in terms of space complexity though might be slower . yes maybe there is no need for setrecursionlimit
The simplest alternative is to store the information you want to stack in RAM , rather than in your CPU callstack .
Simply write a while loop , append candidate pixels to a list , and pop off a new pixel off the list at the start of the loop , until your list is empty .
Thus , I used pickle to dump everything in between various stages in order to do testing on each stage separately .
Although the first dump always works fine , the second one behaves as if it is size dependent .
The dump which creates the problem contains a whole class which in turn contains methods , dictionaries , lists and variables .
There's an open bug report concerning empty ones : #URL
In your case , the triangulation is in fact reduced to 3 triangles , because you have duplicate points and consequently one unique z value at most has to be chosen for identical location .
Thanks GBy I get that as well after following your code example , if I then modify my data to have unique x , y coordinates then I get this #URL which looks better I think ( although I don't actually know what it should look like ! ) if I then try to create a contour using plt.tricontour ( x , y , z ) I get this #URL how do I now actually get a contour ?
Oh maybe my terminology is incorrect , this is what I thought a contour was and is what I am trying to achieve : #URL With regards to the data I just manually chose a set of random unique points from my data to see if the uniqueness was the issue and it looks like it was the thing that was stopping the system from graphing , but I'm not sure what I would use to create a graph similar to the link I have provided ?
What I mean by this is for some realizations and Boolean conditions np.size ( np.where ( cond )) is faster than np.sum ( cond ) , but for some it is slower .
Specifically , if a large fraction of entries fulfil the condition then np.sum ( cond ) is significantly faster but if a small fraction ( maybe less than a tenth ) do then np.size ( np.where ( cond )) wins .
Does it make sense that the time taken by np.size ( np.where ( cond )) increases with the number of entries for which cond is true ?
there is also np.count_nonzero which is much faster than boolean sum on new numpy versions .
So ` np.linalg.det ( t )` returns an ` (8 , 8) ` array , having calculated each ` det ` using the last 2 dimensions .
I have another question now this is working , I have separate columns of different data types ( strings , floats and ints ) and I want to concatenate them without losing their type . hstack and column stack lose the type and generalise them all to strings .
The question is not exactly the same ( euclidean distance there instead of absolute value and also the distances in that question had to be weighted ) but this question seems to me even simpler that that one .
Now the range of ` list1 [ 2 ]` is not equal to the range for ` list2 ` and thus the answer given fails to reject those points ` i ` for which ` list2 [ i ] max ( list1 [ 2 ])` or ` list2 [ i ] min ( list1 [ 2 ])` .
@USER Sorting is ` O ( NlogN )` compared to ` O ( N )` time taken by ` min ` , ` max ` .
So , ` min ` and ` max ` are going to be faster here ( at least theoretically ) .
And binary search won't work here as OP is looking to find the index of item with minimum absolute difference .
Binary search can find minimum absolute difference , no problem .
And max is just returning the largest value in that data structure .
If ` r_ ` , ` g_ ` and ` b_ ` have dtype uint8 , then so does ` r_ + g_ + b_ ` , and if an element is 255 in all three , the sum will be 253 .
Python TypeError : list indices must be integers , not tuple
One python iteration is insignificant compared to making a histogram of 10e4 elements .
You could try writing a numba extension to create a faster histogram perhaps .
I then have a single test array in N-dimensions , that I want to find all of the indices of its locations in the large array .
I want to avoid looping over the entire large array and testing all individual objects to find the indices .
There are multiple locations in the large array where each test array appears , and I need all of the indices .
why dont you just flatten and then reshape the array and then use ` array.take ( indices , mode= ' wrap ')` ?
You can also use ` roll ` , to roll the array and then take your slice : #CODE
In order to assign to this I would have to avoid the call to reshape and work directly with the flattened version returned by the fancy indexing .
To compute the number of unique elements in a numpy array , you can use ` unique ( x ) .size ` or ` len ( unique ( x ))` ( see ` numpy.unique ` ) .
In case it matters : for moderate to large arrays , ` unique ( x )` can be a lot faster than ` set ( x )` , but for small arrays , ` set ( x )` is faster .
I replace each nan with the same random number which is not what I want or I get a broadcast error in the second example .
Normal numpy arrays have access to matrix-style multiplication via the ` dot ` function .
Pandas , how to filter a df to get unique entries ?
I want to filter the dataframe so that I have a unique occurrence of " type " attrybute ( e.g. A appears only once ) , and if there are more rows that have the same value for " type " I want to choose the one with higher value .
In reality my idexes are the IDs and they are unique identifier string like ' M_001 ' how can I restore those indexes ?
Btw I am especially interested in keeping the opening zeros , e.g. ' 080712 ' of first line .
Again , I stress the point that the code above is a trivial example , but the application of what I'm trying to do could have arbitrary indexes where ` a 4 ` and not something based on ` arange ` and ` reshape ` .
Note that the indices you gave are actually ( y , x ) order , which is also what the answer delivers .
` nonzero ` and ` where ` are essentially equivalent , but ` transpose ` turns out to be the wrong one here ( even though it's mentioned in the docs ): #CODE
" 100 points which the sum of all distances between them is maximum .
This might be accomplishable succinctly with ` np.random.choice ` using group sizes to make the probabilities , and then using ` np.random.choice ` again on the group itself , instead of ` permutation ` which forms an array of all the points in the cell .
Adding an additional compress step would make it even slower , right ?
Sum 4D array efficiently according to indices contained in another array
I need to sum all these cubes together into one cube .
I was wondering since I know where the sub-cubes are in the cubes can I sum them more efficiently since the majority of the summing operation would be adding zeros- these cube are of significant size ( > 100^3 ) so if I can it will be a huge saving .
Did you try to use sparse matrix , since your cubes contain a lot of zeros ?
If you know were the sub-cubes are you can use fancy indexing to sum only where you need , for example : #CODE
And here are the indices where I want to sum : #CODE
Which will sum only at points : ` p1 ( 0 , 0 , 1 , 1 )` , ` p2 ( 2 , 1 , 5 , 6 )` , ` p3 ( 4 , 3 , 8 , 7 )` and ` p4 ( 6 , 7 , 9 , 8) ` .
Why they use the transpose in this example ?
I found that to append array to a record array , use mlab recfunctions is OK but numpy recfunctions is NOT OK .
It is a difference between the numpy ` sum ` and the native ` sum ` .
The native run does not take an axis parameter , but does not throw an error either , because it is a ` start ` parameter , which is in effect just an offset to the sum .
Unless you transpose df.T [[ 4 , 5 , 6 ]] = df.T [[ 4 , 5 , 6 ]] , but this seems like cheating ...
no , you don't set new rows that way , use append .
Also , in case I wasn't clear , the intent of my df.loc [ 7:9 ] = val example was not to append rows , but to change the content of existing rows ( with index = 7 to 9 ) .
I would store all your data in a python list and use the append function to add new measurement .
Since you've set the ` alpha ` to zero and aren't plotting the surface tiles , you might want to consider using ` plot_wireframe ` instead , where ` color ` sets the line color ( rather than the tile color as in ` plot_surface `) .
This is not surprising , because if ` c ` is 1 , most of the values of ` exp ( -c*x )` underflow to 0 : #CODE
Using ` numpy.repeat ` and ` numpy.tile ` ( which respectively tile the array horizontally and vertically ) .
` file.readline() ` does not strip EOF characters :
It appears as if the Cython function is not detecting the boundary between the ` c ` and ` d ` fields correctly and thinks as if you are passing in a char array of the sum of the lengths .
Further you can use ` pack ` again with the mask to get the indices and do more involved manipulations .
Unexpected result with max and min functions in numpy arrays
Essentially you just want the dot product of each row with the index row , normalized to a maximum of 1.0 .
When calculating the value for the overall estimator , I wasn't sure if it would be better compute one likelihood curve from all the results and use the max of that ( this is what I ended up doing ) , or to use the mean of all the subset estimators .
@USER -pl This one might help : [ Increment Numpy multi-d array with repeated indices ] ( #URL )
so that in your endresult , other values than 0 and 1 will exist ? or is your resulting array something in which only ones and zeros exist ?
Using numpy you could convert your pairs of indices into a flat index and bincount it : #CODE
Filling array with zeros in numpy
I understand that it is getting filled by zeros , and the first two values are specifying the row and column , what about the third ?
Read ` ( 4 , 3 , 2 )` as : There's a building with 4 floors , each floor has 3 rows and 2 columns of rooms .
Use np.repeat() and np.tile() to repeat / tile the array #CODE
The only thing left to be done is to reshape the result to get it in the desired form .
@USER : Maybe there is some super speed-up for subtraction of arrays in contiguous chunks of memory , which makes it faster than looping over indices which is ( I guess ) what broadcasting requires .
@USER , yeah , I had this as being faster , maybe I timed something differently , but sounds odd that it would be slower , unless the reshape is not possible in place .
You can use ` clip ` :
Could also be done in-place like the clip .
Resampling an array using predefined indices in Python
@USER use ` .T ` method to transpose the 2D array or ` zip ( *that_list )` .
How do I properly reshape the array for using this command ?
As a note ` argmax ` can take an axis argument , so you can broadcast ` rands ` and ` lookup ` against each other to make a ` n x n ` matrix for an N^2 scaling algorithm .
I have extracted the indices for each -1 by search through the main list : #CODE
If you only need the groups themselves and don't care about the indices of the groups ( you could always reconstruct them , after all ) , I'd use ` itertools.groupby ` : #CODE
Since all of the splines ( in a function set ) are created on the same knot values , is it possible to avoid step ( 1 , relevant indices ) in the spline evaluation once it has been performed on the first function of a function set ?
This blew up my algorithm since I had not expected the numpy broadcasting to look inside the function and decide it did not need to broadcast .
You could check the shape of the value returned by ` g ( x )` , and broadcast it if needed .
It's not unusual for ` numpy ` ( and MATLAB ) functions to massage the inputs to put them into an optimal shape for calculation , and then further reshape the outputs to match the input shapes .
This is just a thought , but how about writing a decorator to check the outputs of your energy function - and have it reshape to the input array if needed ( i.e. if a scalar , resize ` x `' s shape ) .
I may be able to find a way to reshape it into the array I want , but I figured that there must be an easier / cleaner / clearer built-in method that I'm missing ?
IMO , using ` dot ` and ` T ` is more legible here , but for unusual summations , ` einsum ` is absolutely the way to go .
@USER i added a few lines after the d2= ... to explain a bit more , what you do with those indices , it is quite simple actually once you get your head around it , ( just write as in a quadruple for-loop like d1 [ i , j ] = ..., and than write what you do with indices , but that in one line as in d2 = ...
Don't you mean the first 5000 * rows * - You seem to be working very hard take a look at tricks like : ` a = [ 1 , 2 , 32 , 3 , 1 , -4 , 44 ]; m = min ( a ); i = a.index ( m ); print m , i ; `
besides looking at builtin stuff like min ( a ) , maybe take a look at numpy arrays , instead of lists , they are much faster ( you say ' open arrays for minimum value and index ' , but you create lists in the line after that )
First of all , you can find the minima indices much easier and faster using numpy's argmin : #CODE
since list indices must be integers , not tuple in python .
Repeating rows in numpy according to a vector of indices
How to fill a numpy array with a gradient with a vectorized code ?
but for a large matrix of shape ( 5 , 1000 ) , ` tile ` is faster : #CODE
Anyway , ` tile ` makes code cleaner .
Create a second array of the same length with the values [ 0 , 0 , 0 , 0.6 ] per row using numpy tile
@USER ` def dG ( thetaf , psi , gamma ) :
return 0.35 * (( cos ( psi )) **2 ) * ( 2*sin ( 3*thetaf / 2+2*gamma ) + ( 1+4*sin ( gamma ) **2 ) *sin ( thetaf / 2 ) -sin ( 3*thetaf / 2 )) + ( sin ( psi ) **2 ) *sin ( thetaf / 2 )`
I am trying to find the root of this function where ` thetaf ` is my variable and psi gamma are in ` [ 0 , pi / 2 ]`
@USER in ` [ 0 , pi / 2 ]` i have roots , i ploted my function for some value of gamma and psi , and i see that there is a solution of my problem
If ` psi ` and ` gamma ` are within ` [ 0 , pi / 2 ]` but you're using ` thetaf ` as your independent variable , you're misunderstanding something : it's ` thetaf ` that needs to be in ` [ 0 , pi / 2 ]` .
` for i , gamma in enumerate ( gammas ): `
` g = lambda thetaf : dG ( thetaf , psi , gamma )`
Take psi = 0 and any value for gamma
Given ` y = dG ( x , psi , gamma )` : #CODE
Would just be ( assuming R has unique rows , for the moment ): #CODE
For example , let's say we wanted to know the indies where the sum of each row was greater than 1 .
If they are way longer , you maybe would like to find first the longest runs of zeros , and then only test those cases that begin with such a run .
C ) Better yet , I could roll ` j ` rather than ` x ` : #CODE
D ) Possible further speed up by constructing an array of rotated ` j ` , and doing the dot product just once .
I tried to do fancy numpy indexing but I constantly get the " too many indices error " .
However , when you do this for all the random positions in r : ` a [( j , tuple ( range ( pos , pos+i ))) , ( j2 , tuple ( range ( pos2 , pos2+i )))] +=1 ` , etc then it complains ` IndexError , too many indices ` or something
Clamping the indices to the range ` [ 0 , featurelength )` , via ` np.clip ` .
But you can directly consume the indices and act on them .
So , I have two lists : One of them is a list of unique arbitrary numbers , let's call it A , and the other one is a linear list starting with 1 and with the same length as the first list , named B , which represents indices in A ( starting with 1 )
But now I have a huge list of random , non-unique integers , and I want to know the indices of the integers that are in A , and what the corresponding elements in B are .
Using nonzero() I can get the indices of the elements in random_list that exist in A .
This correctly gives me the indices , but it's not optimal , because firstly I need a map() , secondly I need a lambda , and finally numpy.where() does not stop after the item was found once ( remember , A has only unique elements ) , meaning that it scales horribly with huge datasets like mine .
I think you would be well advised to use a hashtable for your lookups instead of ` numpy.in1d ` , which uses a ` O ( n log n )` merge sort as a preprocessing step .
Since we already loop through the list to convert the entries , it should be possible to get the Not-None indices in the same go .
This gives you the indices of ` random_list ` that exist in ` A ` ( ` 0 ` , ` 1 `) , along with the indices of the corresponding values of ` A ` ( ` 4 ` , ` 5 `) .
not sure how a numpy solution would compare to pypy ; but I highly doubt that the n log n character of has much to do with it . swapping an element in a C array is like what ; 100x faster than creating and hashing a python object ?
exp ( 100 ) is a big number . ill cook up a numpy solution for comparison
dunno ; if the time constant of the python solution is indeed that small , it may be faster . but that is more likely to be the deciding factor than the log ( N ) id say .
@USER : Basic complexity analysis says that sort has ` O ( n log n )` asymptotic complexity while inserting ` n ` elements in a hash table has expected complexity of ` O ( n )` .
its not unreasonable ; but log ( 10000 ) ~ 10 , and the speed difference of doing things in python versus C is typically much larger than that .
Even for large N , the extra log N need not be your primary concern .
This calculates the same thing as OP , but with the indices in matching order to the values queried , which I imagine is an improvement in functionality .
How to find elements ( and their indices ) where one of abbreviations is equal ** ( double asterisk ) ?
use ` collections.Counter ` , ` join ` the string list and then ` split ` it : #CODE
In numpy append multiple rows using for loop
How can I use a for loop ( or anything faster ? ) to add rows to x with zeros after , such as #CODE
and now concatenate that part to the old array #CODE
alternativly there is the resize function #CODE
It converts the file_number into string and concatenates it with the file extension ( the separator is the dot ) .
0350_randomstringN.csv , right ? and you want the ones with the same number to be read as if they were one file , right ?
What I'd like to do is simply dump that dictionary into the HDF5 .
I don't intend to change the data later on , it should just serve as data dump .
Second , while an ordinary Python ` int ` will return a consistent but nonsense result when compared with a sequence , NumPy types attempt to broadcast the comparison : #CODE
The standard way is to insert a unique ID between the elements you want compared and the elements you don't : #CODE
You'll need to make sure the elements you push in the Huffman tree creation routine have unique IDs , too : #CODE
However , realizing that the ` rfft `' s result is no symmetric like ` fft ` , and provides the result , in an ` N ` size array .
My general confusion arises from how ` rfft ` is related to ` fft ` , and how the mapping can be done correctly while using ` rfft ` .
The fft function makes no assumption on the input values ( they can have both a real and imaginary part ) and therefore no symmetry can be assumed in the output and it gives you a full output vector with N values .
In light of the above , your indices for accessing the output vector are out of bounds , i.e. > 512 .
Assuming the output stores ( n / 2 + 1 ) _complex values_ as n seperate _elements_ in a vector , instead of as a complex number data type , then one should just use the appropriate indices to calculate the magnitudes of the various bins .
Manipulating data from python Numpy array : Using values from one column to sum over adjacent value
I want to sum for each number in the second row here .
I want to sum all values in the first column that have the same value in second column .
For example a sum for each 1 in the second column would be : 2 + 7 = 9
Now , ` weights ` is multiplying the count by some weight which is in this case your first value in a list , essentially making a sum this way .
where 0 is the sum of first elements where the second element is 0 , etc ...
which is an array of sums , sorted by the second element ( because ` unique ` returns a sorted list ) .
resize a 2D numpy array excluding NaN
I'm trying to resize a 2D numpy array of a given factor , obtaining a smaller array in output .
your mod ( stride trick ) :
I got this error caus ' i don't know how to translate it in python : #CODE
In ` numpy ` indices start from 0 .
Anyway , you might want to look at this page #URL in order to translate you code \ thought patterns from Matlab to Numpy .
numpy histogram cumulative density does not sum to 1
" Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen ; it is not a probability mass function .
Well , indeed , when plotted , they don't sum to 1 .
" When I set the bins to 1 , of course , I get an empty chart ; when I set them to the population size , I don't get a sum to 1 ( more like 0.2 ) .
When I use the 40 bins suggested , they sum to about .006 .
Does the area sum to one ?
Get indices of array at certain value python
Do you want to use the indices to generate a polygon of the outline ?
With masked array you could then just make the manipulations that you want and the only elements that are used are the unmasked ones .
The negative axis indices indicate they are the last and second to last , ` axis =( 2 , 3 )` would be the exact same in your case .
All of the reduction ufuncs , e.g. ` sum ` , ` prod ` , ` mean ` ,..., can take an ` axis ` argument .
When calling with multiple axes , the final result is generated in a single shot , without those intermediate ones .
What does the log file say ?
What tests fail and what is the stack trace ?
How to compute an expensive high precision sum in python ?
I would like to compute the following sum .
In an effort to be efficient and only perform this calculation once ( since the absolute difference between X and Y is the same for both X and Y , this should only be calculated once ) ,
I need to find the absolute difference between every value of this variable , so I did , and put the results as they were computed together in a list of lists like this :
But notice how , since I'm working with absolute difference ( e.g. absolute_value ( a-b ) ) ,
And if you want to link to the intersphinx documentation page , use the module name : `` : mod : ` Intersphinx ` `` .
I will like save in txt a join array with the position in x ( 0 to 25 ) and y ( 0 to 43 ) and the value ( that is 14 values ) but eliminate the mased positions to free size .
x=0 , y=12 , max , min ...
In general you can use ` MaskedArray.compressed , but for your specific requirements ( to list the indices together with the data ) you could do it all directly #CODE
or , if you just want the paired indices as tuples you could use : #CODE
With ` i ` and ` x [ i ]` you can easily get anything you want ( and if you want the masked items , just drop the ` ~ ` in the expression using ` nonzero `) .
Regardless , using the ` np.nonzero ` function you can get whatever indices you want : masked , unmasked , before , after , etc .
I think you're asking to save the indices of the elements before they were compressed , and that's what my method above does .
witih respect the mask , I load the mask of a archive _mascara= np.loadtxt ( ' C :\ \ndfd\\wgrib2\\ mask.txt ') after this masked with max= np.ma.masked_where ( _mascara < 0.5 , max ); It is the same that your mask ?
Second question : my ` x ` is similar to your ` max ` after ` masked_where ` , in that they are both masked arrays ; but I don't really understand your question .
I have a list ( or it can be 1-D numpy array ) filled with zeros and ones :
Power spectrum of real data with fftpack on log axis
I already read many discussion about this topic ( comparison between lomb-scargle and fft , Plotting power spectrum in python , Scipy / Numpy FFT Frequency Analysis , and many others ) , but still can't manage it , so I need some tips .
I would like to plot the Power spectrum density showing a peak at this frequency ( on a log x-axis , possibly ) .
Two issues : 1 ) I think ` fftfreq ` only works with ` fft ` , not ` rfft ` ( which has a different frequency axis ); 2 ) ` rfft ` gives a complex result so plot ` abs ( f_signal )` .
You mean I should use ` f_signal = fft ( y )` instead of ` f_signal = rfft ( y )` ?
Either , ` fft ( y )` or ` rfft ( y )` should work since your signal is real , but since ` fft ` has the correspondence to ` fftfreq ` , ` fft ` is probably ( counter-intuitively ) the easiest way to get the frequency axis right .
So : 0 ) make sure the units are correct ; 1 ) zoom in on the x-axis to see the interesting part better ; 2 ) maybe use rfft ; 3 ) take the log if you want .
If ` rfft ` is used instead of ` fft ` ( and ` rfftfreq ` instead of ` fftfreq `) the same plot is reproduced ( in that case , the frequencies values , instead of the module , can be used numpy.fft.rfft )
( fixed indentation ) another thing you can change , to make it perhaps more readable , is instead of writing cArr [ 0 , 1:4 ] , write x0 , y0 , z0 ( and calculate that norm in one calculation , for each of those coordinate combinations , instead of calling linalg.norm 14 times , call it once ( with some broadcasting it should be possible to do , or with axis keyword or so )
The mathematics say that we should have VI * VH = 1 , and VH * A * V = VI * A * V = D , where D is the diagonal matrix of the eigenvalues .
This two work correct , since the off-diagonals are very small and on the diagonal we have the eigenvalues .
Likewise , the ( complex-valued ) matrix of eigenvectors v is unitary if the matrix a is normal , i.e. , if dot ( a , a.H ) = dot ( a.H , a ) , where a.H denotes the conjugate transpose of a .
Does this mean that ` np.power ` and ` ** ` are using floor division ( integer division ) for integer negative exponents ?
Suppose I have an arr = ( n item array ) and a matrix mat = nxm matrix .
I want to multiply each item of arr with its aligned row of the matrix mat .
You can do it with ` arr [ ..., None ] * mat ` ( or ` arr [ None , ... ] * mat ` to multiply along the other axis )
This code , where I create a the histogram with a subset of the DataFrame fails #CODE
If your matrices are that small , the difference between Levinson and ` det ` is likely not very big .
I'd just use ` det ` for this .
We need to speedup ` toeplitz ` and ` det ` calls :
in NumPy 1.8 , ` det ` is a general ufunc , which can calculate may det in one call .
I hope could get the values in b according to the position of the max value in each row in a , the expected output should be : #CODE
You can use the ` unpack ` argument to have ` genfromtxt ` return the transpose of the array .
So you use ` ravel() ` to flatten your 2D arrays : #CODE
You'll obviously need to reshape the output for plotting , e.g : #CODE
For something like a dot product , pandas ` DataFrames ` are generally going to be slower than a numpy array since pandas is doing ** a lot more stuff ** aligning labels , potentially dealing with heterogenous types , and so on .
I think scikit-image has a ` resize ` method ( not sure if it's faster though ): #URL
Using geometry , cos ( theta ) is x / sqrt ( r ) where r is the resultant displacement ( i.e. the hypotenuse of the triangle connecting Mars and the satellite ) .
Here , the size of each item in ` e ` is the same , so you could just use reshape : #CODE
The ` -1 ` in the call to ` reshape ` tells ` reshape ` to fill ' er up with whatever number is necessary to use all the items in ` M ` .
Instead of doing the math , you just put ` -1 ` , and let reshape do the work .
Can I truncate or " zoom-in " on a section of a pyplot figure before calling show , to avoid exceeding the complexity limit ?
Returning specific numpy label indices for an image
I'd like to segment my image using numpy's label and then based on the number of indices found in each label remove those which satisfy my criteria .
Now I'd like to retrieve the indices for each region which has a size greater than 121 pixels ( or one regionator size ) .
I'd then like to take those indices and set them to zero so they are no longer part of the labeled image .
Essentially something similar to MATLAB's regionprops or utilizing IDL's reverse_indices output from its histogram function .
Using the get indices process taken from here I've come to this : #CODE
Numpy then uses the elements of ` labels ` as indices to ` keep_labels ` and produces an array the same shape as ` labels ` .
I see that you want to keep the small ones .
It repeats the process until the sum of floats stored reaches a certain limit .
That allows ` log ( n )` lookups instead of needing to browse the whole list .
One obvious optimization - don't re-calculate sum on each iteration , accumulate it #CODE
As I understand it , you are just trying to sample uniformally from the elements in ` a [ 0 ]` until you have a list whose sum exceeds some limit .
Of course , if the sum of the sample of ` blocksize ` elements is lim then this will fail to give you a sample whose sum is > = lim .
You could check whether this is the case , and append to your sample in a loop if necessary .
Note that concatenating arrays is pretty slow , so it would probably be better to make ` blocksize ` large enough to be reasonably sure that the sum of a single block will be > = to your limit , without being excessively large .
I'm getting a ` ValueError : probabilities do not sum to 1 ` probably because I use a CDF , not a probability distribution but I'll try to work around this .
It appears that by setting it as a large number I could end up by pure chance with a sample that sums up to a _much_ larger number than ` lim ` , when I need that value to be as close as possible to it ( hence the condition to stop adding elements as soon as the sum passes the ` lim ` limit in my original code )
If it's a bit too big there will be some performance penalty because of greater memory allocation etc ., but the largest possible sum you could get out will still be <= to ` lim ` + the largest value in ` refs ` .
Generating a larger sample will still probably be cheaper than generating multiple smaller ones and concatenating them .
If you flatten the array I believe that leastsq will minimize the sum of squares of residues .
Whereas your objective function ( get_mean_distance ) is basically sum of euclidean distances of measured to fitted points .
` import ` won't dump the contents of the module into the current namespace ; you need to refer to ` numpy.whatever_thing_you_want_from_the_module ` .
( Basically adding a small diagonal matrix to Cov . Here delta is a small float and k is dimension of Cov )
That means a zero or near-zero diagonal element , so inversion is impossible .
It's more complex than just looking for zeroes on the diagonal .
For arguments that are passed as scalars , they should then be broadcast as a numpy array of a specified length .
RuntimeWarning : divide by zero encountered in log
I am using numpy.log10 to calculate the log of an array of probability values .
There are some zeros in the array , and I am trying to get around it using #CODE
` numpy.log10 ( prob )` calculates the base 10 logarithm for all elements of ` prob ` , even the ones that aren't selected by the ` where ` .
If you want , you can fill the zeros of ` prob ` with ` 10**-10 ` or some dummy value before taking the logarithm to get rid of the problem .
Color-coding a histogram
I would like to depict the distribution of x with a histogram in MATPLOTLIB using hist() .
Now , I would like to color-code EACH bar of the histogram with a color that represents the average value of y in that set with a colormap .
The " right " option of digitize is available on my Ubuntu machine , but not on my Mac ...
See #URL you can replicate the function of ` digitize ` with one pass through ` x `
As ` ravel_multi_index ` may not be the fastest algorithm ever you can look into taking the unique rows of a numpy array .
Because if we take those indices from ` data ` , i.e. , ` data [ data [ data [: , 0 ]= =0 , 1 ]= =0 ]` we do not get the rows where the first and second number are ` 0 ` , but the ` 0th ` and ` 3rd ` row instead : #CODE
Additionally , it works fine even if your " data " ndarray has indices which are out-of-bounds for your result matrix .
If it's a binary nxn matrix then the determinant is integral , and the maximum absolute value of the determinant for 10x10 is pretty small ( 320 , I think . ) In practice you're probably safe .
I mean smallest absolute value .
@USER I did and I don't see any determinants with absolute value less than 1 except for zero .
@USER : as I said , the determinant is an integer , and 0 is the only integer with absolute value smaller than 1 .
What I said is that the maximum absolute value of the determinant is small , which suggests that no intermediate calculation is going to introduce enough error to invalidate the result .
The determinant is then 0 if one element of the diagonal is zero and nonzero otherwise .
I would check how det() is implemented and maybe , there is no issue with numerical noise , and , in fact , you could just test for det ( M ) == 0.0 and get neither false negatives nor false positives .
Checking det ( M ) == 0 definitely doesn't work .
As the entries in the matrices are either 1 or 0 the smallest non-zero absolute value of a determinant is 1 .
In fact you could do abs ( det ( M )) < 0.5 to test for singularity !
how about test batches while playing with the tolerance arg , then decide on the max acceptable tolerance , rinse and repeat :
Note that ` np.median ` requires a flattened array , hence the reshape .
and my code can not be completed because in optimize.py argument x0 flatten and my vectorising is completely broken ( line 822 in 0.13.2 scipy version ) .
My theta ( it is 2-D array ) should be flatten and all vectors ( 2-D arrays too ) in code should be flatten , so code can work with minimum changes .
The expression ` f ( t ) *exp ( j*w0*t )` is point-wise multiplication of ` f ( t )` and ` exp ( j*w0*t )` , not convolution .
If you scroll up a bit in the link I mentioned above , you'll see a description of the " Time shift Property " , which says that point-wise multiplication in the frequency domain by the complex exponential ` exp ( -j*w*t0 )` gives a shift of ` t0 ` in the time domain .
I'd like to define a function in Python which itself uses a function from numpy , namely , the sine function ' sin ' .
However , if I run the " use_sine_test.py " Python script using the run button , I get a " NameError : global name ' sin ' is not defined " error as shown below .
return sin ( theta )
in other words , with the function definition and the script running it in one " .py " , but I still get the same error message ( " global name ' sin ' not defined ") .
I will upload the histogram image of sample data .
When one works with 2D data , one can only transpose result from h5read #CODE
Every numpy array has a set of strides that tells you how to find any given element in the array , as a simple inner product between strides and indices .
What is however impossible to imagine , is indexing an array as arr [[ 2 , 0 , 1 ]] , and representing that array as a strided view on the same piece of memory . arr [[ 1 , 0 ]] on the other hand could be represented as a view , but returning a view or copy depending on the content of the indices you are indexing with would mean a performance hit for what should be a simple operation ; and it would make for rather funny semantics as well .
For your latter example Numpy is not able to return a view because the requested indices are to irregular to be described through ` shape and strides ` .
My problem is that , depending on the starting point the solutions change and I am not sure that the ones that I found are the most reasonable .
So I have an numpy.array of unknown dimension and I generate a list of indices in order to access a specific element within the matrix .
I wish to set the matrice " m " with two items from each tuple ; the first item , item1 , is each item in the outer tuple [ [( item1 ( 0 , 0 )) , ( item1 ( 1 , 1 )] [ .... ( item1 ( 3 , 5 )) .... ] .. ] and the second item , item2 , is on indexposition [ 1 ] in each inner tuple , for instance ( 3 ( 3 , item2 )) .
Are ` b ` and ` c ` the indices ?
Do you want a specific probability distribution for a row to have 1 , 2 or 3 ones ?
It is not the final answer , but maybe it can help you find an alternate solution using random numbers and permutation .
For the fixed number of ones per rows solution , I also get ` [ 1 , 2 , 1 , 0 , 0 ]` - could the trick be then to use ` > 0 ` masking ?
Rewrite the ` yy ` -part to ` 1-ax ( 1+g ( x ))` and the then part inside the sum to ` 1-yy /( 1-ax ) = 1- (( 1-ax ( 1+g ( x )) /( 1-ax )) = 1- ( 1+g ( x )) = -g ( x )` .
What about inv ?
my_inverse_array = inv ( my_array )
I wonder if theres a way to reshape it back to its original state and then inverse it ?
You will either need to change the way you're generating matrices , or skip the ones that aren't invertible .
' Module ' object has no attribute inv ...
svd in python and R
When I run svd in R it takes few minutes but than I have the result .
Is there some sort of bug in svd in python ?
I used svd different time in python .
My code is only 1-read the matrix 2-run svd .
Error when trying to sum an array by block's
I have a large dataset stored in a numpy array ( A ) I am trying to sum by block's using :
` numpy.add.reduceat ` requires an array of integer indices , but ` numpy.arange ( 0 , A.shape [ 0 ] , 5.0 )` returns an array of float64 values .
Will the reshape or the transposition be faster / what impact will they each have on memory usage ?
Rather than asking us to translate MATLAB code , most of the community would probably be more appreciative if you first explained the problem you were trying to solve .
I would like to find the smallest ( in absolute value ) non-zero eigenvalue of a matrix exactly .
Also , an interesting observation is that the LARGEST eigenvalue of matrix A is equal to the norm of this matrix ( square root of sum of squares of its elements ) .
The SMALLEST eigenvalue of A is equal to 1 divided by the norm of the INVERSE of A .
So , if your matrix is not very large and its inverse exists , you can afford to invert it , then just do it , and compute 1 / norm ( inv ( A )) .
And I don't think the last paragraph is true : [[ 1 0 ] [ 0 2 ]] has norm ?
Perhaps the norm bounds the eigenvalues ?
Matlab gives me a norm = 2 for your matrix .
You refer to the Frobenius norm , which is sqrt ( 5 ) .
I was not correct when I said that the largest eigenvalue is equal to the " square root of sum of squares of matrix elements " .
h ( x ) = y ' ( t ) - integral ( sqrt ( u ( t )) *y ( t ) + y ( t )) = 0 # a nonlinear differential equation
I've tried to translate the problem into python code using openopt and scipy :
y '' ( t ) - sqrt ( u ( t )) *y ( t ) + y ( t )) = 0
` s ` is a vector containing however many elements I wish ( ` [ s0 , s1 , s2 ,...., sn-2 , sn-1 ]`) , and I intend to then create an array ( initial full of zeros ) that is of the same size as ` s ` .
You don't need to create an array of zeros first .
But for this function in particular , I want to create an array of zeros initially ; and then change the zeros .
And this list is a list of ` indices `
I am attempting to perform the calculation ` X.transpose() * W * X ` where X is a 2x3 dense matrix and W is a sparse diagonal matrix .
How does one index a numpy array using another numpy array that holds the indices ?
I would like to select the 1 , 2 and 3rd row of this array , using the indices in this other numpy array , called ` idx ` :
How does one index a numpy array using another numpy array that holds the indices ?
I've seen people use ` sqrt ` from ` libc.math ` to replace the np.sqrt , so I suppose my first question is , are the similar functions for the other methods in ` libc.math ` that I can use ?
This is no surprise as you're using the Python built-in function ` sum ` in both the Python and Cython versions .
is really a vector dot product , i.e. #CODE
I want to subdivide one array into a few smaller ones , they contain the start and stop information of several peaks and need to be saved in separate chunks #CODE
It returns " Unknown mat file type " .
As you can see , the intercept / refract methods always use ray.p() so the newest point , but for some reason it doesn't actually append the new point / direction upon intersecting and refracting with the second spherical element ?
No , because the issue is that the default is shared , so that subsequent instances will be affected by previous ones .
In that same file , ` sum ` is defined as : #CODE
A boolean sparse matrix could easily be written to compress whichever value ( True or False ) constituted the majority .
Yours is degenerate because it only works for matrices that are all zeros or all ones .
Yes , a boolean sparse matrix that can compress either True or False would do the job .
` tet ` contains numbers ( indices ) that refer to the values in ` vertices `
The values in ` tet ` are indices of simplices .
If the target is in a simplex , tet contains one of the 1965 indices , if not , ` tet ` contains the value ` -1 `
a list of indices
How can I replace the values of ` tet ` ( that are indices of the values in ` vertices `) by the values contained in ` vertices ` ?
You might want to edit your question with the exact error , including the [ stack trackback ] ( #URL ) , so we can help you figure out what's going wrong .
More in detail I'm working right now with a Taylor sum series program ( already coded ) and about this line results , corresponding to numpy function : #CODE
I think you could use the Python built-in ` json ` package and the ` dumps ` function
generate unique serie of numbers in range ( 1 , 2 , 3 ... 99 ; 1 , 2 , 3 ... 99 ; ) with no duplicates
I managed to do this but the code looks pathetic and i failed in some tasks , for example by skipping series with duplicated / non unique numbers in an elegant way , or creating one single function that takes the length of the serie as parameter ( for example 3 for 1-99 , 1-99 and 1-99 , 2 for 1-99 and 1-99 and so on ) to avoid handwriting each serie's function .
Numpy : 2D array access with 2D array of indices
and another which is a matrix of data to access at these indices #CODE
and I want to able to use the indices of a to get the entries of ` b ` .
when I would like to use the index pair in the last axis of ` a ` to give the two indices of ` b ` : #CODE
Is there a clean way of doing this , or do I need to reshape ` b ` and combine the columns of ` a ` in a corresponding manner ?
need to reshape the result back again though : b.reshape ( a.shape [: -1 ])
What seems strange to me is that if I ' manually ' apply the actual convolve algorithm to the matrix , it appears that the result instead should be shifted 1 step right ...
[ EDIT ]: After Hivert's answer , in order to understand it , I implemented my own ' convolve ' , according to my previous understanding of the convention for the matrix manipulation : #CODE
This code produces a ' mirrored ' result wrt the real convolve function , that is , if I have the identity kernel shifted left , then this code will shift the image to the right , vs . the real convolve function will shift to left and vv .
How exactly are you calling convolve ?
I did som further experiments with convolve , thanks to the input from hivert , and discovered that if if flip the kernel both vertically and horizontally , my ' poor-mans-convolve ' produces identical results to that of scipy .
I am having trouble making a scatter plot with a log scale for the values .
But if you think it matters ; all sparse matrices store their indices internally in some way or another , which you can access directly without the conversion to a dense matrix
That's not a big deal , though , because we just need the nonzero entries in the sparse vector .
Get those with ` index.nonzero() ` , which returns a tuple of ( row indices , column indices ) .
We only care about the column indices , so we use ` index.nonzero() [ 1 ]` to get those by themselves .
Then , simply index with the array of nonzero column indices and you're done .
This should be the most basic and commonplace thing fft is used for .
After running the ` Wlib.GetImage() ` , Python dies without any errors , the Windows stopped working error comes up , and I can't make any sense from the Windows text error log .
Assuming 64-bit , ` 0x0000000000000064 ` is the value for ` rbeg ` on the call stack .
B is a filled array inizialized with numpy zeros function #CODE
And if I want to reshape C ?
You can reshape C by doing ` C = C.reshape (( N , 1 ))` , or ` ( -1 , 1 )` and numpy will figure out N for you , but know that ` trapz ( B , C )` in that case will return an ( N , ) array .
It takes 0.02 seconds for Matlab to compute the inverse of a diagonal matrix using the sparse command .
In the matlab code you start to count the time after creating the diagonal and converting the matrix to sparse .
I am pretty sure that the time you spend creating a 2D array of size 10000x10000 , storing the diagonal , and converting it to csr is much biger than the time you spend inverting the diagonal .
You are witholding crucial information from your software : the fact that the matrix is diagonal makes it super easy to invert : you simply invert each element of its diagonal : #CODE
Of course , this is only valid for diagonal matrices ...
Exactly , this is only valid for diagonal matrices , therefore the answer does not really work .
When you run inv in matlab for a sparse matrix , matlab check different properties of the matrix to optimize the calculation .
For a sparse diagonal matrix , you can run the followin code to see what is matlab doing #CODE
So matlab is inverting only the diagonal .
If matlab is taken advantage of the diagonal structure of the matrix , but scipy is not ( of course scipy is also using the structure of the matrix , but in a less efficient way , at least for the example ) , matlab should be much faster .
To be sure , as @USER .Escondido propossed , we will try a minor modification in matrix A , to trace the matlab procedure when the matrix is not diagonal : #CODE
Also , I might have hundreds of dictionaries that I want to " concatenate " .
It does not support ` rand `
NumPy rounds the values to ` 0 ` , then finds that since this is a sparse matrix , the zeros should not be explicitly stored .
The linked vertices you can get via the indices in the ` simplices ` attribute .
I'd like to normalise all columns to sum to 1 , resulting in : #CODE
Note that in the case where the sum of a column is 0 , I'd like them to be equally distributed as you see above .
How to efficiently sum the upper left coner of a matrix using numpy ?
Or without pandas , you can get a unique set of years to loop over and use numpy.mean ( numpy.where ( data2 [: , ] == )) .
Since your data seems to be sorted , you should be able to use ` np.where() ` , which will give you the indices of an array that meet some criteria .
So ` rand ` seems to simply be a convenience function
Is there a more efficient way or even a method to load a 2D ` numpy ` array ` data [ n , m ]` into three 1D arrays ` X [ n*m ]` , ` Y [ n*m ]` , and ` Z [ n*m ]` than looping over the indices ?
` X ` and ` Y ` seem to be rather boring wastes of space with indices in them .
fastest way to get NetCDF variable min / max using Python ?
My usual method for extracting the ` min / max ` of a variable's data values from a NetCDF file is a magnitude of order slower when switching to the netCDF4 Python module compared to ` scipy.io.netcdf ` .
The notable difference is that I must first flatten the data array in netCDF4 , and this operation apparently slows things down .
Why do you have to flatten the array ?
Something like : ` ncwa -y max ...
Something like ` eigvals , eigvecs = la.eigh ( mat )` ` principal = eigvecs [: , eigvals.argmax() ]` ` if ( principal > = 0 ) .all() or ( pricipal <= 0 ) .all() : print ' all the same '` ?
Probably easier to text ` mat ` in the first place .
You can easily check that the eigendecomposition of your symmetric matrix ` MM ` is correct , if you exploit ` MM == QQ*DD* QQ.T ` , with ` DD= diag ([ lam_1 , lam2_ ,... ])` being a matrix with the eigenvalues on the diagonal and ` QQ ` being the matrix of eigenvectors : #CODE
digitize array to several lists of bins
So I need to tell digitize , that for the first array element it should check into which bin of ` list_of_my_bin_lists [ 0 ]` that element belongs , for the second element with ` list_of_my_bin_lists [ 1 ]` and so on .
Scipy.linalg.solve max array size
This sum should get inserted back into the center of the 3x3 window , with the original surrounding values left untouched
I did look at some of the ` convolve ` functions , but they didn't seem to do quite what I was after .
I understand what's going on with the ` arr ` indices , though coming from the ` c ` family of languages it's a little crazy one can do that with array indices !
to return indices of where the lines crossed , and which functions were involved .
numpy : Efficiently avoid 0s when taking log ( matrix )
Sure masking away the zeros will prevent ` log2 ` to get applied to them , won't it ?
How can I efficiently compute the element-wise log of a numpy array without getting division-by-zero warnings ?
And disabling the specific ' divide by zero ' warning does not disable the other problem with calculating the log of a number , which is negative input .
` q=array ([ 0.0 , 1.0 ])` and ` log ( q.astype ( float ))` raises the RuntimeWarning on my machine .
2 ) Disabling the specific ' divide by zero ' warning does NOT disable the other problem with calculating the log of a number , which is negative input .
Is there a faster method to populate the diff vert part ( which takes longest ) and / or the distance part which is also quite time consuming ?
Also , trying to do it just on indexes to the verts causes me other issues with further calculations when trying to get back to some boolean tests ( i.e. this is only one of a set of calculations so keeping at the tri point level works best .
In this example through the basic set of loops I'm going through ( ie with all the groups ) the setting up of the diff verts part is about 60-70 % of the time v the actual distance part being about 30-40 %
However , doing the calculations for 3 types of angle calculations also on each tri point are roughly a tenth of the speed .
b ) the actual distance and c ) a Boolean array triverts*ttriverts in size to say which ones were valid ?
In the end the time using this was approximately the same as just the vertdist calculation ie excluding the diff verts calculation .
That was why i was wanting to do it in a series of tests and why at least I thought doing it at the tri point level was better .
EDIT : As @USER mentions in the comment , you can work around this limitation by adding an ellipsis even when all indices are specified : #CODE
I wish to create a histogram , where on the X axis there would be numerous bars , for example one for every .25 of data , so four bars , where the first has the value of 0-0.25 , the second 0.25-0.5 , third 0.5-0.75 and fourth 0.75-1 .
The error comes from the ` N ` variable , which is the number of bars in the histogram .
It allows you to put an error range on top of each of the histogram bars .
Is there an efficient way to dump from RAM onto a disk temporarily ?
I then dump them as arrays by the approximate code : #CODE
As far as I have understood , h5py's ` .value ` method reads an entire dataset and dumps it into an array , which is slow and discouraged ( and should be generally replaced by ` [( )]` .
Now that I think of it , I had to manually convert array indices to ` Slice ` objects once to speed up an HDF5 read .
Was the Slice approach significantly faster than array indices ?
Simple slicing ( slice objects and single integer indices ) should be very fast , as it translates directly into HDF5 hyperslab selections .
The expression ` file [ ' test '] [ range ( 300000 )]` invokes h5py's version of " fancy indexing " , namely , indexing via an explicit list of indices .
We use ` numpy.choose ` to generate an array of payoff selections and multiply that with an array of absolute differences between ` 0.5 ` and the prediction values , then sum .
python log n choose k
Is there a function that stays in log space ?
I don't see it in scipy.misc , and it just feels wasteful to convert to normal space and then back to log .
@USER A memory efficient version wil be ` sum ( not np.isnan ( x ) for x in a )` , but in terms of speed it is slow compared to @USER numpy version .
Is there a way to define the sum of a NoneType and float to be NoneType and keep it in the new array ?
then M [ i , j ] += 1 instead of M [ i , j ] = sum ([( i*j ) *2 ]) , it will return 1 in the columns where h and n co-occur and 0 in the rest .
I'll have to look into custom classes with already-existing ones like NumPy arrays .
The above code works , but would be better to append the data_array retrieved from for loop directly into the numpy array rather than using python list .
Its first and second column are longitude and latitude , so integer , and the other four columns are names and ids , so they may be defined as string .
At this time I am considering only one group ( id == 19 ) , but I will want to do this for all the other ids .
@USER The reason I do not do it that way is because I have hundreds of ids and most of them will be considered later , although I am only considering one id .
Is there a build in task in scipy / numpy ( or any other package in principle ) that uses sinc interpolation ( #URL ) ?
integer - The color of the top-left pixel ( floor function ) .
thanks for the suggestion but I would prefer a native python solution if possible , also if I am not missing something ( which is possible ) these interpolation methods do also not include a sinc interpolation
This will result in a hyperlink ( with the text " Polynomial ") from the documentation of your ` quad() ` function to the documentation of ` numpy.polynomial.polynomial.Polynomial ` .
When I run it , the server displays the following log ( in production this may go to a logging file ) .
My ` bad() ` function does not appear on the stack because it ran successfully .
Then you can check whether a given word exists in the list in O ( W ) time . where W is the max length of the word .
Given that L i.e. number of 150K lists is much smaller than 150K and W is much smaller than 150K no set join will ever be as fast as a Trie comparison .
The problem is cause I have a 2D array that stores some data and , in my code , I have , e.g. , to calculate the gradient for each position value , so I create another 2D array where each position stores the gradient for its value .
In a specific method , I have to calculate the gradient for this data , which is a 2 x 2 matrix ( cause of this I would like to use ndarray , and not a simple array ) , and , to do this , I create another instance of this object to store this new data , in which each point ( pixel ) should store its gradient .
I assume that the default value of some special option ( like ddof in std function ) differs between Python and matlab but I can't find it .
How do I put together a function and a Polynomial with a division operator ?
And I have the following NumPy Polynomial ` q ` : #CODE
As you can see , I want to sum the mixins as shifted by the xs .
" While this might seem to be impressive relative speedups , the absolute speedup from going down this route only allowed us to save some milliseconds of CPU time .
python zeros array based on another : same shape but longer along one dimension
I have variables which number of dimensions can be any value greater than 2 . for each variable I would like to initialise a new variable with zeros with the same shape but longer along the first dimension , i.e. 10 times longer .
IIUC , you can simply concatenate tuples : #CODE
The stack trace is : #CODE
What's wrong with ` sum ` ?
` sin ` will give a float after all .
my problem is that sum will give me a single number whereas I am expectin an array as the summation contains q and q varies [ 0:1 : 90 ]
Note that in Numpy's sum function , you have two parameters : #CODE
The axis parameters tell it to sum in only sum of the dimensions .
Meaning , if you sum along the ` q ` and ` j ` axis , you can still have a vector result in the ` q ` axis .
You can easily generalize it to include ` f_i ( q )` ( a 2D array of the same structure ) , add ` sin ` , etc .
If you can make arrays of ` f_j ` and ` f_i ` , you can probably vectorize the summation , by utilizing array multiplication and the numpy version of sum . which allows you to define what axis to sum over .
I just want to ensure that the parameters I pass into my Logistic Regression are the best possible ones .
But in sklearn they divide the lambda in order to make the coefficient of sum of square of weight equal to 1 .
You can change the NaN values to zero with Numpy's isnan function and then remove the zeros as follows : #CODE
Except you mean I have to creat an condition for the programme to ignore all the zeros ?
If you want an intersection between the two arrays you can loop ; for i data : and get i from first array , and i from second array.But I'm not sure if I follow it correctly , you have some data which has 0 occurences in some columns of your array , if you append the other values to a new array the memory of where in the data those values came from is already automatically stored .
Fast inverse and transpose matrix in Python
Now I want find the inverse and transpose of matrix ` A ` : #CODE
For a tensor it is not clear how to define an inverse or a transpose .
If you want to inverse / transpose a 2-dim array of matrices you might want to look at numpy's tensorinv .
Numpy has the ` array.T ` properties which is a shortcut for transpose .
for the transpose :
to transpose the second and third dimensions ( while leaving dimension 0 and 1 the same )
and it will know that the last two dimensions are the ones it is supposed to invert over , and that the first dimensions are just how you stacked your data .
for the transpose : your method needs 3.77557015419 sec , and mine needs 2.86102294922e-06 sec ( which is a speedup of over 1 million times )
This will simple call inv for each 3x3 matrix in the array ; but the overhead of calling inv without exploiting its 3x3 nature of the matrices may be substantial ; I suspect it would be slower than the solution I posted ; but itd be nice to try .
Your idea of distinguishing memmap by its special attributes works perfectly :D I take the " flush " method as a condition to determine whether it is memmap or not .
Given this adj . list , I would like to select a submatrix which has identical row and column indices that is given by : #CODE
I am reading data ( pixel values to be exact ) from a h5 file and plotting the data in a histogram using numpy .
Im creating a histogram based on a min and max that I set manually ( -40 and 20 respectively ) so the no-data value doesn't show up in my histogram - which is fine .
What is interesting here is we take ` b ` which is a boolean array and convert it into indices so that the normal ` np.take ` operation can work as advertised .
If we examine how long this takes we realize ` nonzero ` is the majority of the total time and fancy indexing takes less then ` nonzero ` : #CODE
The docs explains that even fancy indexing creates an array of indices using some sort of ` nonzero ` like operation ; however , fancy indexing does not involve a python abstraction layer .
And extract could probably be rewritten to use it ( though for non-boolean arrays the nonzero logic may be better ) .
Advanced indexing starts here #URL the simple boolean one is handled earlier though ( also the nonzero calls for non-simple , it is in the prepare_index function ) .
The MATLAB command ` Vi = interp3 ( x , y , z , V , xi , yi , zi )` would translate to something like : #CODE
First sum the indices and orientate : #CODE
Minor comment : in my question I defined ` e_1 ` with ` max ( elem [ 0 ] , 1e-10 )` ( same for ` e_2 `) to avoid numerical errors if either one was zero .
The ` cholM ` matrix is triangular , so very small values on the diagonal would certainly affect the solution , but this is not occuring .
Is there a difference in accuracy using ` inv ` versus ` solve ` ?
There is also a difference between inv , solve , and direct back substitution .
The error in inv seems greater ( as expected ) , but solve and back substitution seem about the same .
numpy finding indices of good values
I want to find indices of elements equal to a set of values .
` == ` doesn't work like that ; when given two arrays of different shape , it tries to broadcast the comparison according to the broadcasting rules .
Some NumPy functions , like ` np.subtract ` , ` np.add ` , ` np.multiply ` , ` np.divide ` , ` np.logical_and ` , ` np.bitwise_and ` , etc . have an " outer " method which can be used to create the equivalent of " multiplication tables " : #CODE
Try a dictionary where the key is the date and the result is an array that you append to .
Then you stack them in z-direction using ` np.dstack ` .
But remember , this is 1D array , so we reshape it to our original image shape using ` X.reshape ( img.shape )` method .
I didn't like having to strip the index of its name , and transposing the values etc . seemed cumbersome .
( 2 ) Collect the parts of the SQL command in a list and do a ` str.join ` in the end , to avoid allocating an increasingly long string each time ( you can't really append to a string in Python as they are immutable ) .
I eventually figured out that they didn't link to the math library where all those symbols where defined ( log10 , sqrt , sincos etc . ) .
Can't see what's missing without a look at the install log .
use numpy to join two columns
I would like to join these the two columns in a third file to be one column like this :
I delete an entire row and after doing so , I want to append the deleted row to the reduced matrix .
Now I want to append the deleted row so that it once again becomes 3x3 .
Note the use of ` np.newaxis ` is necessary to make the single-row array A1 of the same shape as the array to append ( because ` np.append ` requires arrays to have the same number of dimensions ) .
I do know what slicing is but I don't manage it enough to translate this block into numpy arrays .
will return an array of indices , the same length as ` b_lst ` , holding before which item in ` a_lst ` they should be inserted to preserve order .
If so , ` w , vl , vr = eig ( limT , left=True )` .
) The eigenvectors returned by ` eig ` are not unit eigenvectors .
Unit means the sum of the entries is 1 .
Basically you compute your desired poles and zeros , break them up into complex conjugate pairs , and compute the transfer function for each pair .
To see the benefits of this , you need to use ` z , p , k = butter ( output= ' zpk ')` and then work with poles and zeros instead of numerator and denominator .
you have at most 4 in that dimension ( see your reshape line ) , so the index it will count are 0 and 2 ( 1 and 3 are skipped , and 3 is the last element ) .
A vector y satisfying dot ( y.T , a )
I.e. you need to transpose the vectors in ` vl ` .
Thanks Lemming , a question ; I was told to scale my lsh eigenvector by >>> s =vl [: , 1 ] / sum ( vl [: , 1 ])
Because , if you look closely , you'll see , that the sum is nearly zero , hence the result of this scaling is not going to be very meaningful .
If you meant to do ` s=vl [: , 1 ] / sqrt ( sum ( vl [: , 1 ] **2 ))` , then you would be normalizing your eigenvector ( I.e. making it a vector of length 1 ) .
Note , that the eigenvectors given by ` eig ` are already normalized .
@USER Also note , that ` numpy ` already comes with a ` norm ` function ` numpy.linalg.norm ( vl [: , 1 ])` returns 1.0 .
On the scale , I need to use a lhs unit vector so I was trying to scale the sum of the vector to 1 ( I never used a unit vector before , but that is my guess at one is ) I am posting another that involves vector matrix multiplication .
I am searching the form that mask values > 998 and that point is transparency to matplotlib and masking when calc max and min .
Python : Divide values in cell by max in each column
( ` sum ( elements ) ! = 0 ` , what if it is -2 -1 0 1 2 ? That should be result in -1 - 0.5 0 0.5 1 , right ? ) #CODE
Else , it's cell / max ( in column )
If I defined a transpose function / wrote one myself .
But , what happens if we want a division by max value in each row ?
The key : transpose In Numpy : .transpose() or .T
append C :\ libav\usr\bin\ to the ' Path ' environment variable
Presently I find the max ( since max function returns one index ) of each row and set it to zero .
It works if all rows have at most 2 ones but if more than 2 then it fails .
I'd like to use ` np.argsort ` to get the indices of the non-zero elements : #CODE
The output of ` np.where ` is a tuple of three 1D arrays , each giving the indices along a single axis .
If the values are all unique , the ` ser.value_counts() ` is no longer needed .
That part is slow ( Fetching unique values ) .
The crux is that , all indices are unique but the ` values ` can have duplicate values .
It works fine in this example because all the values in ` ser ` are unique .
numpy concatenate two arrays vertically
Because both ` a ` and ` b ` have only one axis , as their shape is ` ( 3 )` , and the axis parameter specifically refers to the axis of the elements to concatenate .
this example should clarify what ` concatenate ` is doing with axis .
You can still do it with concatenate , but it takes longer : #CODE
Still , ` concatenate (( c , d ))` works here .
I tried identifying the unique columns first with this function : #CODE
You're close , you've found all the unique columns .
` ind ` has all the indices you want , but by indexing ind you're just taking one value instead of all of them .
Once the duplicates are all together , you can easily tell which columns are duplicates and the indices that correspond with those columns .
This method requires that the axis you want to unique is contiguous in memory and numpy's typical memory layout is ` C ` contiguous or contiguous in rows .
Plus I do not see lexsort being faster then ` unique ` for a large number of rows .
It looks like all he wants to do is get the indices where an array is equal to a set of integers .
Finding indices given condition in numpy matrix
Now I want to find the indices , where all values are greater than 50 ( not the maximum ) .
Other than iterating through , checking each value and keeping track of indices for values which obey condition and then returning these pairs - how do you do this efficiently ?
which makes it easy to distinguish the large from the small , but now the eyes boggle looking at too many zeros .
I think problem in this code in the following #URL = zeros (( numberOfLines , 3 ))
Its the line circles = cv2.HoughCircles ( img , cv.CV_HOUGH_GRADIENT , 1 , 10 , np.array ([ ]) , 100 , 30 , 0 , 0 ) , where the lat two parameters indicate min and max radii .
To find what to enter in place of `' your-api-key '` , log in at #URL and then visit #URL
And , it is posible take the midle of the max and the min values an put another 10 values .
I edit the answer and include one more petition , if you dont mind that take 10 values between max and min , in the middle take 10 most near .
I will like mask 10 values in the middle of array and mix with 10 max and 10 min values .
logical premise 10 max and 10 min #CODE
However , since my x axis is log , the lines don't look very linear .
mask is Array masked inside only the 10 min element and 10 max inside with a mask TRUE .
On the left I wanted get a histogram oder density function to where eigenvalues occur and how often .
The histogram is done with numpy : #CODE
Additionally to the amount of eigenvalues in each bin I wanted the histogram still to show clearly where eigenvalues occured and where they did not .
It seems that either the histogram or the colormap rounds small values to zero because I get a way too small blue region in the histogram plot .
Ideally , I would like to input 1 and return some collection of 1's and zeros , that I can put into numpy .
I was under the impression that since the absolute values of all elements inside this matrix are less than ` numpy.finfo ( float ) .eps ` , where ` 1.0 + eps = 1.0 ` , this should return ` True ` .
I want to create an array that contains the indices of a subset of ` a_arr ` which steps through the array , selecting elements but ignoring elements that aren't spaced at least ` thresh ` away from the last selection .
Select the indices of the numbers that are the first element in their bucket .
This gives you all the indices except for the first zero , which is always present .
` cumsum ( bincount ( ... ))` counts the size of each group and converts them into indices .
Alternatively , if the order of indices doesn't matter , you could exploit this to get your zero index back : #CODE
Setting the sum of the elements equal to 1 then gives you the unit eigenvector .
Setting the sum of these equal to 1 will then give you the unit eigenvector elements .
Unit vectors you get by dividing with the length ( the sqroot of the sum of the squared elements ) .
s=vl [: , 1 ] / sqrt ( sum ( vl [: , 1 ] **2 )) like this ?
Thanks deinonychusaur , 1 ) How is v1Unit = np.linalg.norm ( v1 [: , 1 ]) different from v1Unit [: , 1 ] / sqrt ( sum ( vl [: , 1 ] **2 )) ?
Also , your last comment is wrong : ` reshape ` needs to preserve the element count .
bc = rem ( floor ( dc*pow2 ( - ( t_length-1 ): 0 )) , 2 );
The reshaping ensures that the bitwise and operation is performed as a sort of ' outer ' operation : we take the bitwise and of every element of the original ` arange ` with every element of the ` powers_of_two ` array .
I am not sure if this is exactly what you are looking for , but ` reduce ` from ` functools ` can be used here to accumulate a state that depends on the " past " values of your input list .
If you wish , you can append a ` [ 0 , 0 ]` to match your desired output in the comments .
It also occured to me that you could change your state from ` [( max1 , diff1 ) , ... ]` to ` ( lastMax , [ diff1 , diff2 , ... ])` and avoid keeping a history of max values for the entire list .
Matplotlib log scale tick label number formatting
With ` matplotlib ` when a log scale is specified for an axis , the default method of labeling that axis is with numbers that are 10 to a power eg .
They can be allocated on the stack and are easier for the C compiler to optimize , but they will be more difficult to access outside of Cython .
The ptr_time() version is likely slower than carr_time() as you used , range ( size ) rather than range ( 10000 ) , on x86 the compiler has 4 register before using memory , i.e. eax = i , ebx = AP , ecx = j , edx = n , nothing left for size , hence extra memory operation occurs , size is dword [ ebp-8 ] , ebp is the stack pointer . in carr version , the code is otherwise the same except the loop compares to a constant .
Since you already have a numpy array , to avoid for loops , you can use ` reshape ` and consider the new dimension to be the bin : #CODE
Just use ` reshape ` and then ` mean ( axis=1 )` .
The log-bin values should lie on the same line as the original ones in the thin part .
I want to append ` n ` copies of ` B ` at the end of ` A ` : #CODE
First lets check out ` concatenate ` routines : #CODE
Note that this is only the case for small arrays , ` append ` , ` hstack ` , and ` concatenate ` should be asymptotically convergent as all of these functions call ` concatenate ` , the main difference is python overhead .
By default , it's all ones .
If you want to know where each value is coming from , then first use ` np.argsort ` , then unravel the flattened indices : #CODE
Will I have to transpose my matrix after running the code as I want the columns to only be affected which seems to be the transpose of your final array .
I have an array ` a ` of data , and another array ` b ` containing indices along dimension 2 of ` a ` .
When you fancy-index a multidimensional array with multidimensional arrays , the indices for each dimension are broadcasted together .
I am having a problem getting my real program to stack by column - it only appends to the rows .
I would like to column stack those sets of data if they are of the same length .
After the stack ( of 2 of them ) what do you want ?
Note that ` vstack ` does ` concatenate ([ atleast_2d ( _m ) for _m in tup ] , 0 )` .
I'll try a reshape on the data or something and see what I can get to happen .
Note also that you can take any result modulo 1 , due to the symmetries of ` cos ( pi*D1 )` .
On this line ( ` return fact * ( altsigns [: zi.size-1 ] * ( sins [: , 1 :] - sins [: , : -1 ])) .sum ( 1 )`) why do you use ` sin ` instead of ` cos ` ?
I thought since the integral was with cosine , one'd use ` cos ` .
Well , the integrand is ` cos ` , but we are now doing the integrals analytically between each jump points of the ` sign ` function .
` int_a^b cos ( x ) dx = sin ( b ) - sin ( a )` .
Re-order numpy array based on where its associated ids are positioned in the ` master_order ` array
As a reaction to comment by Jamie , could you clarify if your ids are strings and do they have to be strings ?
Searchsorted gives you indices where you should insert ids into master_order_ids to keep the arrray ordered .
I silently hoped that ids as strings is a mistake .
Arrays should be constructed using ` array ` , ` zeros ` or ` empty ` .
Note that flatten would make an additional copy , so ravel should be used instead .
@USER you don't have to transpose , just address by column as I'm showing below .
And remove unpacking , as this will transpose the data .
View the docs on genfromtxt() . unpack is used to transpose the data so that it can be returned to variables like x , y , z .
Now that there's no second dimension to transpose the data into , so it remains the same .
The euclidean norm of a complex number is defined as the modulus of the number , and then you can define the distance between two complex numbers as the modulus of their difference .
The matrix ` dist ` will be symmetric and will be zero on the diagonal .
You need to reshape your array first , try this : #CODE
The frequency of the sin is definitely too high , check here .
How to append the list's values to a 3D array - python and opencv
Anyways my question is , I have a set of 30 images and I am calculating the mean of Red , Green and Blue channles for all the 30 images and stored them in 3 different lists , now what I want is to append these 3 different list values into a 3D array as : #CODE
Use numpy.ravel() `` to flatten your array in python
Should I manipulate the data to fill missing values with zeros ( numpy , maybe ) or is there a way to plot this ` matplotlib.plot ` ?
Do you want to do a [ histogram ] ( #URL ) ?
It is not like I need to plot a histogram .
Compare the timing with a list of arrays and the built-in ` sum ` : #CODE
Alice chooses the secret number x , she calculates X=g^x mod p and sends X through an unsecured channel to Bob .
Bob chooses the secret number y , he calculates Y=g^y mod p and sends Y through the same unsecured channel to Alice .
Both can calculate the value Z = X^y = Y^x = g^xy mod p
Hugh already gave you your answer , but I'm going to help you a little further by introducing you to some other common ` import ` conventions you may come across , in particular when working with the data analysis stack .
My first guess would be that you have an error in your gradient computation .
Maybe you should compare it to the numerical approximation of the gradient with ` scipy.optimize.check_grad ` .
Gradient works fine.Actually I had written same script in octave earlier and so I am sure about gradient .
You can use the ` extent ` kwarg to resize the image to an arbitrary size in data-units .
How to get figure for web traffic + how to append column to numpy array ?
I'd like to know how to append a column to a numpy array ?
You can use numpy.hstack to append columns , like this : #CODE
What can I run in my for each to append a value to the column of the numpy array I am making here ?
set the diagonal elements of ` A ` to the value ` 1 ` .
It will return a copy when it is not possible to flatten the array as a view .
and numpy fancy indexing takes the elements at indices ` X ` s and ` Y ` s , and assign a new value to them : #CODE
If you want to modify a ` block ` ( not diagonal this time ) , use broadcasting : #CODE
Your code will set the main diagonal to zero : #CODE
You just need to specify your own bounds , or change the ones already there , and see the difference yourself .
I hoped it would execute some external Python code in terminal and save the output in a numpy array which I could then append to another numpy array to add an extra column .
Skipping specific indices in an array ( or addressing intervals of indices )
I have an array that I want to address specific indices of it by ignoring a few in the middle .
Is there a smart way of addressing intervals of indices for an array ?
It would probably be better to define ` diag_max = min ( xmax , ymax )` , and then set ` lena [ range ( diag_max ) , range ( diag_max )] = 0 ` .
In particular , this reverses the previous list , and so grabs the diagonal from right to left instead of left to right .
So doing ' lena [ range ( xmax ) , range ( ymax )] = 0 ' will set the diagonal of the ' lena ' matrix to zero since you're stepping through the x and y coordinates incrementally at the same time .
In reality the parent set A that I'm dealing with contains 15k unique elements , with 30k subsets and these sub sets range in lengths from a single unique element to 1.5k unique elements .
The full version of the code that I posted above clears the dictionary if len ( z ) < min ( Dict.keys() ) . but that's one of the little logic things that I said I left out for clarity's sake .
Shouldn't ` x = [ B [ i ]]` in the outer loop by ` x = B [ i ]` ?
I agree that a more intelligent implementation is to iterate through the longest unique subsets and accept values that are unique to the set being constructed if a certain percentage of those values are new to the set being constructed .
The usual way to do searches in ` numpy ` is to construct a matrix of all combinations , and then pull out the desired ones with something like sum , min or max .
` bincount ` requires the input to be an array-like object containing non-negative integer data , and if ` max ( c2 )` is much greater than ` len ( c2 )` , it can be highly inefficient .
If not , is there a cheap windowed operation that will amplify the nans in x to the ones that are needed in y so I can use np.where() ?
You could use ` np.where ` to find the index of the NaNs , then use ` np.add.outer ` to include all the neighboring indices : #CODE
At runtime on the first iteration of the loop the problematic line has the addition of an array and a scalar which broadcast to a array : #CODE
In order to calculate dot product between their rows the second one has to be transposed .
I've done test prints of the values at those indices and they print out correctly .
I can't see where my indices are being rearranged , seems like a straightforward programming question to me ...
Instead of the values , it is displaying only zeros .
Use ` enumerate ` to create row indices and unzip ( ` zip ( * ... )`) this collection to get the row indices ( the range ` [ 0 , len ( list ))`) and the column indices ( ` lis `) : #CODE
This is great but how can we find the max per column ?
Basically , I want to dynamically concatenate elements from a vector by indexing .
What is the cleanest way of finding the indices of all entries in a numpy array that matches a pattern ?
Is there a way to neatly extract the indices of all values in an array that matches a pattern ?
The following example produces a list of tuples with indices matching the elements of the array A that are True #CODE
It doesn't matter if the indices are produced as a list of tuples or something else that is easily accessible .
Generate random indices and index both arrays with them : #CODE
To sum up just focus on the second part of the test : I'd like a 1-line test that checks that none of the defined rejection strings is present in a list of values .
My workaround usually is to sum these boolean vectors and equate to 2 , but there must be a better way .
To make sure that my code is not doing anything wrong before the resize , I used a small example on both functions and compared the output .
Perhaps you can first convolve with a Gaussian or other low pass filter prior to resize / resample .
Hi @USER I tried zip ( A , B ) however this did not append every value in A to every value in B , it just create a matrix of length 2 by 6 .
If you are using a ` pandas.DataFrame ` this is achieved by performing a join operation between the two arrays , where each has been given a " key " that is constant for all of the entries .
You need to reshape ` boston.target ` explicitly to have shape ( 506 , 1 ) using ` boston.target.reshape ( -1 , 1 )` where the -1 means infer the number of rows so that the data is reshaped into 1 column .
So ` 1+exp ( -709 )= = 1.0 ` so that ` expit ( 709 )= 1.0 ` Seems fairly reasonable , rounding ` exp ( -709 )= =0 ` .
` expit ( 710 )= =nan ` implies that ` 1+exp ( -710 )= =0 ` , which implies : ` exp ( -710 )= -1 ` which is not right at all .
` expit ( 710 )` yielding Nan could also imply that ` exp ( -710 )` is returning Nan
As can be seen in the code linked to by @USER , when ` x ` is positive , ` expit ` computes ` exp ( x ) /( 1+exp ( x ))` .
The numpy version of ` exp ( x )` returns ` inf ` when it overflows , and then ` inf / inf ` gives ` nan ` .
Values generated will correspond to the indices of ` A ` .
the difference may be negative , ` sqrt ` will make negative numbers be ` nan ` .
Given matrix ` a = [[ 1 , 2 ] , [ 3 , 1 ]]` I tried to minimize the function square norm
this works only for the exact example I mentioned above , if I change bigger array ( 4x4 ) to let's 5x5 or even bigger or change the position to eg 0 0 I get an error : ' ValueError : could not broadcast input array from shape ( 3 , 3 ) into shape ( 4 , 4 )'
I have a function ` ComparePatchMany ` written in numpy , which does some basic matrix functions ( dot product , diagonal , etc ) , and due to the size of matrices I'm using , is too slow .
The function ( ` ComparePatchMany `) , finds the diagonal of the dot product of the difference between two matrices ( ` np.dot ( p_diff.T , p_diff )`) .
What I'm saying is that multiplying a 500 8265 with its transpose should be no problem either .
Pythonic way of getting max of dictionary after applying custom function
Use ` max ` : #CODE
@USER sorry , I'm new to lamba functions , I've read very little about them and your code should work but I think the error is beacuse max built-in method doesn't support numpy.float64 ndtype , isn't it ?
@USER ` lambda ` is just an inline form of the def based based function ( but has some limitations ) , and ` max ` works fine with NumPy scalars as well .
@USER sorry I had a problem and was out of town , your code works great now , I was using " max " as variable in some part of my code and python got confused .
Have you looked at the examples in the docstring for ` norm ` ( #URL ) ?
` numpy.linalg.norm ( x ) == numpy.linalg.norm ( x.T )` where ` .T ` denotes the transpose .
then the Frobenius norm is basically computing #CODE
But overall , are you sure the L2 norm is the appropriate measure for you .
Maybe something like sum (( x1-y1 ) **2 + ( x2-y2 ) **2 + ... + ( xn-yn ) **2 ) would be more appropriate ?
The problem I'm working on asks you to leave L as a variable , leaving room to potentially change what type of norm you want to calculate ( L2 norm or euclidean norm or sqrt dot product , etc ) based on what value you give it .
Padding each group to the length of the longest one with zeros instead of trimming is also OK , if that's easier to do .
This looks like it pads each group to the max length .
Provided the index is unique , you can groupby the ` cumcount ` and take the mean : #CODE
$python example.py -- function cos ( x ) #CODE
How to efficiently join a list of values to a list of intervals ?
This gave me the median ignoring zeros .
This will remove all rows which are all zeros , or all nans : #CODE
And this will remove all rows which are all either zeros or nans : #CODE
Wouldn't ` groupby ` do excess amount of work on indices that doesn't exist in my tuples ?
Does pandas have ` groupby after join ` similar functional ?
So I can right join the df with my list of tuple ( on the multi index ) and then ` groupby ` ?
One day in the not so distant future , all NumPy / SciPy functions will broadcast all their arguments , and you will be able to do ` truncnorm.rvs ( a_s , b_s , size=100 )` , but since we are not there yet , you could manually generate your random samples from a uniform distribution and the CDF and PPF of a normal distribution : #CODE
How can I ignore zeros when I take the median on columns of an array ?
Once you get the nonzero array , you can obtain the median directly from a [ nonzero ( a )]
The function isn't called repeatedly with the indices of each individual cell ; it's called once , with index arrays representing the indices of all the cells at once .
This will make matrix multiplication more tedious since explicit ` reshape ` is required .
Without explicitly reshape like this : ` numpy.dot ( M [: , 0 ] .reshape ( R , 1 ) , numpy.ones (( 1 , R )))`
The difference between ( R , ) and ( 1 , R ) is literally the number of indicies that you need to use . ones (( 1 , R )) is a 2-D array that happens to have only one row . ones ( R ) is a vector .
All matricies are forced into being 2-D arrays , and operator ' * ' does matrix multiplication instead of element-wise ( so you don't need dot ) .
2 ) For your example , you are computing an outer product so you can do this without a ` reshape ` call by using ` np.outer ` : #CODE
2 ) It's not always replaceable by ` np.outer ` , e.g. , dot for matrix in shape ( 1 , R ) then ( R , 1 ) .
For its base array class , 2d arrays are no more special than 1d or 3d ones .
If we reshape an array , this doesn't change the data buffer .
the array ` b ` has the same data buffer as ` a ` , but now it is indexed by two indices which run from 0 to 2 and 0 to 3 respectively .
If we label the two indices ` i ` and ` j ` , the array ` b ` looks like this : #CODE
the array ` d ` is indexed by two indices , the first of which runs from 0 to 11 , and the second index is always 0 : #CODE
But without seeing the actual context that led to the need for a reshape , it's hard to say what should be changed .
Extract the poisition of the nonzero elements in A : #CODE
Then you can use double-indexing ` A [ i , j ]` with any array , not just contiguous ones , and Cython handles everything for you .
( On multidimensional C arrays , like ` double [ ] [ ]` , the C compiler essentially generates the equivalent 1D indexing operation by multiplying the array dimensions with the indices as appropriate ) .
How can I " fetch " the points of the curve ( first finalize it and then get a smoother one ) and append them to an array of type ` Point2D ` ( that I have developed in Python ) ?
It converts the non-zero entries of ` A ` into indices in a flattened array , then uses those indices to copy the non-zero entries of ` A ` into their right positions in a flattened view of ` B ` .
no ; their IS a way to do that though ; you basically have to broadcast the results to the original shape ; when the index is unique is can be done ....
@USER : that's right , but I was trying to break it into " get the group means " and " broadcast those group means up to the dataframe " .
I think you might need to set the index of the returned on the broadcast result ( it happens to work here because its a default index #CODE
AttributeError : ' str ' object has no attribute ' append '
I am wondering if there is a way to use numpy column stack such that when I output it to a another file using numpy.savetxt I get what I display below ?
Simply save the array as the transpose : #CODE
I get ` ValueError : operands could not be broadcast together ` when I try this .
Here are two more things I tried , where arr2 contains zeros : #CODE
you could always try replacing zeros in the denominator array with NaNs , something like : #CODE
I forgot to transpose ` C ` !
The rectangle is defined as ` (( x1 , y1 ) , ( x2 , y2 ))` where all x and y s are naturally array indices .
@USER I want to do a custom transform , not one of the standard ones with cvtColor
I have far too many rows and would much rather some form of optimized ` reshape ` -like magic ( hence the reference to numpy ) .
I'm trying to do some image manipulation in python but I was having some trouble with the stack overflowing .
Are you sure it was the * stack * that was overflowing ?
( This constructs a 2-dimensional array : if you really wanted a flattened version you could call the ` flatten ` method , but that's not necessary as I will explain below . )
The division by 3 is always the same , so it can simply be omitted , and we could use the sum of the colour channels instead .
The sum of the colour channels of an image is a bit like the luminance of an image , except that it doesn't take into account the different physiological responses to the channels ( green is perceived as brighter than red which is perceived as brighter than blue ) .
What strikes me is that the first two elements , which signify the " knots " in the spline , are eight elements , consisting of only two unique values repeated four times each .
Both unique values are from the corresponding diffPoints lists , but diffPoints are both 16 unique elements .
change it to append my information to the csv that is currently there
Open file with append instruction : #CODE
When you reshape or transpose an array , NumPy leaves the data buffer alone and creates a new view describing the new way to index the same data .
flattens the last two indices into one ( with the third index changing fastest ) , so that ` c ` indexes the image using two axes ( c , x height + y ): #CODE
The second reshape : #CODE
flattens the remaining two indices into one , so that ` d ` indexes the image using the single axis (( c width ) + x ) height + y : #CODE
Creates ` n_par ` sized array of all column means of ` mat ` .
Finally , index the ` mat ` matrix using ` ` logical operation and apply ` std ` to that .
you're not calculating the standard deviation of the _values_ where ` mat < m ` , just the standard deviation of the _number of values_ that satisfy that criteria in each column .
You can take all your numbers mod 1 : #CODE
Does matplotlib have a function for drawing diagonal lines in axis coordinates ?
Is there a similar function for plotting a constant diagonal ?
Plotting a diagonal line based from the bottom-left to the top-right of the screen is quite simple , you can simply use ` ax.plot ( ax.get_xlim() , ax.get_ylim() , ls= " -- " , c= " .3 ")` .
However , if you want to be able to zoom using your graph then it becomes slightly more tricky , as the diagonal line that you have plotted will not change to match your new xlims and ylims .
In this case you can use callbacks to check when the xlims ( or ylims ) have changed and change the data in your diagonal line accordingly ( as shown below ) .
Note that if you don't want the diagonal line to change with zooming then you simply remove everything below ` diag_line , = ax.plot ( ...
I'm not worried so much about interactive zooming , just in general the data limits changing as data is added to the plot , or if you wanted to add a diagonal line independent of any other data you're going to plot .
Just so you're aware the diagonal line will not just change on interactive zoom but also when you add data to the plot and change it via , for e.g. , ` ax.set_xlim ( a , b )` .
The whole idea of my answer is that whenever they do change the diagonal line will be modified to take that into account .
Drawing a diagonal from the lower left to the upper right corners of your plot would be accomplished by the following
I implemented an fft algorithm that take I1 and I2 and returns a new image ( I3 ) that is the result of aplying at I2 the transformations that would make possible the match between reference and sensed images .
Also this fft algorithm returns the transformations parameters that I used to transform I2 in I3 .
Unless you're using ` matrix ` objects or the ` dot ` method / function , it's not .
If the arrays are of different shapes , numpy will attempt to broadcast the smaller array into something that still makes sense , then perform the elementwise operation .
You can't use concat in the way you're trying to , the reason is that it demands the data be fully materialized , which makes sense , you can't concatenate something that isn't there yet .
numpy sum is not giving right answer for float32 type
Once I do a sum directly , and then I use numpy.sum .
The direct sum gives the correct result , whereas np.sum is giving something wrong .
If you sum up one axis first , and then sum the result of that .
" IndexError : too many indices " in numpy python
I'm trying to extract the column name that corresponds to the max value in a particular row of a data frame .
If there is a unique max value in the row , then this index is only one element and I can extract the ` float64 ` I need via #CODE
But I've encountered cases where there are multiple records in the `' map '` row with the same value ( the max ) .
A workaround is to take the max / min from the numpy values : #CODE
In Eclipse for example when you just put a dot after an object it shows you everything you want to see .
Is this precision cast away if I decide to take the numpy dot product of A with itself ?
` dot ` works fine when I test it with arrays full of ` fractions.Fraction ` instances , so it'll probably call the appropriate ` mpmath ` methods without trying to cast to numpy floats .
I don't understand for example the dot() documentation when it says " For N dimensions it is a sum product over the last axis of a and the second-to-last of b " .
I don't understand like if V is a N : 1 vector and M is N : N matrix , how dot ( V , M ) or dot ( M , V ) work and the difference between them .
@USER sure ! but if you do dot ( V , M ) and consider it as a matrix multiplication , then it's wrong because V is N : 1 and M is N : N .
But dot ( M , V ) is correct .
However doing dot ( V , M ) still gives a result !!!
but if V is a 1D vector , then dot ( V , M ) still works !
Dimensionality of NumPy arrays must be understood in the data structures sense , not the mathematical sense , i.e. it's the number of scalar indices you need to obtain a scalar value .
The rule " sum product over the last axis of ` a ` and the second-to-last of ` b `" matches and generalizes the common definition of matrix multiplication .
I didn't understand the idea of the sum over the axies .
Then ` A.sum ( axis=0 ) == array ([ 5 , 7 , 9 ])` , i.e. a sum over each column , while ` A.sum ( axis=1 ) == array ([ 6 , 15 ])` , a sum over each row .
" For N dimensions it is a sum product over the last axis of a and the
` np.sum ( arr , axis=0 )` tells NumPy to sum the elements in ` arr ` eliminating the 0th axis .
then ` np.sum ( arr , axis=0 )` will sum along the columns , thus eliminating the 0th axis ( i.e. the rows ) .
The 0th axis had length 2 , and each sum is composed of adding those 2 elements .
So the resultant shape will be ` ( 2 , )` , and element in the result will be the sum of 3 values .
This has bearing on ` np.dot ` , since the calculation performed by ` np.dot ` is a sum of products .
I don't understand what does it mean to go through the axes like sum ( m , axes=0 ) or axes=1 .
@USER : an example of the reshape scheme you mention would help since I am not sure I follow .
Ok I understand that sum ( A , axis=0 ) will sum each column down and will give a 1D array with 3 elements .
I also understand that sum ( A , axis=1 ) will sum each row .
For example sum ( A , axis=0 ) will sum each column from top to bottom , but I don't see elimination or reduction here .
The same also for sum ( A , axis=1 ) .
what i usually do , if i can't find a more suitable name than next , max , sum etc .
Pandas append filtered row to another DataFrame
I apply some filters to ` df ` , which results in a single row of data , and I'd like to append that row to ` df_min ` .
I tried using a loop to traverse ` df ` , and tried using ` loc ` to append the row to ` df_min ` .
You can use ` import numpy ` and ` numpy.array ( )` but it won't add zeros if the rows have different sizes .
It would definitely be faster to avoid joining ` res ` and ` keys ` and instead use ` argsort ` to find the indices that sort ` res ` and use those to reorder ` keys ` : #CODE
You can use ` argsort ` method of numpy arrays to sort keys with indices that would sort other array .
group argmax / argmin over partitioning indices in numpy
I would like a similar function to perform ` argmax ` , noting that I would only like a single maximum index in each partition : ` [ 3 , 4 , 5 ]` with the above ` a ` and ` split_at ` ( despite indices 5 and 9 both obtaining the maximum value in the last group ) , as would be returned by #CODE
THe array is made contiguous by [ ` flatten `] ( #URL ) , which always makes a copy .
I think your code will only work with big-endian data , as you are taking the last two bytes to be the least significant ones that contain the 16bit number .
How to find max value in a numpy array column ?
What I want is the max value in the first column and second column ( these are x , y coordinates and I eventually need the height and width of each shape ) , so max x coordinate is 10 and max y coordinate is 6 .
Clever way to return nearest " floor " binary number in Python ?
My current approach involves creating a list of binary numbers , and searching for each individual closest floor number within the list , but I was wondering if there are more clever approaches .
Slicing 2D arrays using indices from arrays in python
To select the slices , I have the indices stored in arrays .
How big is ` mat ` ?
Size of mat is user-defined , so it can be either pretty small or pretty big .
There's not much you can do with a bunch of slices in numpy , so the first task is to expand them into individual indices .
Then the following code expands the slice limits into lists of ( possibly repeating ) indices and values , joins them together with ` bincount ` , and adds them to the original ` mat ` : #CODE
I tried using ` reshape ` but not getting the desired result .
I don't comment the bit about copying , others will decide , there are many , many ways of using transpose / swapaxes to obtain a new view of the array , but it's the same trick in the end .
The matrix can then be used for further operations ( simple ones like calculating its Transpose , its Determinant etc ... ) .
A ` n x m ` 2D array of ` p ` -vectors represented as an ` n x m x p ` 3D matrix , as might result from computing the gradient of an image
These can even be combined in the case of a gradient of a volume in which case you get a 4D matrix
Staying with the graphics paradigm , adding time adds an extra dimension , so a time-variant 3D gradient texture would be 5D
So you can identify any shelf using 3 numbers : the row , the column and the floor .
A box is then identified by : row , column , floor and the index in the shelf .
An item is identified by : row , column , floor , index in shelf , index in box .
For example I know from R that every operator is actually a function , so + is sum ( x , y ) or something like this .
I want to get a histogram showing the years where meteors have fallout in steps of 10 years .
numpy.histogram just compute the histogram of a set of data , so if you want to get histogram visually , you should use matplotlib [ hist ] ( #URL ) .
There 3 outer loops , but they end up calling ` optimize.fmin ` .
That's fundamental barrier to ' vectorizing ' those outer loops .
` sum ([ prediction [ i ]= =actual [ i ] for i in range ( len ( prediction ))]) / len ( prediction )`
Python - Calculate histogram of image
Given an image ` x ` of dimensions 2048x1354 with 3 channels , efficiently calculate the histogram of the pixel intensities .
This is not exactly related to your original question but consider using a better algorithm for generating the histogram .
Even better would be to use ` sum ( img [ i ] [ j ])` and eliminate the loop entirely .
I'm using ` numpy.asarray ` here to make sure that ` img ` is a numpy array , so I can flatten it to the one-dimensional array ` bincount ` needs .
You can use newer OpenCV python interface which natively uses numpy arrays and plot the histogram of the pixel intensities using matplotlib ` hist ` .
Above specified number of bins not always provide desired result as min and max are calculated from actual values .
Here is updated code which always plot histogram correctly with bars centered on values 0 ..
Take a look at ` inv ` and ` dot ` functions .
To comment on some of the comments to the question : better not use ` inv ` for solving linear systems .
Manual fft not giving me same results as fft
It doesn't look like ` to_csv ` can append onto an existing file .
This would mean I have to carry around a potentially huge data frame in RAM and dump it to file at the end.I guess I can experiment with the feasibility of this ...
The ` mat.reshape() ` method doesn't just return the reshaped matrix it alters m ` mat ` itself .
To avoid this we need to reshape a ` copy() ` of ` mat ` .
open ' myfile.dat ' to append each iteration : 30.6 sec
I managed to get the desired output using your solution , although I built the x0 , x1 ... and y0 , y1 ... matrix using ones as I only have a z0 , z1 ... matrix which was rearanged in a 1D matrix before adding x and y .
I just borrowed their code and wrote my own cov / corr not casting into doubles , since I really only need 32 bit floats.And I ditch the conj() since my data are always real .
dot product up into pieces , and to optimize this , the number of elements is padded to a power of 2 .
Does anybody know of a way to ask numpy for a decent dot product routine , that balances memory usage with speed ...?
You can try and see if ` np.einsum ` works better than dot for your case : #CODE
The internal workings of ` dot ` are a little obscure , as it tries to use BLAS optimized routines , which sometimes require copies of arrays to be in Fortran order , not sure if that's the case here .
UPDATE : Turns out the dot product completed with out error , but upon careful inspection
the output array consists of zeros at 95,000 to the end , of the 151,000 cols .
if you have a lot of zeros you can use sparse matrices #URL
` floor ` is ` math.floor ` from the standard module ` math ` .
newSize then contains a tuple containing fracions that are used to resize the input image to a specific scale , and sm becomes a resized version of the input image .
ValueError : operands could not be broadcast together with
Actually I don't even need the tile .
With ` sm ` I have to expand ( and sum ) 2 dimensions : #CODE
That ` matlab.repmat ` ends up doing a whole string of ` reshape ` and ` repeat ` s : ` a.reshape ( 1 , a.size ) .repeat ( m , 0 ) .reshape ( rows , origcols ) .repeat ( n , 0 )`
The Octave version of ` repmat ` uses ` reshape ` s and ` ones ( 1 , p )` .
` reshape ` adds or consolidates dimensions , ` ones ` expands selected dimensions .
the behaviour of broadcast operations changes depending on vector representation and context : #CODE
One benefit of treating ` ( M , 1 )` and ` ( M , )` differently is to enable you to specify what dimensions to align and what dimensions to broadcast
But what if you want to align ` a ` and ` b ` in ` axis=0 ` and broadcast in ` axis=1 ` ?
` ( M , 1 )` vs ` ( M , )` difference enables you to specify which dimension to align and broadcast .
( i.e if ` ( M , 1 )` and ` ( M , )` are treated the same , how do you tell numpy you want to broadcast on ` axis=1 ` ? )
In the cases you highlighted above , ` c = a.T + b ` and ` c = a + b ` where ` a.shape == ( M , 1 )` would broadcast in a way that intuitively makes sense to me .
You are right in ** 2D ** ` a.T + b ` could specify the broadcast preference , but it would be really hard to apply it for higher dimension operations .
add text in bins of histogram in wxpython
If values in ` col2 ` in ` b ` are not unique , then probably you also need something like : #CODE
You can use the function gradient , which is implemented in numpy .
The gradient is computed using central differences in the interior and first differences at the boundaries .
The returned gradient hence hasthe same shape as the input array .
If you use numpy 1.9 ( currently dev version ) the gradient function uses second order accurate differences on the boundary too so the accuracy is preserved across the whole derivative :) #URL
Probably because the gradient function doesn't know my kx-values .
That is right , but you can also include the differences as an argument to the gradient function . gradient ( func [ 1 :] , diff ( func )) .
The gradient tool from delivers the correct result .
Moreover , the two outputs ( by matlab and numpy ) do correspond one another modulo a permutation of the terms .
If I transpose the matrix to perform the operation in Python I get the same answer as by Matlab .
Computing a binomial probability for huge numbers
I want to compute binomial probabilities on python .
I then tried to use ` sum ( np.random.binomial ( n , p , numberOfTrials )= =valueOfInterest ) / numberOfTrials `
Work in the log domain to compute combination and exponentiation functions and then raise them to exponent .
e ., the log of final probability is pretty small and hence underflow occurs while taking ` np.exp ` .
However , you can still work with log probability .
Also note that when n is this large the binomial distribution is well approximated by the normal distribution ( or Poisson if p is very small ) .
indices of NumPy arrays intersection
My task is to create list of indices of ` arr2 ` array where ` arr1 == arr2 ` .
Otherwise keep an eye on ` arraysetops ` in case someone adds a ` return_index ` parameter to ` intersect1d `
I have a stack of bitmap images ( between 2000-4000 ) that I'm doing a z-projection maximum intensity projection on .
So from the stack , I need to get a 2d array of maximum values for each x , y position .
These arrays are then compared to find the maximum for the stack .
You can just take the result of convolve and apply your expression to it : #CODE
You want to do ` >> k & 1 ` on the result of convolve , then apply ` tostring ` after that .
I would like to sum the contribution from multiple points ( with defined x , y coordinates ) over the grid .
Just from having a quick poke at things like ` .Random .seed ` in R and ` np.random.get_state() ` in ` numpy ` , it looks like ` numpy ` uses unsigned ints whereas R uses signed ones for the RNG's state- I'm not sure if you can really force them to behave the same .
all 0 : ` np.all ( mat == 0 )`
all 1 : ` np.all ( mat == 1 )`
some 0 : ` np.any ( mat == 0 )`
some 1 : ` np.any ( mat == 1 )`
Hey , just an aside , how can I make a matrix like ` mat0 = np.array ([[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 8 , 9 ]])` flatten to an array like ` arr0 = np.array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ])` ?
I've got a better one : ` sum (( set ( mat0.ravel() )))`
If you know it's int dtype , then ( suprisingly ) it's faster to check the max and min ( even without doing these operations simultaneously ): #CODE
Note : this short circuits if min is not 0 .
You can use ` unique ` #CODE
fyi , pandas unique function does exactly the same as numpy but without the sorting .
Now , ` np.unique ( mat0.ravel() )` finds all the unique elements and sorts them and puts them in a set , like this : #CODE
From here if one applies ` np.sum ` on this , namely ` np.sum ( np.unique ( mat0.ravel() ))` we get the sum of the contents of the set , so a good condition to check if only a ` 0 ` or ` 1 ` in each and every cell in matrix is the following : #CODE
The problem I have with that is , there are a lot of peaks , and it's hard to find only the ones I want .
seems to involve Python API calls , and I therefore can't release the GIL to allow the outer loop to run in parallel .
I'm still not totally clear on what the consequences are of declaring ` tmp ` in the outer loop .
If I was able to parallelize the outer loop , would this mean that when I make the assignment ` tmp = tmp_carray ` that this ought to affect where all of the child threads are reading / writing in memory ( which would obviously not work ) , or would the fact that I ` malloc ` a new buffer for each inner loop get around this ?
I'm thinking of just ` malloc ` ing flat intermediate arrays , then writing a C helper function to convert a set of multidimensional indices to a single index into the flat array .
To do that , I am using the indices of the image ( as coordinates ) and the average function : #CODE
This works fine if I take a simple ' image ' , and concatenate it to itself ( horizontally or diagonally ) .
When I move the gradient of the line , the intersection points it finds are not related to the line in question but instead a uniform line .
How to compress a matrix into an array / set in NumPy the fastest ?
How can I make a matrix like ` mat0 = np.array ([[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 8 , 9 ]])` flatten to an array like ` arr0 = np.array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ])` ?
@USER , I just wanted to be able to do this : ` sum (( set ( mat0.ravel() )))`
Use ` reshape ` : #CODE
@USER , If you want to get only unique values , use ` numpy.unique ` instead of ` set ` : ` np.unique ( np.array ([ 1 , 2 , 3 , 3 ]))` .
@USER , And use ` numpy.sum ` or ` sum ` method of the array instead of Python builtin function ` sum ` .
All the examples I have been able to find of mlab.contour3d generate a grid using ogrid , then plot a simple function ( sin etc ) of this grid .
Easy way to generate datas : X , Y= np.meshgrid ( np.arange ( - 250,250 ) , np.arange ( - 250,250 )) and then datas= np.exp ( - ( sqrt ( X**2+Y**2 )-8 0 ) **2 / 50 ) .
I have a 9X51100 two-dimensional array ( 9 different datasets , 51,100 daily files ) and I'm attempting to take 140 yearly averages of the data and append them to a new array .
Any help with either the append or looping problems ?
Are you taking some sort of max or average across the 9 datasets ?
In other words- it is not just taking a min or max .
I am trying to join two numpy arrays .
As far as I can see I should be able to join these columns ?
Take a sequence of 1-D arrays and stack them as columns to make a
This line then throws the error ` AttributeError : resize not found ` .
As ` X ` is a sparse array , instead of ` numpy.hstack ` , use ` scipy.sparse.hstack ` to join the arrays .
So , use ` scipy.sparse.hstack ` when you have a sparse array to stack .
` min ( data2 [: , 0 ])` and ` min ( data2 [: , 1 ])` does not work , strangely .
By ` min ` I meant ` np.min `
Are you taking the ` min ` after the ` view ` and ` reshape ` ?
I've added several examples of taking ` max ` values .
how do you use the indices in a numpy array for calculations
So I want the perimeterpoints function to work , but I am not sure how to run through the indices of my array .
So if I had 10 points that would produce indices of 0 to 9 .
perimpoints = np.hypot ( diff ( xx ) , diff ( yy )) ...
Here we reverse the input to get the " last " elements from unique , then adjust the indexes at the end .
Definition : eye ( N , M=None , k=0 , dtype =)
I am trying to run hstack to join a column of integer values to a list of columns created by a TF-IDF ( so I can eventually use all of these columns / features in a classifier ) .
Finally , I want to join them all together , and this is where our error occurs and the program cannot run , and also I am unsure whether I am using the StandardScaler appropriately here : #CODE
How do you view a numpy broadcast object ?
I then noticed there is another way to use broadcast ; by using the keyword np.broadcast ( a , b )
The documentation states that you create a broadcast image by imputing np.broadcast ( a , b ) .
The ` broadcast ` using your 1st arrays is : #CODE
The ` broadcast ` example isn't meant to used in production .
One thing to be careful about is that if you multiply with a real sine , your down-sampled spectrum will actually be the sum of two mirrored spectra , since a real sine consists of positive and negative frequencies .
demodulate by both a sine and a cosine of the same frequency , so that you obtain 2 spectra , after which taking the sum or difference will get you your spectrum .
demodulate by multiplying with a complex sine of the form ` exp ( 2*pi*i*f_demod*t )` .
Then probably the easiest approach is to only have a single ` numpy ` array : one that contains the sum of the data in your files .
Another option would be to reshape ` tm2tr ` ( doesn't work well for leap years ) .
You might be able to extract the desired time indices from that .
Yes , the data can be set into certain time indices , but I've downloaded the way I did to save time ( and a bit space ) .Thanks again .
My question : How do I minimize error function ( Mean absolute percentage error , MAPE to be specific ) using an optimal alpha ( level smoothing constant ) ?
+1 , although I'd use ` empty ` rather than ` zeros ` , more for conceptual reasons than practical ones .
I have a rough understanding that 2nd moment is sort of like moment of inertia ( Ixx , Iyy ) which is a tensor but I am not too sure how to calculate it and how it would translate into two intersecting lines with the centroid at its intersection .
@USER I tried calculating moments about x , y and then plotting them , but the plot seems completely irrelavant to the image in my brightness distribution ' x_moment = stats.moment ( data , moment=2 , axis=0 ) ; y_moment = stats.moment ( data , moment=2 , axis=1 ) ; import matplotlib.pyplot as plt ; img_plot = ;p lt.imshow ( data , cmap= cm.gray , norm = log_norm , origin= " lower ") ; for yindex , yvalue in np.ndenumerate ( y_moment ): ; for xindex , xvalue in np.ndenumerate ( x_moment ): ;p lt.scatter ( yvalue , xvalue , c= ' r ' , s=10 ) plt.show() `
For example , I have a list of matrix weight error derivatives , and I want to subtract them ( after multiplying by a learning rate ) from my weight matrixs for gradient decent .
average of all rows corresponing to all unique rows
You can probably do this more efficiently by using ` np.histogram ` to first get the sums of the values in ` A [ 1 ]` corresponding to each unique index in ` A [ 1 ]` , then the total number of occurrences for each unique index .
You could of course then stack both arrays , although that will convert ` unq ` to float dtype : #CODE
Essentially , there are 2 unique ID1s associated with every ID0 .
there are 2 unique ID1s associated with every ID0 .
it groups the data-frame by ` [ ' ID0 ' , ' Time ']` and reverses the specific columns ; if there are exactly 2 unique ` ID1s ` in each group , the data-frame will be expanded by the values from the other ` ID1 ` ; #CODE
translate Python in to Unity C# - Maths OR how to find Shortest distance between two skew lines in 3D space
I have quickly ran out of ideas on how to optimise this further , because ` p ` is used within the loop , in all three terms of the equation : when added to itself ; in the dot product ; and in a scalar multiplication .
I guess that was because ( a ) I'm new to ` Cython ` ; and ( b ) I kept most of everything the same instead of , for example , using ` C ` loops in place of the dot product .
This is of course assuming you will be using matrices larger then the ones you describe as this only takes 250us per call as shown .
I may be running your sample program incorrectly , but it looks like the terms in the dot product ` np.dot ( p , b )` are large which makes them very close to one when evaluated in ` np.tanh ` .
Nevertheless , note that ` tanh ( x ) ~ 1 ` for for ` x 1 ` .
But unfortunately , the tanh of the dot product is not necessarily approximately one .
I searched stack overflow for " IndexError : list index out of range " and understand that it's likely an error from trying to access a n-th element of a list that only has n-1 elements .
I really want to get the dot product of tmp1 .
The dot product of two matrices of size ( 3 , 2 ) is not valid .
I thought this was the dot product of tmp1 .
*tmp2 ` is not the dot product in MATLAB .
Everything works great and it prints out the maximum value of the array to the screen perfect , however , when I try to save the max value to a file , like this : #CODE
Funny thing is , when I just dump the whole array to a file it looks fine .
If it does , numpy will compress it .
At first I thought if the array is itself 3 dimensions , ` axis=0 ` will return three elements , consisting of the sum of all nested items in that same position .
If you have a vector and sum over its elements you get a number .
But if your ` int ` s are 32 bit integers , then you will be accessing the memory incorrectly , and probably have a lot of zeros alternating with only the first half of the values in your array .
Given ' start ' and ' stop ' indices , I need to construct an array y using sub-arrays of x .
This is almost 3 times faster than the loop for me , almost all the time difference comes from replacing fromiter with concatenate : #CODE
I was blindly following this post to flatten my list : #URL
I don't want to reshape the arrays .
A reshape like this might do the job #CODE
How can I make a vertical histogram .
I can then generate a histogram of this data and fit a pdf to it : #CODE
I have attached the outputted histogram with pdf .
Check that all rows in numpy array are unique
I want to extract the rows that are unique ( or the first occurrence ) and the rows such that for a row ` i ` , there are no other rows ` j ` in the array where ` A [ i , :] == A [ j , [ 2 , 1 , 0 , 3 ]]` ( or the first occurrence ) .
For the unique rows , you can use python sets to remove duplicates .
This will now loop through A , check that it is not already in B and append it .
Once the tree structure has been built , go back and collect all the branches and leaves into the array structure and by definition , they will be unique .
As each entry is read , if A [ 3 ] is not found already in a particular list , append it .
Find the unique rows in ` As ` ( the sorted array ) .
View it as a structured array where each row is a single element ( since ` np.unique ` will otherwise flatten the array first ) #CODE
And use them to get the rows that were unique in ` As ` from the original array ` A ` : #CODE
File " C :\ Python26\lib\ site-packages \numpy\lib\ arraysetops.py " , line 178 , in unique
For each k , the sum featureParams + transitiveParams will be executed .
Basically all you need to do is broadcast your ` features ` array to your ` Params ` arrays .
Note that I used ` keepdims ` instead of the reshaping after the sum .
[ 0~1 , 1~2 , 2~3 , ...., max-1 ~ max ]
Compare Numpy and Matlab code that uses random permutation
I'm comparing the MATLAB ` randperm ` function with the output of the equivalent ` numpy.random.permutation ` function but , even if I've set the seed to the same value with a MATLAB ` rand ( ' twister ' , 0 )` and a python ` numpy.random.seed ( 0 )` I'm obtaining different permutations .
I've to say that the result of MATLAB's ` rand ` and numpy ` numpy.random.rand ` are the same if the seed are set like above .
While the random number generator is identical , the function which converts your random number stream into a random permutation is different .
Just implement your own permutation algorithm in exactly the same way in python and matlab , for sure you can find some recipes on the web .
Then you can dump the different objects i.e. ` Knm0 ` , ` Kn0 ` , etc . or try different pieces like ` Knm0 [ u , :] ` and see which one throws the error , what the object looks like , etc .
The most common outlier tests use " median absolute deviation " which is less sensitive to the presence of outliers .
However , a common , not-too-unreasonable outlier test is to remove points based on their " median absolute deviation " .
@USER every where you are using median , but ` diff ` is calculated as L2 norm ( ` **2 ` ); median is the value which minimizes L1 norm , whereas in L2 norm ' mean ' is the center ; i was expecting that if you start with median stay in L1 norm .
do you have any reason ` **2 ` would work better than absolute values in calculating ` diff ` ?
@USER .nouri - Now that you mention it , it does seem very odd to use the L2 norm for this ( and it's not in fact the median absolute deviation , then ) .
If I just change things to the L1 norm , it doesn't work quite correctly , at any rate ...
The ` med_abs_deviation ` calculated in the code is different from the definition of the mean absolute deviation .
Logical indexing in Numpy with two indices as in MATLAB
What is the correct way to logically index with two indices in numpy ?
You can use the outer product : #CODE
To simplify , imagine 2-dimensional arrays like the ones shown below :
Firstly , use the [ timing tools ] ( #URL ) that come with python rather than trying to roll your own .
I access random indices using the following ( assume list1 is the list that contains the lists . Each list within list1 is of the same length , so I just equated list2 to one of the lists ): #CODE
In addition , is there any way to write a block of code that would try the x and y indices and see if it works and if there is an error involved , just pick two new random indices ?
I had added in print statements to catch which indices gave index errors .
What is ` max ( len ( x ) for x in lists1 )` ?
wouldn't ` sum ` do the job ?
Edit : if you are looking to sum ' vertically ' #CODE
numpy dot returns invalid values for large arrays when using scipy.linalg.blas.sgemm
Notice the zeros ?
From 35468 on , the array is zeros .
i guess I start learning :) Keep an eye out for new threads on that :)
How to create a list of counts from a unique list and a Counter object ?
And I have a unique list of strings .
And I need to yield a list of counts from the counter and then put zeros to the other string instances that have 0 counts .
I suppose you could use the ` Counter `' s ` update ` method to add 0 to all of the keys in the unique list #CODE
try reading the file and then [ transpose ] ( #URL )
I have not used them but assume converting the below lists ( and its append operation ) into numpy arrays is trivial .
In this case you could just reshape it : ` data.reshape ( len ( data )) .tolist() ` .
How often do your indices change ?
The row indices stay constant for a couple of dozen observations ( more precisely : I have k variables which are divided into an active and an inactive set and I need to check which variable of the inactive set fits best -- i.e. the row indices stay constant as long as I am checking the variables in the inactive set )
Lets do something where we make a 1D array of indices that satisfy our conditions for a n-dimensional grid .
Lets use this functional to build indices , hold them , and then use take to return your desired output : #CODE
So building our indices and keeping them appears to work , now for timings : #CODE
Anyhow if you keep these indices for more then 10-15 iterations it could help some or if you add an additional dimension and take all inactive datasets simultaneously .
Vectorized implementation of an image convolve function
I know that ` numpy ` includes a convolve function .
Also , although scipy has a 2d convolve , numpy alone does not .
Ouch it is half done how to know when it is string np.array ([ ' abcd ']) .dtype return dtype ( ' |S4 ') S5 , S6 depdending of string length , how to get the ' |Sxx ' information to decode ?
Assume the given array is all zeros .
Yeah ; the ones are the ' stencil ' .
Use ` reshape ( -1 , 1 )` to reshape ` ( n , )` to ` ( n , 1 )` , see detail examples : #CODE
What will that ( reshape ( -1 , 1 ) ) do to an arbitrary ( n by k ) numpy array ?
When you have a ` ( n , )` array , with the total size of ` n ` , ` reshape ( -1 , 1 )` will infer the first dimension to be ` n ` , in order to keep the total size unchanged .
( ie ., obviously not point me to write the histogram function in C / C++ ? )
This means any new operations that might create a new array I make sure to use ` reshape ` to convert it back to fortran order .
This seems very error-prone because I have to pay close attention to any operation that creates a new array and make sure to reshape it into fortran order since the default is usually C order .
I'm afraid that I'll always be getting bitten by missing calls to ` reshape ` that forces things in C order .
If you're having to use reversed indices , it's because you treated the array as C-ordered when you read it in and initially reshaped it .
Notice that the only difference is specifying the memory layout to ` reshape ` .
The memory layout of the two arrays is still different ( reshape doesn't make a copy ) , but they're treated identically at the python level .
Basically , this is where your reversed indices come from .
The reason for that is that the ` order ` kwarg to ` reshape ` tells numpy what order it should assume the underlying array is in ( basically , ` order ` should only really be used for 1D inputs ) .
What is the most elegant way to append np.nan to the arrays so that they match the sizes ?
File " functions.py " , line 18 , in rand
@USER : You changed more than just adding ` self ` to the method signature then , because ` def rand ( self , a , b ): ` , then ` self.rand = ( b-a ) * random.random() + a ` , then ` return self.rand ` works fine .
You can use some preprocess to remove zeros , here is an example : #CODE
remove all zeros in ` a ` , and calculate the new index : #CODE
Maybe you can also find a more fancy way to compute the ` l ` and ` r ` indices ...
I spent 15 min just looking at them but couldn't come up with anything better than incremental :)
Also , for binomial coefficient , we always have ` k=2 ` , so it's just ` n* ( n-1 ) / 2 `
If you compute the full cartesian product of differences , flatten the resulting 2D array , and create your own indices to extract the upper triangle , you can get it to be " only " 6x slower than ` pdist ` : #CODE
You can also speed up your solution , creating the indices yourself , and using take instead of fancy indexing : #CODE
I had expected that ` A.any() ` would be much faster ( it should need to check only one element ! ) , followed by ` A.max() ` , and that ` A.sum() ` would be the slowest ( ` sum() ` needs to add numbers and update a value every time , ` max ` needs to compare numbers every time and update sometimes , and I thought adding should be slower than comparing ) .
I'm curious , why would you think ` max ` should be faster than ` sum ` ?
I could understand expecting both to be equally fast , or ` sum ` faster than ` any ` .
@USER Because for ` sum ` , for every entry , it needs to ( 1 ) add two numbers ( 2 ) update a value , whereas for ` max ` , for every entry , it needs to ( 1 ) compare two numbers and ( 2 ) sometimes update a value .
Conditional expressions are often slower than arithmetic ones because of the way CPUs are built : #URL
@USER : Adding and comparing are typically the same speed , while _branching_ ( required by max but not sum ) is far , far slower .
Max is marginally faster than sum .
` max ` has to store a value , continuously checking for potential updates ( and the CPU needs to do branche operations to effect these ) .
` sum ` just churns through the values .
So ` sum ` will be quicker .
So how do I do append a new row to an empty array in numpy ?
I just want to know whether it is possible to append to an empty numpy array .
Sometimes it's cleaner to write code like this since the append operations are in a loop .
Won't work if the second array has dimension > =2 like ones (( 2 , 2 )) .
Then be sure to append along axis 0 : #CODE
In fact , if you're going to be appending in a loop , it would be much faster to append to a list as in your first example , then convert to a numpy array at the end , since you're really not using numpy as intended during the loop : #CODE
note : the array is created with ` random.randint ( min , max , len )` , from ` numpy.random `
When using SVD how do I unsort the scale factors along Sigma's diagonal ?
I want to get some information about the extent of this ellipse so I decomposed the matrix using svd : #CODE
I found out that the reason is because the implimentations of svd I've tried ( cv2.SVDecomp , and numpy.linalg.svd ) sort the elements on the diagonal of S .
The V matrix of the SVD decomposition A = U * S * V ' , where the ' symbol indicates transposition , encodes the permutation of the axes of A .
S - scaling that always happens along x , y ( this means x coordinates are scaled by sigmaX and y coordinates are scaled by sigmaY ); thus it was important to pre-rotate data in case scaling is need to be done not at X , Y axis directions but at a different directions ; it is like you know you are gonna get squeezed and you turn your side towards that force ; note that after scaling the circle now turned into an ellipse with its axes oriented along x , y axis ; For example , we may expand at x direction ( coordinate ) by factor 1.5 and squeeze at y direction by 0.9 ;
To sum up SVD represents any matrix multiplication as 3 consecutive operations : remap , squeeze , orient .
So I have an array of values in which I would like to take out only certain ones to put in a new list or array .
It can compress binary data very efficiently .
A practical suggestion , suitable for many kinds of reasonably continuous data : denoise -> bandwidth limit -> delta compress -> gzip ( or xz , etc ) .
Delta compress is just y [ n ] = x [ n ] - x [ n-1 ] .
Say I want to resize an array of shape ( 100,100,100 ) into an array of shape ( 57 , 57 , 57 ) using linear interpolation .
You will need to reshape your data and create another array with the original indexes , or point coordinates #CODE
Using numpy I sum two more dimensional arrays : #CODE
The usual mathematical notion for indices uses subscripts and / or superscripts .
But in numpy , integers have a fixed width ( as opposed to python itself in which they are arbitrary length numbers ) , so they " roll over " when they get larger than the maximum allowed value .
` marker= ' s '` means to use a square instead of a dot , ` s= 0.25 ` sets the dot smaller .
Usually a single executable packaged by py2exe will catch warnings and traceback and save them into a log file .
With numpy , I got back an array with many missing values , that were just zeros .
Ray has expertly noticed that the blas I'm using via cblas , must be 32 bit and not able to access the array indices .
Writing a double sum in Python
I have data for the two indices ` ( n , m )` and for the coefficients ` c_{nm} ` and ` s_{nm} ` in a .txt file .
A sum is simply a ` for ` loop whose body adds the result of a calculation to a variable .
likewise , a nested sum is made of such for loops nested
I'll wait for more details before trying to answer , but a generator expression along with [ ` sum `] ( #URL ) is likely to be more suited to this problem than a ` for ` loop in Python .
sometimes it works , sometimes it does not even run and and gives me core dumps .
One solution I can come up with is use ` numpy ` sorting , which uses ` n log ( n )` sorting algorithms .
You can use the key parameter of the max function .
Now suppose if we are given individual images as numpy arrays and we have to concatenate them as a single numpy array ( basically doing the reverse of the code above ) , how can we proceed ?
use ` vstack() ` and ` reshape ` : #CODE
First function returns LAB color , e.g. [ 53.798345635 , - 10.358443685 , 100.358443685 ] , second function I will explain more detail in question in 5 min
If you're making many , many images at once , keep an eye on memory usage .
A co-worker told me , that I could simply avoid it , if I got the fft for shifted frequency values ` ( np.arange ( N ) - N / 2 . + 1 . / 2 ) / T ` .
Is there a way to specify the output grid of the numpy fft ?
If you integrate a function with a nonzero DC component with coefficient A0 , the resulting function includes a term of the form A0*t , which is not in the space of periodic functions to which this Fourier technique applies .
Theoretically , the imaginary parts should all be 0 ; in practice they will be very small but nonzero because of normal inexact floating point arithmetic .
Is the best way to reshape it to 1-d , shuffle and reshape again to 2-d or is it possible to shuffle without reshaping ?
I load up a large stack of large images and get an array of shape ( num-rasters , h , w ) where num-rasters is the number of images and h / w are height / width of an image ( which are all the same size ) .
It works fine but my ' res ' variable ( the stack of images ) is several Gigs in size and even with a ton of ram ( 32Gigs ) , the operation takes it all .
the take-home for me is that ' swapaxes ' does the same thing as a transpose but you can swap any axes you want , whereas transpose has its fixed way of flipping .
it's also nice to see how a transpose behaves in 3-d ... that is the main point for me ... not needing to ravel . also , the transpose is an in-place view .
I am having trouble getting ` np.piecewise ` to work for multiple dimensional plotting due to broadcast errors .
Then I concatenate them using : #CODE
Assign same value to different ndarray indices - Python
I think it depends on the distribution , some distributions have a finite number of parameters and those encode the Probability of a certain event from happening .
But your new method will turn the zeroes into the product of the nonzero elements ( because they'll take the case where the zeroes are replaced by 1 . ) This is much easier to see if you use a 4x4 matrix to test , which has a row with two nonzero elements and two zeroes .
Indeed it does , but the large matrices result in zeros everywhere .
To generate the new point distributing by gaussian function , I tried the numpy.rand.normal ( mean , std ,.. ) .
When I tried numpy.rand.normal ( mean , std ,.. ) , this generate some random points over the mean value .
I want new points over my old ones with the given STD .
If you want to generate random points for these blue ones what do you do ?
Add the blue ones to the ones you want to ones you want to offset from .
Why is sin ( pi / 4 ) different from cos ( pi / 4 ) in python ?
Probably just floating point arithmetics , when I do ` numpy.allclose ( sin ( pi / 4 ) , cos ( pi / 4 ))` I get ` True ` .
Since " pi / 4 " can't have an exact floating point representation and cos ( " pi / 4 ") ^2 > 0.5 , it only makes sense , that sin ( " pi / 4 ") ^2 < 0.5 , since the sum should equal to 1.0 as closely as possible .
Fun fact : using ` numpy `' s ` sqrt ` , too : #CODE
thanks seems sin ( pi / 4 ) is not evaluate as sqrt ( 2 ) / 2 ...
@USER - Most likely , cos ( pi / 4 ) also isn't .
Because floating point representation isn't always exact and functions like cos and sin are calculated by approximate numerical methods , it is unreasonable to imagine that the results will be bitwise identical .
I need to choose a random element that is not zero from it and calculate the sum of its adjacent elements .
My other idea is to just sample from the index and then check if the element is non-zero , but this is obviously quite wasteful when there are a lot of zeros .
You can use ` lattice.nonzero() ` to get the locations of the nonzero elements [ nonzero docs ] : #CODE
which returns a tuple of arrays corresponding to the coordinates of the nonzero elements .
I'm looking for an efficient way to pick randomly between ` 1.0 ` , ` 1.5 ` , ` 6.0 ` ignoring all zeros while retaining the index for comparison with another ` array ` such as ` y ` .
Oddly enough , a very similar question was just asked [ here ] ( #URL ); you can use ` .nonzero() ` to get the indices of the nonzero elements and then choose one of those .
You could avoid storing the indices ( which if the matrix is sparse is going to be negligible storage anyway ) by looping and drawing a new random number each time a nonzero value is spotted but that's going to be a lot slower .
Another trick is to use ravel rather than flatten ( as this doesn't create a copy ) .
It seems that numpy.mat() ignores some characters in its input string : in this case " math " and " sqrt " .
If you get the 4th stack frame instead of the 1st , you can just replace ` np.matrixlib.defmatrix._eval ` with ` eval ` ( after adding in the globals and locals ) and make OP's original code work as-is : #URL
Use ` numpy.random.randint ` to generate a random column of 100 1s and 0s , then use ` tile ` to repeat the column 5 times : #CODE
You want to create a temporary array of zeros and ones , and then randomly index into that array to create a new array .
In the code below , the first line creates an array whose 0'th row contains all zeros and whose 1st row contains all ones .
The function ` randint ` returns a random sequence of zeros and ones , which can be used as indices into the temporary array .
This can't be accomplished with a sum because that would make position three twice what it should be .
np.maximum seems like it potentially could be the most efficient , but some of the values are negative so it grabs the zeros .
This one line takes every row of ` m2 ` , multiplies it by ` m3 ` ( elementswise , not matrix-matrix multiplication , since your original R code has a ` * `) and then takes colsums by passing ` axis=0 ` to ` sum ` .
Take a sequence of arrays and stack them vertically to make a single
Imagine that a , b and c are square and stack to form a cube .
Finally , you could always transpose the array if you wanted to ' rotate the cube ' : #CODE
This gave me one use of dstack ... not dstack actually but dstack() .T .... but I am really curious as to why numpy guys made dstack ( obviously not only for getting its transpose and using it ) .
and this method ( getting dstack's transpose and then getting individual elements ) is not valid for higher dimensions
Until now I'm getting the max value with #CODE
What I'm trying to do is that the max value became 1 .
So , I would like to do a translation such that 9 becomes 1 ( in positive case just dividing the values by it max value ) , and 0 ( when it is the max value ) becomes 1 ( with translation method , e.g ) , which I know hot to do , but I guess numpy may have a solution for do this thing in its package .
What I'm trying to do is that the max value became 1 and the minimum 0 .
Ok , so you want to normalize by the largest magnitude ( i.e. , absolute value ) .
Of course , this still would give an error if ` a ` consists of entirely zeros -- But the problem isn't well posed in that case .
If performance is really critical , you might what to create your own ` ptp ` and save ` np.min ` to a temporary variable so you don't calculate it twice .
Note that the OP has a problem when the max value is 0 ; I don't think this helps in that case .
Matlab indices for arrays start at 1 whereas Python index starts at 0
Do you want element-wise multiplication , inner , outer products ?
Unless you transpose the second matrix before multiplication .
If you need mathematical matrix multiplication ( dot product ) , use numpy.dot ( see examples behind the link ) .
You can transpose your array with ` a.T ` .
As the OP is already shaping the arrays , they could just ` reshape ( 5 , 1 )` one of them
For the dot product , see ` leeladam `' s answer .
I'm trying to optimize over a 3D array , where all of the entries along the z-axis sum to 1 ( that is , the sum of all entries with the same ( x , y ) coordinate is 1 for all choices of x and y ) .
The easiest way to fix your code would be to transpose ` c ` : #CODE
These numbers are the sample indices where the event is occurring in the signal .
Basically , all I want to do is make signal1Indicator = 1 for all indices that fall between the left and right columns of signal1Data .
For the first one , you can remove the outer loop and the ` if `' s by making use of slicing : #CODE
Examining the log file it seems that this is what caused the failure #CODE
And if possible , I would like to avoid a reshape .
Why do you want to avoid a reshape ?
I'm executing this code on a GPU , and I was originally unsure of the performance impact of a reshape .
( This is very similar to a reshape . )
I'll end up using the reshape solution , but your answer addressed all of the constraints I posed in my question .
You do not need to use ` unpack=True ` here , you are only loading one line , and you don't need to transpose multiple lines into columns here .
Setup with a medium sized array and few values to clip : #CODE
Same array size with more values to clip : #CODE
What you really want is a " find first nonzero element " routine which is not currently in numpy , but aimed for numpy 2.0 .
and my second question how can I get only all unique point ? for example in above example just a list contains [ 2 , 3 , 4 , 5 ] , thanks in advance
What makes a point unique ?
But I'm looking for a more efficient way to do this ( especially when I'll have arrays of thousands of elements at the end ) : I suspect the ` concatenate ` used here to recreate lots of new temporary arrays , and that would be un-efficient .
How to plot log normalized image using imshow ( ) ( matplotlib ) ?
How to pass these ` norm ` and ` cmap ` parameters in matplotlib to ` plt.show ` or ` imshow() ` ?
Perhaps ` lm ` is a typo for ` ln ` , the natural log ?
If you know how many if these entries you have ( `' Irend '` , `' IloadI '` etc ) you can reshape ` a ` : #CODE
tried to do this without dropna() and with min periods but didn't give me the correct result , if you can make it work would love to see it .
I don't know if it does all the things you want , but scipy.stats has analogs to numpy's histogram functions that let you calculate statistics for each cell , see #URL
I'm trying to create a new FITS file out of two older ones using PyFITS .
or if you want to create a new image out of the sum of both inputs : #CODE
After some investigation it turns out that my data does not fit well in log space , which scipy uses in its MLE fitting algorithm , so I need to try something like GMM instead which is only available in statsmodels .
I've been told by a long-time expert on the subject matter that this data is not well represented in log space , hence my exploration into alternatives .
In this case there is no x , or depending on the model it would be just ones .
And that's about the max you can expect from this .
I want to group the y array by the sorted , unique values of the x array and summarize the counts for each y class .
Where the first column represents the count of ' 0 ' values for each unique value of x and the second column represents the count of ' 1 ' values for each unique value of x .
It is equivalent to ` a.shape = a.shape + ( 1 , )` so that you can " broadcast " with it .
Is there a built-in limit to the size of arrays that can be broadcast together ?
When I use an x that has a large number of unique values ( over 10,000 ) , the comparison ` x [ y == 0 ]= = np.unique ( x ) [ ..., np.newaxis ]` returns a single bool value as if Python is testing whether the objects are the same .
I have a list of list of numbers and I want to sum all the numbers ( regardless the list of lists ) .
And unless you already have a numpy array , the builtin sum is actually faster most of the time .
When it comes to sum the elements of the array it will use the objects ` __add__ ` operator .
Try and sum a list of two or more of them and see what is produced .
I know I need to flatten my image , but the shape is 512x512x3 .
How do I truncate this ?
If there are three bands ( which is the case for an RGB image ) , you need to reshape your image like #CODE
The total amount of variance in the image is equal to ` np.sum ( S )` , which is the sum of eigenvalues .
and want to take the dot product of row 1 of a with row 1 of b , row 2 of a with row 2 of b and so on .
However , @USER ' s suggestion of using ` sum ` is arguably more readable for folks not used to Einstein summation notation .
` einsum ` is 3x faster than ` sum ` on my system .
Your question is confusing because you say the arrays have shape ( 3 , 71216 ) , but your examples show the transpose ( 71216 , 3 ) .
Anyway , it sounds like you just want this , not a dot product : #CODE
I want to check if some elements are ones or not .
The reason is quite likely to be the fact that fetching a single value from a sparse matrix in CSR ( or CSC form ) , given indices ( i , j ) , is very expensive .
Algorithms for these sparse matrix representations aren't usually designed to do that : they're designed to use the indices they find as they go through the arrays sequentially .
In CSR , when you look up a row , you effectively get an array of column indices and the corresponding values .
If you're fetching a single value , you have to do a linear search through the little array of column indices ( unsorted in general ) to see if it's there ( otherwise the value is zero ); if found , you then pick the value out of the value array and return it .
This is much less of a problem for algorithms like SpMV that use the column indices as they find them .
If you implemented SpMV like dense MM , fetching every value , it would be horribly horribly slow even if you had some oracular magic way of skipping the zeros .
@USER .B . the above question is significantly different from mine ; it asks for both min and max , and it is for 2D matrix
@USER Well the sorting approach might be faster for smaller data sets , but for larger ones the heap method should be faster .
I have a loop to generate millions of histograms in python and i need to store them all in one folder in my laptop is there a way to save them all without the need of pressing save button each time a histogram generated ?
The doc string for the argmin() method says it returns " indices " .
But " indices " is neither .
In numpy , most stuff can be done without for-loops , by using the axis argument on some methods , or just clever playing with indices .
The first line creates a Boolean array with the same shape as ` a ` that is ` True ` in all entries that correspond to ones in ` a ` .
result is numpy.array that consists of two columns , first column are indices ( numbers 1 through n ) and second column values from ( 0 , 9 ) .
Numpy : Get minimum value indices in array , ignoring specific value
I'm trying to get the indices of the minimum values in array , such as : #CODE
Then by typing : ` np.linalg.norm ( p2-p1 )` i am getting ` 1103.4963114787836 ` as their euclidean norm which doesn't seem reasonable compared with ~78klm from the gcd .
2 ) The funny looking plot is because the returned values from the fft are not ordered by increasing values of the frequency , and it happens that the last frequency in the returned values are at the middle frequency in the plot , so that line just ends there .
My attempt to compute it is to fetch both rows of the ` x ` and ` y ` nodes from ` A ` and then sum them .
- initialize AA ( Adamic-Adar ) matrix by zeros
- calc ` d = log ( 1.0 / k_deg )` ( why log10 - is it important or not ? )
A small warning though : with this approach you really need to make sure that the values on the main diagonal ( of ` A ` and ` adamic_adar `) are what you expect them to be .
Also , ` A ` must not contain weights , but only zeros and ones .
` np.einsum ` gives more control over ` dot ` operations .
Be careful about using ` sum ` with numpy arrays .
It's probably ( ? ) those two that are slowing you down ( mostly ` sum ` vs ` numpy.sum `) .
I'm not optimistic about the speed for large arrays , since there will be iteration in ` unique ` and ` split ` in addition to the comprehension .
The solution offered from this question was very helpful and indeed works when the number of unique values of x is small .
However , as I the number of unique values of x increases some odd behavior happens : #CODE
Once the number of unique x values exceeds a certain threshold , the type resulting from the broadcast is a bool instead of an array .
Is an upper limit to the size of a broadcast operation in numpy ?
Replacing n with ` n = [ 1000 , 10000 , 46340 , 46341 ]` ( the sqrt of ` sys.maxint `) fails at 46341 but not 46340 .
` reshape ` can be used to group data into chunks to calculate stats : #CODE
This would call the function ` np.loadtxt ` which would load the file ` GPBUSD1d.txt '` and transpose ( " unpack ") it .
On the other hand , a compound data type does return nonzero , thus True : #CODE
python concatenate numpy arrays with the method concatenate()
I want to concatenate numpy arrays .
maybe you want to do ` concatenate ([[( j , i ) for j in xrange ( i )] for i in xrange ( 1 , n )])` ?
the error is because ` concatenate ` expects a list / tuple as input , not a single array .
Let's see what you are trying to concatenate : #CODE
If this is what you actually want ( which I doubt ) , just remove ` concatenate ` .
When fitting a straight line to a set of data , weighted with errors , I was expecting polyfit to return a 2x2 covariance matrix from which I could square root the diagonal elements to find the uncertainty in the coefficients , but I don't .
cov : bool , optional
Return the estimate and the covariance matrix of the estimate If full is True , then cov is not returned .
Try adding the absolute path to the GPBUSD1d.txt file .
What's a absolute path ?
The absolute path is just the path from your root directory to that file .
Could it be that you want to concatenate `` np.zeros ( len_h )`` and `` np.asarray ( x )`` ?
By looking at the data plotted in the figure , one would estimate by eye that the value for ` b ` ( ie : the ` x ` value where the data set stabilizes ) should be around ` x=300 ` .
you can use a penalty value for the norm of parameters , and use ` fmin ` : #CODE
edit : playing with the penalty function and the norm order , it gives a very good fit at #CODE
Even restraining it via a max value only results in that max value being returned , so it's of no use .
I also used einsum to take the product then sum along the last axis .
Could somebody please explain the way that I should ' reverse ' a roll , or what I'm doing wrong ?
I tried looking into ` np.where ` ( to get the required indices , which I can then subscipt L with ) , but that doesn't seem to do what I need .
I updated mine with a detailed explanation of the array broadcast magic .
Then we get rid of the last column of 2 , and transpose the array , so that the dimension is ( 2 , 4 )
Line of best fit for a log graph in python 3.3
I start with K people and then I have a graph which plots log ( k ) vs .
I need to find the gradient of this graph but also want to do a line of best fit , any help would be most appreciated !
try to do ` from numpy import cos , exp ` instead of using ` cmath `
I need to find a fast algorithm to find the indices of the borders
Are there many more zeros than xs , more xs than zeros or about the same amount of both ?
The junks of Zeros have a known max length <= ~ 5000 Values --- relative small Number of borders
If the OP's array starts or ends with a chunk of zeros , you're going to need to prepend / append 0 / -1 to ` borders ` .
@USER ' s answer gets you 90% of the way there , but if you actually want to use the indices to chop up your array , you need some additional information .
You've found the indices where the array changes , but you don't know if the change was from ` True ` to ` False ` or the opposite way around .
Otherwise , you'll wind up extracting the segment of zeros instead of data in some cases .
Like this script which fit Gaussian to 1D histogram but I want to fit Gaussian to 2D histogram
( To me that makes no sense : the contours are derived from the histogram of the data , why not fit directly to the data ? ) Also what do you intend to do with the fitted Gaussian ?
Pandas to sum and group by date index
numpy array division and append
I dont believe I am understanding how to append in numpy
Build diagonal matrix without using for loop
So is there a way to construct a diagonal matrix like the one shown by matrix A without using a ` for ` loop ?
The suggestions also seem to have the same problem that I am having with the diagonal .
Notice that the number of ` 0 ` is always 3 ( or a constant whenever you want to have a diagonal matrix like this ): #CODE
numpy broadcast from first dimension
In NumPy , is there an easy way to broadcast two arrays of dimensions e.g. ` ( x , y )` and ` ( x , y , z )` ?
Its still an extra line of code , but I don't think you can avoid that , and at least its more human-readable than the reshape based solution .
how about use transpose : #CODE
` numpy ` functions often have blocks of code that check dimensions , reshape arrays into compatible shapes , all before getting down to the core business of adding or multiplying .
They may reshape the output to match the inputs .
f [ ' len '] [ ' click '] / sum ( f [ ' len '] [ ' click ']
( 1 ) is > f [ ' len '] [ ' click '] / sum ( f [ ' len '] [ ' click ']) and
( 2 ) is > f [ ' sum '] [ ' click '] / sum ( f [ ' sum '] [ ' click '])
Have you tried ( f.astype ( ' float ') / f.sum() ) .sort ([ ' len ' , ' sum '] , ascending=False ) [: 3 ]
ValueError : Cannot sort by duplicate column sum " btw- why are we typecasting " f " to float ?
So far I've got code to generate a random time series , and I've got code to calculate the max drawdown .
I've created a mask of X and Y , for example : ` mask = [ Y [: , :] 100 , X [: , :] 50 ]` and I've created a sum of these masks : #CODE
I have a the mean , std dev and n of sample 1 and sample 2 - samples are taken from the sample population , but measured by different labs .
I tried using the scipy.stat module by creating my numbers with ` np.random.normal ` , since it only takes data and not stat values like mean and std dev ( is there any way to use these values directly ) .
There is sparse array data structure maybe you have access to some sort of sparse matrix data type that could save you a lot of memory if your matrix contains lots of zeros .
You can dump an image as a numpy array and manipulate the pixel values that way .
If you concatenate two arrays , it won't know how to move from one memory location to the other .
The numpy function ` numpy.where ( A == ' A ')` returns a set of indices indicating where in ` A ` there is the letter `" A "` .
We then use those indices to slice the ` A ` array and do a reassignment .
I am using numpy and pandas to attempt to concatenate a number of heterogenous values into a single array .
These all have the same number of columns ` ( 0 )` , why can't I concatenate them with either of the above described numpy methods ?
It appears I can concatenate the two existing arrays : #CODE
However , the integer , even when casted cannot be used in concatenate .
What does work , however is nesting append and concatenate #CODE
Assuming that all the pixels really are ones or zeros , something like this might work ( not at all tested ):
Python PIL glitches when I resize with ' Image.ANTIALIAS '
If I resize an image in python using #CODE
When you view an array with a different dtype , you are reinterpreting the underlying data ( zeros and ones ) according to the different dtype .
Or If I have ` second = numpy.array ([ 0.62 , 0.61 , 0.97 ])` and ` third = numpy.array ([ 0.49 , 0.72 , 0.97 ])` I want the sort array return indices like [ 0 , 1 , 2 ] so the first array after sorting is like this ` [ 1 , 2 , 3 ]` why ?
( 1 ) numpy : frequency counts for unique values in an array
Update : Also , as suggested in ( 2 ) , I could use bincount , but I fear that might be inefficient if my indices are large , e.g. ~ 250,000 .
As @USER pointed out ( below ) , np.unique sorts the array , in O ( n log n ) time , at best .
Turns out ` itemfreq ` sorts the array , which I'll assume is also O ( n log n ): #CODE
It seems to me it would be O ( log n ) at best ( but I could be wrong here ) .
@USER Yes , this is ` O ( log N )` , and I forgot to mention it also expects the data to be sorted .
@USER Note however that it's ` O ( log N )` where the search is done by efficient C code on C types .
if you can afford to do the ` np.unique ` ( it sorts the array , so it is n log n ) call with ` return_index ` , you can work with small consecutive integers as indices , which usually makes things easier , e.g. using ` np.searchsorted ` you would do : #CODE
This happens because ` scipy.integrate.quad ` returns the estimated integral value AND the estimate of the absolute error , as a tuple ` ( y , abserr )`
are lists of couples of values , and you're plotting both the integrand and the absolute error ( that is in fact very close to zero in the whole domain ) .
That far in the tail you're better off using a complemented error function ( erfc ) or , possibly , erfcx , which is the complemented error function scaled by ` exp ( x**2 )` .
You can compute this with the ` sf ` method of the the ` norm ` object of ` scipy.stats ` .
` tile ( X , N )` will do it .
The ( num ) Pythonic way is not to do this but to use [ broadcasting ] ( #URL ) instead of ` tile ` and ` repmat ` and the like .
If you call ` np.tile ( a , 3 )` you will get ` concatenate ` behavior like you were seeing #CODE
So to separate it out into a stereo stream , reshape into a 2D array : #CODE
I'd rather not create ~8000 separate files , but I'd also rather not pad out a full ` (8 000,400 0 , 2 )` matrix with zeros if I can avoid it .
Because ` all ` is just a list of pointers to your numpy arrays , nothing has to be padded with zeros or otherwise copied .
Lists , on the other hand , don't broadcast operations across their elements .
` list1 + list2 ` will append ` list2 ` to ` list1 ` , so you get a new list with ` len ( list1 ) + len ( list2 )` elements .
` array1 + array2 ` will calculate the sum for each corresponding element in ` array1 ` and ` array2 ` .
t = Time-Time.min() gave an error of " AttributeError : ' list ' object has no attribute ' min '" and t = Time-min ( Time ) gave " TypeError : unsupported operand type ( s ) for - : ' list ' and ' float '" .
However , running the list through ` minimum = min ( Time ); for time in Time : t.append ( time - minimum )` worked .
As I know the ` numpy.take ` can only get values with indices .
If you want the indices where this occurs , you can use ` np.where ` : #CODE
L continues to contain all zeros .
You get a list of lists which you then translate to 2 dimensional array .
how to add two numpy.array with indices or mask
When we use ` == ` with two arrays like this , ` a [ 0 ]` is " broadcast " to act like an array with shape ( 3 , 9 ) , filled with copies of the first row .
The error is ` ValueError : operands could not be broadcast together with shapes ( 0 ) ( 9095 )` .
You want to sum them , or concatenate them ?
I want to concatenate them .
maybe concatenate (( a , b ) , 1 ) or hstack (( a , b )) See also #URL
@USER : Thanks for the catch , forgot the brackets as concatenate takes the arrays to be concatenated as a list .
you always get the second column and then stack it to itself , itmeans each time you overwrite it .
You have to define another array beside than ` col_sel ` and stack ` col_sel ` to it .
If you want to stack them vertically and write them in a 2D array , at the end the number of columns is equal to the ` CVS ` files : #CODE
Used reshape to make rows into columns .
I've tried to tile B in different ways to achieve the same results , but nothing works .
If I didn't get the indices wrong , I have 5 matrices of shape 4x4 , and 3 vectors of length 4 , and am computing the 3x5 quadratic forms of each matrix with each vector .
This doesn't reproduce the double ` dot ` of the OP , since ` dot ` uses the 2nd to the last dimension of the 2nd argument .
In any case , he should be able to get what he wants by fiddling with the indices .
For N dimensions it is a sum product over the last
Often in testing I like to use different sizes in each dimension ( e.g. ` z= ... reshape ( 3 , 4 , 5 )`) , so errors jump out at me .
` y.T ` ( transpose ) is ( j , 1 ) .
So the 2nd dot combines ` ( 1 , k , l )` with ` ( j , 1 )` , last dim ( ` l `) with first ` j ` , resulting in ` ( 1 , k , 1 )` , your 3d array : #CODE
You just need to systematically track the dimensions as they pass through ` dot ` .
To get a smaller dimensional result , you need to squeeze out the 1's or reshape the result .
FWIW , in your original notation it would be linspace ( min , max , n+1 ) ( not n )
I concatenate all the data together for batch processing , but I need an index array to extract the results back out .
True ; changed the code to handle zeros using add.at .
The consequence it has on the code if any . everything passes the eye test and unit test .
You could then put a ` try ... except ` around the line in your code that raises the exception , and use the ` except ` clause to log the value of relevant variables .
How to get values from a 2-d Array with indices
How can do it with a indices [ 1 , 2 ] .
So , it's an indirect sort that returns the indices that would sort an array .
thanks for the pointer on making uniform [ 0 , 1 ) . one thing which confused me was how can you divide a double with a number greater than the max value of a 64bit number ?
And of course , what should be your first option for your actual indices , although it will not work if they are not all consecutive integers : #CODE
Since there is no copy being made , the reshape is memory efficient .
` array2 ` is already of shape ` ( sizes*sizes*sizes , 3 )` , so this reshape does nothing .
To me it seems that the most obvious implementation in a compiled language such as C would be to simply loop through the array , test each element and save the ones that pass the test .
How do I concatenate an array into a 3D matrix ?
If you're going to insert a large amount of zeros , it may be more efficient to just create a big array of zeros first and then copy in the parts you need from the original array .
Is there a better way to get the coordinates of the max value ?
If you bin your data first , then you can use convolution and fft . statsmodels fft based KDE only works for 1d , but there are 2d recipes in python available .
I am hoping to make a histogram similar to the last figure in this link :
I have a list of 100k items and each item has a list of indices .
There are x-1 of these ( one for each of p ( 1 ) , ..., p ( x-1 )) so the system either has a unique solution or none at all .
Intuitively , it seems like there should always be a unique solution .
The two last equations would be unique since they would feature p ( x ) and p ( x+1 ) , so those terms would be ommitted ; the column vector for the RHS of Cramer's rule would then be ( 0 , 0 , ..., 0 , 1 / 2 , 1 / 2 ) , I believe .
Then I need to sum the first column data ( from first line to , for example , 100th line ) .
Use ` .argsort() ` it returns an ` numpy.array ` of indices that sort the given ` numpy.array ` .
You can now call ` .argsort() ` on the column you want to sort , and it will give you an array of row indices that sort that particular column which you can pass as an index to your original array .
The indices are returned in increasing order .
So ` in1d_index ` is ~650x faster ( for arrays of length in the low thousands ) , but again note the comparison is not exactly apples-to-apples since ` in1d_index ` returns the indices in increasing order , while ` ismember_rows ` returns the indices in the order rows of ` a ` show up in ` b ` .
as what unutbu said , the indices are returned in increasing order
I personally found it useful to sample binary colors orthogonally to the image gradient ( that is on the both sides of it ) .
I tried to transpose the ` r [ 0 ]` vector but still got the same issue .
I want to take this list of dates and append them to three separate lists indicating whether they were before , during or after a certain event .
I'm having trouble with the indices , and was also wondering whether there was a built-in NumPy function for the iteration .
This is the histogram :
The first option will repeat the first or last item for indices out of bounds , the second will wrap around , as if your array was circular and had both ends connected .
In scientific python that would be an outer product ` np.outer ( x , y )`
So to get the same outer product , you need to expand the dimensions of ` x ` and ` y ` .
One way is with ` reshape ` .
It is based on an indexing notation that is popular in physics , and is especially useful in writing more complicated inner ( dot ) products .
To solve my problem I had the idea to open the tempfile in ` mode=a+b ` with the goal to append the new arrays from the loop .
I don't think you can use ` .npz ` files in append mode .
reshape returns a ndarray , this is documented and expected behaviour , it wouldn't make sense to be able to reshape from a 1-D array to a N-D array without changing the type see : #URL the element type should be unchanged though
After the ` reshape ` the array has two dimensions , meaning indexing in the same way gives you a row of the array ( which has type ` numpy.ndarray `) .
@USER : ` reshape ` may return an array with ` ndim ` equal to one , but by supplying ` ( 3 , 1 )` as argument you ask to return an array with two dimensions .
You can use numpy digitize method to bin your data into bins : #CODE
Conditions : no tuples in final arrays , and each array contains a unique combination of the tuple elements .
This is because a kd-tree kan find k-nearnest neighbors in O ( n log n ) time , and therefore you avoid the O ( n**2 ) complexity of computing all n by n distances .
I think it would be better to define Z as a single array filled with zeros before hand and then fill it with your values .
To sum elements up , we have binary operator ` np.add ` , and moreover ` np.sum ` dealing with multiple elements .
` sum ` and ` prod ` are implemented as calls to ` add.reduce ` and ` multiply.reduce ` , and all binary ufuncs have the ` .reduce ` method , so even if ` any ` and ` all ` were not explicitly defined , they would be easy to produce .
I would like to plot a distribution-graph of the values , like this ( here it is done for a binomial random variable ) :
How to calculate exp ( x ) for really big integers in Python ?
The regular python math and numpy modules will overflow on exp ( 300000 ) .
But when I'm doing the sigmoid function , ` exp ( -300000 ) + 1 ` , it outputs just ` 1 ` .
` print exp ( -300000 ) +1 `
a bunch of zeros , and then the digits from exp ( -300000 ) .
Also if you are using the sigmoid - you can try to compute tanh instead ( it is the same function , but is " directly " computable )
+1 for mentioning ` tanh ` .
` decimal ` module from stdlib allows to compute ` exp ( -300000 )` with desired precision : #CODE
append data to list within list
( As an aside , I disagree with dpkp's suggestion of using a " defaultlist " since I think it's an overly complicated , non-standard , and advanced solution to a beginner problem . Instead , if you want a workaround to this problem where a list is automatically created to append to , please post that as a separate question with more specifics about why you want it -- and I think you'd probably want a defaultdict using integer keys , but that should wait for the full question . )
It sounds like you are saying : if there is already a sublist in my list ` L ` at position ` somevalue ` , then append ` x ` to it .
Calculate Summation over all values of z in ` z^k * exp (( -z^2 ) / 2 )` #CODE
For the second use ` part2 = map ( math.exp , -z**2 / 2 )` and ` return np.dot ( part1 , part2 )` no need to transpose 1D arrays .
We define a function for the thing we want to sum , use the ` map ` function to apply it to each value in the list and then sum all these values .
It does run faster with dot : #CODE
CSV data - max values for segments of columns using numpy
I want to be able to return this file so that for each increasing number under the header ' time ' , I fine the max number found in the column speed and return that number for speed next to the number for time in an array .
We then determine the unique time values that occur and find the rightmost entry in the array for each time value .
Then ` np.unique ` can be applied to find the unique values : #CODE
That way , the function would not be slowed down by an unnecessary transpose if you wish to use it to count the number of distinct rows .
I checked the histogram and x is correct .
I would expect the algorithm to ( at least sometimes ) find the correct ` mu ` ( i.e -10 , 5 , 10 ) with std ~= 1.0 .
I forgot to take the square root of when calculating ` std ` .
If your data has a lot of zeros in it , you should use scipy.sparse matrix .
It is a special data structure designed to save memory for matrices that have a lot of zeros .
Select indices where x is nan while y is between two values
Now I would like to select the indices where ` x = np.nan ` only where ` y ` is between ` 0 ` and ` 10 ` .
The computation is quite eavy , but also quick ( 30 sec max ) , but can become longer ( several hours ) if we use custom parameters space exploration .
If you really want to squeeze miliseconds , you could use bottleneck's ` nanstd ` function instead , because it is faster .
Before your loop , define ` std , _ = bn.func.nansum_selector ( xs , axis=0 )` and use it instead of bn.nanstd , or just ` func.nanstd_1d_float64_axisNone ` if you are not going to change the dtype .
Some basic changes I would make : currently , you recalculate the sum of squares , mean , and count for each partition on every iteration , which makes this " cross-validation style " algorithm quadratic .
You then take ` - 0.5 *n* np.log ( 2* np.pi *sigSq ) - 0.5 *n ` and calculate that based on the updated values of n , mean and sigSq ( you will need to calculate the stddev from the sum of squared values ) .
If you don't need the actual log likelihood , you can focus on only maximizing ` proflike ` and then calculating ` lr ` outside the loop , saving a few multiplies -- ( you can fold the 2 * and the 0.5 * in the functions together if you do some basic algebra regardless )
The right hand ones would also need to be compensated for as well .
I would like to set a new column " MyColumn " where if BC , CC , and DC are less than AC , you take the max of the three for that row .
If only CC and DC are less than AC , you take the max of CC and DC for that row , etc etc .
Note : you can take max of a subset of columns : #CODE
also how does ` np.diag ( cov )` look like ?
How is ` cov ` generated ?
Whenever a column or a row of a matrix is full with zeros , this makes the determinant to be zero and the matrix is singular , therefore your cov matrix has no inverse .
I would recommend you to double check the reason why the calculation of your covariance matrix leads to rows and columns full of zeros , even in the main diagonal .
Actually I generated covariance matrix by calculating the frequency of di tri and tetra nucleotide word frequency in a sequence .
and difreq = AA , AT etc tri = AAA , ATG etc and tetra = AAAA , TGAT etc . this way I generates frequency matrix and thereby covariance matrix .
What I want to do is for each unique value in col 1 ( 0 , 1 , 2 , 3 , etc . . . . ) I want to view col 2 and find the minimum value ( I also have to do this for max , but similar idea ) within that column , and return that row that has the minimum number in col 2 for each number in col 1 .
I've tried to search up each unique value in col 1 but I dont know how to do that based on values for each in col2 .
Cumulative sum of variable till a given percentile
I would like to sum up all the values in an array till a given percentile .
The 90th percentile is ~611 and the cumulative sum till then is 461
This can be done with an algorithm similar to flood fill , using a stack : #CODE
For stack [( 3 , 2 )] it ultimately ends up finding 17 cells within our requirements .
It took me a bit of fiddling to get the whole scipy stack but I'm sure I have it now , just wondering why copy-pasting their code and then running it would give an error ( I'm sure they wouldn't put code with a bug on their site ) .
How did you install the scipy stack ?
I strongly suggest that you don't try to assemble a stack yourself , as it is quite challenging to do .
Efficiently convert a vector of bin counts to a vector of bin indices
I need to obtain a vector containing the time bin indices for each event .
I'm thinking I want something like a skeleton or medial axis ( from skimage ) and then look at the distribution of distances along the nonzero elements of the resulting array .
I just tried ` numba.jit ( lambdify ( x , sin ( x ))) ( 1 )` and it worked .
For instance , for ` lambdify ( x , sin ( x ) , ' numpy ')` , ` sin ( x )` is converted to `' sin ( x )'` ( the string form here is the same as the regular string form , but they can differ , e.g. , because of function name differences between SymPy and NumPy .
This is then added to ` lambda ` , giving `' lambda x : sin ( x )'` .
In other words , it evaluates `' lambda x : sin ( x )'` in a namespace where ` sin ` is ` numpy.sin ` .
As far as I know , numba.jit and numba.autojit just translate bytecode , so they should work fine on a lambda .
You could list the indices explicitly , of course : #CODE
Alternatively , you can use indices to remove as #CODE
I want to use either some basic python or numpy to read that data from col 1 and find all the ones that have 0 and group that row up together in a list or something , and then all the ones that have a 1 in col1 , etc .
If this is a problem , you could add a buffer of zeros around your array with : #CODE
It doesn't happen to cause a bug here , but it'd still be better to compute both sets of indices before modifying ` Z ` .
How to build a dict of indices of occurences of elements in a list in Python ?
What is the fastest way to convert the indices of occurrences of all elements in a list to a dict ?
In this sort of case it makes more sense to append to the return value of ` B.get ( x , [ ])` , which modifies it in-place .
You have to assign the object returned by ` get() ` to ` B [ x ]` and append to it in different statements .
the output array which I'm append has the size of 180x20 ... but the RAM load-factor increases 0 , 12GB each time.Is there a more efficient way to store arrays , without tempfiles ?
Since both ` reshape ` and ` transpose ` return views of the original array whenever possible , it may also be worthwhile to explicitly make a copy of the result .
In your case there are 3 depth : ` l [ ... ]` slices , reshape , and transpose .
3 ) If so have you tried writing some tests for your functions ` sumproduct ` , ` flatten ` , etc ?
As ` mean1 ` is a 1D array ` transpose ` does nothing as there is nothing to transpose .
Also , you could reshape ` t ` and ` v ` to let numpy handle the indexing maths , but I don't know if this is significant .
The ` reshape ` not so much , which is a pity because I thought it was a really sweet idea and it simplified the code .
How do I flatten axis i and j so that I can do numpy.argmax ( F , flattenedAxis ) ?
From there , the same basic command works to get the restricted indices .
You can then add the start of the restricted indices to the result to get them in terms of the whole matrix .
If you have a list of the x and y indices you want to restrict by , you could do : #CODE
" One Hamming Distance " : hamming distance between the sub-array of myLabel which are equal to " 1 " and sub-array of each vector ( I have created sub-array of each vector by selecting value from each vector based on indices of " myLabel " where the value is ' 1 ')
" zero Hamming Distance " : hamming distance between the sub-array of myLabel which are equal to " 0 " and sub-array of each vector ( I have created sub-array of each vector by selecting value from each vector based on indices of " myLabel " where the value is ' 0 ')
Now , the indices in either ` ham ` array will be the indices in the second axis of the ` allLabelPredict ` array .
I don't exactly get your solution , the point in my question is that I have two different hamming distances and I want to sort based on these two ( it's important to be separated each of them shows me different thing ) now I want to minimize both at the same time or if it's impossible select one value for one and minimize second one ( for example give vectors that have zeros hamming distance of 0.2 and minimum value of one hamming distance )
If you want to minimize both , you may as well take the hamming of the entire vectors , since the sum of the two hamming distances is just the total hamming distance .
I am not sure either I don't understand your solution or you didn't understand my question , for example why allLabelPredict [ myLabel == 0 ] when I am working on each vector which is allLabelPredict [: , i ] and why are you not just using scipy Hamming distance function to calculate hamming distances and at the end where did you give me vecotrs which has best zeros and one hamming distances ( or as I said earlier oneHam = 0.25 and minimum value of zeroHam )
It doesn't give unique random rows .
Try the resize method .
You need to sort the columns by decay values ?? of the diagonal elements ,
not simply the diagonal elements in the reverse order , they need to sort by decay , and simultaneously change the columns depends places to which column the copyright of a diagonal element
not simply the diagonal elements in the reverse order , they need to sort by decay , and simultaneously change the columns depends places to which column the copyright of a diagonal element
` a.diagonal ` to get the diagonal values with ` [: : -1 ]` to get them in reverse order
A common approach for testing convergence to a min is the RMSD ( Root-Mean Square Deviation ) test .
Anyway , the point is that there's no fundamental reason why the OP couldn't use one of a number of different multithreaded BLAS libraries to accelerate dot products .
As for Pandas : I'm not sure how it does dot products .
First of all I would also propose to convert to bumpy arrays and use numpys dot function .
I tried hacking together something that would just append pixels as it crossed them , but it was inefficient , inelegant , and non-ideal in a number of ways .
Keep in mind that for an array there doesn't exist a diagonal slice without some weird zigzag ( or alternatively , interpolation ) no matter how elegant your implementation is .
Especially if you don't have much more code than what you've presented here it might be pretty easy to translate over .
then you can make use of the functions np.in1d() which will give you a True / False array which you can just sum over to get the total number of True entries .
My guess would be that you're hitting the object layer instead of generating native code due to the calls to ` sum ` , which means that Numba isn't going to speed things up significantly .
It just doesn't know how to optimize / translate ` sum ` ( at this point ) .
I would try to expand out the sum in the inner loop as an explicit additional loop over elements , rather than taking slices and using the ` sum ` method .
Find the indices of array elements that are non-zero , grouped by element .
The first two columns are indices in den landcover array and the last is the number of adjacent building cells .
If I use zeros and then ` a [ i , j , z ] = 1 ` , ` a [ 0 , 0 , 0 ]` is ` 1 ` but should be ` 0 ` .
I'm looking for an efficient way to split an array into chunks that have a similar histogram .
Not only this but each chunk should have a similar size and a similar sum on a given function f : #CODE
Let's start with a very weak assumption that similar histograms are defined in a following rudimentary way : for a set of integers S1 , the histogram ` H ( S1 )` is similar to an histogram ` H ( S2 )` of set S2 if ` sum ( S1 ) = sum ( S2 )` .
The following function returns a two-dimensional numpy array ` diff ` which contains the differences between all possible combinations of a list or numpy array ` a ` .
For example , ` diff [ 3 , 2 ]` would contain the result of ` a [ 3 ] - a [ 2 ]` and so on .
In fact , the array you're looking for is a diagonal of the matrix that ` difference_matrix ` returns .
Python array sum vs MATLAB
@USER You should check the code again , MATLAB is adding the ones in a single call .
It looks like if reshape ` x ` , ` y ` , ` z ` to square matrix , you can do a ` contourf ` plot : #CODE
If the grid is 84*81 , you can make the heatmap as described , but reshape your data in to ` (8 4 , 81 )` rather then a square matrix .
( Minor : if A can have duplicate elements , choosing the complement of the indices and having S2 contain all the elements in A not in S1 aren't the same thing . )
I might bypass indices entirely , instead permuting the elements and then splitting the results : #CODE
The result is a 4X4 matrix and all diagonal elements are 0 because they are the same .
Shouldn't you join before using the results ?
But for whatever reason this is happening anywar I need a solution for a situation like this ` T.inv ( T.dot ( A , B ) + const * eye )`
How about this quick and dirty mapping from lists of strings to indices ?
PS -- Didn't know about ' valid ' option for convolve ; that's a much better approach .
one problem with this - is that I needed to change closep [ 19 :] > smas because i got an error : ' ValueError : operands could not be broadcast together with shapes ( 522 ) ( 503 )'
ValueError : too many boolean indices '
ValueError : operands could not be broadcast together with shapes ( 19 ) ( 503 )
According to your link ( #URL ) you need to be mindful of the indices ...
Whenever unsure , the safest way is to grab the diagonal elements of the covariance matrix rather than calculate ` var ` separately , to ensure consistent behavior .
2 quick and possibly elementary questions : Why are you multiplying V with its transpose instead of just multiplying the two variances ?
Just read all the data into an array , then reshape the array to have 3 rows .
The ` -1 ` in the call to ` reshape ` will be replaced by ` reshape ` by whatever number makes sense .
The strange thing is indices get properly appended to the list ` stack ` , but not list ` counterStack ` .
Indices will get added to stack twice .
You're setting ` counterStack = [( 1366 , 1264 )]` in each iteration of the ` for ` loop but stack is only set to the base index once .
Move ` counterStack = [( 1366 , 1264 )]` to right below the line ` stack = [( 1366 , 1264 )]` and you should see what you want .
you can also use python bindings for win32 to set the max memory usage of a process [ Memory limits ] ( #URL )
Given a flatten NxN array in numpy , I'd like to find the minimum value , and its offset in the array .
You can't , with built-in ` min ` function , but you can easily write a loop to do it .
But I would like to know if it is possible to list all offset which has the same min value ?
There's also no need to call ` array ` here ; ` zeros ` already returns an array .
ValueError : operands could not be broadcast together with shapes ( 2 ) ( 3 )
to sum up : #CODE
Here's another way of producing the ' flat ' outer product : #CODE
Before the final ` reshape ` this produces a 2D matrix .
I checked similar problem at sum of N lists element-wise python
I am looking for a way to sum the color values of all pixels of an image .
Would anyone please help me how can I sum the colour values of all pixels of an image .
If you want the mean value , just replace ` sum ` with ` mean ` : #CODE
Maybe you should use the mean of the colors , and the sum of the pixels : ` tot = np.einsum ( ' ijk , k -> ' , arr , np.ones ( 3 ) / 3 . )`
Actually my brightness scale is min = 0 , max = 1.189e-15 for one source and min=0 , max = 3.938e-18 for the other .
You can simply index a csr matrix by using a list of indices .
Now you can generate ` N ` random indices with no repeats ( no replacement ) using numpy's ` choice ` : #CODE
Then you can grab those indices from both your original matrix ` m ` : #CODE
Assuming your ` for ` loop breaks the 1000s of points correctly into chunks of 10 ( which I don't see in the example ) , to create an array within an array , you need to make ` MaxRSS ` a list and then append things to it : #CODE
sounds close , but I get this error : AttributeError : ' numpy.float32 ' object has no attribute ' append '
Instead of looping over 10 points at a time , if you have enough memory to read the entire dataset into an array , then you could reshape the array to a 2D array with 10 values per row , and the take the ` max ` along the rows : #CODE
my_array [ 0 ] .dtype shows Attribute Error : ' list ' has no attribute dtype , I deleted one pair of outer parenthesis and I think it might have been causing some of the problems .
What the stack trace ?
The important thing is that any value > 1 gets broken down into ones .
Is ` reshape ` what I want to do here ?
` reshape ` with a shape of ` ( 1 , 1 )` means you want a 1 by 1 output .
I want to product a matrix with a vector and sum it with a bias .
UPDATE : I've managed to improve the Gaussian fit and get the Lorentzian working , and even ( I think ) managed to calculate the sum of the residuals in each case .
And I think you don't need the ` intensity0 ` parameter , it is just going to be ` 1 / sigma / srqt ( 2*pi )` , because the density function has to sum up to 1 .
Numpy slicing : all entries except these ones
The function numpy.hstack allows you to horizontally stack multiple numpy arrays .
I'm using NumPy , and I have specific row indices and specific column indices that I want to select from .
Fancy indexing requires you to provide all indices for each dimension .
You are providing 3 indices for the first one , and only 2 for the second one , hence the error .
if you go and just join them all into a single list , you destroy that order ( you can no more use moments or similar on it ) .
So we use numpy's ` vstack() ` to stack them together , followed by ` squeeze() ` to remove any redundant axis .
Error `' utf-8 ' codec can't decode byte 0xb5 in position 4158 ` don't seem to happen in the beginning of the file .
newx and newy must have the same shape , so you must reshape them .
Either it is the upper limit of the sum , or it is the index in the array .
The output at position n is computed with a sum up to n .
@USER , yes that was my misunderstanding : I thought ` n ` was some constant limit that OP was introducing as the limit to the sum , hence my first comment .
` tup2d = ( s1 , s2 )` is a single object and ` mat [ tup2d ]` is the same as ` mat [ s1 , s2 ]` ...
I'm practicing with a simple function to compute the sum of an array of doubles .
Strangely my sum function is not giving the correct output .
You need to specify the correct type for sum .
I'm getting an error < string > : 149 : RuntimeWarning : invalid value encountered in sqrt while generating a list
probably ` xs**2 ` returns a number ` 1 ` sqrt with negative number will return nan ( not a number ) #CODE
If i am right numpy provides complex numbers functionality which i think is the only way to represent sqrt ( x ) where x 0
also if you want numpy to give an error on such cases you can use ` np.seterr ( all= ' raise ')` and would give ` FloatingPointError : invalid value encountered in sqrt ` instead of ` nan ` . more info here #URL
dot product of two 1D vectors in numpy
If you want an inner product then use ` numpy.dot ( x , x )` for outer product use ` numpy.outer ( x , x )`
Most efficient way to create an array of cos and sin in Numpy
I need to store an array of size ` n ` with values of ` cos ( x )` and ` sin ( x )` , lets say #CODE
The arguments of each pair of cos and sin is given by random choice .
I believe the answer to this question , without overly complicating the code , is no , since numpy already optimizes the vectorization ( assigning of the cos and sin values to the array )
And all in log representation ...
Ah , you mean in the log representation , right ?
I have got a list containing values like the ones below #CODE
I want to loop through the list and sum the numpy.arrays if the labels are the same .
And you'll get your two lists , the first containing the chosen ones and the second containing the rest .
And depending how that is implemented , np.argsort ( random.randint() ) may be a faster way still to generate a permutation index .
The best way to say " thank you , that helps a lot " on stack overflow is to accept the answer :-)
In another questions ask for replace nan for zeros , mean , etc .
Calculate sums with large binomial coefficients
The problem is that the binomial coefficients are too large I think so it fails .
Here's an reformulated version which shifts the values around a bit ; we can find successive values of the binomial series for each n instead of recomputing from scratch each time , and I've pushed the power-of-a-sqrt into a single operation .
It looks like I need some way to avoid calculating these massive binomial coefficients explicitly .
With ` exact=False ` it uses a gamma function for fast computation , but you can force it to calculate the coefficient explicitly with ` exact=True ` .
It is a credit to stackoverflow and python that I could download , install , and learn enough numpy to translate my code in about 30m !
For my purposes I need to align the time series ( such that the maximum value is centered , pad them with zeros ( so they are all the same length ) , and normalize them before I can do the clustering analysis .
Of course , we have lots of zeros here , so a sparse matrix seems the ideal way of storing the data .
numpy : broadcast matrix multiply accross array
For the ` ( 3xNxM )` case you can reshape to ( 3x ( N.M )) , ` dot ` and reshape the result back , similar to my answer here
I'm sure in your case the dot is faster , because there is less overhead and it will probably use some blas routine , but as you stated that you want to expand to even more dimensions this might be the way to go .
@USER I want the columns to be the the m length subsequences i.e. look at the transpose
You can take advantage of the following broadcasting trick , to create a 2dim ` indices ` array from two 1dim ` arange ` s : #CODE
I've also tried the numpy fft ( fast fourier transformation ) on a similar array ( ones ( 1 ) and zeros ( 2 ) , but i just dont get how to interpret the resulting float numbers .
You can build an array containing 0 ( zeroes ) at the places in your time-span where you don't have time-stamp data , and containing 1 ( ones ) at the places where you have time-stamp data .
If you have it reversed , then you can either use ` np.column_stack ( XX )` or transpose ` np.asarray ( XX ) .T `
I have also tried the reshape function with no success : #CODE
You should know that reshape takes a tuple as an argument .
However I'm more interested in how to implement the medoids-individuation step ( second step in [ 2 ]) , consisting in selecting , cluster by cluster , the sample that minimizes the sum of the distances from the other samples belonging to the same cluster .
Maybe I should transpose and copy the matrix ?
Numpy fft result is unexpected
This is really a question for the DSP stack exchange ( #URL ) .
And plot the magnitude of the fft vs frequencies : #CODE
The simplest way to do this is to write flatten / unflatten functions .
Calculate histogram of distances between points in big data set
I would like to calculate histogram of distances between these points taking into account periodicity .
@USER .K . thanks for tip , I just played with profiling and it looks like 50% of time my code spends on this line : diff [ abs ( diff ) > np.pi ]= 2* np.pi-abs ( diff [ abs ( diff ) > np.pi ])
In that case , you are computing diff [ abs ( diff ) > np.pi ] twice .
In a quick test it proved significantly faster to use ` diff * diff ` .
A little faster still is replacing ` np.sum ( diff*diff , 1 )` by ` np.einsum ( ' ij , ij -> i ' , diff , diff )` .
Also , I'd replace the einsum line and the line above it ( diff2 ) with ` np.sqrt ( np.einsum ( ' ij , ij -> i ' , diff , diff ))` .
I would like to obtain the distribution ( median and std ) of ` sum ( y-x )` distances between the points in ` Y ` and ` X ` .
if one ` y ` point is ` ( 2 , 4 )` and one ` x ` point is ` ( 3 , 5 )` the ` sum ( y-x )` distance would be ` 2-3 + 4-5 = -2 ` .
Now just grab the median and std : #CODE
which is simply the sum of log of probability density function of Gaussian distribution .
And we will discuss how to translate it into ` python ` in the end .
I did resize the images to * ( 60 , 40 ) * before taking the hog features .
Just tried this , but on the line * tmp_hogs [ i , :] =fd* , I get the following error : ** ValueError : could not broadcast input array from shape ( 1728 ) into shape ( 256 ) ** .
I need to compute the partial sum ov ` values ` for each group .
BTW , do you know if there is something similar for the ` min ` operation ?
Below I have generated a noisy sin wave and Fourier transformed it as well as generated the frequency data .
I would like to convert it to a normal distribution with a min of 0 and a max of 1 .
It doesn't make sense why the normal distribution means ` a min of 0 and a max of 1 .
Oh i'm an idiot , i jus twanted to standardize it and can just do z = ( x- mean ) / std .
Oh I'm an idiot , I just wanted to standardize it and can just do ` z = ( x- mean ) / std ` .
If you want to get the indices to that particular array ( so that the lengths match ) , you could use ` np.arange ( len ( np.arange ( -3 , 3 , 1.2 )))`
I do need to sum / average across all the rows .
There are also things like speed , where for example if you sum the elements of a Numpy array compared to the elements in a list it will be faster .
Then I pass these values to the coords list as indices for coords itself , which , in return , holds the filtered x and y indices for your 2D array called ` data ` .
The polynomial results will still expose logarithmical growth ( since ` log x^n = n * log x `) , but the exponential curve will transform into a proper straight line ( ` log exp ( x ) = x `) .
If the time cost t of your algorithm is polynomial in terms of size n , i.e. approximates t = k*n m , then a log-log plot , a plot of log t against log n should be a straight line with gradient m ( and intercept log k ) .
If the time cost t of your algorithm is exponential in terms of size n , i.e. approximates t = k*e m*n , then a log-linear plot , a plot of log t against n should be a straight line with gradient m ( and intercept log k ) .
Once you have the parameters , the gradient m and intercept c of the straight line fit in log-log or semi-log space , you can transform m and c back to your original coordinate space to get the best fit curve in terms of your original data ( the details are shown in the example program below ) .
However this can be a bit misleading because it reflects the fit in the transformed space ( log t vs log n or log t vs n ) , so it's easier just to look at the resulting equations and eyeball the curves .
An alternative quantitative measure would be to sum the errors between your original data and the corresponding points on the best fit curve .
I then use ` groupby ` to sum the strings : #CODE
I have a 21x23 zeros array and I want to count occurences by adding one .
In order to convert it to a 2-D array , with lets say 2 columns , I use the command ` reshape ` with following arguments : #CODE
As the ` reshape ` docs say : #CODE
` sol ` is a permutation , for example [ 1 , 2 , 3 , 4 ] . the function is called when I am generating neighbor solutions , so , a neighbor of [ 1 , 2 , 3 , 4 ] is [ 2 , 1 , 3 , 4 ] .
I am changing only two positions in the original permutation and then call ` deltaC ` , which calculates a factorization of the solution with positions r , s swaped ( In the example above r , s = 0 , 1 ) .
This permutation is made to avoid calculate the entire cost of the neighbor solution .
Before trying to present a solution , it would be great if you could tell us whether this operation has to be done for all ` r , s ` and whether ` sol ` is a fixed permutation of indices or not .
Maybe I have confused you , sol must be a permutation of the solution , if the problem is of size 4 , sol must be a list of len 4 .
Tha max size would be 90x90
Then I want to take a log of all those values .
( therefore Im dividing all the values in each array by .25 then taking the log of that number )
If we imagine you just want to sum an array across multiple processes : #CODE
This approach is slower than a plain ` result = V / D ` without checking for zeros using ` D [ D= =0 ] = float ( ' inf ')` , but it gets better with increasing matrix size .
In this case it's ok to change ` D ` because it's a temporary calculated by ` dot ` .
Calculating ` eig ` will use at least that amount again .
If you are not using all of them , you should only calculate the ones you need , e.g. using ` scipy.sparse.linalg.svds `
python get a list of length X where the sum of all values is Y
I need to find all the combinations of lists who has their sum equal to X in the shortest possible time .
Yes , I know that this will be slow no matter what , but i'm hoping that a pair of fresh eye could see something that I'm missing , or could come up with something new ( python not being my primary language :) )
" I need to find all the combinations of lists who has their sum equal to X [ with length Y ] .
Supported classes include gamma , norm , weibull , bde , em ' .
Finding the indices of the rows where there are non-zero entries in a sparse csc_matrix
I am interested in finding the indices of the rows where there are non-zero entries , in the 0th column .
How do I get the indices I'm looking for ?
CSR and CSC matrices are stored as three arrays : ` data ` , ` indices ` , and ` indptr ` .
The ` indptr ` array stores the offsets of each row / column into the ` indices ` array .
This example shows the sum function in python being used with a numpy array , and how slow it is in comparison with a c function .
Is there a quicker pythonic way of computing the sum of a numpy array ?
Is there a faster way to calculate the sum in python ?
Besides operations with nan's this does not appear to be much faster then numpy's sum .
Python : How to find the a unique element pattern from 2 arrays ?
where ` B ` is a unique pattern within ` A ` .
To sum it up , a quick hack looks like this : #CODE
For larger alphabet , 4-byte int representation can be used . in that case representation must be large enough to always have a leading zero , thus 2-byte for values up to 256 , 4-byte for values up to 256**3 , 8-b yte for values up to 256**7 , etc . and there shouldn't be too many zeros , in which case an offset can be used .
you cannot append to a struct array ; the underlying datastructure of a struct array is a C-struct , not a dynamic python type .
Let the index array be called ` indices ` .
` indices == i ` is a boolean , you can't use that as an index . you probably need to find the indices of ` i ` in ` indices ` , if your interpretation of the question is correct .
I already have indices in array form .
@USER : ` indices == i ` is a boolean NumPy array , so it can be used for indexing .
does something like ` [ x for index , x in enumerate ( pcar ) if indices [ index ] == i ]` , then ?
I have an numpy array with 13 invalid ( logs of zeros ) numbers that I have masked and 3 valid numbers : #CODE
Here is an idea how to vectorize this if there aren't too many unique elements .
can numpy's digitize function output the mean or median ?
Timing for a single outer loop - Demonstrates efficiency of vectorized calculations
Did you intend to use ` * ` as ` dot ` product ?
I know I can use clip , for instance ` a [: , 1:3 ] .clip ( 2 , 6 , a [: , 1:3 ])` , but
1 ) clip will be applied to all elements , including those 3 .
2 ) I don't know how to set the min and max values of clip to the minimum values of the 2 related elements of each row .
And your assignment would probably be clearer if you did something like ` b [: , :] = b.min ( axis=1 , keepdims=True )` , or if your version of numpy does not support ` keepdims= ` , something like ` b [: , :] = b.min ( 1 ) [: , np.newaxis ]` instead of the transpose trick .
We first defined a ` row_mask ` , depicting the ` 3 ` condition , and then apply a ` min ` imum along an axis to find the minimum ( for rows in ` row_mask `) .
The ` newaxis ` part is required for the broadcasting of a 1dim array ( of ` min ` imums ) to the 2-dim target of the assignment .
the code includes ` divide by 0 ` and ` log ( inf )` .
stack = ' C :\ Users\Cian\Documents\ eiffel.jpg '`
@USER the problem with ` time ` starting at any arbitrary values is that the ` exp ` of it will return a huge number , such that the sensitivity of the optimum parameters to the values of ` x ` gets closer to the numerical precision threshold .
I need to find the starting point of the rise phase , marked here by the red dot .
Therefore when working with ` np.int8 ` data the matrix has to be more sparse , since indices take up disproportional amount of memory .
Here is the Error log I get
Identifying range of x array indices to set corresponding y array values to zero
I want to identify the array element indices between t= 0.5 to t= 1.5 and t=3 to t=4 in order to set the corresponding y values to zero ( and put this in ` ynew ` array ) .
I am having trouble with getting the indices into the empty array ` tnew ` , as I just get an output of ` [ ]` .
Sorry , I wanted to identify the corresponding ` y ` array values - the indices are specified in ` tnew ` .
So the values in ` ynew ` are extracted from the corresponding indices of ` y ` , * after which * they're set to zero in ` y ` ?
The difference is performance will increase as the size of the array increases because ` np.searchsorted ` does a binary search that is O ( log n ) vs .
Then I used the indices to create the array .
` arr = np.array ([ lines [ 1 :]) ` creates a flat array of each element in ` lines ` starting at the 2nd element , so I resized it using the ` dim x dim ` you provided with the ` resize ` function .
To leave out some frequencie values : You can get the frequencies values of the fft wih ` np.fft.fftfreq ( binsize , 1 . / sr )` .
( It creates two arrays holding row and column indices ) .
Is there a function that I can use to reshape the ` A ` matrix into the form required by ` ab ` ?
That being said , if you have the full matrix diag() can pull out the diagonal ( by default ) and the bands above and below .
There is a ` scipy.sparse ` matrix type called ` scipy.sparse.dia_matrix ` which captures the structure of your matrix well ( it will store 3 arrays , in " positions " 0 ( diagonal ) , 1 ( above ) and -1 ( below )) .
You can get all the min values of the columns by doing ` np.min ( data , axis=0 )` and the same for max .
I would not do it in one line , because you're calculating the same ` min ` twice .
I would make a new matrix , padded with ` ( n-1 ) / 2 ` zeros around it : #CODE
If you actually want to have the submatrix overlap with the data matrix with only one row , then you'd need to pad with ` n-1 ` zeros ( and in that case even vs odd ` n ` won't matter ) .
" Reasonably fast " means better than looping over indices , e.g. : #CODE
This function will return a list of couples of indices , telling you which rows correspond to each other and is a lot more efficient than the double for loop used in your code .
Explanation : The function ` compare ` calculates pairwise euclidean distances using the binomial formula ` ( a - b ) ** 2 == a ** 2 + b ** 2 - 2 * a * b ` .
This formula also works for l2 norm and scalar products .
The ` nnz ` attribute gives the number of nonzero elements in the sparse array .
Since SQLite3 only provides only a few data types ( slightly different ones than what you said ) , you need to map your input variables to the types provided by SQLite3 .
So either you throw an exception when the type of consecutive values for the same column do not match , or you try to convert the previous values according to the new ones ; but occasionally you may need to re-read the input .
I'd like to be able to correlate this to other data I have in Pandas for the same periods , e.g. the number of " Foo " Events during that time .
Or just get the floor and ceiling : #CODE
Then we can perform an outer join on the two tables , and fill the NA values the join created with 0s : #CODE
On the newly joined " Events " DataFrame , we then create an " AvailabilityDelta " column that is the sum of the " Login " and " Logout " columns ( from the login and logout DataFrames +1s and -1s we added above ): #CODE
Finally we can create an " Availability " column by performing a cumulative sum on the " AvailabilityDelta " column .
How can I draw plot of formula : x ( t )= cos ( 2 ? 10t ) +cos ( 2 ? 25t ) + cos ( 2 ? 50t ) +cos ( 2 ? 100t )
x is a 600*120 training set , with zero mean and unit variance , with max value 3.28 and min value - 4.07 .
X is the training set , with zero mean and unit variance , with max value 3.28 and min value - 4.07 .
Try to append the following code snippet : #CODE
I want to calculate the sum of their ratio : #CODE
To sum values except ` divide by 0 ` , #CODE
I'm trying to resize numpy array , but it seems that the resize works by first flattening the array , then getting first X*Y elem and putting them in the new shape .
Similar thing happens when I try to upsize it say to 7 , 7 ... instead of " rearranging " I want to fill the new cols and rows with zeros and keep the data as it is .
I believe you want to use numpy's slicing syntax instead of ` resize ` .
` resize ` works by first raveling the array and working with a 1D view .
You start with an array of zeros and fill in appropriately #CODE
During the update step , this involves computing the Hessian matrix ` H ` , the gradient ` g ` , then solving for ` d ` in ` H * d = -g ` to get the new search direction .
You don't need ` dot ` , just ` Hsig - z * M.T * ( N * M )` , but I don't know whether it's faster .
This uses the ` sparse dot ` .
It takes longer to generate the data than to do that double dot .
I get close to a 4x speed-up in computing the product ` M.T * D * M ` out of the three diagonal arrays .
If ` d0 ` and ` d1 ` are the main and upper diagonal of ` M ` , and ` d ` is the main diagonal of ` D ` , then the following code creates ` M.T * D * M ` directly : #CODE
But suppose I wanted those innermost elements to lie side-by-side instead ( this would only work because every dimension of the source arrays is the same , including the one I want to ' stack ' in ) , producing a result with shape ` ( 2 , 2 , 4 , 2 )` as follows ?
The best approach I have is to reshape each source array first , to add a 1-length dimension right before the last : #CODE
Scipy labels and measuring max pixel in each label
So I want to measure the max pixel and average of pixels in each label ( in multiple array ) using scipy .
So for each label I want to find the min value of the pixel , the max value of the pixel and the average of the intensities ( the area of each label I can count ) for img and merge .
And I want to be able to make an histogram with these counts per label ( connected area in img ) .
It seems you want to use log scale y-axis , you can try #CODE
@USER I calculate the natural log values of your first several intervals , and the log results are close to linear values , so I thought it's log scale you ask for .
Can you explain what you mean by " the log results are close to linear values " , I am not able to get it
So you need to write some function that convert a poly parameters array to a latex string , here is an example : #CODE
Find the indices of non-zero elements and group by values
I wrote a code in python that takes a numpy matrix as input and returns a list of indices grouped by the corresponding values ( i.e. output [ 3 ] returns all indices with value of 3 ) .
Any way to do it without loops or optimized way to get the pairs of indices per value ?
To find all nonzero elements in an array , by necessity you have to check all the elements .
Regarding the data , it is a 2D matrix with values ranging from 0 to max .
I know the value of max and the matrix has all the values between 1 and max ( I don't care about the zero here ) .
The values corresponding to a previous clustering operation and therefore have no clue about the maximum number of indices to expect since the input of the clustering operation is a user file .
Here's an O ( n log n ) algorithm for your problem .
This is the correct order for numbers that isn't left padded with zeros .
` s_m = sum ( s ) / len ( s )` should give a float point in normal case , your output looks strange to me ...
When I would expect to get an array of indices to use in " MinB " .
Numpy array indexing with partial indices
I am trying to pull out a particular slice of a numpy array but don't know how to express it with a tuple of indices .
Using a tuple of indices works if its length is the same as the number of dimensions : #CODE
So is there a way to use a named tuple of indices together with slices ?
So to pull the first and third column from the following matrix I use column indices 0 and 2 .
which creates a 2x2 matrix of ones .
It seems that numpy's ` mean ` function doesn't know what to do with std :: vector .
Perhaps if you restated it to be " the smallest * nonzero * floating point ...
Plot histogram with x-axis not rendered when bins are logspaced
I am trying to plot the histogram of an array elements .
So I was thinking to plot the histogram on a log x-axis .
The axis are showed clearly but no histogram appears on the plot .
I have a large square matrix which is mostly zeros .
What about rows / columns that have some nonzero and some zero entries ?
First rows with all zeros , then columns with all zeros .
Now , to count nonzero elements , you have to define " element , " more carefully , because a jpg image has three elements per pixel .
During Gaussian elimination , sparse matrices can undergo a large amount of fill-in , where zeros become non-zeros .
For example , one non-zero along the top row could potentially result in all the zeros below it up to the diagonal being filled in ( set to non-zero ) .
Add a reshape to the answer @USER linked to get a 2D array .
Assign sampled multinomial values uniformly at random
I am using ` np.random.multinomial ` to sample a multinomial distribution ` M ` times ( given probabilities ` [ X_0 X_1 .. X_n ]` it returns counts ` [ C_0 C_1 ... C_n ]` sampled from the specified multinomial , where ` \sum_i C_i = M `) .
scipy LU factorization permutation matrix
However , the functions in scipy relating to LU factorizations ( ` lu ` , ` lu_factor ` , ` lu_solve ` ) seem to involve a third matrix P , such that A = PLU and P is a permutation matrix ( and L , U are as before ) .
What is the point of this permutation matrix ?
But every square matrix has at least one row permutation with an LU decomposition .
Basic algorithms avoid this by searching for the entry with the largest absolute value in the pivot column and switching the corresponding row with the pivot row .
This switch can be expensive , so often the largest absolute value entry will have to be bigger than the pivot's absolute value by some factor , e.g. 10 , for the switch to occur .
Note : Since P is a permutation matrix , P^T = P^ ( -1 ) .
Changing the permutation ( aka reordering ) can also affect the number of non-zeros in the L and U factors .
I had to rename some things , as well as add a call to ` Image.fromarray ` when you resize to 360x360 .
I can do it pretty easily myself , but ` mean() ` exists which is basically ` sum / len ` ...
Assuming ` x1-x4 ` are approximately normally distributed around each mean ( 1-sigma uncertainty ) , the problem is turning into one of minimizing the sum of square of errors , with 3 linear constrain functions .
So , want to change the values of those indices not included in result as np.nan
Make another array of all ` np.nan ` , and then replace the values at the indices in ` result ` with the real values : #CODE
If you want to get a list of all indices that are not in ` result ` , you could do this , though I think the above is probably better : #CODE
This works by turning each pair of indices into one element of a structured array , so that they can be compared as set elements to a similar array of all indices .
Regarding the requirement that the integer indices returned by using_filters be used in this solution , I think there is a better alternative .
However , in my case , I have many group ids ( j's ) to extract .
How to predict using a multivariate regression function that is the sum of other regression functions
But if I don't have a final regression function created by using ** fit() ** , and instead I've just taken the coefficients for the final function from the sum of all the coefficients of the previous regression functions , then how can I force the summed coefficients to be a new regression function on which I can then use ** predict() ** ?
An alternative that I've looked into is to just draw all the splines onto a PyPlot plot , and then dump the final image to a numpy array that I can use for other calculations , which seems to run a bit faster , ~ 0.15 seconds to draw 100 splines .
The problem is that the linewidth parameter seems to correspond to pixels on my screen , rather than the number of pixels on the image ( on the 256 x 256 grid ) , so when I resize the window , the scale of the line changes with the window , but the linewidth stays the same .
whose gradient is this :
Here are the implementation the objective functions and the gradient in python ( theta becomes ` x ` here ): #CODE
Your gradient function is likely incorrect .
As @USER . pointed out as a comment , I made a mistake in computing the gradient .
First of all , the correct ( mathematical ) expression for the gradient of my objective function is :
Here's my updated gradient : #CODE
` scipy.optimize.check_grad ` : showed that my gradient function was producing results very far away from an approximated ( finite difference ) gradient .
Then I have to make a histogram for the ' ZBEST ' of the corresponding objects obeying the above conditions .
At last I want to slice the ' ZBEST ' into three slices ( zbest 0.5 ) , ( 0.5 zbest 1.0 ) , ( zbest > 1.0 ) and want to plot histogram and ' M_B ' vs ' UB ' diagram of them separately .
How to center labels in histogram plot
i would like to plot a histogram of it .
This gives me a histogram with the x-axis labelled ` 0.0 0.5 1.0 1.5 2.0 2.5 3.0 .
@USER Yes I want to get rid of the default labels and add the ones I described .
Find number of zeros before non-zero in a numpy array
I would like to return the number of zeros before a non-zero in ` A ` in an efficient way as it is in a loop .
By adding a nonzero number at the end of the array , you can still use np.nonzero to get your desired outcome .
I would expect performance to be similar to nonzero , except that we avoid the construction of nonzero's output array .
so the concentration of a point on the next step depends on its previous value and the ones of its two neighbors .
I'd say just read them one at a time and join them together after all of the files have been read into its own array .
If you want to append , make sure your array uses fortran format .
I have plotted a histogram with Matplotlib , and by visual inspection the values seem to be normally distributed :
Fitting the histogram with the suggestion from askewchan : #CODE
Fitting the histogram with the suggestion from user user333700 : #CODE
What if you plot over your histogram with ` plt.plot ( bin_edges , scipy.stats.norm.pdf ( bin_edges , loc= values.mean() , scale= values.std() ))` ?
@USER : I have plotted that over the histogram and I have put the result in the post under Edit 3 .
One possibility is to visually inspect your data by plotting a ` normed ` histogram with a large number of bins and the pdf with ` loc =d ata.mean() ` and ` scale =d ata.std() ` .
Kolmogorov-Smirnov is based on the maximum absolute difference between the cdf .
chisquare for binned data would be based on the weighted sum of squared bin probabilities .
I tried to use ` resize ` on an array in this way : #CODE
it gave rise to an error , saying that , ` ValueError : cannot resize this array : it does not own its data `
My question : why after applying ` reshape ` the ownership of array is changed ?
The ` reshape ` does not create a new memory and it is performing its operation on the same array memory !
I can check ` ndarray.flags ` always before applying the ` resize ` method .
Calling the ` resize ` function like ` ndarray.resize ` tries to change the array in place , as ` b ` is just a view of ` a ` this is not permissible as from the ` resize ` definition :
To circumvent your issue you can call ` resize ` from numpy ( not as an attribute of a ndarray ) which will detect this issue and copy the data automatically : #CODE
` cannot be broadcast to a single shape `
There are a few things you are not showing , like what is the function ` A , and the full traceback ( including the actual code , ` sum ( pvals [: -1 ]) > 1.0 ` is not here ) .
As a result , sometimes the output has a sum greater than ` 1 ` ( e.g. ` 1.0000024 ... `) .
Also , I'd recommend replacing ` list ( vec ) .index ( 1 )` with ` vec.argmax() ` , which is the same in the case that ` 1 ` is the max value of the array .
I am trying to ensure that the sum is leq 1 , but I am having problems there as well , see my other question : ( #URL ) I guess I should try with python's sum rather than numpy's .
Now , I was trying to ensure that the parameters I passed multinomial had a sum = 1 , but I did it like this : #CODE
I have been assuming that lon / lat positions are absolute , and the projection should be handled by the basemap instance - is this not the case ?
You can use smarter algorithms to flatten the argument list so to include also non-iterable elements ( i.e. the ones constituted by just one item ) .
I don't think they do , but you can use pyOpenCV : #URL or roll your own .
Grouping indices of unique elements in numpy
I want to get the indices where each of the element occur .
The following code borrows a lot from the implementation of ` np.unique ` for the upcoming version 1.9 , which includes unique item counting functionality , see here : #CODE
To get the positional indices for each values , we simply do : #CODE
Note that ` unq_count ` doesn't count the occurrences of the last unique item , because that is not needed to split the index array .
But I can't see how to turn this pseudocode into actual valid Python . abs and max don't operate over an array of tuples where each element in the tuple is itself an array of floats .
` np.fabs ` will simply return an array of the same shape but with the absolute value for each element in a multidimensional array .
QR decomposition is not unique if the matrix is not invertible .
Mathematically , non-correlated data means that cov ( a , b ) = 0 .
` cov ` is the numpy array , ` mean ` is the mean in the row direction .
use ` scipy.special.gammaln ` instead of the ` factorial ` , use sums and exp at the end , and the precision will be much higher .
I need to sum the max and the min of this array : #CODE
Now I need to compare this value ( ` g_1 `) with the sum of the two other elements of the array ( I don't know what value is the max when I program this part of the code ) and I don't know how to do that .
The sum of the other two elements of the array is simply the sum of all elements of the array , minus the max and min values : ` sum ( r ) - g_1 `
The way I wrote it , ` g_1 ` will always be the max plus the min , if you hardcode the max index as you propose it will only work properly for 4 item arrays .
Explanation : ` ind ` are the indices of all the samples where the signal is below the lower or above the upper threshold , and for which the position of the ' switch ' is thus well-defined .
I would like to transpose all of these elements ( ` N*N ` matrices ) in a vectorized fashion .
Finally reshape to ` ( 9 , 3 )` .
( is there ? ) In my version I think data is copied in the final reshape step
I want to plot a bar chart or a histogram using matplotlib .
Note : you can transpose your numpy array with ` my_array.T ` instead of ` zip ( *my_array )` , and the ` zip ` produces the tuples that you found out ...
When you need to transpose the results from ` np.loadtxt() ` , just pass ` unpack=True ` and you get the same result : #CODE
You seem to want to transpose the array .
where ` rows_to_keep ` can be a list or a 1-D array with the indices to keep .
TIL boolean indexing does not broadcast , so I had to manually do the broadcasting .
Sorry , didn't notice this because I did the ` astype ` after the ` broadcast ` when I was testing , so it worked with the copy :P
Why is the histogram method so much faster ?
In reality , my A and B arrays are both 96100 entries long , and the histogram method runs ~10x faster than the one using nanmean .
They both work well , but you're right , the histogram method is a lot faster .
I'm looking for a clean way to transform a vector of integers into a 2D array of binary values , where ones are in the columns corresponding to the values of the vector taken as indices
You are trying to implement a permutation matrix .
See also this question in [ #URL Was wondering if there is some function in scipy.linalg that implements the permutation matrix .
Read about the Yale Format and scipy's csr_matrix to better understand the objects ( indices , indptr , data ) and usage .
Note that I am not subtracting 1 from the indices in the above code .
Use ` indices = numpy.array ([ 1 , 5 , 3 ]) -1 ` if that's what you want .
I have two arrays , and I want to append them into a new array but I need the masked information to be kept .
same as append , losing the masked information
It lists from 0 up to max degree in the graph , which is what ` value ` should return as well .
I used ` A1 = ( A + offsets ) * ( priorita / norms )` but I'm getting ValueError : operands could not be broadcast together with shapes ( 18 , 11 ) ( 18 )
The sum and division in parenthesis are working fine , but the multiplication is returning ` ValueError : matrices are not aligned `
Another solution is to use ` nonzero ` : #CODE
find the absolute diff on each element
already have all header names listed then you can use " join " and
just append the row on the top of the file .
I'm confused here because I'm not putting conditions on what indices to retrieve according to the values , but I need a condition dependent on the indices themselves .
If you really want to eliminate just one row , you could , alternatively , use ` np.concatenate ` to join two basic slices : #CODE
Doesn't this condition produce an array of indices ?
unutbu's answer works , but I find placing the computation in the indices ... icky .
Iterate over the ' rows ' of the transpose : ` for col in A.T : print col.reshape ( -1 , 1 )`
Concerning the ` reshape ` solution , you can enhance the syntax like this : #CODE
I'm not sure what your actual goal here is , but you probably want to transpose the matrix , assuming your goal is standard matrix-vector multiplication : #CODE
Is there any other way to speed up dot product ?
So , your options to improve dot are :
You are doing the dot product of two vectors .
Well , use ` min ` .
If there will ever be duplicates in the integer , you'll want to use ` min ` with a key : #CODE
When I use min it tells me ValueError : The truth value of an array with more than one element is ambiguous .
Originally , I was thinking I could just iterate through the column , collect the indices that match the offending entries , and use that to subset my original array .
Further , even if I did find the correct indices , I'm unsure how to go about the subsetting .
Iterate through the numerical indices of the rows checking your condition .
If the condition is not met , add the index to the list ` keep ` which contains the row indices to keep .
Here is an example which prints the array before slicing , the list of indices to keep , and the array after slicing .
In response to @USER ' s answer : The particular einsum I'm wanting to is ` numpy.einsum ( " ki , kj- ij " , A , A )` - the sum of the outer products of the rows .
All ` einsum ` operations on a pair of 2D matrices are very easy to write without ` einsum ` using ` dot ` , ` transpose ` and pointwise operations , provided that the result does not exceed two dimensions .
The clip -method may help you : #CODE
As explained in the other answers , too large values get ' wrapped around ' , so you need to clip them by hand to the minimum and maximum allowed values before converting .
You can have your full Python stack in your home folder , so no need for root access for anything .
Then , using ` numpy ` , I convert one of the lists into a 2x5 array , and the other into a 5x2 array and take the outer product to arrive at a 5x5 array like this : #CODE
or even shorter ` return sum ( potential ( np.sqrt (( x [ i ] -x [: i ]) **2 )) .sum() for i in range ( N-1 ))`
Keep in mind that this will still give you ` NaN `' s when there are zeros in the denominator .
Creating a histogram for the data in Python
I have created a function in Python that computes the histogram of data .
You only need to pass the data once to get the histogram , not once per bin ...
It looks like the only thing missing in your code was that ( unlike the leading bins which are half-open ) the last bin in the numpy histogram is closed ( includes both endpoints ) , whereas all of your bins were half-open .
The single weight would simply become 1 when they're normalized to sum to 1 .
OLS solution using pinv / svd #CODE
Adding a row to a numpy array : ' numpy.ndarray ' object has no attribute ' append ' or ' vstack '
I have a 2D numpy array to which I want to append rows at the end .
I've tried both ` append ` and ` vstack ` but either way I get an error like : #CODE
or it says there no attribute ` append ` ...
It's better to use lists and append , or create a large array first and then assign into a slice inside the loop .
In some cases you need to check what the orientation of the matrices are , sometimes it's necessary to transpose
In other cases , we usually stack the model so that y is 1D and first part of it is ` z [: , 0 ]` and the second part of the column is ` z [: , 1 ]` .
ValueError : operands could not be broadcast together with shapes ( 10 , 1 , 2 ) ( 10 , 3 )` What should I do to be able to use 2D weights ?
iterate over columns of z and weights or stack them .
And the dot products :
The fastest you'll get with the numpy / scipy stack is a specialized function just for this purpose ` scipy.spatial.distance.cdist ` .
If I understand the problem , you're really asking how to encode the categorical variables such that they can be properly interpreted by the nearest neighbors algorithm .
I didn't think that I can just append they to different array and this way solve my problem .
I am trying to calculate relative vorticity i.e dV / dX - dU / dY and I am currently using the gradient function on numpy .
I would like to know if there is a better way of doing this instead of trying to reshape the array when I want to do dU / dY .
I noticed an inconsistent behavior in ` numpy.dot ` when ` nan ` s and zeros are involved .
Is this specific to the ` dot ` function ?
The result of dot depends on the blas library you are using .
finding max number of consecutive elements using vectorization
As a part of my project I need to find if there are 4 or more consecutive elements in a vector and also their indices . currently I am using the following code : #CODE
It's ` O ( n log k )` where ` n ` is the number of elements in the list and ` k ` is ` GROUPSIZE ` , so don't use it for massively large ` GROUPSIZE ` .
nice trick . but how can I get the indices ?
it should output the indices in the original matrix i.e. [ 2 , 3 , 4 ]
Python slices are defined as ` start ` to ` last+1 ` , so we could increment that array by one , add a zero at the beginning and the length of the array at the end and have all starting and ending indices of consecutive sequences , i.e. : #CODE
Taking the differences from consecutive indices will give us the desired length of each consecutive chunk .
Because all we care for is the differences , rather than adding one to the indices , in my original answer I chose to to prepend ` -1 ` and append ` len ( a ) -1 ` : #CODE
Say that in this array you identify that you want the indices of the chunk of ` 5 ` items , the one in position ` 3 ` of this array .
To recover the start and stop indices of that chunk you can simply do : #CODE
also I also need the indices of those elements .
sum array inside for loop and call it outside of function
Im trying to sum an array that has been populated by a loop , but i can only call the last value in the array .
That value is the last value in the array of 5200 records , rather than the sum of them .
I understand the global variable , but i don't understand why i am only getting the last value in the array , rather than the sum of 5200 records .
But , when I create three 1 billion length python arrays , and convert them to numpy array , then append them .
In the original implementation of sparse matrices , indices where stored in an ` int32 ` variable , even on 64 bit systems .
So whenever your matrix has more than ` 2^31 - 1 ` nonzero entries , as is your case , the indexing overflows and lots of bad things happen .
I'm trying to create multiple rows of three arrays and then stack them vertically so that I would have 16 rows of array a above 16 of b above 56 of c .
Multiply number of distances in distance matrix prior to histogram binning
If you have an array of counts of the same shape as the array of locations , you can pretty easily weight the histogram with that .
For a 10,000 coordinate array pdist only takes ~ 0.8 seconds ( slow , but borderline acceptable ) , but indices takes ~8 seconds and histogram takes about 5 seconds , which is too long .
I don't think you can speed up ` histogram ` though , I think it's already as fast as it can easily get , but you could search elsewhere or ask another question .
If this is the case , then you have 3 diff . equations , and not ` 6 ` as in your ` func() ` ( a sum of lists is the concatenation of the lists , not the list of the sums ) .
To understand what's going on , first we negate the array , turning the ` True ` s into ` False ` s and viceversa , then we reverse the order , then we accumulate the result of OR-ing the array : this will be ` False ` until the first ` True ` , and will remain ` True ` thereafter . reversing this array , we have a boolean indexing array that will get rid of all trailing ` True ` s .
Maybe it is possible to beat the summation : I'd be curious to see how fast is my ( askewchan's really ) ` numpy.argmin() ` solution ( ` argmin() ` is essentially a different way of calculating the sum , but without additions ) .
This is likely due to the fact that ` sum ( takewhile ( ) )` loops through elements without NumPy , handles an iterator , and performs many general Python integer sum .
The only elements that are scanned are the necessary ones ( no need to look before the first ` False ` found ) but , again , this is done with a slower ( non-Numpy ) method .
Numpy : efficient array of indices to " bump " array
Given a length ` n ` array of indices in ` 0 ...
I was wondering why ` numpy.linspace ` does not have the option dtype , as for example ` numpy.array ` , ` numpy.zeros ` or ` numpy ones ` .
Then you may need to reshape mu ( see CT Zhu's answer ) .
I think you just need to reshape the ` mu ` by ` mu= np.array ( mu ) .reshape ( 100,100 )` , and plot it by ` plt.contourf ( n_range , mass_range , mu.T )` #CODE
Where x is the straight on view , y is the horizontal view and z is the birds eye view .
So in this small example there should be 108 unique values I believe ( 9*12 ) ...
D stores the 6 unique elements of the 3x3 symmetric matrix for every voxel in my image .
stack images as numpy array faster ( than preallocation ) ?
I often need to stack 2d numpy arrays ( tiff images ) .
For that , I first append them in a list and use np.dstack .
Arrays in temp are all 2D and I want to concatenate to get a 3D array .
Second order gradient in numpy
I am trying to calculate the 2nd-order gradient numerically of an array in numpy .
There's no universal right answer for numerical gradient calculation .
Before you can calculate the gradient about sample data , you have to make some assumption about the underlying function that generated that data .
You can technically use ` np.diff ` for gradient calculation .
Return the gradient of an N-dimensional array .
The gradient is computed using second order accurate central differences in the interior and either first differences or second order accurate one-sides ( forward or backwards ) differences at the boundaries .
The returned gradient hence has the same shape as the input array .
gradient : ndarray
I think the problem I am having most in python is appending the ones matrices with X1 and X2 since they are np.arrays .
' appending the ones matrices with X1 and X2 ' doesn't sound right .
You are indexing X1 and X2 with the ones .
It is overloaded with different indices and very hard to read .
It might be possible to also vectorize the outer loops , but by far the biggest gain is usually obtained by focusing on the inner loops only .
Simple loops that just compute a sum ( ` accum = 0 ; for item in vector : accum += item `) should be vectorized like ` accum = np.sum ( vector )` .
Conditional summing in a loop can be converted to a vectorized sum with boolean indexing , so ` accum = 0 ; for i in range ( n ): if cond [ i ]: accum += vector [ i ]` can be replaced with ` accum = np.sum ( vector [ cond ])`
Below is some code which uses a callback to print out the current azimuthal and elevation angles , as well as append them to a list for further use later .
Resample and resize numpy array
@USER , all such lists ( regardless of size ) will have the same standard deviation with an error given by the standard error , and an equal ( not low ) probability of being higher or lower than the actual population standard devation , which is ` 1 / sqrt ( 12 )` .
The standard deviation for a uniform distribution is ` ( b - a ) / sqrt ( 12 )` where ` a ` and ` b ` are the limits of your distribution .
In your case , ` a = 0 ` and ` b = 1 ` , so you should expect ` std = 1 / sqrt ( 12 ) = 0.288675 ` for any size sample .
Perhaps what you're looking for is the standard error , which is given by ` std / sqrt ( N )` and will decrease as your sample size increases : #CODE
@USER perhaps you can reshape this to a 2-D array and perform the interpolation in the way you want
Based on your comments you can achieve what you want if you reshape ` data ` , interpolate using the ` DataFrame.interpolate() ` method and then return the array to its original value .
I'm trying to write Cython code to dump a dense feature matrix , target vector pair to libsvm format faster than sklearn's built in code .
Add the matrix and its transpose to fill out the LD region
where ` b ` is nonzero : Wipe out the rows and place a 1 in the diagonal ( this represents an initial condition )
where ` b ` is zero : Sum the matrices along ` axis=1 ` , and place the negative value of the sum into each diagonal ( this balances out sinks and sources )
They don't handle item deletion , transpose , etc .
I am looking for a function like append that I normally use to create a vector with all the solutions that I generate , but this time each solution is not a single value but an array of 6 values and I am not able to generate what I want .
You can append an array to an array of arrays , what's wrong with that ?
When I reshape the data as data.reshape ( 3 , 25 ) , can you teach me how to apply cubic interpolation ?
you can " filter " the ` np.ndarray ` doing ` cond= new_data.shape [ 1 ] - np.sum ( np.isnan ( new_data ) , axis=1 ) > =4 ` , and then creating the dataframe like ` pd.DataFrame ( new_data [ cond , :]) ` , this will accept the ` cubic ` interpolation , but in this case some of the ` nan ` s still do not vanish ...
Also , once I've got a 3x2 array , how do I stack 3x2 arrays on top of each other ?
In this case , ` curr_g ` does end up with ` ( 3L , )` , but it doesn't stack up alright .
I wanted it to get assigned ` temp_g ` only when it's empty , but it seems to be doing the reverse ... and it only holds the current value of ` temp_g ` , but doesn't stack up with the previous value .
The problem is in how you calculate ` curr_g ` then , and not on how to stack the arrays together .
When it's empty I need to simply assign it the value of ` temp_g ` , and when it's not empty , and it already holds the previous ` temp_g ` , I need to stack the new ` temp_g ` onto it .
you can reshape it to ` ( 3 , 1 )` with ` temp_g = temp_g.reshape ( 3 , 1 )` , but you will end up with a final array with size ` ( N , 3 , 1 )` , not ` ( N , 3 , 2 )` .
Use ` choice ` to choose the 1dim indices into the array , then index it .
Choosing indices is the 1dim problem ` choice ` knows how to handle .
Unless I am wrong , aren't sparse matrices the ones with many zeros ?
Doing that just keeps the array filled with zeros .
Fitting a Gaussian to a histogram with MatPlotLib and Numpy - wrong Y-scaling ?
I have written the below code to fit a Gaussian curve to a histogram .
You need to normalize the histogram , since the distribution you plot is also normalized : #CODE
Note also that I changed your sample data , because the histogram looks weird with too few data points .
If you instead want to keep the original histogram and rather adjust the distribution , you have to scale the distribution such that the integral over the distribution equals the integral of the histogram , i.e. the number of items in the list multiplied by the width of the bars .
Is there a way that rather than normalizing the histogram to the distribution , I could just not normalize the distribution in the first place ?
You can of course multiply the distribution by the total number of items divided by the number of bins in the histogram .
I'd like to efficiently sum the first N non-missing values of a pandas DataFrame .
And I wanted to sum the first 3 non-missing values per row , I could loop over the dataframe as follows : #CODE
However , to get the row sum , one needs to specify axis=1 .
Python , random sample from a vector's non-zero indices
I want to find randomly sample k indices of its zero position .
sorry , i wrote the wrong code , your answer works for nonzero , how to make it work for zeros ?
However , you could use the more intuitive ` nonzero ` function : #CODE
I don't see why the degrees of freedom need to be changed for ` std ` for this purpose : #CODE
Well the ` pandas '` versions of ` mean ` and ` std ` will hand the ` Nan ` so you could just compute that way ( to get the same as scipy zscore I think you need to use ddof=0 on ` std `) : #CODE
Names are identical on both sides ( and the data is simmetrical around it's main diagonal ) .
Numpy : 3D to 2D maxtrix reshape with flattened order along one axis
I'd like to read in very large binary files ( GB+ ) from disk and do a reshape for further processing .
I want to reshape the data in a way that they are in shape ( F*N , B ) , so that I can easily operate on blocks .
If you want to merge the first two dimension then together , you can apply an appropriate reshape : #CODE
I have it starting with the the second record ( index 1 ) in my range and then comparing to the max of values from index 0 up to i .
It would be also more efficient to store the current max value instead of performing ` np.max ( x [: i ])` ( which is linear ) .
You can get the indices of the positions where ` g ` is increased with : #CODE
You would need to forcibly reshape both of them .
Also , the transpose of a 1D array does not change the array .
To circumvent this you either need to use ` np.exp ` or sum elements together to return a single scalar .
The error message is ` IndexError : arrays used as indices must be of integer ( or boolean ) type `
However , this can be replaced by anything else you may be interested in , such as the mean , sum , etc .
@USER : If you reshape the 4D array to combine the last to axes into 1 axis , then perhaps you can do the rest of the computation with one function call ....
Note that number of rows per ` a ` grouping in the actual data is variable , so I cannot just transpose or reshape ` pdf.values ` .
You can use reshape : #CODE
You are attempting to sample from the distribution represented by the * histogram * ` A `
Aah , true , my bad -- I thought it gives back indices .
What we're doing is building a population defined by the histogram A and then randomly sampling from it .
If the real-world case has N large and sum ( A ) small , and / or you need to sample A many times for a fixed A , this should be better .
Each element of the ` vals ` array will be an index picked from the ` choices ` array , so you need to add 1 to the appropriate element of ` B ` for each of these chosen indices .
applying arithmetic , such as sum , to bool values , treats them as integers 0 and 1
You can use ` argmax ` along axis=1 to find the index of the first ` True ` value in each row , and then construct a new array and assign ` True ` in those indices : #CODE
To make this work , we can mask the indices : #CODE
You can also automatically get tracking out of it if you put in a 3D array ( i.e. stack each map into a 3D array ) .
I have tried to increment the stack depth by :
Modifying you example ( group by random integers instead of floats which are usually unique ): #CODE
Usually , If I need single values for each columns per group , I'll do this ( for example , sum of ' a ' , mean of ' b ') #CODE
If we were to append ( instead of prepend ) `' 2R : '` , would this function handle its task significantly differently according to whether ' S100 ' leaves enough space or not ?
@USER , According to an experiment ( #URL ) , ` add ` ( both prepend / append ) truncate trailing parts according to the size specified .
go to the log file and paste it somewhere , usually there are enough detail there , then we can help .
The histogram density takes bin widths into account and only the integral over ` bin value * bin width ` actually sums to one .
You need to group columns with command grouby and also to play with command stack and unstack .
numpy - how to outer join arrays
Basically the equivalent of a SQL outer join ( where the ' pos ' field is the key / index ) #CODE
It sounds like the join key is not just pos , but also any shared column names .
I think the off diagonal terms should ( ? ) still be zero , though , even if the rows aren't unit vectors .
If they are complex , then ` A.dot ( A.T.conj() )` will be a diagonal matrix .
If the rows are orthogonal , the R factor will be diagonal .
The columns of Q corresponding to non-zero diagonal entries of R form an orthonormal basis of the range space .
If the columns of X=A T , i.e. , the rows of A , already are orthogonal , then the QR decomposition will necessarily have the R factor diagonal , where the diagonal entries are plus or minus the lengths of the columns of X resp . the rows of A .
For step 2 one would need to compute the number of ones for each row .
Now I will go through each element of this vector to check which elements are zeros .
If you would like for 2 to be a relative min , you can add the mode= ' wrap ' argument to the function call .
I tried by many things but the I have bad histogram , if do you think there is
Clipping a histogram plot on the y axis to ( 0 , 1 ) makes no sense because histograms count events , and usually you'll have more than 1 event in many of the bins .
It sounds like you want to scale the histogram to divide the number of events by the total number of events .
As I suggested , dividing the histogram by 1 / #events will give you the frequency of the bins , and ` pandas.Series.quantile ` makes a good inverse CDF .
When you say " I am trying to plot y between ( 0 , 1 ) or between ( 0 and lees than 1 )" , you mean that you want the bars to sum up to 1 ?
as seen here : plotting histograms whose bar heights sum to 1 in matplotlib
Note : Normed = True allows you to have the integral over the bars equals to 1 ( rather than the sum of the bars )
Since D has some zeros then I will get a division by zero error .
Using the eigendecomposition of the transpose for example .
I've included my INITIAL ` cprofile ` log here #URL As I interpret it , it is saying most of the time is spent in ` _nonLinEq ` and ` _jacNLE ` which are essentially made up of the code in 1 and 2 respectively .
The basic idea of the code is updating multinomial distribution parameters based on initial estimate ` m ` , observations ` x ` and an inverse covariance matrix ` W ` representing our confidence in the intial estimates .
If you n is large this will be a fairly expensive computation since you are flattening , copying to a new array and then turning that create a new matrix with that array as the diagonal .
First of all : ` np.sum ` It ' s strange but it can even be slower than pythons built-in ` sum ` -function .
I once tried to avoid it by either multiplying the array you want to sum up with a ` matrix ( ones ( N ))` * or writing a function " in " ` numba ` .
[ EDIT :] * I predefined ` onesN = matrix ( ones ( N ))` so I would not have to build it every time I just wanted to sum up an array .
Numpy fft freezes for longer samples
Also , if I explicitly specify the length of the fft the function works fine , but it detects wrong frequencies ( it detects frequencies shifted by one or two semitones higher ) .
For samples where fft_length is smaller than 216090 numpy.fft works fine , for any longer fft , the algorithm hangs but it continues after I press ctrl+C in the terminal .
The FFT algorithm used in ` np.fft ` performs very well ( meaning O ( n log n )) when the input length has many small prime factors , and very bad ( meaning a naive DFT requiring O ( n^2 )) when the input size is a prime number .
Likewise B [ l ] is a 1D array of length M . and I want to multiply transpose of A [ l ] into B [ l ] .
I want to multiply transpose of A [ l ] which would be NxM into B [ l ] which would be Mx1 and get a Nx1 array
Pyramidal 3D histogram in matplotlib ( as in 1976 historical movie about SVD )
By using numpy and matplotlib , it's possible to easily plot a 3D histogram :
ProcessRawData : To reshape the byte list into an array that matches the pixel orientations
Data is unique in that every block of 14 rows represents 14-bits of one row of pixels on the device ( with 32 bytes across @ 8 bits / byte = 256 bits across ) , which is 256x256 pixels .
Let's see if anyone else has anything to add to squeeze any more frames out of this with anything else .
I really appreciate your help and eye for detail - Thanks !
np.diff offers the option of calculating the second order diff , but the gradient doesn't .
Critical point is the point where the first derivative ( or gradient in multi-dimensional case ) of a function is 0 .
` numpy `' s ` diff ` function is good for this case .
You passed negative weights and as the docstring says , the sqrt of weights is used .
this is my first stack overflow question so please pardon any ignorance about the forum on my part .
using histogram counts in scatter
this my code to and i want to use histogram data to plot scatter where y axis is counts center from the histogram , is there any direct command or way to do this ?
Do you just want the same plot but without the histogram bars ?
Me E thank you for taking time to look at my question , i want the scatter between ( t2.compressed() *10**10 ) as x , and number of occurrences as y .. which i should get it from the histogram ..
Then why do you plot the histogram instead of only calculating it ?
This both calculated the histogram AND plots it .
From the comments I gather you essentially want to remove the histogram from your plot ?
This was you only calculate the histogram and don't plot it ( ` A ` is exactly the same return as ` hist ` also calls ` numpy.histogram ` .
There are still some gotchas : for instance , if you're expecting an outer product ( that is , in Octave X is a column vector , Y a row vector ) , you could look at using ` numpy.outer ` instead , or be careful about the shape of X and Y .
The looping over indices way #CODE
Good question , I should be more clear : I mean comparing using numpy as intended vs . looping over numpy arrays with indices .
so your variable loop are python object not c ones
Although you didn't reach it because of the error , your transpose operation also will not work as expected , since the transpose of a 1D array does nothing .
Second , you are doing transpose the hard way .
Next , in the matlab version you transpose the first w0 : #CODE
And the histogram returns the bin edges , which you don't want .
Also not an error , but you can use arr.real and arr.imag to get the real and imaginary parts in python , and arr.sum() to get the sum .
Python lets you broadcast low-dimensional arrays to higher dimensions , so you can do something like : #CODE
Would I then have to turn 0.99986900000000001 into a string and truncate to actually get 6 decimal places when I write it out to a file ?
Your ` %pylab inline ` call is overwriting the ` sum ` builtin , which messes up your timings .
I could use zeros instead of NaNs , but I'm making a vector plot and with zeros basemap / matplotlib's quiver() function places a point at all the zero vectors .
I could fill with zeros , zoom , and then go back and flip the zeros to NaNs , when I do this it doesn't leave the zeros as zero , it fills them with very small values .
For a 2D array numpy expects a list of row indices and then column indices , with ` zip ( *ix )` you are providing this : #CODE
Bivariate Legendre Polynomial Fitting to find orthogonal coefficents
If that is the case , the fit is a sum of many bases ` x^a*y^b ` , each multiplied by a coefficient .
Matplotlib 3d Bar chart : Error : cannot concatenate ' str ' and ' float ' objects
I am getting error in my code : " Error : cannot concatenate ' str ' and ' float ' objects " .
TypeError : cannot concatenate ' str ' and ' float ' objects
I want to display variation of ' DURATION ' ( we can take sum of duration ) with respect to ' type1 ' , ' type2 ' and ' type3 ' in 3D bar chart at the same time in the chart .
please provide stack trace for memory error
I need ` numpy ` to perform ` fft ` functions .
What I want to do is to sum these outputs from each node to one big waveform that will be the input to each receiver for demodulation etc .
the transmitters send asynchronously and therefore a ` start_clock ` and an ` end_clock ` has to be maintened per transmitting node so as to sum the waveforms properly
I can't fix ` end_clock [ j ] - start_clock [ j ]` but I would be able to produce column aligned outputs for example ` aligned_output ` with total length max ( end_clock ) ( as in the waveform ) if I could somehow pad with zeros before start_clock and after end_clock ..
Is your code to compute at a point the sum of all the Green functions ?
If it's really about size of the arrays and you can't use ` dot ` , it might be worthwhile looking into ` numexpr ` .
@USER You are likely better off writing a few simple loops where you complete your dot product in chunks .
@USER , what if I need to convolve ` output [ j , :] ` with ` attenuate [ i , j , :] ` , where ` attenuate ` would be a 3d array ( no_nodes x no_nodes x no_samples_for_channel_model ) ( suppose that attenuate is a channel model representing the channel that the signal from transmitter ` j ` goes through to reach receiver ` i ` .
I found the dot function for dot product but if i want to multiply a.dot ( b ) with these shapes : #CODE
I tried also transpose function but it doesn't work .
Transposing either array does not work because it is only 1D- there is nothing to transpose , instead you need to add a new axis : #CODE
To get the dot product to work as shown you would have to do something convoluted : #CODE
You can rely on broadcasting instead of ` dot ` : #CODE
Or you can simply use outer : #CODE
By dirty hacks , I mean inconsistent ones .
Instead of dynamic memory allocation via C we have a single global array with integer indices ( also at global scope ) , but otherwise it's much the same .
All the " intelligence " and utilities , like error cross checking of the user-input , help pages , tutorials howto , etc . would be implemented in the Python wrapper , which would parse these inputs , translate them to the corresponding commands for your Fortran program , send them and wait for the results .
Two things : First the roll axises is an interesting idea ; however if you run the python loop solution it is actually faster then your roll axis solution .
What I want to achieve is to dot product each example in A and B and sum the result : #CODE
Don't name your variable ` sum ` , you override the build-in ` sum ` .
In fact a solution based on ` map ` and ` sum ` is , albeit simpler , even slower : #CODE
Building the columns of a numpy array - why does the first column remain full of zeros ?
But my array actually turns out with the entire first column being zeros , like this : #CODE
Calculation on 2D array rejecting special values ( knowing the indices )
@USER the indices start at ` 0 `
Fastest way to sum two lists of items with each other in python
I want to create a new list ` list_3 ` such that every element ` i ` is the sum of the elements at position ` i ` from ` list_1 ` and ` list_2 ` .
For every ` i ` such that ` 0 = i len ( list_1 )` , if the item at position ` i ` ( i.e. ` list_1 [ i ]`) is 0 , then the sum of ` list_1 [ i ]` and ` list_2 [ i ]` should also be zero .
That doesn't sum to 0 if the item in ` list2 ` is zero .
Should be ``` sum ( t ) if t [ 0 ] !
Here is an example that illustrates both a regular sum and a sum with a conditional , using zip : #CODE
python pickle dump and load private variables
So I've got this code to put the constant " outer " elements together with the changing ones : #CODE
numpy 2d array max / argmax
I can get the indices for the maximums using A.argmax ( axis=1 ) , in that case I would get : #CODE
How can I use the ' indices ' array to get an array of maximum values for each row in the matrix ?
You can fancy-index using the indices ` np.arange ( len ( A ))` on first dimension ( since you want a value per row ) , and your indices ( squeezed ) , which correspond to the index on each row , on the second dimension : #CODE
, which people feel my question is a duplicate of , the author actually asks how to make the elements of each row sum to one .
This is different than normalizing each row such that its magnitude is one ( the sum of the square of each element equals one ) .
Since you essentially want to make the euclidean norm of each row equal to 1 , with recent versions you could just do ` result_norm = result / np.linalg.norm ( result , ord=2 , axis=-1 ) [: , None ]` .
Therefore the gradient / derivative function is not a smooth one and we wouldn't be able to use many of the other methods provided by ` scipy ` .
If I understand correctly , you want to take a 2D array and tile it ` v ` times in the first dimension ?
Another alternative is ` np.tile ` , which behaves slightly differently in that it will always tile over the last dimension : #CODE
Numpy : operands could not be broadcast together with shapes
ValueError : operands could not be broadcast together with shapes ( 3,150 ) ( 150 , 4 )
A sum of products ?
The three components on the left correspond with the subscripts for ` gamma ` , ` xbar ` and ` xbar ` , the operands being passed to ` np.einsum ` .
` gamma ` has subscript ` nk ` , just as in the formula you posted .
That tells ` np.einsum ` to sum over ` n ` .
This tells ` np.einsum ` that we wish to advance the ` k ` subscript in lock-step , but ( since it appears on the right ) we don't want to sum over ` k ` .
Reading the numpy documentation for ` np.fft.fft ` , it mentions that if ` A = fft ( a )` then ` np.abs ( A )` is its amplitude spectrum and ` np.abs ( A ) **2 ` is its power spectrum .
EDIT #2 : For posterity , np.fft and the matlab fft are identical except that the former has zero-based indexing and the latter has one-based indexing .
If the docs are too general for you , I'm sorry to say but you're out of your depth implementing your own PSD , even with a library fft available .
In Matlab , what I would have to do I believe is just ` abs ( fft ( x )) ^2 / L^2 ` , where ` L ` is the length of my signal .
What I would like to do is to sum the Y values corresponding to each bin without doing a " for " loop .
Assuming that your y-values are at the corresponding position , i.e. , ` y [ i ] = f ( x [ i ])` then you can use ` numpy.digitize ` to find the indexes of the bins that the x-values belong to and use those indexes to sum up the corresponding y-values .
then sum up the values in y : #CODE
What would be the max / min values of the fourier transform performed on a list as described above be ?
You can see the spike at index ` 1 ` indicating a sin wave with period equal to the number of samples was found .
If you wanted to get the unique elements out of ` a ` and do not care about the order in ` b ` , i.e. ` b ` and therefore ` c ` will look differently , then it can be simplified to #CODE
If your array ` a ` has ` m ` elements and ` b ` has ` n ` , the sorting is going to be O ( n log n ) , and the searching O ( m log n ) , which is not bad .
Since the array ` b ` contains unique elements , equality with an element of ` a ` can only ever be with one single element of ` b ` .
it doesnt correlate ?
You just need to decode the string back into ASCII , so it would just be : #CODE
Basically , you're saying you want ` position-width ` if it's nonzero , or ` 1 ` otherwise .
Create a matrix which lower triangle ( ` np.tril `) is filled with values of your array ` nums ` and your upper triangle ( ` np.triu ` , with second parameter 1 , so the diagonal stays free ) is filled with the maximum of the array .
And really , using ` max ` wasn't too bad ( it only adds an O ( N )` term onto an already ` O ( N^2 )` algorithm .
For any NumPy universal function , its ` accumulate ` method is the cumulative version of that function .
( ` zfill ` pads a string on the left with zeros to fill the width . ) #CODE
( e.g. , there is a function ef and egradf that evaluate the function and its gradient , respectively )
I have data wherein I have a variable ` z ` that contains around 4000 values ( from 0.0 to 1.0 ) for which the histogram looks like this .
I have been using ` numpy.random.normal ` but the problem is that I cannot set the range from 0.0 to 1.0 , because usually normal distribution has a mean = 0.0 and std dev = 1.0 .
These keyword arguments are ` loc ` ( mean ) and ` scale ` ( std ) .
` y = np.random.normal ( loc=mean , scale=std , size=1 )` returns an array of size 1 so I am simply removing the first ( 0th ) value of it to append to the list .
Otherwise you'd append 4000 numpy-arrays with 1 element each .
Of course I picked arbritrary values for ` mean ` and ` std ` which could effect the levels at lower values but there ** is ** a difference in shape .
get the min and max just to show how np.percentile works #CODE
I have tried what you told , but I cannot show you the histogram plot that I got here in the comments .
Since in this case , the cdf could be obtained by a cumsum on the histogram , rejection sampling seems overkill
If you can approximate the cumulative density function for the distribution ( for example by taking cumsum of histogram ) then sampling from that distribution becomes trivial .
You can reconstruct ` a ` and ` b ` taking the unique elements : #CODE
Finding indices in Python lists efficiently ( in comparison to MATLAB )
I have got difficulties to find an efficient solution to find indices in Python lists .
I basically want to find indices for a list of floats occurring in a ( longer ) list of floats if that makes sense .
If you need the indices as an array instead of a list , use ` np.array ( indices )` after that .
If my array b would look like the following : ` b = np.array ([ 0.9 , 2.1 , 4.2 , 2.9 , 1.1 ])` , is there a way to find the indices where ` b [ i ]` is closest to one item in ` a ` ?
What happens when ` max ( b ) > max ( a )` ?
You can easily remove all ` b `' s smaller than ` min ( a )` or greater than ` max ( a )` in a step before the sorted-search ( or afterwards ) .
The next filter uses the data after the first filter has run and checks if the sum of the last twelve time steps exceeds a certain value .
When both filters are done I want to sum up the values , so that at the end of the year I have one 400x700 array with the filtered accumulated values .
The older arrays that are filtered I could then add to the yearly sum and delete , so that I do not have more than 16 ( 4+12 ) time steps ( arrays ) in memory at all times .
The main use I would want is to able to use the system installs of pyside / pyqt / pygtk for putting together the scientific python stack in a virtual enviroment .
numerically stable way to multiply log probability matrices in numpy
I need to take the matrix product of two NumPy matrices ( or other 2d arrays ) containing log probabilities .
or other combinations of tile and reshape also work but run even slower than the loop above due to the prohibitively large amounts of memory required for realistically sized input matrices .
In your example I don't think that ` scipy.misc.logsumexp ` is doing what you think it is - according to [ the docs ] ( #URL ) the ` b= ` parameter is actually a scaling factor for ` exp ( a )` , i.e. ` np.log ( np.sum ( b* np.exp ( a )))` .
It will be CPU-time-faster to expand the dot product and rewrite it in pure Cython ; but much developer-time-slower .
@USER I really wonder where you encountered this problem ; the result of that dot product is no longer a matrix of probabilities .
I.e. , it pulls out the max before starting to sum , to prevent overflow in ` exp ` .
The same can be applied before doing vector dot products : #CODE
The final form has a vector dot product in its innards .
This creates two ` A ` -sized temporaries and two ` B ` -sized ones , but one of each can be eliminated by #CODE
I need it because I would like to compute the hyper volume of some function of a histogram .
My legend currently has an entry for each stack , `' a '` , `' b '` , and `' c '` .
It's implied in the nature of your graph that the sum of two sections represents the combination of two samples ( by the way did you really mean ` blue + green = ' combination of a + b '` , because ` a ` is red in your example ) .
How about use traceback to print the stack ?
( Of course I could introspect the stack and extract that information manually but I was hoping for something simpler ... )
numpy machine precision for dot product ?
This is particularly an issue because I am taking dot products of vectors and it seems like the results is affected , i.e. a is treated as a ' negative ' number .
a dot product can be a negative number when angles are ** obtuse **
In the first dot product the angle is about 180 degrees , and the dot product should be approximately negative the product of the lengths , which it is .
In the second dot product the angle is 90 degrees , and the dot product is zero .
relative difference ( rtol * abs ( b )) and the absolute difference atol
are added together to compare against the absolute difference between
The point is that if I have two arrays , a and b , with a bunch of - 2.22044605e-16 in them , and I take their dot product , the results are different ( see the addition in the question )
Using ` max ` is a brilliant idea , but your implementation is not taking full advantage of numpy .
Essentially what this does is reshape to take the maximum in pairs of two horizontally , then shuffle things around again and take the maximum in pairs of two vertically , ultimately giving the maximum of each block of 4 , matching your desired output .
Average of iterative sum of arrays with nan elements
Possibly , I could also change the way to sum the elements , if ` nansum ` is not the best way , but this is how I did it until now .
In that case the OP do not sum iteratively .
You can't simply keep track of the number of NaNs in a , take the sum of b after the loop and divide by the total number of non-NaN elements ( ` len ( whatever ) * a.size - totalNaNs `) ?
In case yes , you should roll back your question to the previous review .
To be precise , I want an additional column that has the values that correspond to the indices that the first column contains : ` df [ ' j '] [ 0 ] = grid [ df [ ' i '] [ 0 ]]` in column ` 0 ` etc
I want to print numpy arrays nicer , using the indices into the array ` [ 0 1 ]` indexes row zero and column one :
which can be ` reshape ( 5 , 5 )` .
No need to reshape , unless it is to ` ( 5 , 5 , 2 )` .
and I want to group each similar lines ( with the same five columns in the middle ) and show in the output the minimal of the first column and the average , median , mean , min , max ,...
but nothing really seems to work , it only work for the min and max but to get each result I have to only use one argument ... like this : #CODE
for the grouping it works well , i have problems with the average / min / ...
Np :) I've updated the answer a bit , it's easier and probably faster to use numpy to transpose and cast to float than using ` map ` and ` zip ( *lst )`
I need to get the cumulative sum across a numpy array such that , for
each value , I generate the sum of all values that are less than or
The pattern is : for each value , sum all values over entire array that are less than or equal it .
something like ` [ sum ( x for x in arr if x <= a ) for a in arr ]` ?
shouldn't it be ` [ 11 , 4 , 4 , 7 ]` ? or did I missed the part where only unique values must be summed ?
@USER , Where did OP say that only unique values must be summed ?
Essentially , what I'm trying to do is sum all values across the entire raster ( which has been converted to a 2D array ) that are less than or equal to the value / pixel in question .
Whether or not they are unique values .
Where of course ` a ` is your array flattened , that you would then have to reshape to the original shape .
If the sum only considers items once no matter how much they appear , then , #CODE
For a 2D array ( assuming you want the sum of the whole array , not just the row ): #CODE
I've got the ` x ` and ` y ` values , and want to plot them in a 2d histogram ( to examine correlation ) .
Why do I get a histogram with limits from 0-9 on each axis ?
Firstly you should have 5 bins in your histogram ( it's set to 10 as default ): #CODE
I have a data frame in which I want to identify all pairs of rows whose time value ` t ` differs by a fixed amount , say ` diff ` .
For example , if ` diff = 22.2423 ` , then we would have a match between rows 4 and 7 .
But as I have a log of values ( 10000+ ) , this will be quite slow .
Further , I want to look and check to see if any differences of a multiple of ` diff ` exist .
So , for instance , rows 4 and 9 differ by ` 2 * diff ` in my example .
If I can do this , then I can simply compare ` df.t ` , ` df.t - diff ` , ` df.t - 2 * diff ` , etc .
If you want to check many multiples , it might be best to take the modulo of ` df ` with respect to ` diff ` and compare the result to zero , within your tolerance .
On method 2 , you are comparing and looping using Python ; but on method 3 , you're much closer to the processor and using optimized instructions to compare and sum .
Python replace zeros in matrix with ' circle ' of ones
I can create a large matrix of zeros and replace a rectangular region within the matrix with ones like so : #CODE
How would I replace a circular region in the matrix with ones ?
How do i create a histogram out of it with ` bin sizes=1 ` in the interval ` 0-10 `
Do you mean the histogram of the keys , or the histogram of the values ?
You can use the ` keys ` as bin positions and the ` values ` as weights of the histogram , doing the following : #CODE
Histogram of sum instead of count using numpy and maplotlib
Both Matplotlib's ` hist ` function and NumPy's ` histogram ` function have a ` weights ` optional keyword argument .
I am looking for an efficient way to perform a matrix multiplication ( dot product ) of two time-dependent 2D matrices to end up with one time-dependent 2D matrix .
Likely in this case the actual frequency is really close to a DFT bin , so that the value of each DFT bin is close to the maximum and we do not see the zeros , and then we do not see the scallops .
The LSB of 16-bit PCM has a signal power of about -90dB , so if this is what you're using , the noise floor isn't going to be any lower than that .
If using single precision floating point , you can probably looking at a noise floor about ~-13 8d b .
numpy multidimensional indexing and diagonal symmetries
Note that the last line contains ` iy ` and ` ix ` indices in inversed order , essentially creating a transpose of the lower triangular matrix .
Also , why don't understand why you use indices 1 and 2 and don't reuse ix and iy ?
Is it really sufficient to only use 2 indices and not 4 , i.e. all 55*55 combinations ?
The adding of different axes is very important , since this way numpy will generate the cartesian product of the indices .
I suppose that the problem could be with memory aligning or something like that when the dot products is calculating , but that's just a thought ...
Ok , the problem was with uninitialized vector output , that when called from c++ contained zeros , and random stuff when called from python .
( Btw . you wouldn't need the explicit conversion in the return statement ) The append operation isn't that expensive , but it can sum up .
This generates random numbers in chunks and saves the good ones .
Probably it's much better for perfomance to prepare an numpy array ( using zeros ) and fill it without creating python auto-growing list .
The benefit of this is that you get a 2D array off the bat -- I don't believe you can coax ` numpy.fromstring ` into giving you a 2D array -- You'd have to somehow figure out the dimensions and reshape it .
can anyone tell me how to make the cov works with large variables ?
@USER I only have 4GB of memory , the X.T occurs in numpy library calculation of cov .
@USER when I try to call np.dot ( data_mat , data_mat.T.conj() ) it gives me ValueError : array is too big . the dot function can't handle ( 39906x39906 ) .
Slice a Pandas dataframe by an array of indices and column names
I want to pass an array of indices and column names and get a list of objects that are found in the corresponding index and column name .
This seems to work okay for a simulated pure sin wave , but when performed on actual datasets , the amplitudes are always attenuated by some amount .
Numpys fft assumes that dt is the same for all your data .
So now i want to append b to the last column of a
The sum of the elements of ` V ` is always 1 .
One way I thought to solve this is by summing all the elements of ` E ` and then stop the loop when the value of the sum is below a threshold , but that resulted in a zero as the result of the sum in the very first iteration because some elements were negatives and other positives .
Edit : I just thought of using the absolute value of ` E ` .
and 2d array of numbers ( so I can use np.savetxt and dump it into a file ) .
If all you wish to do is dump ` x ` and ` y ` to a CSV file , then it is not necessary to use a recarray .
When I was trying to solve a scientific problem with Python ( Numpy ) , a ' shape mismatch ' error came up : " shape mismatch : objects cannot be broadcast to a single shape " .
In the above code , ` ff ` has 7 values for each node and I was trying to sum up the 7 values to get a new nx*ny array .
When you index using arrays of indices ` a , b , c ` like in ` ff [ a , b , c ]` , ` a , b , c ` must have the same shape , and ` numpy ` will build a new array based on the indices .
Check if the ` sum ` method produces the result that you need : #CODE
Looks like you want to sum ` ff ` over the last dimension , with the 1st 2 dimensions covering their whole range .
But you can get the same result without looping , by using the ` sum ` method .
More specifically , I have a ( y , x ) array , which I want to tile multiple times to create a ( z , y , x ) array .
Unless you do some sort of ` dot ` product on the y or x dimension , you could still end up with memory error .
One simple trick is to use ` np.broadcast_arrays ` to broadcast your ` ( x , y )` against a ` z ` -long vector in the first dimension : #CODE
@USER that's interesting to know , although I'm sure that that using ` stride_tricks ` directly is still more efficient than allocating another array just to broadcast against .
If you still need to squeeze more time , you can use bottleneck's ` mean ` and ` std ` functions , that are faster than Numpy .
My problem , I don't have min and max value with normalize and I want keep shape of value .
` data ` is going to change a lot , but I want ' counts ' to reflect the number of times the max has appeared in each position : #CODE
I'm also happy to turn the indices array into an array of 0's and 1's , and then add it to the ` counts ` array each time , but I'm not sure how to construct that .
I'm attempting to perform a simple task : append an array to the beginning of another array .
Is it possible to have some problems when I reshape ?
As such and provided that your compiler support C++11 , you could use std :: shuffle to shuffle the pixels of your image ( see example below ) .
#URL ( fft )
Finding which rows have all elements as zeros in a matrix with numpy
Some of the rows of the matrix have all of their elements as zero and I need to get the indices of those rows .
So each value indicates whether the corresponding row contains a nonzero value .
To get the row indices , we use the ` where ` function .
It returns the indices where its argument is True : #CODE
Edit : basically I need that to avoid a division by zero when doing a normalization by the sum of the elements of a row .
Some rows are all zeros , so doing the summation ( which is zero ) then dividing by the summation will give a division by zero .
What I'm doing is that I find out which rows are all zeros and then I want not to do the normalization operation for those specific rows .
I want to find the largest k elements in that array along with their indices .
The solution is to use ` numpy.argsort ( arr ) [: : -1 ] [: k ]` , which yields the required indices .
Determining the median does not require a full ` O ( n log ( n ))` sort and can be done in [ linear time ] ( #URL ) , but this is not implemented in numpy as far as I can see .
If you look at the call signature for ` np.matrix.sum ` you see that its ` sum ` method has no ` keepdims ` parameter : #CODE
So some subclasses of ` ndarray ` may have ` sum ` methods which do not have a ` keepdims ` parameter .
But I'm thinking that in general there must be a way to concatenate pieces of the array at least .
Such that if I added another pandas.Series log line , I would get another row and so on .
So after playing a bit with this I came up with the following , but still can not concatinate / append .
It is important to note that whatever solution you find to append to your dataframe , you will not be able to avoid a full copy of your dataframe unless you have preallocated memory by defining an empty frame of sufficient size which you fill up sequentially .
Use broadcast to calculate the different , use ` sum ( axis=1 )` to sum every row : #CODE
Assume that the number of unique conditions ( whether I'm slicing by month or year ) are than the total number of elements in the ` t ` array .
Also it's really not practical to just do the mod at the end when large numbers are involved .
max value in Python
Using the normal ` max ` function when you already have ` numpy ` arrays available is not the most efficient way .
Simple implementation of NumPy cov ( covariance ) function
I was trying to implement the ` numpy.cov() ` function as given here : numpy cov ( covariance ) function , what exactly does it compute ?
Then I simply use gradient descent for this .
However as far as I know , one can use more sophisticated optimization methods than gradient descent that don't need parameters to tune or care about .
Jacobian ( gradient ) of objective function .
If jac is a Boolean and is True , fun is assumed to return the gradient along with the objective function .
If False , the gradient will be estimated numerically . jac can also be a callable returning the gradient of the objective .
As mentioned by @USER in a comment , you can find some example with and without gradient here .
Some [ examples ] ( #URL ) , both - with and without gradient .
This works because it knows how to broadcast #CODE
I would like to append my numpy array in a loop .
I would like to append x with 3 element long array in order to get Mx3 matrix , but my array is appending in one dimension ...
append works differently in numpy than list.append , it basically concatenates the new and old elements into one array .
Also , the array you append must have the same number of dimensions as the array you append to .
I need to randomly generate a series of numbers according to the binomial distribution .
Or do I need to saddle up and roll my own ?
Alternately , if you are stuck on Windows , consider using the Normal Approximation to the binomial distribution .
Consider using the [ Normal Approximation ] ( #URL ) to the binomial distribution .
The indexing will return an array , though ` x_vals ` will have only a single value if the ` y_val ` was unique .
I want to recover the unique indexes without repeating , and thus their corresponding arrays .
How can I recover the unique elements from list A with their corresponded numpy array in list b ?
I am currently recovering nothing because of my shortminded approach of recovering the unique elements of A but not enough information to recover the np.arrays from B
Should integers be unique across all sublists or just within a single sublist each ?
The integers are unique within each sublist :)
First , get unique indices of ` A ` , then take them from ` A ` and ` B ` #CODE
It is part of ` numpy ` so does ` from numpy import fft ` work ?
However you might be trying to use numpy fft , in which case you need to use from numpy import fft
You might try importing scipy and numpy before fft but I think that doesnot help .
These arrays have potentially different indices .
To produce this vectorized version , all I did was replace the various indices into ` X ` , ` Y ` , and ` Z ` with full ` a , b , c , d , e ` -style indexing , inserting ` None ` where missing indices were found .
The ` max ` and ` exp ` operations are fundamentally different ; ` exp ` ( and other operations like addition , ` sin ` , etc . ) is an elementwise operation that is embarrassingly parallelizable , while ` max ` requires a parallel-processing scan algorithm that basically builds up a tree of pairwise comparisons over an array .
It's not impossible to speed up ` max ` , but it's not as easy as ` exp ` .
Anyway , the ` theano ` implementation of ` max ` basically consists of the following lines ( in theano / tensor / basic.py ): #CODE
where ` max_and_argmax ` is a bunch of custom code that , to my eye , implements a max+argmax operation using ` numpy ` , and ` CAReduce ` is a generic GPU-accelerated scan operation used as a fallback ( which , according to the comments , doesn't support ` grad ` etc . ) .
Not surprisingly , finding the max is memory bound .
I wish to speedup ` the max ( X , axis=0 )` operation including the copying overheads .
As I tried to tell in the answer , I don't think you will be able to get speed up for max reduction if you include the transfer time with any system , not just Theano .
You could set a min / max as well using the first range for validation .
If we flatten your lists then we have : #CODE
The following example will additionally take care not to set diagonal entries : #CODE
If you would like to set the diagonal as well , then remove the second argument in ` np.triu_indices ` .
' np.tril_indices ` takes a second argument , the diagonal offset .
ValueError : operands could not be broadcast together with shapes ( 5 , 5 ) ( 5 , 23 )
this is referring to the linux environment variable ` $PATH ` which lists the directories in which to look for executables for commands when you don't specify an absolute path . this will contain a list of comma separated paths eg : #CODE
It only works until you log out of your terminal instance or run ` deactivate `
In response to your note ... the ` set() ` function is a remnant from my application where I am looking to extract unique elements from a letters-like column .
I have an image consisting of 100 pixels . for each pixel , I want to pad it with zeros ( if on the edge ) so that it's in the center , concatenate with neighboring pixels and generate a new 10x10 image .
So the pixels on the edge need to be padded with zeros so that I can still generate a 10x10 image .
yes , that's correct . how do I concatenate np.zeros ( array ) to the pixels on the edge , whereas this is not required for the pixels in the middle .
At the same time , we need to pad with zeros only if the pixels are on the edge , e.g. ` padded [ i+9 ] [ j+9 ] = im_array [ i ] [ j ]`
You can reshape your data by day and then perform means along only the desired axis .
To do this it is easiest to either strip the array down to whole days ( or you can pad it out to whole days ) .
Now to get the mean by day you can transpose the array and perform the mean only long the desired axis : #CODE
How to broadcast a function over a numpy array , when dtype =o bject ?
How do I broadcast a function over the actual numerical values without having to manually traverse the array ?
` numpy ` will happily broadcast over python lists .
The reason ` sin ` doesn't broadcast in this case is that python lists don't define ` sin ` at all .
Of course , handling arrays containing NaNs should be done with care , e.g. you probably want to use ` nanmax ` instead of ` max ` , etc .
@USER maybe ... but what patch coordinates would translate into your code ?
I can use ` numpy.dot ` directly , out of the box , because it uses the last axis in the first array and the second-to-last axis in the second array , and those are exactly the ones I need .
If one needs different or several ones , one can use ` np.tensordot `
Get indices of intersecting rows of Numpy 2d Array
I want to get the indices of the intersecting rows of a main numpy 2d array A , with another one B .
Where this should return [ 0 , 2 ] based on the indices of array A .
from numpy in1d for 2D arrays ? but I get a reshape error .
Yes I tried k [ np.in1d ( k.view ( dtype= ' i , i ') .reshape ( k.shape [ 0 ]) , k2.view ( dtype= ' i , i ') .reshape ( k2.shape [ 0 ]))] from #URL But I get a reshape error .
I have a list of arrays of different length , I want to combine them to a matrix to largest dimension , and fill zeros at the ends .
` total_charge ` is the sum of the charges in each group .
` zerolocs ` gives the indices where ` 0 ` occurs in ` index_data ` : #CODE
Use ` np.add.reduceat ` to sum the charge groups .
I am new to Python and am uncertain why I am seeing memory usage spike so dramatically when I use Numpy ` hstack ` to join together two ` pandas ` data frames .
As shown in this thread it is not possible to append an array in place and this would not be efficient since there is no guarantee to keep the extended array continguous in memory .
Fancy indexing ( using a list of indices to access elements of an array ) always produces a copy , as there is no way for numpy to translate it into a new view of the same data , but with a different fixed stride and shape , starting from a particular element .
but when the last statement is executed ( i.e. b [ numpy.isnan ( b )]= 1 ) both a and b become arrays of ones #CODE
how can I set array b to ones and array a to NaN .
I need to do this with any other functions like ` log ( x+1 )= 4 ` or ` log ( x ) -log ( x+1 )= log ( x )` .
log and exponentiation are inverse functions .
When you have something like ` log ( x+1 ) +log ( 4-x )= log ( 100 )` you can solve this analytically by usage of the log laws .
` log ( x+1 ) + log ( x-4 ) = log (( x+1 ) * ( x-4 ))` then ` log (( x+1 ) * ( x-4 )) = log ( 100 ) => ( x+1 ) * ( x-4 ) = 100 ` which is just a quadratic which is straightforward to solve .
Since ` log ` is a non-linear function , you will need to use a non-linear solver like ` scipy.optimize.fsolve ` .
While the ordering of samples can affect stochastic gradient descent learning algorithms ( like for example the one for the NN ) they are in most cases coded in a way that ensures internal randomness .
How to fill upper triangle of numpy array with zeros in place ?
What is the best way to fill in the lower triangle of a numpy array with zeros in place so that I don't have to do the following : #CODE
Digging into the internals of ` triu ` you'll find that it just multiplies the input by the output of ` tri ` .
So you can just multiply the array in-place by the output of ` tri ` : #CODE
Like ` triu ` , this still creates a second array ( the output of ` tri `) , but at least it performs the operation itself in-place .
But note that you can still specify a diagonal : #CODE
Error with hex encode in Python 3.3
Additionally , for string formatting you'll need to decode the result .
Also , if your Y's are binary ( all zeros and ones ) , you should look at using Logistic Regression .
That's much faster than the three separate ones you had , and arguably clearer too .
The exact code of ` norm ` can be checked [ here ] ( #URL ) , it should be equivalent ( but slower ) to calling ` dot ` and ` sqrt ` .
Since ` cov ( X , Y ) = cov ( Y , X )` , the covariance matrix should be symmetric .
The sum of the probability densities over the
Find the index corresponding to the max value of a numpy array
I have a ` numpy.array ` of data called f , I know the max value in it is ` f_max=max ( f )` but I would like to know the index in the array corresponding to the maximum value .
The easiest way would be to find the ` max ` and then , look for its index .
You could also use ` enumerate ` to get a list of indices and the values corresponding to them and choose the max among them .
I updated the code above , the goal is to efficiently apply my_func to all the pairs of indices in a .
What do you mean by " all pairs of indices " ?
If your array contains millions of elements and your code is possibly written in a way that creates temporary arrays , are you sure the run time of 15 min is not coming from the need to use swap space on the disk rather than in memory ?
So then I'd imagine you might want to concatenate these : #CODE
It's also possible to do this concatenate loop just after ` numpy.mgrid ` to halve the amount of work : #CODE
In the first part I refer to ` target ` as a 4D array , where 2 dimensions specify ` subs [ 0 ]` and ` subs [ 1 ]` , and the second ( " It's also possible to do this concatenate ... ") has it as a 3D array where one axis specifies * both * ` subs [ 0 ]` and ` subs [ 1 ]` .
This function calculates , for each element of ` x ` , the sum of that element itself with other elements of ` x ` given by a list of those elements .
The final sum for a element x [ i ] should be #CODE
The return of the function is the list with each calculated sum .
Edit : If it is helpful , I always sum 3 values per element , so the sublists of ` n ` are always of lenght 3 .
Now , it becomes like ` x [[ 1 , 2 , 3 ,... ]]` which will return the row of ` x ` corresponding to each indices .
Or I can append this to my answer as an update ...
It is new to me that we can pass any dimension array as indices .
I have a numpy array that I wish to resize using opencv .
One solution is to clip the results to [ 0 , 255 ] .
Should I use INTER_CUBIC ( and clip ) , INTER_AREA , or INTER_LINEAR ?
Since I want to resize an image to a larger size , I want an interpolation method that minimizes artifacts and / or aliasing .
now you can just strip the last element , if you want
This shows that for the carefully chosen constant of ` 1.46 ` , a full 12-13 % of answers are wrong with the ` diff ` variant !
My answer is the best unless you're finding the average of two values whose sum exceeds ` 1.7976931348623157e +308 ` or is smaller than ` - 1.7976931348623157e +308 ` .
Mine * is * faster , but it's also perfectly accurate , whereas the ` diff ` version is not .
Cython to accelerate loops with array broadcast
This solution generates a broadcast error , which is why I added the reshape : " ValueError : could not broadcast input array from shape ( 3 , 1 ) into shape ( 3 )"
` np.where ` returns indices that match the condition .
The strategy is to get indices upper than ` threshold ` and remove the consecutive .
prints ` [ 2 9 16 ]` , the indices corresponding to first entry in a sequence greater than the threshold .
Edit : As pointed out by @USER , the convolution can be replaced with ` numpy.diff ( thresholded_data )` , which is conceptually a bit simpler , though in that case the indices will be out by 1 , so remember to add those back in , and also to convert ` thresholded_data ` to be an array of ints with ` thresholded_data.astype ( int )` .
If using ` np.diff ` watch out that you use it on ` thresholded_data.astype ( int )` , since otherwise all differences will just evaluate to ` True ` , also the ` -1 ` ones .
@USER On rechecking , the timing difference is not substantial and is actually the other way around ( 11ms for the ` diff ` vs 12ms for ` convolve `) .
For matrix dot , inner and outer products you need to use ` numpy.dot ` , ` np.inner ` and ` numpy.outer ` explicitly .
It seems you want ` np.where() ` combined with the solution of this answer to find unique rows : #CODE
` argmax ` on bools shortcircuits in numpy 1.9 , so it should be preferred to ` where ` or ` nonzero ` for this use case .
I have tried ` np.interpolate ` by splitting the data into two arrays : first with ` gradient ( y ) 0 ` and another with ` gradient ( y ) 0 ` , however this yielded incorrect values .
For example , you can do the permutation and slice easily with CSR or CSC sparse formats .
I want to make a collection of multinomial random variables which I can later sample using mcmc .
They just shows that the components are multinomial random variables but I think I can't use the components with mcmc to get the posterior distribution .
I have calculated log probability in some other non-pymc function .
If the model is correct , then the log probabilities predicted by model must be similar ( if other parameters ( like tau in my case ) are same ) .
just one last query , I will need to redefine logp function because there are data elements with all entries 0 eg [ 0 , 0 , 0 ] and their log by default comes out to be -inf which I don't want .
But I am getting the difference between log probabilities as precisely 2234.7574041 for every single datapoint , which is awfully suspicious .
You could always do a linear fit in log space .
You mean instead of y = A*exp ( -bx ) +C , I could linearise by log ( y ) = ln ( A ) + bX .
Fitting the original data will give equal weight to the absolute error in each point .
Fitiing in log space will give equal weight to the relative error in each point .
You can improve the numerical condition of the model my moving the constant ` A ` into the exponent , ` exp ( -K* ( t - t_0 )) + C ` .
Fitting in log space won't help either ( and is not really an option as long as you have the additive constant ` C ` in your model ) .
Smoothed 2D histogram using matplotlib and imshow
I try to do a 2D histogram plot and to obtain a " smooth " picture by a sort of interpolation .
This code will rise : ` IndexError : too many indices ` .
I was thinking I could reshape the matrix to a matrix and then loop through the last column adding the desired values .
However , I wasn't sure how I could reshape a matrix this way ( i.e. adding a column ) .
I guess this could be done with a for loop in the function to append the result of each element back on to some form of RESULT array , but this seems a bit clunky .
I have a numpy array which looks like : ` [ 3 , 65 , 7 , 83 , 2 , 4 ]` and I want to keep indices ` [ 1 , 3 , 5 ]` .
I then assign these ( in my own scikit-code where clf stands for a svm.SVC-classifier ) to clf.dual_coefs_ and properly change clf.support_ too ( which holds the indices of the support vectors ) .
I don't think you can be anymore memory frugal than to generate a mask of the rows that you want to keep ( either boolean or a list of indices ) , and make a copy of each array , ` A = A [ I , :] ` .
use ` isfinite() ` and ` sum ( axis=-1 )` : #CODE
Ultimately , I'd like to pass a 3d array of values sorted along an axis , a 2d array of values to search for and have the function return a 2d array of indices .
Are you wondering about time differences of copying or of the dot product ?
@USER I compare dot product time due to something like #URL but seems using A.T using some OS caching .
The indices for N are unique , but the indices for M may appear more than once .
Missing in the question : the obvious ` M [ w [: , 0 ]] += N [ w [: , 1 ]]` fails because the indices in ` w [: , 0 ]` are not unique , so values get overwritten .
How do you then group this matrix by ` w [: , 0 ]` and sum its values ?
The CSR constructor builds a matrix with the required values at the required indices ; the third part of its argument is a compressed column index , meaning that the values ` N [ 0 : len ( N )]` have the indices ` m_ind [ 0 : len ( N )]` .
* That * gives me ` ValueError : non-broadcastable output operand with shape ( 1 , ) doesn't match the broadcast shape ( 1 , 1 )` from the ` M [ np.unique ( m_ind )] += update ` part .
ValueError : operands could not be broadcast together with shapes ( 224,224 ) ( 180,180 )
@USER - one naive way would be to loop over all combinations of length ` n ` from ` 0 ,..., k-1 ` , and count each whose sum is ` r ` .
and then just reading off the ones you are interested in ?
or , in other words , ` ( k-1 ) *n+1 ` so that the results don't wrap around at the ends ( or at least so when they do they only add zeros to affected elements ) .
Often its length should also be a power of two since this is required by the FFT algorithm ( implementations that don't require it to will pad your input with zeros until it does ) .
Approximate your data as the sum of a number of coefficients times basis functions ( for example B-splines with non-uniformly chosen knots - or a radial basis function network of well-placed Gaussians ) .
and projecting y_new = dot ( Bnew , c )
interp1d does not seem to scale very well as shown below ( y-axis is log time per point [ most negative values correspond to fastest computations per interpolation point ] , x-axis power of 2 in ` N `) .
I = A [ A [: , 1 ] == i ] [ 0 , 2 , 3 ] --> IndexError : too many indices
If I needed all axes , I could use ` nditer ` , but if I need only the specific ones , I have to do it manually : #CODE
If you're not using the absolute value , you'll need to play with the sign depending on which time you want to be considered ' first ' .
If you have an array ` diff ` of the time differences ranging between 0 and 24 hours you can make a correction to the wrongly calculated values as follows : #CODE
Polar / Euler Form ( A * exp ( i * phi ) )
How do I search for indices that satisfy condition in numpy ?
it will store in ` rows ` the indices where all the row contains ` True ` values .
" Maybe it was this process of ` Hessian [ i , j ]= diff ( diff ( function , x_i ) , x_j )` which caused the trouble .
Maybe it was this process of ` Hessian [ i , j ]= diff ( diff ( function , x_i ) , x_j )` which caused the trouble .
The trace of the hat matrix is the sum of its eigenvalues .
For GCV you need the vector of diagonal elements of the hatmatrix which can be calculated without calculating the entire ( n , n ) hatmatrix .
Am I understanding correctly that via singular value decomposition that the trace of the hat matrix would be equal to the sum of the squares of s_i , where s_i is the i_th diagonal of S from the SVD of X ?
All diagonal elements will be of the form ` s_i ** 2 / s_i ** 2 == 1 ` .
Also , @USER correctly states that you need the whole diagonal to compute GCV ( more on that e.g. [ here ] ( #URL )) .
If you need the diagonal , then do ` ( X.T * np.linalg.inv ( X.T.dot ( X )) .dot ( X.T )) .sum ( 0 )`
Your intention to perform GCV , however , necessitates the full knowledge of the diagonal of the hat matrix .
As can be seen , ` only_diag ` contains the diagonal .
So to sum up : you may be able to get away with using the jit decorator instead of the njit decorator until numba properly supports calling ndarray.sort() in nopython mode , as long as you're not sorting arrays inside a loop .
I want to keep this as a line , instead of a scatter plot or histogram .
You can also refer to Numpy : Creating a complex array from 2 real ones ?
` dk [: , 0 ] = np.array ([ sqrt (( PC_t-pc ) **2 / np.var ( PC_l )) for pc in PC_l ])`
I think we are defining precision differently - usually I consider the error _relative_ to the dimensions , not an absolute error .
@USER that makes sense ; I was referring to an absolute error , but as you mention , relative probably makes more sense .
If you sum over ` i ` , then it doesn't mean anything to index by ` i ` outside of the sum ( ie , in ` m ( x_i )` ); that is , there's no ` i ` on the rhs .
If you read the reference I added ( this is the exact page #URL ) at the end , it seems the sum is over the points located within distance h , not over the same i .
Basically , though , you just want to sum over pairwise measures , which you can probably do easily enough in numpy .
That is , summing up to index ` ihval ` is the sum over ` N ( h )` in the equation , since this includes all pairs with ` h ` s below the desired values .
Otherwise , one would feed ` zh ` and ` zsh ` into ` cov ` for every new ` h ` , and all the products would be recalculated .
Maybe you're right but I interpreted it as the average value within distance h for each value inside the sum in the definition of Cov ( h ) .
As for the difference in m , I think it still can be interpreted as you did , an average over pair of values separated by distance h ( given a tolerance value ) or as an average over each value considered in the sum of Cov ( h ) .
( Also , though , if you have the full derivation you should be able to check for the correct interpretation , which should be based on the math rather than the words surrounding it , or you could ask at the stats stack exchange . )
This is the best solution for me as I have in fact 7 dimensions and multiple swap or roll is difficult .
The canonical way of doing this in numpy would be to use ` np.transpose ` ' s optional permutation argument .
In your case , to go from ` ijkl ` to ` klij ` , the permutation is ` ( 2 , 3 , 0 , 1 )` , e.g. : #CODE
Since I don't want the normalized version of the fft , I need the normalization factor to " undo " the normalization .
When you do the forward FFT you get an expected DC term that is the sum of the inputs , and if the inverse brings you back to the original , then there must only be a 1 / N " normalisation " .
I believe the norm of ` numpy.fft.irfft ` is 1 over array length .
From this I believe that the norm of the irfft is 1 / n and from rfft it is 1 .
Shouldn't you be looking at the sum of ` x ` rather than the maximum value ?
The Series must also have a ` name ` to be used with ` join ` , which gets pulled in as a new field called ` name ` .
You also need to specify an inner join to get something like ` isin ` because ` join ` defaults to a left join . query ` in ` syntax seems to have the same speed characteristics as ` isin ` for large datasets .
@USER , the link I provided gives a pretty good intro into using Cython with pandas : #URL I would probably try to use join or hash tables before you get into that though .
It's better if you have a somewhat unique algorithm you need to optimize .
I want to translate the following group coloring octave function to python and use it with pyplot .
UPDATE : The above plot was log log scale but it happens on a linear plot too .
So I am convinced that it is indeed just a log scale effect .
` lambda x : sqrt (( x [ 1 ] -X_0 ) **2+ ( x [ 0 ] -Y_0 ) **2 )` ,
` distances = map ( lambda x : sqrt (( x [ 1 ] -X_0 ) **2+ ( x [ 0 ] -Y_0 ) **2 ) , my_points )`
VlueError : operands could not be broadcast together with shapes ( 2 , 0 ) ( 2 , 2 )"
and looked for all of the indices for ` t_events ` in ` t_signal ` e.g. using ` numpy.serachsorted `
This saves me the memory for ` t_signal ` and I do not have to go through the whole signal to find my indices .
Since your signal is defined on a regular grid , you could do some arithmetic to find indices for all the samples that you require .
For absolute values that include uncertainty in y ( and in x for odr case ):
In the ` scipy.odr ` case use ` stddev = numpy.sqrt ( numpy.diag ( cov ))`
where the cov is the covariance matrix odr gives in the output .
` p , cov = numpy.polyfit ( x , y , 1 , cov = True )
errorbars = numpy.sqrt ( numpy.diag ( cov ))`
My guess is that the functions mixing relative and absolute values .
diagonal elements of the covariance matrix cov ( 0 , 0 ) and
cov ( 1 , 1 ) ,
I have thought of using the absolute value in order to achieve this , but the , how would I get back my negatives ?
will assign each element in ` flow ` whose absolute value is too close to zero .
python child process crashes on numpy dot if PySide is imported
hangs printing only ' before dot ..
if you decrease shape of arrays going into ' dot ' ( e.g. from 128 to
( ! ) if you increase shape of arrays going into ' dot ' from 128 to 256
But here is a stack of updates to keep history for others who may encounter this bug ( e.g. I started with matplotlib , not with pyside )
Update : print ( os.getpid() ) before dot operation gives me pid that I don't see in ' top ' that apparently means that it crashes and multiprocessing waits for a dead process .
Yes it is issue with combining multiprocessing , pyside and blas numpy dot .
this is a general issue with some BLAS libraries used by numpy for ` dot ` .
Threading is not an option - dot operation takes up only ~10% of the execution , so threading doesn't give me speed up I want .
Could use ` a [ a [ ' meta '] == ' metadata2 '] [: , : -1 ]` to strip the meta column .
You transpose ` x , y ` -> ` y , x ` on the second line of ` plot_streamlines ` .
I have a sparse matrix ( 22000x97482 ) in csr format and i want to delete some columns ( indices of columns numbers are stored in a list )
Explicitly generating all column indices is not really ideal if you have a huge number of columns
If you have a very large number of columns then generating the full set of column indices can become rather costly .
You may have to transpose the output , it's hard to debug this kind of code without an example .
Try to strip the actual optimization algorithm , and only show the code that creates the arrays in a loop , using them in a way that still causes the leak .
The function is designed to take several images and several filters and convolve all the images with all the filters .
Return a k length list of unique elements chosen from the population
I want to concatenate numpy matrices that have different shapes in order to get an array with dimension=3 .
may be would be better to say I'm looking for to have an array of matrices or to concatenate different matrices having different shapes
Python pypy : Efficient sum of absolute array / vector difference
It has to calculate for a large number of lists / vectors / arrays the pairwise sums of absolute differences .
Can you also try ` sum ( abs ( a-b ) for a , b in izip ( v1 , v2 ))` ?
Your ` np_sum ` function walks both arrays three times ( subtract , abs , sum ) unless PyPy notices and can optimize it .
This is quite fast and I already dropped the sqrt calculation since I need to rank items only ( nearest-neighbor search ) .
Note that it relies on the binomial formula #CODE
I'm having to convert the image into a numpy array , sum the pixels then divide before converting back to an image .
where R is the curvature radius , K the conic constant and ` r = sqrt ( x +y )` .
I tried creating a healpix map of the Gausian , multiplying the two numpy.ndarray and getting the sum ; this is too slow .
Something like expand the huge_map in spherical harm ., expand the Gaussian in spherical harm ., multiply the coefficients , transform back , get the sum .
And possible modifications like : a ) work instead with log of the huge_map and log of the Gaussian , then the operation to by done to the coefficients is clearly sum .
NumPy append to 1-column vector
I don't think ` append ` is the right method here .
do you want to replace the zeros with the values ? or are you trying to make the array longer ?
yes , replace the zeros with the values .
Having trouble plotting 2-D histogram with numpy.histogram2d and matplotlib
Here's a code that generates 100 random points and attempts to plot them in a 2-D histogram : #CODE
Compute the bi-dimensional histogram of two data samples .
First , I create a lower triangular version of the covariance matrix that excludes the diagonal ( using numpy's ` tril `) : #CODE
And then I stack the dataframe : #CODE
I also want bins to have a width of .5 so that I can have a bin from 10.5 to 11 or 24 to 24.5 etc ... because otherwise , python outputs the histogram with the bins random and undetermined .
I need to transform it into a percentual value related to the diagonal : #CODE
If you dont mind switching the diagonal around , you could do this : #CODE
E.g , it is not clear to me , which numbers you use to obtain a std . dev . of `` 1.00 `` .
Than take the std along the thrid axies : #CODE
How to normalize a histogram of an exponential distributionin scipy ?
Strangely , no matter what I do I can't seem to scale the histogram so it fits the fitted exponential distribution .
For some reason , the histogram takes up much more space than the probability distribution , even though I have normed=1 .
My problem is just with the normalization of either the histogram or the probability distribution .
Can't you sum areas and integrate the fit to check which one is not normalized ?
Access matrix by list of matrix indices in Numpy
I have a list of matrix indices and want to access a matrix by these indices .
This should result in ` [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 0 , 0 , 0 ]]` , but unfortunately I always get "` list indices must be integers , not list `" .
` indices ` is a regular list of tuples and can't be used to get an element of your regular ` mat ` .
What you can do is iterate over your list to get your indices : #CODE
seems reasonable to me , you could add that if he wanted to do ` mat [ indices ] = 0 ` the indicies would have to be converted into ` [[ 2 , 2 , 2 ] , [ 0 , 1 , 2 ]]` by for example ` zip ( *indices )`
The standard method , assuming your errors are normally distributed , is to use the sum of the squared residuals .
` B ` matrix has integers ( min value -1 ) .
I need to plot a histogram between the elements of matrix ` B ( X-axis )` and their frequency they are listed as ON in matrix ` A ` ( in the corresponding positions ) .
The whole thing runs in 0.3 sec without passing it through unique , and in a little over 6 sec when using it .
After this to plot histogram I use unq_val and ung_freq in matplotlib ?
The internal workings of dot are a little obscure , as it tries to use
@USER transpose of a matrix will only create a view , therefore the memory layout is the same .
As the matrices grow , the dot product dominates , and you don't see the copy effect .
Also note that you can index with a list or tuple of slices and indices : #CODE
ArcGIS / Python : How to sum rows based on a common ID
So , I would like a way to sum values in a table based on a common identifier ( in the table below , the field ' IWUP ') .
I'd like to sum the PPG data for each Site and Month and produce something like this : #CODE
This overwrites your original array and you will find zeros at all positions where there is equality between neighbors .
LED strip matrix for FFT with Python and Raspberry Pi
I'm stuck with Python and a LED strip .
The LED strip , which has the WS2801 chip and is addressable via SPI , has been arranged as a matrix like this : #CODE
By the way , the LED strip is doing fine with the PixelPi examples like fading and chasing .
You cannot resize NumPy arrays that share data with another array in-place using the ` resize ` method by default .
" You cannot resize NumPy arrays that share data with another array in-place " -> where does my numpy array share data with another array ?
to subset to the max value from each a1 / a2 group to get #CODE
Numpy 2D dot operation , indexing and iteration .
On my system it gives a memory error : you are computing a huge array and the throwing away everything but the diagonal , I'd be surprised if it was advantageous in any way .
In case it is relevant , note that the last reshape triggers a copy , so what you get is a new array , not a view into the old one .
I can't quite see why reshape has this effect of partitioning an array .
It seems that your code is basically saying " I want 2 arrays , and each of these must have 2 more arrays with dimensions 5X5 " Another question : Why does the last reshape produce a copy , but not the first ?
The first reshape creates the 4 blocks , the transpose rearranges them , and the final reshape joins them back into a 2D array .
Because the ndarray data is stored in a single contiguous memory block , that last reshape requires moving data around , not just changing how it is viewed .
naive filtering using fft in python
Program1 records RECORD_SECONDS from microphone and writes information about ` fft ` in ` fft.bin ` file .
In addition , I figured out , that every , even very little change in ` fft ` causes Program2 to fail .
e.g. I have the proportion p =[ 0.1 , 0 , 0.3 , 0.6 , 0 ] of elements e =[ 0 , 1 , 2 , 3 , 4 ] ( elements are here identified by there indices )
In the first one , the function can change elements of the global list and append to it .
But why can the second function change a value of a global ndarray , but can not append to it ?
Now we can decide which ones we want to patch : #CODE
Creating Arrays in numpy using zeros
In particular , a polynomial written naively might involve a lot of cancellations , which leads to numerical errors if the sum of the terms is much smaller than the sum of the * absolute values of * the terms .
Numpy equivalent of dot ( A , B , 3 )
If I want to take pairwise dot products along the third dimension , I could do so like this in matlab : #CODE
and in matlab it gives me ` std ([ -1 , 0 , 1 ]) = 1 ` .
So the real answer is ` sqrt ( 2 / 3 )` which is exactly that : ` 0.8164965 ...
I think the np.std() is just universal std .
If it is a sample std , it should be N-1 .
Is there a function for sample std ?
The Numpy docs for ` std ` are a bit opaque , IMHO , especially considering that NumPy docs are generally fairly clear .
` ( In english , default is pop std dev , set ` ddof=1 ` for sample std dev ) .
Build a ` scipy.spatial.KDTree ` from the points tx , ty , tz and then use nearest-neighbour look-up in the infinity norm for each point in ox , oy , oz to see whether there is any point close enough .
The sum in the last expression is exactly the Discrete Fourier Transformation ( DFT ) numpy uses ( see section " Implementation details " of the numpy FFT module ) .
I have a large symmetric matrix ` A ` of dimensions ` ( N , N )` ( ` N ` is about twenty million ) , and for sure I cannot store this matrix ( 50% components of ` A ` are zeros ) .
For example ` A [ i , j ] = cos ( i ) *cos ( j )` .
This might be a silly question , in fact is certainly is , but cos ( i ) cos ( j ) repeats and , since A is symmetric , there are some tricks that you should be able to use .
` multiply by N ` - a dot product the reduces the dimensions , or broadcasted ?
i.e. ` cos ( i ) *cos ( j ) *N ( i ) -> M ( i , j )` or ` sum ( i ) ( ... ) -> M ( j )` ?
The only optimization of algorithm I can think of is take into consideration that ` f ( i , j ) = cos ( i ) *cos ( j )` is symmetric function ( ` f ( i , j ) = f ( j , i )`) .
I can see that I could pre-process the file to strip out the commas - I'd like to avoid that if possible but would welcome suggestions if this is the only way .
Maybe something is expecting an absolute path ?
It needed an absolute path instead .
So , if you want to be sure to convert a pathname given by the user in " shell language " 1 into an absolute path usable with python's file objects you should do : #CODE
I'm wondering whether there is a way to simply add a dense vector to all the rows of a sparse matrix represented as a ` csr_matrix ` in ` scipy.sparse ` and returning a sparse matrix , ie trying to sum only the non zero element of the sparse matrix .
` sum ` is obviously a dense matrix but with the zero numbers summed too .
The point is to leave the zeros out from the computation .
It is better to specify that I'm looking for something that performs the log-sum-exp trick , doing a simply succession of exp elem-wise , summing the rows and doing a log elem-wise is trivial in ` scipy.sparse ` .
Less trivial is computing in a clean way the max along rows and subtracting it as each element in the a sparse matrix row is subtracted the corresponding max vector elem ( retaining a sparse matrix in the end ) .
In some cases it's enough to operate on all of the nonzero values ; in this you need to select them row by row .
and ( a permutation of ) the values of the actual row would be obtained by appending ` X.shape [ 1 ] - len ( X [ i ] .data )` zeros to that .
A column-wise version is much trickier ; it's probably easiest to transpose the matrix and convert back to CSR .
UPDATE Ok , I misunderstood the question : the OP is not interested in handling the zeros at all , so the above derivation is useless and the algorithm should be #CODE
For column-wise , transpose , convert back to CSR and apply the above .
Actually in the examples you linked ` zeros ` is used ** outside ** the decorated function , through an explicit out parameter .
multiple numpy dot products without a loop
Is it possible to compute several dot products without a loop ?
Unfortunately , ` dot ` doesn't work that way .
This solution still uses a loop , but is faster because it avoids unnecessary creation of temp arrays , by using the ` out ` arg of ` dot ` : #CODE
` np.tensordot ` would do a sort of outer product , multiplying every matrix in ` a ` with every matrix in ` b ` , giving rise to 2 axes of size 100 .
The desired result is the diagonal of that , so ` np.tensordot ` calculates too many coefficients .
I feel extremely bad about admitting it , but I never understood the magic that is happening with the indices , therefore I unfortunately need help .
@USER : Do I get you correctly : Given a grid ` z ` and an arbitrary index ` i ` and a history array of grids ` H ` , you want to find ` min ([ H [ k ] [ i ] for k in len ( H ) if H [ k ] [ i ] > z [ i ]])` , only for all ` i ` and in an efficient manner ?
``` history ``` contains ** all ** * guesses * that have been made , ``` history [ ..., 2 :] ``` , as well as the global min and max ?
` higherZ = b [ indices ]` in your code , while in mine it was ` higherZ = history [ indices ]` .
numpy.array.__iadd__ and repeated indices
and list of indices with repetitions : #CODE
and another array i would like to add to A using indices above : #CODE
Do i have to iterate over indices ?
at ( a , indices , b=None )
specified by ' indices ' .
to ` a [ indices ] += b ` , except that results are accumulated for elements
Obviously it's even more simple if A starts off as zeros : #CODE
NumPy map calculation depending on the indices
You could also look at ` meshgrid ` , ` mgrid ` , and / or ` indices ` : #CODE
Now calling read_input prints samples just the way I want it ( append npoints to global samples each time ) and returns npoints .
there are metrics that describe kind of a " distance between distributions " that take both mean and std into account , e.g. the [ Kullback-Leibler divergence ] ( #URL )
When adding or subtracting you add the absolute errors .
Assuming you follow that , the error in ` m ( Y ) - m ( X )` is ` std ( X ) + std ( Y )` , which as a relative error is ` ( std ( X ) + std ( Y )) / ( m ( Y ) - m ( X ))` .
Add the relative error in the denominator ` std ( X ) / m ( X )` and you have the relative error of the whole .
I won't run the timing tests because you already have an answer you like , but I suspect the ` norm ` method you first started with would be comparable in speed to the methods suggested by @USER ( if norm will work for you generalized problem -- I'm not sure how you want to expand your problem statement of ( 2x1 ) * ( 2x5 ) to 4000x3000 ) .
Slicing arrays with meshgrid / array indices in Numpy
The problem is you can't slice arrays with arrays of indices .
You need to broadcast the index yourself , for example : #CODE
Why does numpy not automatically flatten the multidimensional array to a vector like this : #CODE
I had already read the documentation and saw the ` max ( iterable [ , key=func ])` but couldn't quite understand the meaning of the iterable myself .
The Python function ` max ` does not expect just lists as an argument but instead accepts an ` iterable ` object as shown in the docs here : #CODE
Ah I see thanks , so the ` iterable ` is like an ` interface ` which is being implemented and thus ` max ` works for that type of datatypes all the same ?
The max function has the following arguments : #CODE
Numpy does not flatten the array as the iterator will loop , by default , over rows and not over elements .
As an extra option , you can replace ` max ( b )` by ` b.max() ` .
Why is ` max ` implemented in so many ways ?
Numpy supports both a scripting style of programming ` max ( a )` and an objected oriented programming style ` a.max() ` .
In the specific case of max , python already has the procedural style max , and not ` list.max ` .
Novice in python , I'm trying perform an optimal interpolation and , somewhere in my code , I need to multiply two arrays as follow : Cgd * inv ( Cdd )
It solves ` Ax = b ` , which gives ` x = inv ( A ) b ` , but this is more stable than solving for ` inv ( A )` and then multiplying it by ` b ` .
you can find ` Cgd * inv ( Cdd )` by ` np.linalg.solve ( Cdd , Cgd ')` , and then taking transpose .
` Cgd '` is transpose of ` Cgd ` .
This will not work as expected if ` rhs ` is a view that overlaps with ` lhs ` ( like a transpose ) .
After some processing I got an array with following atributes : max value is : 0.99999999988 ,
min value is 8.269656407e-08 and type is : ` type ' numpy.ndarray ' ` .
The bottleneck of the code is the integration part I was doing with ` scipy.integral.quad ` an then concatenating bunch of ` numpy.array ` and ` sum ` over each row of a 2D array .
Try using a log scale .
ValueError : shape mismatch : objects cannot be broadcast to a single shape
I am transforming a flattened array into a matrix ( n , m ) with the method reshape .
What is wrong with " numpy concatenate " ?
The second argument to ` concatenate ` should be the axis you want to concatenate on , numpy thinks you're trying to concatenate on axis ` x_copy ` .
Take a look here to see how concatenate should be used .
To make @USER ' s comment more explicit : You forgot brackets around the arrays you want to concatenate .
My input is actually a list of unequal length vectors , and I put them in a big numpy 2d array filled with zeros for missing values . and the second input is a int array that indicates the length of each vector in the first parameter .
The idea is to apply a filter centred at several points across an image , and then take the sum of the values of the result at each point .
Could I just convolve the two and then take the results at each point across ?
There would be no need to sum either in that case AFAIK ?
gaussian sum filter for irregular spaced points
I got the tip that this method is called a " Gaussian sum filter " , but so far I have not found any implementation in numpy / scipy for that , although it seems like a standard problem at first glance .
A pure NumPy / Python alternative would be to use a dict to map the column names to indices : #CODE
Each row tuple / " triplet " is unique in each array .
Using the assumptions you have that the rows are unique in each matrix and that there are common rows , here is one solution .
The basic idea is to concatenate the two arrays , sort it so the similar rows are together and then do a difference across the rows .
The essence of the program is some image processing , where it will convolve a 640x480 uint8 with a complex128 of twice the size .
So to get proper results , the arrays are padded with zeros to a common shape which , in your case , will be ` ( 1280+ 640-1 , 960+ 480-1 ) = ( 1919 , 1439 )` .
Use regular convolve instead of fftconvolve .
Use scipy.weave and do your own convolve operation ( Thus avoiding numpy memory management ) .
3D numpy array into block diagonal matrix
I am looking for a way to convert a nXaXb numpy array into a block diagonal matrix .
You can add a call to ` rstrip ` to remove the trailing zeros : #CODE
The idea is taht the first one is a stack of 12 [ 256x256 ] arrays and the 2nd one is a stack of 1d scalars .
Short of concatenating explicitly field by field , is there a way to concatenate the two ?
How to create a diagonal multi-dimensional ( ie greater than 2 ) in numpy
will create a diagonal 2-d matrix shape =( len ( L ) , len ( L )) with elements of L on the diagonal .
Here's one way I thought of , using the flat indices .
For a more general approach , you could set the diagonal of an arbitrarily sized array doing something like : #CODE
You can use ` diag_indices ` to get the indices to be set .
@USER No , because ` MinVec ` is the element-wise sum ( * not * the concatenation ) of two slices of ` N ` .
Btw , generally ` np.min ` is preferred over the Python built-in ` min ` , since it's faster .
Also , ` numpy ` [ provides several ] ( #URL ) ` min ` variants that can be useful .
You can append the value , then minimize .
No , it doesn't - from the numpy docs : [ Note that append does not occur in-place : a new array is allocated and filled . ] ( #URL )
Although the OP doesn't explicitly say so , the code implies that MinVec is a numpy array , so it doesn't contain an ` append ` method .
To also make this array 2D , you can also use ` reshape ` , just as with ` B ` .
I've put in the right ones now .
( The ones that give the blue curve in the first pic )
Broadcasting matrix-vector dot product
I tried if ` dot ` , ` matrix_multiply ` , ` inner ` , or ` inner1d ` would fit the bill , in combination with ` transpose ` , but I didn't quite get it .
I'd like to assign each pixel of a mat ` matA ` to some value according to values of ` matB ` , my code is a nested for-loop : #CODE
and it costs about 40~70ms for a mat of size 640*480 .
In your C++ code , at every pixel you are making a function call , and passing in two indices which are getting converted into a flat index doing something like ` i*depthImageCols + j ` .
I am trying to make a pixel histogram for an image .
I wanted to trim this list using list comphrehension and keeping only the values greater than some max value .
What should I do to keep only values in a list that are greater than a specified max value ?
Is it possible to tile a numpy array in 2 dimensions with a single call
You are not using the power of pandas here , just use `` .isin `` , and or / join , see this for benchmarks : #URL
For example , if you wanted the sum or count of the data in each bin you could do : #CODE
As ` sin ` always is from -1 to 1 , all you need to do is set the volume less than 127 .
For older Python 3 versions , replace ` statistics.mean ` with ` sum ( values ) / len ( values )` .
All this does is pre-process the lists so that the short ones are stretched to the maximum length ( ` maxlen ` , which can be changed if you wish ) .
The code works fine until I try deleting the radio quiet quasars indices from the RA and Distance arrays to get the remaining radio loud ones , and vice versa to get radio quiet ones .
You use ` np.where ` to find indices on your original ` radioflux ` array , but then use those indices to remove elements from ` radiocut ` , which is a filtered version of ` radioflux ` .
When filtering the ` distancecut ` and ` anglecut ` arrays , you are not using the ` loud ` and ` quiet ` arrays , which hold the indices , but ` radioloud ` and ` radioquiet ` , which hold radio flux values .
You see the matrix size in the frist column , followed by mean , min and max of execution times gained by running the matrix matrix multiplication 100 fold : #CODE
The oberved min and max values for ATLAS are no outliers , the times are distributed over the given range .
The given times come from a different run , so avg , min and max values differ slightly .
What happens if you put ` numpy.random.seed ( 0 )` in ` setup ` before the ` rand ` call ?
I also tried plotting a pixel histogram , I see that the pixel value stops at 256 since pixel values are 8 bits but I don't quite understand its significance .
Numpy find indices of matching columns
I'm looking to find the indices of A by matching columns in B .
Then you use ` np.where ( np.in1d() )` to get the indices : #CODE
Sorting is ` O ( n log n )` , so no , you can .
Yes , I know , but in this way indices change on both arrays .
This is ` O ( len B log len A )` .
The same happens if you append ` a ` to a list .
One way to solve this is by creating a new array object each time we append the array to the list : #CODE
Polynomial fits are not going to help you , as far as I know .
Did you try creating a histogram of the data ?
Create a histogram
Use ` array ` , and use ` numpy.dot ` or the array ` dot ` method for matrix multiplication .
@USER : Use ` dot ` for that .
Now if I convert ` row ` to array , and squeeze out the extra dimension , its shape is ` ( 5 , )` , and ` num ` is now a scalar , and the ` if ` works fine .
The cross-validation iterators return indices for use in indexing into numpy arrays , but your data are plain Python lists .
( Alternatively , you could extract the given indices with a list comprehension or the like , but since scikit is going to convert to numpy anyway , you might as well use numpy in the first place . )
I am not an expert on eigenvalues , but I'm not sure if you're approach is feasible : All the algorithms I know of do not return the eigenvalues in a specific order , so when you have one , you don't know if it is one of the smaller ones .
I have a 1D array and I want to use ` numpy bincount ` to create a histogram .
I want to get the indices where the rows contain the string ' bar '
should give me the indices [ 0 , 3 ] followed by broadcasting : - #CODE
In theano , we can easily configure the function , e.g. , sqrt to work on float32 or float64 as follows : #CODE
IIRC , you should use points equally spaced in log scale .
The size of C is determined as follows : if n1 = length ( h1 ) and n2 = length ( h2 ) , then mc = max ([ ma+ n1-1 , ma , n1 ]) and nc = max ([ na+ n2-1 , na , n2 ]) .
Your general idea of vectorizing is of course a good one , but python is surprisingly fast as-is , and with maths inner loops matter , outer loops not so much .
Then you can manipulate this any way you want ( e.g. calculate the absolute value ) , which gives you a boolean array .
To find which rows have any hits ( absolute difference .1 ) , use ` numpy.any ` : #CODE
Python loops add quite a lot of overhead , that is non-existent if you can translate things to pure C .
I had to reshape the result , but other than that , it worked perfectly : ` colorImg = np.take ( np.asarray ( cMap ) , img , axis = 0 ) .reshape ( img.shape [: 2 ] + ( 3 , ))`
Faster alternative to numpy.einsum for taking the " element-wise " dot product of two lists of vectors ?
can you consolidate the ` einsum ` calls - concatenate arrays ?
have you tried the [ reshape ] ( #URL ) function of numpy ?
Should be pretty direct with pandas ` stack ` and some string parsing .
@USER I've never used stack before but actually it looks really useful .
The reshape suggestion ended up being what worked the best .
For each dimension I have as input either a binary vector , or a vector of indices .
note that the submatrix you get is a new copy , not a view of the original mat .
I believe you don't need any ` coordinate ` function , only make ` a [ i ] , b [ j ]` a tuple or list and ` append ` ( not ` add `) to ` out ` .
I need to construct a 2-dimensional array D [ i , j ]= f ( xi ) *f ( xj ) where i , j are indices in [ 0 ,..., n-1 ] .
I thought that " numpy.indices " would help me ( see Create a numpy matrix with elements a function of indices ) , but I admit I am at a loss on how to use that command for my purpose .
For example , do ` i ` and ` j ` range over all indices , or are there specific ones you want ?
We can use exp ( . ) as an example .
You can use fromfunc to vectorize the function then use the dot product to multiply : #CODE
Anyway , what I really wanted to say was that you want to find the outer product : #CODE
In [ 136 ]: %timeit op_version ( inputs , testing , x00 ) ( NOTICE : x00 is a reshape of x0 )
OK , I was pretty sure I had tried that to get rid of the loop , but I probably got the sqrt bit missing .
cumsum per group in column ordered by second column append to original dataframe
I want a cumulative summation of the third column per each unique group in col1 ordered by col2 .
Will give a hierarchical index solution , but I could not find a way to take the resulting cumulative sum column and attach it to the original dataframe without multistep merge commands .
I am trying to use numpy.savetxt to dump data in to varieties of format for each column .
I have a 2-dimensional array of ones and zeros called M where the g rows represent groups and the a columns represent articles .
Now , I would like to convert M into a square a x a matrix of ones and zeros ( call it N ) where if an article " art1 " is in the same group as article " art2 " , we have N ( art1 , art2 )= 1 and N ( art1 , art2 )= 0 otherwise .
N is clearly symmetric with 1's in the diagonal .
In order to compute something relevant to the gradient , I need to do the following :
If NumPy is backed by a good BLAS , ` dot ` will blow ` einsum ` out of the water for matrix multiplication .
some of the things I need to do can't be done with ` dot ` ( unless I make a few changes to the matrices and then change them back , which I'd rather not - because most of the code was written by someone else , and I'd rather not change it too much ) .
Once you have that written out , its just removing all the for-loop constructions and just keep the final C [ i , j , k , l ] +=A [ m , n , o , p ] *B [ p , q , r ] ( but with the right indices at the right places ofc ) .
since there you got two indices for the vector and two for the matrix ? while you should get one index for the vector and two for the matrix
Wouldn't work without the indices since ` shape ( S )` is ` ( X , 1 )` , not just ` ( X , )`
Just write it out in nested for-loops , as if you didnt know numpy could do this , and than keep the indices for the numpy einsum .
Anyhow , it might be faster to just create a list of indices that you want to keep or remove , and remove all entries in one go after the loop .
will return the indices of the maximum value in the array .
You can optionally specify and axis to get all maximum indices along the chosen axis .
This is not the same thing , this is a diagonal extraction .
Since you didn't mention it , I must be missing something , but won't ` ix_ ` work , e.g. ` y [ ix_ ( xs , ys )]` , which you can then reshape ?
If you want your result to be ( 6 , 3 ) just reshape after indexing : #CODE
Cython sum v / s mean memory jump
I have been trying to work with Cython and I encountered the following peculiar scenario where a sum function over an array takes 3 times the amount of time that the average of an array takes .
The sum is around 500K .
On a side note don't use sum as a variable name , it shadows the builtin sum method .
I used double instead of FLOAT_t and sum took 1.17 ms avg was 1.16ms
The funny thing is in the above function If i pass an extra argument and just divide sum by it , then the times are the same again .
I do not see any difference in performance on memory usage for avg and sum .
You should read ( and possibly parse ) all of them , and use ` min ( ... )` and ` max ( ... )` to get the smallest and the largest .
I would like it if there was some way to loop over the file and read the coordinates , then skip the momentum information , and strip away all of the extra information as well such as I= , X= etc .
In this array you will access the momentum , I and ( X , Y , Z ) using the first , second and third indices : #CODE
In this example the transpose of ` Q ` is not used , so you will probably not get the memory error .
Each element in the list is probably more than 64bits long while using indices you will get 32 bits per stored element .
I have a Pandas Dataframe of indices and values between 0 and 1 , something like this : #CODE
I'm reading in an image and using ` skimage.transform ` to resize it .
I want to resize it to have width : 60 , and height : 30 .
resize returns the changed image .
Do , as it is usual in cross correlation in statistics , and weight them by the inverse of the norm of the residuals ( fast to compute , as it is already in the output ) .
Your first code example is ** not what you're actually running** , because it misspells the ` transpose ` function and will not print anything at all .
( perhaps trial subtraction of a delta from rnum , till sum of trues before and after the operation is within some error margin ) #CODE
I could be wrong bu it seems that you can't append with ` to_csv `
To append to a csv use append mode ( see this or this question ) .
Note : The algorithm written in this form is numerically less stable , since we let numerator and denominator become individually very large , while previously we were dividing early to prevent this ( even at the cost of a sqrt ) .
I'm trying to implement a function that returns the max at each position of a dataframe or series , minimizing NaN .
Numpy extract values on the diagonal from a matrix
In that post , I extract elements which are bigger than zero from the input matrix , now I want to extract elements on the diagonal , too .
fill diagonal of the mask to True
scipy odeint : sum of conservative ode equations does not remain zero -- is this normal ?
However , for larger ones , I get the following issue .
In order to confirm this , I would like to ask : is it possible at all for a non-conservative system of equations to first print an ode sum that is close to zero , but later print values that are not close to zero ?
Another way to think of the question : in the larger system of equations , the ode sum initial prints out to be approximately zero , before increasing .
You can't use the numpy reshape for a simple reason : you have data duplicity in your original array ( time and positions ) and not in the result you want .
Before and after a reshape the number of elements must be the same .
So you want a 2 dimensional array with the inner dimension containing all of the data , and the outer dimension ordered by lon , lat , time .
Then translate the column order of the inner array .
Next sort the outer dimension on the inner dimension .
I would convert these to pandas Periods instead of Timestamps and then diff these #CODE
I see you're using a 16-bit float which has a max value of ` 65504 ` .
The arithmetic mean is the sum of the elements along the axis divided
Numpy function to find indices for overlapping vectors
I need to truncate them all so that the domains are the same ( i.e. x1 = x2 ) , and y1 represents the appropriate range to go with the new x1 , y2 is also truncated to go with the new x2 .
Or should this ` y3 = x1 [ 4:10 ] **2 + sqrt ( x2 [: 6 ])` really be something like ` y3 = y1 [ 4:10 ] **2 + sqrt ( y2 [: 6 ])` ?
` A ` contains either ` -1 ` , or indices of sets of 4 values in B #CODE
actually I was wrong about the structure of A , it should be simpler now with A containing indices of values in B .
global name ' sqrt ' not defined
I'm trying to test the function in ` iPython ` but I keep getting the error `" global name ' sqrt ' not defined "` .
Then use ` np.sqrt ` instead of ` sqrt ` .
How to generate Gaussian's dot in specific range by numpy ?
How can i generate Gaussian's dot in range of 0 to 2 ?
I was wondering whether there's a much faster method which will allow me to roll 10k vectors of size 100 ( say ) within less than 0.6ms , that is , 100 times faster .
Where do you get ` exp ` and ` sse.erfc ` from ?
Typically one forms the log-likelihood and assumes independence , so log p ( x | a ) = sum ( log ( pdf ( x [ i ] , a )) , i , 1 , n ) .
You need to use a minimization function , giving it log p ( x | a ) as the function to be minimized , and a as its free parameters .
I want to append the first array to the second one like this : #CODE
What this essentially does , is transpose your array so that it's ` array [ column ] [ row ]` , and then takes each columns , and pairs them with the sortKeys you provided in a list of tuples ( the ` zip ( sortKeys , a )` bit ) .
You can use numpy.argsort to get a list with the sorted indices of your array .
` argsort ` returns an array of indices that would sort the array .
Next , I want to append , Concatenate the dataframe to the the one prior in a cumulative concat function ..
Lastly , after all the concatenations are completed , I want to group by price and get the sum of count at each price ... this is where the problem really occurs .
If I change the step size of the search from 0.1 to 0.01 then it will return a constant nonzero value in those cases .
This would bork any numerical method that tries to find the largest root on the interval [ 0 , 1 ) , since for any root there are always larger ones in the interval .
The ` sin ` function should have roots at 0 , , 2 , 3 .
I thought , the indices [: : 2 ] should be [ 0 , 2 , 4 , 6 ] isn't it ?
What is the most simple and proper way to fitting data to sinc function using by numpy ?
I did some work and finally i got a data that its shape looked like sinc function and i tried to search how to fitting graph to sinc function using by numpy and i found this :
Can you give me more friendly way to fitting graph that give me a curve like sinc function ?
operands could not be broadcast together with shapes ( 36 , 1 ) ( 6 , 1 )
Each pixel in the convolution is the sum of the values of the shift gabor filter times the image pixels .
The variance is a bit more difficult since that is the sum of the squares , and of course , you need to calculate the sqaures before you calculate the sums .
where ` rolling_sum ` is a sliding sum ( i.e. the algorithm above without the last division ) .
how to find indices of k smallest numbers in a multidimentional array ?
I want to form an array containing indices of k smallest values in an array : #CODE
If you want the positions of the global minima , flatten the array firts : #CODE
Recovering the 2D indices will take some time , but it will still be faster .
I have two Numpy arrays which contain indices of maxima and minima from another array .
These indices come from intensity values in an image row .
The resulting vector is , e.g. , 4 , 3 , 2 , 1 , 0 , 1 , 2 , 3 , 2 , 1 , 0 , 1 , 2 , 1 , 0 ,... where the zeros correspond to the minima positions .
absolute value of the matrix gives the distance between each maximum to each minimum
how to replace an array containing indices with values from another array ?
I have an array b containing indices of an array a .
I want to insert values of another array c in the array b with same indices .
## this will create an array with indices of 2 smallest values of a1 #CODE
but its obviously not the right way as array a handles these indices as values
If you need it in the same dimensions as ` a ` you can reshape it : #CODE
Consider how much time it takes just to allocate a ( 1000 , 1000 ) -shaped array of ones : #CODE
Try ` mod = sm.WLS ( y1 , self.X , w ); np.isfinite ( mod.wexog ) .all() `
Is Numpy making a sum somewhere in the background ?
Yes , according to the documentation ` np.mean ` and ` np.ma.mean ` should be equivalent , but in my system they are defined in different places and the standard accumulator size of ` np.ma.sum ` for the sum when calculating the mean is the standard size of an integer in the system , which may be ` int32 ` , while for ` np.mean ` the standard dtype for the accumulator is ` float64 ` .
Pandas uses the index " line up " operations in that the operation will apply only to the common indices .
The ` squeeze ` method converts the NumPy array , ` a.loc [ ' 2005 ']` , of shape to ` ( 1 , 6 )` to an array of shape ` ( 6 , )` .
@USER You're right , based on what you said the ` squeeze ` method is a must for this to work .
Or probably downsize the data first ( does ` reshape ' do that without a copy if you specify a smaller size than the original ? ) and then pick some columns ?
Load the data from the file and reshape it to the shape you want ( four columns and as many rows as it takes ):
Another thing that catches my eye is that you are iterating through a numpy array with a for loop .
The reshape command was something I overlooked and the upgrade to 64-bit allowed for the creation of larger files .
You can ` flatten ` , make your changes on one row , and ` reshape ` : #CODE
In this case , you will have to calculate the indices yourself .
( 223.251 , 58.05 ) by using this equation : B=sin ( D ) sin ( d ) + cos ( D ) cos ( d ) cos ( R-r ) .
B =((( sin ( 58.05 )) * ( sin ( 58.0551391 )) + ( cos ( 58.05 ) *cos ( 58.0551391 )) * ( cos ( 223.251-223.25190261 ))))
Profiling log using ` %prun ` of my function is posted at bottom ( only a top few lines ) .
It seems the most time consuming part is ` nonzero ` of ` numpy.ndarray ` .
Because these are irregular operations , I can't use merge / join .
In the meantime , do you know which ` pandas ` function rely heavily on ` nonzero ` method in ` numpy ` ?
practially all indexing functions use `` nonzero `` , which btw is one of the most heavility optimized functions .
Numpy : get values from array where indices are in another array
Moreover , I have a nxk array , say b , that contains indices between 0 and m .
For fast access on indices of matrices or vectors I recommend using the hdf5 file format with [ pytables ] ( #URL ) .
With that file format you can easily and fast access indices in data files without any problems .
Is the indices list sorted ?
If you would anyway read almost all pages ( i.e. your indices are random and at a frequency of 1 / 1000 or more ) , this is probably faster .
On the other hand , if you have a large index file , and you only want to pick a few indices , this is not so fast .
( 1 ) Just sum the array elements instead of trying to compute the numerical integrals .
Then just sum those elements to get the cumulative probability .
( ahem ... I'll try again . ) @USER I've executed the code you showed above , and I see that ` sum ( sum ( prob_zgm )) * dx * dy ` ( where ` dx ` and ` dy ` are the grid widths in each direction , i.e. range /( number of steps )) yields a number very close to 1 , which says that your function ` p ` is a joint probability density , not a conditional density .
So to get the conditional density , you just need to extract a slice ( i.e. a row or a column ) from it , and normalize that slice so that ` sum ( slice*a ) *b ` where ` b ` is the step size along the slice ( so ` b ` is either ` dx ` or ` dy `) and ` a ` is the normalizing factor .
Once you have the conditional density , you can form the cumulative sum ( don't bother with trapz ) .
A discrete approximation to the conditional probability ` p ( z | m = m [ j ])` is just the ` j ` -th column of your array ` prob_zgm ` , scaled so that the sum of the column times the step size in the ` z ` -direction ( i.e. , ` ( 1.5 - 0.0 ) / len ( grid_z )`) is equal to 1 .
No need for ` diff ` then ...
Random integers from an exponential distribution between min and max
I would like to generate random integers on an interval min to max .
There are a number of suggestions for this e.g. Pseudorandom Number Generator - Exponential Distribution as well as the numpy function ` numpy.random.RandomState.exponential ` , but these do not address how to constrain the distribution to integers between min and max .
Numpy's implementation generates strictly positive integers , i.e , 1 , 2 , 3 ,..., so you'll want add ` min-1 ` to shift it , and then truncate by rejecting / throwing away results ` max ` .
It's possible to do this without rejection , but you'd have to create your own inversion , determine the probability of exceeding ` max ` , and generate uniforms to between 0 and that probability to feed to your inversion algorithm .
( Actually , only the first letters of mode names matter when giving the mode for ` convolve ` or ` correlate ` . )
let's first look at convolve : #CODE
With complex numbers ` correlate ` conjugates the second vector prior to the calculations above .
I am trying to get access to each of the subtrees that would result by dropping the greatest edge weight if that edge was not an outer edge of the minimum spanning tree .
As a next step , I'd like to predict confidence intervals ( for example 95% confidence for floor and ceiling values ) for what I predict the next values will fall in .
So in addition to the line below , I'd like to be able to generate two additional lines which represent a 95% confidence that the next value will be above the floor or below the ceiling .
C is a diagonal matrix of ` d x d `
In practice I want to do bilinear interpolation when I flatten the image from a 2-D pixel map to a 1-D radial average .
Find the floor of the radius as matrix index
Find the remainder from the floor for the bilinear interpolation of the pixels
This gives the radial sum ( rather than average ) Integrate [ f [ r ] r , { r , 0 , rmax } ]
The hard step conceptually ( for me ) to vectorize the problem was to collapse the N**2 pixel image into a N*sqrt ( 2 ) radial sum .
Convert from radial sum to average .
I haven't tested the fast vectorized solution properly ( against an analytic function ) but it looks right by eye .
Then ` ^ ` or ` + ` that into the original , depending on whether you want to make zeros or twos out of them : #CODE
I don't think this will be fast with short arrays , but it might be with large ones .
To be noted though that convolve can be done with the [ scipy implementation ] ( #URL ) which will scale in a better way .
Python Numpy nonzero
you get a 1D array , so if you had less than 50% of zeros in ` X ` , ` len ( X )` will increase .
What I was trying to do basically is get rid of all nonzero entries in my array .
@USER Then you can use fancy indexing instead of ` nonzero ` .
I'm trying to use scipy.ndimage.filters.generic_filter to calculate a weighted sum from a neighborhood .
reshape does not work " in place " meaning you have to assign to change a variable .
You should be able to just do ` return sum ( a*weights )` .
Finding zeros of a complex function in SciPy / NumPy
ValueError : operands could not be broadcast together with shapes ( 1 , 2 ) ( 20,100 )
But I keep getting the error message " ValueError : operands could not be broadcast together with shapes ( 1 , 2 ) ( 20,100 ) .
When you append a new root to the list , check that the previous root is not in the list by , e.g. , calculating ` np.amin ( np.abs ( np.array ( a ) -b ))` where ` a ` is the list of existing roots and ` b ` is the new root .
If you only want to draw the zeros , replace the number 5 ( number of contours ) with ` [ 0 ]` ( plot only the listed contour ) in the ` countour ` calls .
( iii ) There may be , but there are no general methods for finding all minima / maxima / zeros of arbitrary functions .
` .T() ` means transpose in ` numpy ` .
It seems that ` numpy.take ( array , indices )` and ` numpy.choose ( indices , array )` return the same thing : a subset of ` array ` indexed by ` indices ` .
` numpy.take ( array , indices )` and ` numpy.choose ( indices , array )` behave similarly on 1-D arrays , but this is just coincidence .
` numpy.take ( array , indices )` picks out elements from a flattened version of ` array ` .
` numpy.choose ( indices , set_of_arrays )` plucks out element 0 from array ` indices [ 0 ]` , element 1 from array ` indices [ 1 ]` , element 2 from array ` indices [ 2 ]` , and so on .
For example , ` numpy.take ` and ` numpy.choose ` behave similarly when ` indices ` and ` array ` are 1-D because ` numpy.choose ` first broadcasts ` array ` .
To sum up , here are my questions :
It's this final ` squeeze ` that turns an array with shape ` ( 1 , )` into ` ( )` ( or ` ( 1 , 2 ) - ( 2 , )`) .
You can easily add that dimension back in with a ` reshape ` .
Want to append 2 2d arrays in numpy
I'm trying to append 2 2d numpy arrays #CODE
Maybe you could fill the first rows up with zeros .
So you could resize ` a ` so that it matches the dimension of ` b ` and then concatenate ( the empty cells are filled with zeros ) .
I stuck to my list append answer
You can easily construct the tuple at runtime : ` ( 0 , ) * n ` yields a tuple of ` n ` zeros .
Then the inner loop ( for each pixel ) is fast , and the outer loop ( for each row ) does not really matter .
@USER indeed , an " absolute epsilon " approach is what I usually take .
Then you can just create the coordinate pairs by accessing elements both from both arrays with identical indices .
But apparently ` dstack ` and the like require exactly matching dimensions , they will not broadcast them , which I think is a shame .
fit in the log space
well , the range in each string can be made same but the first number in the string should always be + 1 more than the last number in previous string in list , and the since max value is 165340 the last number of the last string in the list should not go more than the value .
It will be more efficient to append to a list , and build the array from that list of lists .
What this means is that lists are designed to be very efficient with the use of ` append ` .
This is a sample loading shape that i use to correlate with
You can index numpy arrays with either logical arrays or arrays of indices like just like in matlab
In general , the functionality of ` repmat ` is handled in numpy by intelligent broadcasting , that doesn't require calling ` tile ` manually .
The ` mu [: , None ]` array has size ( 100 , 1 ) , and ` numpy ` is smart enough to " broadcast " it to size ( 100,100 ) to calculate ` centered ` .
Regarding ` x ` and ` y ` - the order of values is different , which you'll see if you try to flatten them : #CODE
That is MATLAB arrays are arranged like ' F ' numpy ones , not the default ' C ' .
I had to convert each submatrix to ` lil ` since ` csr ` has not implemented ` reshape ` ( in my version of ` sparse `) .
Many of the numpy functions have many lines of python code that reshape and massage the inputs , before performing the central action .
Here's how I usually deal with this : First I store the input shape and check if the input is scalar , then I apply something like ` np.atleast_1d ` and do my computation , in the end I reshape the result to match the input shape ( E.g. transform back to scalar ) .
If you choose to stick with integer indices , you can always insert a new singleton dimension using ` np.newaxis ` ( or equivalently , ` None `) : #CODE
Or else you could manually reshape it to the correct size ( here using ` -1 ` to infer the size of the first dimension automatically ): #CODE
I'm pretty sure there's nothing built in to ` pandas ` , but if you have the full stack installed you can use ` scipy ` : #CODE
Since the number of plots is so large , and plotting them takes so long , I've used ` fork() ` and ` numpy.array_split() ` to break the indices up and run several plots in parallel .
@USER That is just transpose , not conjugate-transpose .
" AttributeError : exp " while using numpy.exp() on an apparently ordinary array
A mixture distribution is just a number of component distributions , each with a weight , such that the weights are nonnegative and sum to 1 .
Result will have the same ` indices ` and ` indptr ` attributes as ` Y ` and this makes me think that there should be a shorter or faster way .
You are using ` X.dot ( T.T )` to compute the outer product of ` X ` and ` T ` , so I assume ` X ` and ` T ` are stored as * 2D * numpy arrays with shape ` ( 350 , 1 )` and ` ( 23000000 , 1 )` ( and not as 1D arrays with shape ` ( 350 , )` and ` ( 23000000 , )` .
Convert ` Y ` to COO format ( so the row and column indices of the nonzero data are readily available ): #CODE
Compute the equivalent of ` X.dot ( T.T ) - Y ` , but only for values where ` Y ` is nonzero : #CODE
If you want to stay away from Cython , building a diagonal index array and using ` np.bincount ` may do the trick : #CODE
If you always iterate over the smallest dimension , and have the vectorized sum over the largest , it should perform even better , shouldn't it ?
If you have a Fortran contiguous array , transpose it , then reverse the order of the output .
When doing this , you will also want to make sure that your values aren't generated by an outer product .
If your values come from an outer product , this operation can be combined with the outer product into a single call to ` np.convolve ` .
What does your stack trace look like ?
According to the documentation of the function , s is the elevation matrix , a 2D array , where indices along the first array axis represent x locations , and indices along the second array axis represent y locations .
Set values on the diagonal of pandas.DataFrame
I have a pandas dataframe I would like to se the diagonal to 0 #CODE
now I want to set the diagonal to 0 : #CODE
You may be able to squeeze some performance by replicating the source of ` np.intersect1d ` in your code , and instead of checking the length of the return array , calling ` np.any ` on the boolean indexing array .
It's right in the stack trace : ` / usr / local / lib / python3.2 / dist-packages / numpy / _import_tools.py `
Good eye , I assume .
For example , to draw red markers instead of blue ones : ` scatter ( x , y , c= ' r ')` If you only want single color per data series , you may also use ` plt.plot ( x , y , ' r . ')` ( ` r ` defines the color , ` . ` that we want separate data points . )
This , of course gives an error , as the indices given to ` cleanup ` do not match the indices expected .
Segmentation Fault on ndarray matrices dot product
I am performing dot product of a matrix with 50000 rows and 100 columns with it's transpose .
I use dot method of numpy for multiplying the two matrices .
The dot product works well on matrix with 30000 rows but fails for 50000 rows .
Is there any limit on numpy's dot product ?
Any problem above while using dot product ?
There is , for example ` memmap.dot ` which is optimized for taking dot products of memmapped arrays .
You will need at least three such arrays ( X , Y , and result ) plus any temporary space dot may need .
Indeed they are in repositories , but since I already had ATLAS , synaptic warned me that installing those libraries can break the other ones .
The non-zero values are found in the ` .data ` attribute of the matrix , and you can get their corresponding row / column indices using ` x.nonzero() ` : #CODE
How to find min / max values in array of variable-length arrays with numpy ?
` np.min ` and ` np.max ` are working , but numpy doesn't have support for ragged arrays so ` np.array ( data )` is making a one-dimensional array of objects , and ` np.min ` is giving you the smallest ` object ` -- the same as you would get if you had used Python's builtin ` min ` function -- the same goes with ` np.max ` .
You could use ` np.vectorize ` to make a vectorized version of Python's builtin ` max ` and ` min ` functions #CODE
use ` scipy.spatial.distance.cdist ( mat , vec [ np.newaxis , :] , metric= ' cosine ')` , basically computes pairwise distance between every pairs of the two collections of vectors , represented by rows of the two input matrices .
I found some statements about that here : #URL " [ ... ] matrix dot product it can use an optimized implementation obtained as part of " BLAS " " and the other one : " Finally , scipy / numpy does not parallelize operations like
I'm trying to go through my directory and generate an array from each csv file ending in " _chisqtype2_sheet.csv " and append it to the list chisq2data .
check on indices of matrix / array and then check for some condition
Note that you need to pass ` k=-1 ` to ` numpy.tril_indices ` to not include the diagonal .
ultimately yes . but i also googled for ways to loop indices in numpy w / o calling for loop or a fast C API loop .
I think numpy doesnt stil have enumerating through indices . say you just want to print the indices of an array in numpy . say where ([ i , j ] in a is sm condition , result1 , result2 ) ; You cud do where ( array ) instead
This is just a positive side effect of looping through indices to check for row , col conditions . perhaps anyone could use it for some other meaningful purpose where tril and triu methods are not the case .
To complicate matters , when I try to run the script from the terminal in order to save any error message , the log records no error message and indeed stops much earlier in the script ( despite being complete if ` toarray() ` is not called ) .
If the variable holding your CSC matrix is called ` mat ` , can you print ` mat.shape ` and ` mat.nnz ` and share them here ?
You cannot call ` toarray ` on a large sparse matrix as it will try to store all the values ( including the zeros ) explicitly in a continuous chunk of memory .
Converting to the contiguous array representation would materialize all the zeros in memory and the resulting size would be : #CODE
How to slice a numpy array by a list of column indices
If the column indices are irregular then you can do something like this : #CODE
So you need to transpose it to get the output array in the desired format .
For example , ` float ( mat [ ' Temperature '])` will have the same result as ` mat [ ' Temperature '] [ 0 ] [ 0 ]` .
All results depend on the previous ones indeed , so we need to keep the data in the memory .
I am struggling finding a quick , pythonic way to merge all of my arrays into a dataframe , and filling missing data with zeros .
This should solve it , using ` merge ` function and ` outer ` method #CODE
reshape to the correct shape ( number of days , 17 )
In testing this most of the time is spent in calculating the indices ( around 400 ms with an i7 processor ) .
The structure seems to be a single outer array with an array within it of pixel arrays ( 3 number )
I had a typo in my code on the row with reshape .
The point is that ` a.reshape ( x , x )` returns reshaped version of ` a ` ( does not reshape it ) , whereas ` a.resize (( x , x ))` or ` a = a.reshape (( x , x ))` really change the shape of A .
Is there any good ways to find all value indices in an sorted array A close to several targets ?
Use numpy.searchsorted() can allow us to find indices close to several targets efficiently :
What I could do is just loop through idx to get boolean indices like [ A == A [ idx [ 0 ]] , A == A [ idx [ 1 ]] ,...
One thing is I could first find the unique set of the array with numpy.unique() . to find all same values .
Then searchsorted() on that unique array , which might save some time .
If one can build an alternative unique() , to get a indices list for each unique value in A ([[ indices have value ua [ 0 ]] , [ indices have value ua [ 2 ]] ... ]) .
get the closest indices and the closest values : #CODE
With the information of the full stack trace report the bug to the ubuntu team .
How do I align the significant values ( not zero ) in array2 with the ones in array1 ?
This must be done in order to post-process arrays in a meaningful way e.q. calculating average or sum etc .
translate the image by the offset of the center of gravity
( Or have a look at : python numpy roll with padding ) .
A word of warning about the alignment procedures : The simples ( COG , correlation ) fail , if you have an intensity gradient across the image .
Due to this you may want to look for edges and then correlate .
That means I have to translate all my results into long arrays - this does not take advantage of me actually having structured grids
If you do not find a good one , roll your own !
The points are defined on each axis by : ` floor ( a ) -1 ` , ` floor ( a )` , ` floor ( a ) +1 ` , ` floor ( a ) +2 ` etc . for each axis .
But what if my calculation says requires lets 5 significant figures but they involve multiplying really small numbers like dot ( 1.6 e-19 , 6.626 e-34 ) are you saying this would still give me an accurate result up to 15 significant figures ?
pandas apply np.histogram to reshape dataframe
I want to get the normed histogram of each column of a pandas dataframe .
I usually used " v = [ len ( list ( group )) for key , group in groupby ( newa , key= np.isnan ) if key ]" and print max ( v ) .
But now I get the problem , that the bars do not seem to stack properly :
So I basically just created a matrix filled with zeros , declared a list with the unique letters I want to compare .
I really appreciate the reply though and I also learnt the transpose function from your reply .
I need to put in a matrix some words that are taken from a list , based on their indices .
cuv_list is the list of words , and mat_index contains the indices that correspond to the ones in the cuv_list
The mat_index matrix contains indices , that are used as " pointers " to the cuv_list .
fast dot product on all pair of rows
I have a 2d numpy array ` X = ( xrows , xcols )` and I want to apply dot product on each row combination of the array to obtain another array which is of the shape ` P = ( xrow , xrow )` .
You can always extract the iteration from the ` Parallel ` and write it as a for loop saving directly into a result array ( the ` zeros ` from above ) and avoid having all the intermediate matrices in memory .
What I want is to find the indices of array b elements which match those in array a .
More info : I want the indices to match the order of array a too .
However , I want the indices in the same order as the elements in a .
Alternatively , you could create a dictionary mapping the individual elements from ` b ` to their indices .
Then you can use this mapping for quickly finding the indices where elements from ` a ` can be found in ` b ` .
I just created a separate set and used : ` [ indices [ c ] if c in b_set else -99 for c in a ]`
Lookup in ` dict ` is O ( 1 ) as well , so you can just do ` indices [ c ] if c in indices else -99 ` , or use ` get ` with a default value , i.e. ` indices.get ( e , -99 )` .
Considering the ` min ` and ` max ` values of both matrices we cannot have ` 4294967292 ` as maximum value of the difference matrix .
I have also done similar operations in Matlab and the difference ` diff ` and maximum value ` diff.max() ` are consistent .
So , if you want to mimic Matlab's behavior you'll need to clip the result #CODE
python numpy ValueError : operands could not be broadcast together with shapes
In other words , if you are trying to multiply two matrices ( in the linear algebra sense ) then you want ` X.dot ( y )` but if you are trying to broadcast scalars from matrix ` y ` onto ` X ` then you need to perform ` X* y.T ` .
On the other hand , if you're likely to have more zeros than ones , stepping through the list and manually multiplying powers of two and adding them will be fastest .
My method of stepping through the list backwards adding powers of two is still faster if you have all zeros , but otherwise , @USER ' s method is definitely the fastest , and my implementation didn't even include his caching optimization .
Using ` matplotlib ` , I'm trying to generate a histogram from a list of values .
Assuming you've omitted " import matplotlib.pyplot as plt ; import numpy as np " , the code you've provided produces an accurate histogram .
I think I might just not understand how to create a histogram correctly , since my output matches internet examples when I just copy and paste code .
Your code is correct -- if your dataset integers are indeed upper-bounded by 200 , then it will produce the desired histogram .
The histogram is correct .
Anything is possible , but the implication seemed pretty clear that the indices ( column numbers ) were determined by the position they first occur , excluding duplicates .
This assign integers to the alphabetically sorted unique content .
To try and get further I looked up the .DAT header format to understand how to grab the bytes and translate them - how many bytes the data is saved in etc :
You will need to look at the LV code to see exactly what kind of data you're saving and how the write file function is configured ( byte order , size prepending , etc . ) and then use that document to translate it to the actual representation .
You can use the np.einsum function to do that . that will let you give a letter ( index ) to each dimension of the arrays you supply as a string and use the einstein sum notation to process .
I have a numpy array to append in a file every time and it turns out to be 80 by 80 matrix in file .
Numpy , sort rows of a matrix putting zeros first and not modifying the rest of the row
I would like to sort it by rows putting the zeros in each row first without changing the order of the other elements along the row .
For each line in ` a ` , you find the nonzero indices and prepend some zeros to match the size .
I had to put log scale and everything looks fine .
numpy determine mean diff between arguments
using NumPy's ` mean ` and ` diff ` methods is much faster : #CODE
It should translate quite easily into Python .
Is there a function in Python that samples from an n-dimensional numpy array and returns the indices of each draw .
However , I will be dealing with large n-dimensional arrays and can not afford to reshape them just to do a single draw .
Note that since your array is contiguous in memory , ` reshape ` returns a view as well : #CODE
` reshape ` it to ` ( n , 2 )` .
` squeeze ` also gits rid of the singular dimension .
SciPy instead chose to extend the signal at either edge by padding zeros .
There is definitely an argument to be made that it should not pad with zeros , but instead with the boundary values , but it should be noted that no boundary strategy is going to be perfect ; the ideal way to deal with boundary issues is going to depend on your particular signal .
If you have to read and write CSV , you will need to roll your own logic .
the range of the color gradient is controlled by ` norm =p lt.Normalize ( 0 , 10 )`
If so sum it up .
Does anyone know of an efficient way to get the sum of precipitation from such a dataset ?
All conditions can be described as ` delta = ( 100 + tprep - ( tprep-1 )) mod 100 `
All your three conditions can be described as one ` delta = ( 100 + tprep - ( tprep-1 )) mod 100 `
or even as ` delta = ( tprep - ( tprep-1 )) mod 100 ` #CODE
I have data of four weeks and sum precipitation currently on hourly base .
will change all ` nan ` s into zeros .
The remaining bits , which are in the trailing significand field , encode the payload , which might be diagnostic information ( see above ) .
There is a possibility of multiple rows of B satisfy this condition for each i , in that case max or sum of corresponding rows would be the result .
The slice notation represents all indices along the first dimension ( just 0 and 1 , since the array has two rows ) , and the 4 represents the fifth element along the second dimension .
The trick , if you want to get out something of the same dimensions you started with , is then to use all slice indices , as I did in the example at the top of this post .
If I were to histogram this it would have much greater bin values at lower energies .
While Python is a beautiful programming language with a very friendly community , getting started with the scientific Python stack can be quite a hassle .
How can I keep trailing zeros in a list or np.array ?
Python automatically convert it in [ 0.2 , 0.22 , 0.22 , 0.12 , 0.33 , 0.33 ] but in fact the zeros are significant figures in my calculation so I would like to keep them in the array .
I know that I can import decimal and use Decimal for a single value but how can I keep all trailing zeros in a list or np.array ?
Either use decimals , or strings if you care about those zeros .
( To that end , the computer may store ` 0.2 ` as ` 0.20000000000000000000001 ` just because of precision issues , but this is not actually possible to truncate at the storage level due to the limitations of binary representation . )
min function raising ValueError exception
I'm trying to use the built-in function min from Python 2.7.3 with the following list as an input : #CODE
It's strange because min works on other arrays in the same form as this one , but this particular set of numerical values causes the error .
That error doesn't come from ` min ` .
Instead of using min ( x ) , it was needed to use min ( x , key=lambda i : i [ 0 ]) .
so you wanted the min of just the first element ?
What I need to do is find values between the ones in the look up table and I need it to work if I don't know the dimensions of the look up table before hand .
I had to add an extra step to squeeze dimensions that only had one value , but this seems to work .
I can't find a reason why calculating the correlation between two series A and B using ` numpy.correlate ` gives me different results than the ones I obtain using ` statsmodels.tsa.stattools.ccf `
Say I have ` f ( a , b , c , d , e )` , and I want to find ` arg max ( d , e ) f ( a , b , c , d , e )` .
But now assume instead , that I want to solve ` arg max d f ( a , b , c , d , e=X )` : Instead of optimally chosen ` e ` for every other input , ` e ` is a fixed and given ( of size AxBxCxD , which in this example would be ` 10x10x100x10 `) .
Also , notice that instead of ` tile ` you could make use of broadcasting .
@USER , oh right they're supposed to be indices ..
Well indices for ` e ` can only be 0-9 because that dimension is only length 10 .
Maybe ` floor ( arange ( 0 , 10 , 0.1 ))` ?
The original array is read with the list function , and when I try this I get TypeError : list indices must be integers , not tuple
@USER -- yes because that's what ` histogram ` returns .
So I know what the gradient of a ( mathematical ) function is , so I feel like I should know what ` numpy.gradient ` does .
Return the gradient of an N-dimensional array .
What is the gradient of an array ?
The docs do give a more detailed description : ` The gradient is computed using central differences in the interior and first differences at the boundaries .
The returned gradient hence has the same shape as the input array .
As you can define the discrete derivative of a monodimensional array ( x [ i+1 ] -x [ i ]) / h in the simplest case , with h typically 1 ) , you can define the discrete gradient ; it's often used in image algorithms ( see #URL ) .
So , the gradient between the points in the array ( assumes a y delta of 1 ) is : #CODE
You could find the minima of all the absolute values in the resulting array to find the turning points of a curve , for example .
Then gradient is nothing else as matrix differentiation
For a good explanation look at gradient description in matlab documentation .
The gradient is computed using central differences in the interior and
ValueError : operands could not be broadcast together with shapes
` np.power ( a , [ -6 , - 8]) ` but this raises ` ValueError : operands could not be broadcast together with shapes ` .
Should I somehow call the flatten generator inan iterative way ?
I need to calculate ` exp ( x**2 )` where ` x = numpy.arange ( 30 , 90 )` .
Then exp ( -10 ) will be not ` overflow ` !
I can't think of a way to join the slices cleanly ; however , I think using the composite is the way to go .
You can use ` np.r_ [ ]` to join the slices into an array : #CODE
Note that ` np.hstack() ` is used to obtain a single array with all the indices that will be removed .
Out of this stack I'd probably take ` numba ` or plain C depending on the rush I am having .
I need all the indices ( or all the values , or a mask ) of array A where the values of array B are in a range and the values of array C are in another range and D in yet another .
I want either the values , indices , or mask so that I can do some as of yet undecided statistics on the values of the A array in each bin .
If you expect that ` means [ i ] [ j ] [ k ]` will always be a single value and not a list you can use append instead of extend .
append does work with a non-iterable ( e.g. a float )
Purpose is design verification for logic design of high performance div , log , exp functions etc .
It will also work if they are both arrays that can be broadcast .
My generic solution for this is using an index field and increment indices plus handling overflows .
Can't you just use convolve instead and look for all zero-crossings with a suitable 1st derivative kernel ?
I simplified the code to use one kernel for both ` convolve ` s as @USER commented .
For the choice of `" valid "` as option to ` convolve ` .
The Counter class builds a dictionary of unique values and counts their occurrences from a list : #CODE
As you mention I need an absolute width and not a width perpendicular to the line !
saving big matrices in python : zeros saved instead of data
Thus when I load the saved matrix ( numpy.load ) only a small portion of the first data are stored and the rest of the matrix is filled with zeros .
What's the max filesize your filesystem allows ?
The problem of the zeros seems not to arise for a computer with sufficient place on the disk so I think zeros are written when no place can be given anymore to " actual " floats .
The script multiplies two pairs of matrices ( each pair is the same in terms of ` shape ` and ` dtype ` , only the data differs ) in different ways using both ` dot ` function and ` einsum ` .
These effects remain for both ` einsum ` ( numpy implementation , as I understand ) and ` dot ` ( uses BLAS at my machine ) .
And what can be done to flush denormals to zero in such cases ?
I care only about the dot product from BLAS .
Alternatively , simply replacing the tiny values in ` C ` with zeros might work , but whether the resulting accuracy loss is significant and / or acceptable would depend on your application .
I gave the best answer to pv because they did help answer the problem as stated .
User pv helped me move the cut of the Hankel function to the negative imaginary half axis , but my plot of sigma is still off .
I assumed you'd precalculate the indices and work from there .
I have nothing to do with the downvoting , but it is more clear , as well asfaster , to use the boolean array for indexing without the call to ` np.where ` to convert them to indices , i.e. : ` mw [ mw < 4.3 ] = mw [ mw < 4.3 ] * 0.67 + 1.03 ` .
I need to subtract a number from an array x1 from all the numbers in an array x2 and check if the absolute value of the result is less than 0.01 , if it is then append x1 to a new array .
This uses ` numpy.tri ` to get an array with ones below the diagonal and zeros above , and subtracts this from 1 to get an array with zeros below the diagonal and ones above : #CODE
Use ` numpy.tril_indices ` to generate the indices of the lower triangle , and set all those entries to zero : #CODE
Thanks I have changed my code to append each individual " open , high , low , close , date " to a separate array o = [ ] , h = [ ] , l = [ ] etc .
I solved this problem by saving each file separately instead of all together in one big file , this way I could use a for loop to read each separately , operate on them , and then append all the outputs .
` delta [: 1 ]` works because you're asking for just the first element , but delta [: 2 ] will be more than a single integer , and so the multiplication is interpreted as a dot product .
Numpy PIL Python : crop image on whitespace or crop text with histogram Thresholds
How to format in numpy savetxt such that zeros are saved only as " 0 "
How can I format and save such that the zeros are saved only as 0 and the non zero floats are saved with the `' % 5.4f '` format ?
if you need a dense array you can do ` m.toarray() ` , where you can see the zeros ...
For example , here's a pared down function that writes compact zeros : #CODE
Fancy indexing arrays are broadcast to a common shape , so the following also works , and spares you that final reshape : #CODE
I did try to roll my own using numpy and scipy , but I haven't had much luck yet , my knowledge of signal processing is lacking .
The way I printed it here , I can simply do ` dNew = append ( d , expectedOutput , axis=-1 )` .
There might be nicer ways , but a combo of ` take ` , ` diag ` and ` reshape ` would do : #CODE
Comparisons with @USER Paolinis ' alterative , adding reshape to it to match the sought output : #CODE
I said " more efficient " because your solution passes through a N*N matrix of which you discard all but the diagonal .
But as pointed out in the comments , ` append ` is also inefficient since it make a copy of the array contents .
If this is indeed the problem , one solution would be to read and parse the file in chunks , and then concatenate the numpy arrays .
I don't know much about numpy , but doesn't ` append ` always create a copy with the argument added ?
I use the ` resize ` function in my internal implementation .
However , I was forced to insert a kludge in the Lomb-Scargle function to add jitter to the frequencies , so they don't exactly match up to the FFT ones .
Apply numpy nonzero row-wise ?
I have a 2d boolean array from which I'm trying to extract the indices of the true values .
Numpy's nonzero function decomposes my 2d array into a list of x's and y's of positions , which is problematic .
Is it possible to find the column indices of the ` true ` elements while preserving the row order ?
Or if you want to get the indices for the ` True ` s row-by-row , then #CODE
If you want to know the indices of columns with a ` True ` on row 3 , then : #CODE
For small sizes it works , but for larger ones where len ( array2 [: , 0 ]) is something like 2**17 ( and could be even larger ) and len ( array1 [: , 0 ]) is about 2**14 .
Now the problem is that while using loops to manipulate the data it took around 13 min . how can I reduce this time .
Does it blow up on all inputs or only big ones ?
If you want to correlate all 27000 lines with each other , that makes 5.4G using float64 .
Unfortunately , the ` numpy.corrcoef ` does not do this automatically , and we'll have to roll our own correlation coefficient calculation .
So , you need to specify ` axis=0 ` when taking ` min ` , otherwise ` numpy.min ` would be the min on all the elements of the array , not each column separately : #CODE
If the calculations you're doing in ` funct ` are pretty quick to finish , the overhead you're adding may end up taking longer than the sum of all the calculations , which would make the regular map faster .
Column wise sum V row wise sum : Why don't I see a difference using NumPy ?
I'm still not sure about tracing down the cause of this as ` perf ` shows about the same rate of cache misses ( sometimes even a higher rate ) for the faster row sum .
Row sum ` n=10k ` #CODE
Column sum ` n=10k ` #CODE
Taking the min rather than the mean , I'm now consistently seeing a similar magnitude of difference to you with 1.8.1 .
It's interesting to see that when I go back to ` nloop=1000 ` , ` nreps=3 ` I actually see a slightly * greater * rate of cache misses for the row sum ( 17% vs 13% ) , even though it's faster than the column sum .
When comparing BLAS libraries , you definitely want to use intensive operations like matrix-matrix dot products as the point of comparison , since this is where you'll spend the vast majority of your time .
I've found that sometimes , a worse BLAS library will actually have a slightly faster vector-vector dot operation than a better BLAS library .
What makes it worse , though , is that operations like matrix-matrix dot products and eigenvalue decompositions take tens of times longer , and these matter much more than a cheap vector-vector dot .
I think these differences often appear because you can write a reasonably fast vector-vector dot in C without much thought , but writing a good matrix-matrix dot takes a lot of thought and optimization , and is the more costly operation , so this is where good BLAS packages put their effort .
The same is true in Numpy : any optimization is going to be done on larger operations , not small ones , so don't get hung up on speed differences between small operations .
EDIT : After thinking a bit on this , yes I would say that the first time that timeit executes the sum , the column ( or the row ) is fetched from RAM , but the second time that the operation runs , the data is in cache ( for both the row-wise and column-wise versions ) , so it executes fast .
I doesn't benefit much from any caching , i.e. repeated sums are not much faster than the initial sum .
So the next element to sum up is already in the cache .
Column sum speed is first and foremost limited by memory bandwidth .
elements in an array from the sum
If we sum the elements of the array it will give 15 .
For instance I want the sum of the first 3 elements so I'll have 6 .
My problem is I want a new array where it will give me the first 3 elements from the sum ( let's call that variable my_sum ) .
The sum of the list is a single number , what do you mean by " elements of the sum " ?
the sum of ` [ 1 , 3 , 5 ]` is the same as the sum of ` [ 1 , 2 , 6 ]` ...
What I mean by elements of the sum is that from A= [ 1 2 3 4 5 ] , I know how to get the sum of the array which is 15 .
If I sum the first 3 elements of the array I get 6 .
What I want is after getting the sum of let's say n numbers .
I want to have an array where this gives me the elements from array A that gives the sum that I want .
@USER do you mean you want to find some sub-list within the list that has a specified sum ?
@USER but as CommuSoft has pointed out , there can be multiple combinations of elements from your original list that sum up to the given number .
My problem really is to add the first few elements that give a specified sum , so I have 1+2+3 that gives me 6 .
Then from that , I want an array that will return [ 1 2 3 ] knowing that the sum of the first 3 elements is 6 .
I tried it and I got the following error : " AttributeError : ' numpy.ndarray ' object has no attribute ' append ' "
Also , if your original image is truly linear , then you'll need to make the reverse gamma correction : #CODE
With matplotlib , the sanest approach IMO is to normalize every integer-typed image to the range 0-1 as soon as you get it , and only then perform math operations ( for example , correcting gamma like this : ` im **= gamma ` [ can you believe this works ?! ]) .
If you wanted the rightmost value of a ' linked cluster ' , you could extend this solution as linked = delta > threshold , A [ diff ( linked )= =-1 ]
The function is not called for each cell , but once with all indices .
` outer ` will perform the outer product of two vectors , exactly what you want except from the function .
I have confirmed that my objective function ( " calc_res ") is indeed returning a scalar as required ( in my case , the sum of the squared residuals ) , but I still get a ` setting an array element with a sequence ` error .
What is the best way to test whether an array can be broadcast to a given shape ?
` broadcast_arrays ` is what I was looking for in order to get laziness , but I still need a verification stage - how can I test if shape A can be broadcast to shape B , assuming I don't have an array of shape B in hand ?
And a single traversal over an array is always going to be faster than an ` O ( N log N )` sort , so sorting an array and then looping over it is not going to make it any faster .
I want to multiply ` ( A1 , B1 )` and ` ( A2 , B2 )` to form a ` (8 0 , 30 )` matrix , the difference here is that ` A1 ` is defined as the transpose of ` A2 ` , with ` 10000 ` rows in ` A1 ` but ` 80 ` row in ` A2 ` .
How to crop same size image patches with different locations from a stack of images ?
Suppose I have an ndarray ` imgs ` of shape ` ( num_images , 3 , width , height )` that stores a stack of ` num_images ` RGB images all of the same size .
Assume , I have sparse matrix t of zeros .
To get that , you'll want to ` flatten ` the array : #CODE
How to pythonically get the max of a numpy argwhere function
I have read a Matlab file containing a large amount of arrays as a dataset into Python storing the Matlab Dictionary under the variable name ` mat ` using the command :
` mat = loadmat ( ' Sample Matlab Extract.mat ')`
Is there a way to utilize a command similar to this to obtain all corresponding elements within the arrays inside of the ' mat ' dictionary variable ?
As you note , ` mat ` is a dictionary .
To decode a binary answer from a socket connection in Python I would do : #CODE
Could you expand on that in your question and possibly dump some of your raw data with the numbers you think are in it ?
For every label get the max and min of each coordinate .
Edit : Now storing only updated max and min per coordinate for every label .
This removes the need to maintain and store all these large lists ( append ) .
After this operation we have six vectors giving the smallest and largest indices for each axis .
you're first problem is that you've set your index ` j ` to run from ` 1 ` to ` n ` and then tried to access ` f_j [ j ]` when ` f_j ` only has valid indices from ` 0 ` to ` n-1 `
The easiest way is to use the Polynomial class .
The problem is that ` np.where() ` returns the indices .
why unable to concatenate two arrays in python ?
when I perform concatenate operation it shows an error #CODE
Than you can concatenate over 2nd dimension , obv
That's not a transpose , but you do want to reshape the array to ` ( 1 , 195 )` .
Ask NumPy to transpose it ( with ` transpose ` or ` T `) , and you'll find no change .
As a working example , lets just say I call the ` sum ` function on each ` array ` #CODE
numpy : unique list of colors in the image
Another way that might be of practical use , depending on your reasons for extracting unique pixels , would be to use Numpy s ` histogramdd ` function to bin image pixels to some pre-specified fidelity as follows ( where it is assumed pixel values range from 0 to 1 for a given image channel ): #CODE
If for any reason you will need to count the number of times each unique color appears , you can use this : #CODE
Finding False-True and True-True row indices
I can find the indices of all values marked ` True ` by simply entering : #CODE
Now you can use ` numpy.where() ` on this neww array to retrieve all the desired indices .
Assuming I understand you , and you're looking for the indices where either the element is True or the next element is True , you can take advantage of ` shift ` and use #CODE
The way I create my own format is construct a list of list using append operation .
TypeError : list indices must be integers , not list
Finding unique columns in an HDF5 dataset
There is a huge amount of redundancy in the columns ( 97% of the columns are not unique ) .
My current solution consists in loading blocks of the dataset into a numpy array and using a dictionnary to store the unique columns and the metadata .
A fast way to find nonzero entries by row in a sparse matrix in Python
I am trying to find the indices of nonzero entries by row in a sparse matrix : scipy.sparse.csc_matrix .
to each row to get the nonzero column indices .
But this method would take over an hour to find the nonzero column entries per row .
Getting all the row and column indices separately can be done as follows : #CODE
But you can also get both sets of indices simultaneously for any of these sparse types , this may be the easiest : #CODE
If you need to find values in a specific row , csc and csr matrices will return the nonzero entries sorted by row , coo seems to return its indices ordered by columns .
If you'd like the indices as ` ( row , column )` tuples , you can use ` zip ` .
What form do you want these indices in ?
` nonzero ` on the whole matrix has the same numbers , but they aren't split into sublists .
In a ` coo ` format , the same indices are there , but the order is different .
You can override this behavior by using the arguments ` vmin ` and ` vmax ` ( or ` norm `) of ` imshow ` .
But since the pythonic way doesn't deal with simple things like indices , I have no clue how to do that .
This produces a forward fill by taking the indices we'd ordinarily use : #CODE
Finding the ones we want to keep : #CODE
Zeroing the indices that we don't want to keep : #CODE
Here it is in pure python assuming that your first index is less than the max .
so is the size of the matrix you want to build determined by the max value found in ` T ` and ` F ` ?
Numpy : how to roll 1 " row " in an array of arrays
For instance , I want to pass GRID into a function with a row number and number of positions to roll , and then return the result .
How can I roll the single row independently ?
either pass the columns around and roll them before constructing ` GRID ` or break ` GRID ` back into columns , roll one , and reconstruct ` GRID `
I want to be able to arbitrarily pick one of the rows and roll the values of that array only .
In the above example , let say you want to roll COL02
Let's say we want to roll the first row one place to the right : #CODE
Why np.searchsorted returns negative indices .
After getting these indices I basically do something like this #CODE
EDIT : if you dont want to remove first 3 elements play with the step and ` np.s_ [ 0 : -1 : step ])` because array indices start with 0
So , you can open file in append mode and write to it : #CODE
You can broadcast the 2D mask against the 3D array , so that its size is expanded along the 3rd dimension without actually duplicating it in memory : #CODE
Yeah , it's not a function I use that often , since you can almost always broadcast implicitly .
To be honest , I think the ` masked_array ` constructor ought to broadcast the mask against the array internally - I might consider making a pull request .
Finally , try fitting log ( y ) to 1 / T , rather than y to exp ( 1 / T ) -- that is , take the logarithm of your y-data before attempting the fit .
Is there a quick command to find all of the matching indices in b that contain a value in a ?
Error while calculating dot product in Theano
How to resize or reshape a volume image in every dimension ?
Is there a vtk filter or maybe a numpy reshape function that allows me to do this ?
Here is some python code that can somewhat reshape an image , but the data is just spread all over the image .
Although you figured out how to reshape with numpy , there are filters in vtk that can perform that task ( maybe it helps because you do not have to copy the image into a numpy array ) .
You may need to strip the string columns further , and then process the array-like values as you need .
numpy sum of squares for matrix
So i defined a cost function and would like to calculate the sum of squares for all observatoins .
Taking the sum of sqares for this matrix should work like : #CODE
If you would take the sum of the last array it would be correct .
Theta is a array 2x1 of zeros that I m trying to multiply to X .
I should get 97x1 array of zeros to the output for predictions .
` b [ 0 ]` is an array containing the indices , and ` b [ 0 ] [ 0 ] == 24 ` .
For now , most sparse matrix representations assume 32-bit integer indices , so they simply cannot support matrices that large .
This gives you a vector of number of occurrences of values 0 , 1 , 2 , ... max ( label )
Creating and zeroing the output vector is relatively fast , and it is easy and fast to compress back with ` np.nonzero ` .
It seems like a fairly common issue for time series , but despite messing around with groupby , diff , and the like -- and exploring SO -- I haven't been able to come up with much better than the below .
This problem can be transformed to find the continuous numbers in a list . find all the indices where the series is null , and if a run of ( 3 , 4 , 5 , 6 ) are all null , you only need to extract the start and end ( 3 , 6 ) #CODE
I can make a view onto that array that reflects a 2d sliding window , but when I reshape it so that each row is a flattened window ( rows are windows , column is a pixel in that window ) python makes a full copy .
I added later here extra printing the max value of both var .
Seems like I get the data on both axis scaled by overall ( from both axis ) max and minimum .
It is the same when I try on other data with bigger diff . in scales .
You could just use the contents of the ` events ` column to make boolean indices , and use these to index into ` kipp_macht ` and ` luftb_hydgr_add ` : #CODE
Or you could get the corresponding row indices and use these to index into ` kipp_macht ` and ` luftb_hydgr_add ` , although this is a slightly more roundabout way to do things : #CODE
Just want to look how variables correlate in between .
sympy error - AttributeError : sqrt
sqrt is defined in the math module , import it this way .
You are using a sympy symbol : either you wanted to do numerical sqrt ( in which case use numpy.sqrt on an actual number ) or you wanted symbolic sqrt ( in which case use sympy.sqrt ) .
Each of the imports replaces the definition of sqrt in the current namespace , from math , sympy or numpy .
Can't append global NumPy array from other module
It's better to append to a list , and build the array from that .
Or concatenate arrays .
I just looked at the ` append ` code .
It actually uses ` concatenate ` .
Try creating an array with the ' TEXT ' data , and concatenate that to ` test_array ` .
The append method actually copies the entire array to a larger physical space to accommodate the new elements .
Each time you append this way , the copy gets slower because you need to allocate and copy more memory each time it grows .
That's why the id returns a different value every time you append .
Also append seem to like lists of tuples , not lists of lists .
Numpy could not broadcast input array from shape ( 3 ) into shape ( 0 ) inside the method
I have this method above which creates an array which contains three random numbers between indices ` i*3 ` and ` i*3+3 ` and zeros for the rest .
Do you really have 50M unique bonds ?
Ok , so what I'm trying to do is a scale space on a 1D set of data where the entire data set presumably is taken from a sum of gaussians function .
But when doing this you should include the fact that the off diagonal terms are twice as important as the diagonal terms .
I think you shouldn't add ` 2* ` the outer product , but instead the outer product and its transpose , to get the same output as when both loops would run to ` nq ` .
Does the " -> jl " part tell it to explicitly sum over i and j even though they are not repeated indices ?
In my mind ` ij , kl ` says take the full " outer " product and then ` -> jl ` says to " collapse " to the second and fourth dimension ( by summing the first and third dimension ) .
Exactly ; the rhs of einsum contains all indices retained after the product ; all omitted indices are summed over .
This gives more explicit control over summation behavior than the standard Einstein summation convention of repeated indices .
I am using this code to get fft of a signal but how can I plot the fft .
And it plots both the signal and the fft .
Of course " join " and list comprehension can be used here as well .
create an array of bytes and decode the the byte representation using the ascii codec : #CODE
In these cases however you do have to append the right length to the data type ( e.g. ' S3 ' for 3 character strings ) .
Python - Matrix outer product
I want to generate another matrix ` C : m * n ` , with each entry ` C_ij ` being a matrix calculated by the outer product of ` A_i ` and ` B_j ` .
NumPy ufuncs , such as ` multiply ` , have an ` outer ` method that almost does what you want .
A more NumPyish way of doing this could be to search the break-point indices : #CODE
Given a ` 4D ` array ` M : ( m , n , r , r )` , how can I sum all the ` m * n ` inner matrices ( of shape ` ( r , r )`) to get a new matrix of shape ` ( r * r )` ?
Other options include reshaping the first two axes into one axis , and then calling ` sum ` : #CODE
or calling the sum method twice : #CODE
For example , for slightly larger arrays ` M ` , calling the ` sum ` method twice may be fastest : #CODE
Compared with ` sum ( sum ( M ))` ( answered by Cyber ) , is ` np.einsum ` faster ?
I added a timeit benchmark for ` sum ( sum ( M ))` .
in this case , ` p ` would be a size ` ( 2 , )` array containing ` [ gradient , intercept ]`
You can use ` join ` operation on the ` frequency ` object you created , or do it in one stage : #CODE
np.where can be inside pd.rolling_apply for a window of 100 indices backwards . yes 100 indices backward will be the assumption of max lookup before it finds the sweet spot .
If you are looking for full stack ( which will generate plots for you and you can host on web server ) look at bokeh.pydata.org .
It is not just a virtual env , it is full stack python , with C Base libaries already included . , i will update my answer with more details .
Once all the data has been parsed and processed I load the files and concatenate them and save the output as one large hdf5 file .
If you do not include an intercept ( constant explanatory variable ) in your model , statsmodels computes R-squared based on un-centred total sum of squares , ie .
Therefore , unexplained variation in the reduced model is the un-centred total sum of squares .
You are only passing a single column to ` permutation ` , so that is all that gets shuffled .
Actually the sorting seems to be fast , the problem lies with my final loop where I increase the values by indexing via the sorted indices .
You could probably speed things up quite a lot by directly modifying ` a.data ` rather than iterating over row / column indices and modifying individual elements : #CODE
As @USER rightly points out , since you're only interested in the indices of the ` k ` largest elements and you don't care about their order , you can use ` argpartition ` rather than ` argsort ` .
But you can strip it down to just the relevant parts , even if they're all throughout the code , and just enough scaffolding to connect them up .
Does it seem odd that 71 sec are spent in norm @USER .py and 170 sec are spent elsewhere ?
@USER NumPy's ` norm ` has been changed to do exactly that in recent versions ( 1.8.0 , I think ) .
` for i in column : ` will give you the elements of the list , not the indices .
I am looking for a way to perform double integration on sampled data using numpy trapz or a similar function from the scipy stack .
The output array seems to be oriented towards the diagonal of the resulting grid , while it should rather return an array that looks like blurred input array .
Now I open it with append option and write the 2D array values : #CODE
( Ignore the binned histogram part for this exercise . )
the default is , that the colorbar goes linear from min ( z ) to max ( z ) .
Just , the first condition can be replaced by ( updated ` min ` to ` minimum ` . see @USER ' s comment ) #CODE
If you find yourself doing this repeatedly with the same condition , you might use a trick like storing the indices for which the condition is true , e.g. #CODE
Let me translate how it sounds .
For the size of data you're looking at , any speedup from being more complicated in pandas will be less than a blink of an eye .
Finding the consecutive zeros in a numpy array
What I would like to find the start and the end index of the array where the values are zeros consecutively , for the array above the output looks as follows #CODE
well , i can just go through the array with a loop and mark the start and end indices , I thought people who know python would understand it easily
I've changed the requirements a bit , so the return value is a bit more " numpythonic " : it creates an array with shape ( m , 2 ) , where m is the number of " runs " of zeros .
The first column is the index of the first 0 in each run , and the second is the index of the first nonzero element after the run .
With this format , it is simple to get the number of zeros in each run : #CODE
You may also be interested in ` np.gradient ` , although this function takes the gradient over all dimensions of the input array rather than a single one .
I have unique indexes ` 0 , 1 , 2 ,...
Think about what the fft would look like .
The irr function in the numpy library calculates the periodically compounded rate of return that gives a net present value of 0 for an array of cash flows .
So you can't change the shape to something that results in a nonzero number of elements .
Think about it : If you have a stack of 0 sheets of paper , there's no way to turn that into a sheet of paper by deleting the z axis ; if you have a stack of 2 sheets of paper , there's _also_ no way to get a sheet of paper by deleting the z axis there are two sheets of paper ; which one is " the " sheet ?
It seems to me that if you have code that wants an array , it's going to want to broadcast ` n_colours ` and ` crop ` and ` downsample ` and so on over that array .
Also , it seems like it would be _better_ for you if ` ImageWrapper ` could hold an array of > 3D , in which case calling methods on it would effectively broadcast the call over each 3D subarray , rather than making you iterate over each 3D subarray manually .
I'm trying to find the dot product between a huge matrix and itself .
That worked , but still I need the huge dot product !
Wouldn't A*B be the same as the sum over i of A*bi , where bi is the ith col of B ?
have you tried using the ` dot ` attribute of the sparse matrix instead of ` np.dot ` ?
The first thing to realize is that in this specific case ( ` a * a.T `) you are essentially calculating the dot product of each row ( a 36154-element vector ) with each row .
( This leaves you with approximately 50 000 000 000 vector dot products . )
The dot product is easy to calculate by finding the intersection of the sets of non-zero elements in ` a [ c ]` and ` a [ r ]` .
Call the dot product as a method of the sparse matrix : #CODE
For sparse arrays ` * ` is the ` dot ` product .
The real issue is the ` scipy ` version , not how the ` dot ` is called .
Unfortunately , it looks like your method doesn't completely work in python 3 . for some reason , it doesn't remove the BOM from the beginning of the file , and thus crashes trying to encode the BOM into ASCII .
I used another method allowing me to clip that off , but your method seems cleaner than the way I hacked it , if I could at least figure out how to remove the BOM before sending to genfromtxt ...
Is there a way to reshape the arrays so that I have 40 arrays of 3x50 arrays ?
If it is a list , or array with ` dtype =o bject ` , then you have to iterate over items , and reshape each one .
Since the items in an ' object ' array could be anything - strings , arrays , lists , dict - there can't be a ` reshape ` that applies to all of them ' at once ' .
Another way to look at it : ` reshape ` just changes an attribute of the array .
I was curious in knowing if a version of ` reshape ` function exists that would do this .
I often find that works better than histogram intersection .
Where does log ( b , 2 ) come from ?
How do I sum only certain elements from an array ?
I want to sum only the elements from the array ` out ` that have a value less than 0.49 , but I'm not sure how to implement a filter for that criteria .
I would use masked arrays and then just sum along the axis : #CODE
This will sum only those larger than 0.49 rather than only those less .
SVD decomposes the matrix ` X ` effectively into rotations ` P ` and ` Q ` and the diagonal matrix ` D ` .
Vh , as described , is the transpose of the Q used in the Abdi and Williams paper .
One question is how do you define " correctly " - particularly since you plotted it on a log scale I'm not sure I'd trust the value at xmax given the noise well before reaching it .
In that case , correctly , means to have a smoothed plot in log / log scale with the smoothed curve going in the average value of the lasts points
I'm writing a python function to append data to text file , as shown in the following ,
How do you calculate the mean values for bins with a 2D histogram in python ?
I tried using an approach similar to what is shown here ( binning data in python with scipy / numpy ) , but I was having difficulty getting it to work for a 2D histogram .
If you know the bin edges beforehand , you can do the histogram part in the same just by adding one row .
Use ` weights ` parameter to sum values : #CODE
So all you need is to divide in each bin the sum of values by the number of events : #CODE
- In the y-axis , related to the histogram and the %s ( normalized to 1 )
I've managed to escalate the y-axis in a histogram by plotting it individually with the instruction weights = weights and setting it into the plot , but I can't do it here .
Also why import the whole scipy.stats and then again import just norm from it ?
Right , actually it does what I asked about the axis in the title and works perfectly with a histogram .
I mean that the pdf as the one I plotted in the question , when I try to adapt it to your code and plot it there , on the same figure as the histogram , the result is not a Gaussian / bell curve .
do you want to fit a guassian bell to the histogram ?
Fit the the histogram into the bell and measure it with %s in the y-axis , I was searching for that exactly .
Do you want a histogram of a standard gaussian or of your data ?
- Desirably the histogram of values fitted into it
Hi again Max , do you know how to ' cut ' the plot of the histogram between two limits while mantaining the P.D.F the exact way as it is now ?
Shouldn't the next line where I reshape the array just put it back into a 37 by 100 array ?
I'm getting the error on the second line of the code , where I reshape ` events ` .
I then want to reshape events back into a ( 37,100 ) array .
Since Python executes each statement line-by-line , if ` mp ` and ` func ` are identical up to the call to ` reshape ` , then one should not cause an Exception while the other does not .
Another option is to use the ` transpose ` function for your numpy array : #CODE
The ` transpose ` method will return you the transposed array , not transpose in place .
The best way to do this is to use the ` transpose ` method as follows : #CODE
You could also sum the values , which will give you the number of alarms in each time bin : #CODE
I have a large dataframe and found that it was a lot faster to use ' sum ' with an inequality function : ` df.resample ( ' 15min ' , how= ' sum ') > = 1 `
You can stack 1D arrays as if they were column vectors using the ` np.column_stack ` function .
I produce a histogram where I put the weights in the parameter #CODE
Now we can create the cumulative fraction by taking the cumulative sum of weights : #CODE
I tried to broadcast one of the arrays , and then applying the shapely function : #CODE
In my understanding using Numpy arrays is only fast when they contain basic types like for example ` bool ` or ` double ` , not general Python objects like the ones from the Shapely package .
In order to do this , the gradient of the cross entropy ( cost ) function is being computed and is used to update the weights of the algorithm which are assigned to each input .
Yes , LR maximizes the cross-entropy , but the scikit-learn implementation uses [ coordinate descent ] ( #URL ) , not gradient descent .
It contains the optimization algorithms used which include newton conjugate gradient ( newton-cg ) and bfgs ( broyden fletcher goldfarb shanno algorithm ) all of which require the calculation of the hessian of the loss function ( _logistic_loss ) .
The function also computes the gradient of the likelihood , which is then passed to the minimization function ( see below ) .
You can't really append to a Numpy array , so if you want to increase the size you have to allocate a new array first and then copy the old and new data into it .
The solution is to append to arrays sparingly or use a Python list ( if you don't know the size of the result beforehand ) .
You could just use indices , and copy the non-negative data into new lists : #CODE
Attribute error : exp in python , works for some functions and not others
When you apply ` np.exp ` an array of dtype ` object ` , it goes through looking for an ` exp ` method on each element individually , and that gives you the ` AttributeError ` you're seeing .
Python 2.7.8 gives a slightly better error message that at least mentions the type of the object it failed to find the ` exp ` attribute on : #CODE
I have tried to do it without importing numpy ( so it's just exp ( etc ., etc . ) and with from pylab import * instead of the matplotlib command and it gives me the same error message .
That will be considered a stack of ` N ` ` MxM ` arrays .
This means that if for instance given an input array a.shape == ( N , M , M ) , it is interpreted as a stack of N matrices , each of size M-by-M .
Similar specification applies to return values , for instance the determinant has det : ( ... ) and will in this case return an array of shape det ( a ) .shape == ( N , ) .
I have the following set of indices ; #CODE
If you transform you array of indices like : #CODE
Numpy : Stacking masked Arrays and calculating min / max
I'm working with masked arrays and I want to calculate the max of different arrays / columns .
it automatically gives NaN padding ( outer join ) if you set parameter join to inner , it would give no padding .
Yeah I know it gives NaN padding , but only on the indices the joining is done over .
I use a program which dumps large output 3-D arrays in the .mat format .
How to read a v7.3 mat file via h5py ?
You can just flatten your array to get an array of arrays .
I have to following set of indices #CODE
and store the result to a 9 by 9 zero matrix ( or sparse matrix ) to obtain let say B with correct indices ;
If it is one-dimensional , then you can just pick a point , say ` Sample1 ` to be zero , and calculate the absolute location of every other point relative to that .
Once you understand how the ` == ` operator and ` nonzero ` work , look through the other functions in the same section as ` nonzero ` and you should find two shorter ways to do this .
The use of ` np.where ` is not strictly necessary as arrays can be indexed using the bool array produced from ` I == I0 ` below , but having the actual indices as ints is useful in some circumstances .
Transform a 3 columns ( x , y , result ) Python Pandas DataFrame to a DataFrame of result values with x ( unique ) as row and y ( unique ) as column
I know how to get unique x values : #CODE
and unique y values with : #CODE
It's not even true : ` ndarray.resize ` will resize a NumPy array under the right circumstances .
Is there a way to simplify this , e.g. automatically omit values with negative indices like Price [ -5 ] ?
When taking the log of a specific column within a numpy array , i.e. , ` logSFROIIdC = np.log ( data_dC [: , 9 ])` the compiler returns the error : #CODE
Now , I know why this happens , i.e. , log ( -1 ) = Math Error .
Now ` log ( -1 )` doesn't generate a warning : #CODE
Since Y is binary ( 1 and 0 ) then the sum of Y / count per bin would be the % of occurrence .
You can think of each constraint as a hyperplane , intercepting each axis at ( sum constraint constant ) / ( coefficient for that axis ); if the coefficient is 0 , the hyperplane is parallel to that axis (= = " intercepts at infinity ") .
In order to cull as many constraints as early as possible , you want to start by comparing against ones whose hyperplane is ( a ) as close to the origin as possible and ( b ) parallel to as few axes as possible , as it can only cull other hyperplanes also parallel to that axis .
I suggest you sort the list by ( number of axis-parallel axes ) then ( sum of non-infinite axis intercepts ) .
Now that I think about it more : since the sum of intercepts is just a heuristic , I still don't exactly know when I can break the loop , right ?
The only opportunity for that is when the maximum coefficient on row ` j ` is smaller than the sum of the coefficients on row ` i ` , but this will barely help me in my problem , I fear .
If I have an extended form of the above array with shape ( 528 , 32768 ) and the values in the array are now completely random and I need to group values at the indices of 1s , 2s , 3s ....... together with no sorting ; what would be the best way ?
This is a different view on the array up until the transpose .
On the second reshape numpy has to copy .
As you see , dividing the data into the appropriate compartments works with a single ( the last ) reshape .
The rearrangement is done with ` transpose ` as follows : #CODE
The trick I ended up using ( employing reshape and argsort ) has been for some reason deleted from this thread .
If I stack them all vertically , I get an array of dimension ( 528,327 6 8) .
In a broader problem , I want to plot a function ` f ( x )= sqrt ( x )` .
The actual function could be something like ` f ( x )= sqrt ( x+sqrt ( x ) +x^ ( 1 / 3 )) /( x+sqrt ( 2x ))` .
Different std in pandas vs numpy
Here is a log file for this install failure .
I edited the post to add this link to my log file for the install errors : #URL
I want to use this code to read multiple ` data_in ` files and append the statistics into a new row of `' output_file.txt '` .
I'm using the ` reshape ` function as suggested in a previous post here , so every batch of statistics is stored in a new row .
I get the Error message : ` IndexError : too many indices `
Is this error caused because I'm using the reshape function ?
` IndexError : too many indices ` I'm running Python2.7.8 |Anaconda 2.0.1
To get around this issue you likely need to broadcast your arrays slightly differently .
Basically change the outer dimension to reflect the changing ` rgb ` values .
Here r1 and h1 should be ` r1 = ( min ( z ) + max ( z ) ) / 2.0 ` and ` h1 = max ( x ) - min ( x )` , i.e the max and min values so that I get one volume for each slice
You can use ` volume = sum ( volumes )` to get the final volume of the shape .
I only need ` r1 ` and ` h1 ` to be the ` max ` and ` min ` values so that I get one volume for each slice and not all 50 values for each slice .
As I have mentioned above , I want to append the black points of the binary image to ` self.bPoints ` and the white ones to ` self.wPoints ` array whose shape is ` ( 2 , 2 )`
Basically , this uses the ` np.where ` function to determine all the indices which are ` 0 ` or ` 255 ` .
I've looked at numerous sources online , and they all indicate that to translate Matlab's ' mldivide ' , you have to use ' np.linalg.solve ' if the matrix is square and nonsingular , and ' np.linalg.lstsq ' otherwise .
As a result there is no unique solution , and the result of both programs are correct .
Does anybody know why I am getting these non-sense zeros ?
I have to get rid of these two using ` a = a [ 0 : -2 ]` so that when I reshape them into rows and columns using ` a_reshape = a.reshape ( nrows , ncols )` I don't get an error .
a set with all non-zero column indices on that row
a dictionary with the non-zero indices as keys and the corresponding non-zero values as values
calculate the sum of multiplications of the non-zero elements ( i.e. the intersection above )
The complexity of set intersection should be O ( min ( m , n )) where m and n are the numbers of non-zeros in each operand .
For example , in the above code , ` featureSelector ` might be an instance of ` sklearn.feature_selection.SelectKBest ` or ` sklearn.feature_selection.SelectPercentile ` , since these classes implement the ` get_support ` method which returns a boolean mask or integer indices of the selected features .
When one performs feature selection via linear models penalized with the L1 norm , it's unclear how to accomplish this .
` sklearn.svm.LinearSVC ` has no ` get_support ` method and the documentation doesn't make clear how to retrieve the feature indices after using its ` transform ` method to eliminate features from a collection of samples .
The general way of extracting the correct indices should then be ` np.where ( svc.coef_ ! = 0 ) [ -1 ]` , i.e. the last index , which will work for both binary and multiclass problems .
I think this is because ` unique ` returns a numpy array , not a Pandas structure , so it tries to use a numpy datatype .
It seems that when working with aggregate functions ( min , max ) over masked arrays the potential for the result to be a MaskedConstant can cause real headaches .
I am trying to safely accumulate min over many distinct ma's .
In your code , you can test the return value of the ` min ` method with ` testC is masked ` .
AttributeError : ' numpy.ndarray ' object has no attribute ' append ' : Image processing example
In numpy , ` append ` is a function , not a method .
Note that the ` append ` function creates and returns a copy !
The problem is that ` append ` is not a * method * of the ndarray class .
[8 0 :] [ 105 :] it returns me an array with the corresponding values of allpix but when I set [8 0:1 : 200 ] [ 105:1 : 200 ] it returns me the original one roipix ( i.e with only zeros ) without any change .
There are * two * sets of slice indices in the square brackets , since you're indexing into two dimensions .
Since the default start is 0 and the default step size is 1 , you can just specify the stop indices for each dimension , i.e. ` allpix [: 80 , : 105 ]` .
Regarding ph1 , it is the result of summing each of the three bars ( r , g , b ) per bin , hence it represents a summed version of the original histogram .
The reason for the error that you re encountering is that OpenCV's ` cv2.compareHist ` function expects an Nx1 column array of bin counts , while the Numpy ` histogram ` function returns a tuple of the form ` ( bin_counts , bin_edges )` .
( See here to get some info about the OpenCV histogram functionality as exposed via Python bindings . )
If you re comparing colour images via their histograms the multidimensional ` histogramdd ` function is actually more appropriate , as ` histogram ` actually flattens the input array before binning .
In both cases the following function will map the Numpy histogram output to a corresponding OpenCV-compatible form : #CODE
You'll have to create a new array each time to insert an item to an existing key , better use list which supports append operation .
Second , tuples have a special status when used as indices into numpy arrays .
When you put a tuple inside the square indexing brackets , numpy interprets it as a sequence of indices into the corresponding dimensions of the array .
In the special case where you have nested tuples , each inner tuple is treated as a sequence of indices into that dimension ( as if it were a ` list `) .
Would you change operations on dimensions like ` reshape ` and ` swapaxes ` ?
This means I would look at a figure like this and then eye ball where the dashed lines should be .
( A call to ` ravel ` , ` squeeze ` , or ` reshape ` could have been used instead . ) #CODE
It has 1 pass ( well 2 passes if you count the max ) over ` digitized ` so it has complexity O ( n ) .
@USER that's really an implementation detail , is there any reason that digitize could not be ( and I would argue should be ) implemented as a binary search ?
Simpler way to create a matrix / list of indices ?
I wonder what could be the easiest way to create a bi-dimensional array , that has for each row the indices to another multi-dimensional array .
For example , let's say I have a cube 4x4 , the " indices matrix " would be the following : #CODE
OK , the easiest way to do that isn't to get all the indices from ` arr ` and then filter it based the ones whose corresponding value in ` arr ` passes a test condition ; instead , just apply the test to ` arr ` to get a boolean array , and then get the indices of the true values in that boolean array .
If I understand your question , there is : ` indices ` : #CODE
but you can flatten it and transpose it : #CODE
So , we just transform the array into a boolean array of " element is positive " for each element , which is just ` arr 0 ` , and then use ` nonzero ` to get the true indices of that boolean array : #CODE
` sqrt ( abs ( abs ( x ) - 3.0 ) /( abs ( x ) - 3.0 ))` - why would you do this ?
The values of the array inside the sqrt are always 1 , -1 , or NaN .
I am trying to use ` numpy.where ` to find the indices I want .
Add another dimension to each so they can be broadcast against each other : #CODE
If you want the indexes to be the same as the underlying array , just reshape to the shape of ` a ` : #CODE
Comparing equality between sum and constant
I have calculated a histogram slice using numpy histogram by ` N , a = np.histogram ( z , bins=50 )` .
If this were a graphical histogram , you'd be doing your math with the little ticks on the bottom of your graph instead of looking at the height of the bars .
The same does not work for a histogram slice !!
I must add I compute sum of morphology opening with bigger and bigger radius of structure element : out = Opening ( in , R=1 ) + Opening ( in , R=2 ) + ...
Typically when using ( at least when I use ) opening or closing algorithms the input consists of structure of zeros and ones or TRUE , FALSE .
After some researche I turned to Opening implementation using convolution -> multiplication of Fourier transforms - O ( n log n ) , and no so big memory overhead .
Should I transpose the array ?
I would need to enter a unique value for each element .
I am not sure if I understand the question correctly and am wondering to what extent it is well-defined , because I am not sure these groups are unique .
My intuition is that it's unique , but maybe my intuition is not generalizing to higher dimensions .
To make this more clear , notice what happens when I inflate the values of of the x mat #CODE
How to stack multiple Astropy Tables
vertically stack the variables after the for loop gone trough the loops .
However , the dot product will be way too large : #CODE
What you can do is split up the problem and perfrom the ` dot ` operation in pieces ;
The dot product between these arrays means that you calculate the dot product of each possible pair of vectors .
On my computer dot multiplying a ( 300,100 ) array with a ( 100,100 0 ) array takes approximately 1 ms .
The nice thing about taking a dot product is that you can do it piecewise .
Also , your life is much easier if you only sort items within a row ( transpose your matrix if you need ) , as then a simple ` sort ` sorts the row very efficiently .
That was an eye opener .
Then you need to build a complete stack of applications with some tool sets and scripting languages .
However , this doesn't produce what I expect as my ellipse is always centered at ( 0 , 0 ) thus I expect the ones to be in the center of my array but they appear in the top left corner .
In your question you used pixel indices which have 0 , 0 in the corner so you get an ellipse in the corner .
Assuming the two dataframes have the same columns , you could just concatenate them and compute your summary stats on the concatenated frames : #CODE
Since your function is scalar , the Jacobian reduces to the gradient .
The gradient ( Jacobian ) is then #CODE
And you can use ` updates ` in ` theano.function ` in the standard way ( see theano tutorials ) to make your gradient descent , setting ` x1 , x2 ` as shared variables in givens , by hand on the python level , or using ` scan ` as indicated by others .
Do you just want to append zeros at the top and bottom of the array ?
Or resize it ( like you resize a picture ) ?
Possibly the outer for loop might be vectorized as well , but I guess this is much harder .
I need to sum the values of those dictionarys , if both keys are the same , otherwise i want to append the keys and the values .
Then uses numpy.intersect() to find the common keys in both dictionaries and adds the values of B to the values of A at that indices .
Afterwards i take the invert of the intersection and append both the uncommon keys and the value to dictionary A .
I realized when i made up this simple example , i made it too simple : actually i am not doing this for only the one key ' value ' , but i also have two more keys ( lets call them ' y ' and ' z ') , where i want to sum up the values in the same manner as i do for the key ' value ' .
Determine sum of numpy array while excluding certain values
I would like to determine the sum of a two dimensional ` numpy ` array .
How can I sum over the elements in my two dimensional array while excluding all of the 2s ?
I'm trying to write a Python function that takes an image as input and performs bilinear image interpolation to resize an image .
No : ceil ( x_coord )= floor ( x_coord ) , so xc-x_coord =0 AND x_coord-xf =0
I have switched to ` xrange ` for the outer range , to avoid building the list , and ` np.arange ` for the inner range , to create arrays .
If you look at the numpy io source code , you'll find that ` savez ( file , * args , ** kwds )` is just a wrapper that calls the function ` _savez ( file , args , kwds , compress )` .
If you want to get a numpy array back , just use the concatenate function : #CODE
You can then flatten that index array to only the True values using an array's ` .nonzero() ` method : #CODE
For one element only , you could then flatten that using ` .r_ ` : #CODE
You can also use .nonzero() method or the nonzero function : #CODE
Instead of trying to concatenate the vectors and then putting the resulting huge vector through numpy.percentile , is there a more efficient way ?
Filter pandas Dataframe based on max values in a column
In this example , my resulting array would have two elements , one with the sum of the counts from the channel 0 to 39 , and the other elements with the counts in the 40th channel .
How can I return a 2D numpy array of indices of values from a 1D array using a where clause ?
Then I would suggestion using ` np.unique ` since it is sort of nicer but it would skip indices not present in ` a ` .
Because indices start at 0 not 1 .
( notice the index 7 which points to an extra element in ` a ` associated to ` max ( a )`)
I would take ` numpy.mod ( datenum , 10000 )` for each value , but the numpy ufunc mod takes two array-like arguments .
But how would I specify ` 10000 ` for ` mod ` , then ?
Due to the broadcasting rules , 10000 is arraylike , and you can just pass it to ` mod ` or ` % ` as if it were an array : #CODE
Maybe you could be even clearer about no use of apply could broadcast ( if that's the right word ) the second argument if the ufunc expects an array .
@USER szl : Well , actually , it could broadcast .
Taking norm of HUGE matrix in less than a second : NUMPY , PYTHON
Write a function to read a certain row from that file , square and sum all the numbers and return the result .
When ` map() ` returns , sum the resulting list and take the square root of that sum .
np.linalg.norm as of 1.9 does not use BLAS / ATLAS , its just sqrt ( sum ( x.conj() *x ))
the simple norm is a memory bound problem which are not very efficient with numpy .
np.linalg.norm as of 1.9 does not use BLAS / ATLAS , its just sqrt ( sum ( x.conj() *x ))
There are some vectors in my a which have norm as 0 .
Since all angles will have values between 1 and -1 and I need only 10 max values this will help me .
No I want to preserve the indices because I want indices of 10 largest angles in the end which are further matched in other list .
So preserving indices is must for me .
nans sort higher than anything else so when you are sorting you can just strip the top np.count_nonzero ( np.isnan ( d.ravel() ) values
Wow ... that was an eye opener ... !!!
How can I use the above norm with non numpy array input ?
the first argument to ` norm ` should be an array_like object .
Simultaneous matrix transpose for large array of matrices
However , how can I simultaneously transpose all the 3x3 matrices ?
I think using newaxis ( or None does the same thing in slice notation ) is fine , but you can look up " outer " , e.g. , ` numpy.multiply.outer() ` .
possible duplicate of [ Numpy : outer product of n vectors ] ( #URL )
It looks like you just need to flatten this down a level .
Or reshape ` y ` #CODE
Plain ` nditer ` iterates over each element of ` y ` ( ` nditer ` does not give you the indices ): #CODE
It accomplishes a ' shallow iteration ' by creating a ` zeros ` array over selected dimensions .
First , here is the stack trace : #CODE
Is it better to work with arrays than matrices in general , since the numpy dot product seems equivalent to matrix multiplication here .
However , when I use scipy's dot functionality , no such performance boosts can be seen and still one one core is used .
Does anyone know how I can make BLAS also work with scipy's dot functionality ?
well it's csr's own dot function
Do your matrices have some known sparsity structure , e.g. diagonal , banded , block ?
This works because in your example , both ` param1 ` and ` param2 ` are vectors ( 1D arrays ) , so you can use ` outer ` and the result will be a 2D array , not 3D .
universal function ( " ufunc ") , you can use its ` outer ` method ; #CODE
indeed . outer does multiplication , whereas addition is called for
@USER All universal functions support outer .
And is there any difference in terms of memory usage of speed by using the numpy outer function rather than broadcasting and np.newaxis ?
It could be that usinf ufunc , outer directly is faster because it looks like it uses less operations .
Why do you only switch to float32 right before you take the norm ?
Finally , the ` T ` is a transpose , since it looks like you're switching the indices around when you increment the cells of ` movearray ` .
I have also made sure that numpy and scipy have their BLAS libraries linked -- whereas this does not have an effect on the csr_matrix dot functionality as it is implemented in C .
Even though ` X ` is sparse , ` X * X.T ` probably won't , notice , that it just needs one nonzero common element in a given pair of rows .
It can be done in numerous ways , the easiest way is to set some threshold ` T ` under which you treat ` a , b ` as ` 0 ` and compute the dot product by yourself , and create an entry in the resulting sparse matrix iff ` a , b T `
You can either approximate it ( as stated in the answer ) , buy more RAM or compute the required dot products " on the fly " instead of a matrix .
This link has some good starter code , and you could iteratively calculate chunks of your dot product and write to disk .
I now simply tried to use ` SparseRandomProjection ` with the default ` eta= .1 ` and then multiplied the matrix with its transpose on some sample rows .
Ok so I need to find a way to compute the dot of two sparse matrices with PyTables .
I would begin by creating an HDF5Store of zeros on disk , then slice some part of each of your X , X.T matrices , do the dot , then slice into the correct part of the HDF5Store to assign the result .
getting indices when comparing multidimensional arrays
You can do lookups in O ( log n ) time and insertions and deletions in constant time , and you can wrap up the code for yourself so that the usage looks pretty much like a ` defaultdict ` .
fastest way to get lookup table indices with numpy
I built ( with help ) some code that takes pixel values from a m x n x 3 ` numpy.ndarray ` ( an RGB image ) , compares the pixel values with those of a ( positional ) lookup table , and outputs the pixel values indices of the lookup table , as follows : #CODE
The ` lut ` array in the above code is a dummy for a linear color scale representing velocity that I need to digitize , as found below :
Things I use to digitize the scale are :
A LUT is normally a function of the image values , not the indices .
That expression itself is part of a larger math equation so I can't just take the log of it .
I will then need to make a histogram of each of those two variables based off the markov chain .
I know it's rounding down to 0 because I get the error : RuntimeWarning : divide by zero encountered in log .
Can you just work with the log of the probability ?
Can't you use a log transform and avoid the ` exp ` calls ?
` min ( A ) sum P ( yi=1|A )
Do I need to optimize a vector and then reshape the vector into the matrix within the optimization function ?
For the constraint , I can make a function that returns ` A.T *A - eye ( d )` , but I need to check that this is all zeros .
Should I also reshape it as a vector , and will the constraint section of ` optimize ` know that every part of that vector needs to be 0 ?
Acces all off diagonal elements of boolean numpy matrix
Suppose there is a diagonal matrix M : #CODE
Does anybody know a simple way to access all off diagonal elements , meaning all elements that are ` False ` ?
@USER That will , however , work when ` M ` is a diagonal matrix , as specified in the question .
Sorry I didn't read the stack trace so I was looking at your loops to see if anything looked questionable , I would focus on the ` generate ` and ` seed_test ` code , print the lengths and content of the params to see if they look valid
How to see that a numpy array of zeros is not empty ?
What's the reason behind the fact that zeros are not considered as actual values by numpy ?
What's the reason behind the fact that zeros are not considered as actual values by numpy ?
So , zeros are considered as actual values , but they're actual false values .
*** In C-family languages , it's generally zeros and NULL pointers that are falsy .
It seemed counter-intuitive to me that a float value would be returned as False ( especially in the case of an array , in which getting zeros as a results is very possible ) , but if that's the way it is , fair enough !
Takes a sequence of arrays and stack them along the third axis
This is a simple way to stack 2D arrays ( images ) into a single
First of all , ` a ` and ` b ` don't have a third axis so how would I stack them along ' the third axis ' to begin with ?
@USER But why do I end up with three sheets of paper when trying to stack two on top of each other ?
If we stack 3D arrays rather than 2 D: #CODE
You are initializing ` h ` to be an array of zeros the same length as ` x ` , but then you are assigning a scalar value to ` h ` in the ` for ` loop .
It should compute the frobenius norm of a 3D array .
Do you also now if there is a possibility to use nansum instead of sum ?
You then proceed to transpose them and that may cause the problem ( although it shouldn't - ` .broadcastable ` gets tranposed accordingly , but maybe it is still getting muddle ) .
Alternatively , take a look at this answer to a similar question : How to format in numpy savetxt such that zeros are saved only as 0
/ sqrt ( 2*pi ) / self.sigma *exp ( - 0.5 * (( log ( value ) - self.mu ) / self.sigma ) **2 ))` should be : ` -log ( 1 .
/ sqrt ( 2*pi ) / self.sigma / value*exp ( - 0.5 * (( log ( value ) - self.mu ) / self.sigma ) **2 ))`
Plotting ROC curve - too many indices error
I don't understand , what has too many indices ?
Both the ` y_pred ` shown in your other question and ` y_test ` are 1-d , so the expressions ` y_pred [: , i ]` and ` y_test [: , i ]` have too many indices .
I would stack all the arrays together into a 3D matrix of 3000x3000x9 , sum over the last axis , then use cumsum to find the > .5 breakpoint .
If you are afraid of using a lot of memory ( ` cumsum ` uses roughly 640 MB ) , you can do the sum one image at a time : #CODE
This needs 4 or 8 bytes per pixel for the cumulative sum ( depending on the type of floats you use ) and 1 byte per pixel for the counter .
` spell ( kind= ' dry ' , how= ' max ' , threshold= 0.7 )` .
And it would return a wrapped version of the ` spell ` function that uses your new arguments as " defaults " instead of the ones created at the function definition .
For a proper multidimensional array ( rather than just a list of 1D arrays as in your example ) , this sort of ' chained ' indexing will still work , but it's generally faster and easier to use a tuple of indices inside the square brackets .
You can either create a list of a predetermined ( but not fixed ) size ahead of time and then index into that , or you can append items to the end of the list : #CODE
I prefer the ` append ` approach , but that's just my preference .
Too bad the ` diff ` function does not have a ` fill_value ` otherwise this could be a one-liner .
I read your question but I do not fully understand what your problem is , so maybe you can clearify that ( What are the problems ? Post a stack trace ) .
POSIX timestamp is not unique around leap seconds .
My actual problem need to encode strings in a data frame , as I do in the following step : #CODE
Pandas : compute mean or std over entire dataframe
It's not a problem for the mean , but it is for std , as the pandas function uses by default ` ddof=1 ` , unlike the numpy one where ` ddof=0 `
You could convert the dataframe to be a series with ` stack ` :
This works because ` stack ` will convert your data from 5x3 to 15x1 , thus treating it as if it were all in the same column and ensuring that the correct degrees of freedom is used for the standard deviation .
Refer to the documentation on ` stack ` here and here
How can I maximize the Poissonian likelihood of a histogram given a fit curve with scipy / numpy ?
A way to do this is to create a histogram of the data and then fit a curve to this histogram .
The method ` scipy.optimize.leastsq ` does this by minimizing the sum of ` ( y - f ( x )) **2 ` , where ( x , y ) would in this case be the histogram's bin centers and bin contents .
In statistical terms , this least-square maximizes the likelihood of obtaining that histogram by sampling each bin count from a gaussian centered around the fit function at that bin's position .
You can easily see this : each term ` ( y-f ( x )) **2 ` is ` -log ( gauss ( y|mean=f ( x )))` , and the sum is the logarithm of the multiplying the gaussian likelihood for all the bins together .
The Poissonian comes very close to the Gaussian distribution for large values of f ( x ) , but if my histogram doesn't have as good statistics , the difference would be relevant and influencing the fit .
You could use something like ` find ` to retrieve the indexes of those entries that satisfy the logical condition , then negate them to find all the remaining ones , but for sparse matrices , isn't there a more compact way ?
Here's a function that duplicates ` np.where ` , when ` cond ` , ` x ` , and ` y ` are matching sized sparse matrices .
You can call figsize() before it's displayed to change the size , but I don't think there's a way to resize them afterwards .
There is no way ( currently ) to resize the plots by simple mouse drags .
You can resize the plot by mouse in ` ipython notebook ` .
As a side note : while you can resize the plot in ` ipython notebook ` , it resizes the image with the existing ( low ) resolution .
That is to say , I want to concatenate all the items in the middle dimension .
I've been playing with the ` concatenate ` function to no avail .
I know that the correspondence for real-valued dft is X_ ( k1 , k2 , k3 ) = X*_ ( N1-k1 , N2-k2 , N3-k3 ) where * is the conjugate transpose .
Try investigating this answer for [ Generate all unique permutations of 2d array ] ( #URL )
` similarity [ i , j ] = sum ( leafs [ i , k ] == leafs [ j , k ] for k in range ( len ( leafs [ i , :])) '
Also note that this solution is inefficient : ` proximity ` is symmetric , and the diagonal is always equal to ` ncols ` , but this solution computes the full array , so it does more than twice as much work as necessary .
The second parameter to the resize method is the interpolation / filter used when resizing .
In other words ` conv2d ` doesn't take the max over the whole pooling region , but rather the element at index ` [ 0 , 0 ]` of the pooling region .
I know that the max pooling operation is frequently used .
To print a numpy array of the x values of vector3d with indices 1 , 2 , 3 , 4 .
From the Rbf documentation , there is the attribute ` norm ` for defining a different distance norm , could I have to use a spherical distance to get this to work ?
I want to count the number of unique elements in neighbour cells of each cell .
As before , you find the number of unique entries in ` uniques ` .
This is very lucky , because then the neighbour on the right hand side gets reflected onto the left hand side , and thus no unique values are introduced .
Exactly , which corresponds to your calculating the non zero jumps in the ` diff ` .
The only reason I see for which ` np.bincount ` is restricted to 1D is to eradicate ambiguity : `" Should I apply along axes or should I do a joint histogram "` , where clearly the latter becomes infeasible quick
Doing a proper benchmark is difficult , but it seems that they are close to each other with small * n* , ` sort ` and ` diff ` taking maybe 30 % less time than ` bincount ` .
use ` diff ` to the 4th axis ( the sorted array )
calculate the sum of non-zero differences along the 4th axis
The difference vector is then ( 1 , 0 , 0 , 1 , 1 ) , and the sum of non-zero elements ( ` ( diff ( v ) ! = 0 ) .sum ( axis=4 )`) gives 3 .
So , there are 4 unique neighbours .
I like the use of ` diff ` on the sorted array !
numpy 1.9 implements a more accurate sum for 1d array which improves the result a bit ( I get 0.22 ) but it does currently not implement a numerically robust algorithm for the variance .
Next , I want to join this dataframe with the the larger dataset , based on the entry being between the two ` Equal_Span ` columns .
With two calls to ` bincount ` you get the sum and the number of items per bucket : divide them and you get the bucket average .
This should give you the size of all the arrays accesible by Python : ` 1e-6 * sum ( x.nbytes for x in globals() .values() if isinstance ( x , np.ndarray ))` .
I would like a cell's value in the new array to be the sum of the values in this block in the old array .
` np.einsum `' s advantage usually comes from vectorization ( SIMD ) of the sum and product operations .
It's particularly bad with ` numpy ` , as the namespace includes things like ` all ` , ` min ` and ` random ` that shadow built-ins and standard library modules , leading to subtle bugs .
` plot ( x , sin ( x ) , ' r- ' , x , cos ( x ) , ' b- ')`
However , suddenly I received this error stack trace : #CODE
Maybe the scipy dot function does some multithreading ?
But I think the scipy implementation does not do that compare to numpy's dot function .
I could store the result of the dot beforehand and then assign the result , but this is a memory overhead .
I know the indices of the tuples I need to substitute - stored in a list called " indices " , of length " li " .
Hmm , sorry , maybe I wasn't clear - colours [ li , :] only substitutes the tuple with index " li " , but I need to substitute all tuples with indices in this list called " indices " ( and li=length ( indices ))
However , it will work with the list ` indices ` .
so a kind of diagonal shift : #CODE
It seems that the time is spent in rebuilding the indices .
Especially with hierarchical indexing it is possible that the complex indices are rebuilt after each shift .
" ( i ) The nonzero rows come first - otherwise there would have been row exchanges - and pivots are the first nonzero entries in those rows .
( ii ) Below each pivot is a column of zeros , obtained by elimination .
Rectangular matrices may well have zeros in diagonal positions after elimination .
Unlike square matrices , zeros in the diagonal do not imply that the matrix is rank deficient or has no inverse .
If you're doing a lot of these , it makes sense to encode the renumbering in a dictionary for fast lookup .
Is there a numpy function to convert from log ( e ) to log ( 10 ) ?
Is there a single numpy function convert log ( e ) numbers to log ( 10 ) numbers ?
Just divide by ` log ( 10 )` .
I chose Python , because I expect to finish this task in few working days max .
TypeError : list indices must be integers , not list
You probably want to return the sum of squared errors rather than the vector of squared errors : #CODE
It's worth pointing , though , out that ` fsolve ` uses MINPACK [ ` hybrd.f `] ( #URL ) which minimizes the euclidean norm of the vector , so the actual ' objective function ' used internally would be equivalent to the square root of mine .
Different shapes are separated by zeros - so that if you were to plot every point in the array , you would see a map of the various shapes .
I need to count and identify these shapes but I only want to include the ones with an area above a certain threshold .
If you want diagonally adjacent elements to be considered as connected , you can pass a 3x3 array of ones : #CODE
I want to get the sum ( Price * ( 1 / Signal ) ) group by ' Sector ' .
I can find mean or sum of Price #CODE
but not get sum ( Price * ( 1 / Signal ) ) , which is what I need .
But if you subtract its y-value at a point from it and compute the root of the resulting function f = gamma ( 1+3 / x ) / gamma ( 1+1 / x ) ** 3-y0 , then you get the inverse x0 of ' expr ' at the point y0 .
TypeError : list indices must be integers , not tuple .
Hence use data type as ` a10 ` , Where 10 being the max length of your string .
Note : set operation returns unique values
Return the sorted , unique values that are in only one ( not both ) of the input arrays .
Slicing n-dimensional numpy array using list of indices
and I have two lists of indices corresponding to the 2nd and 3rd dimensions : #CODE
On closer inspection , I think I misunderstood the question : ` second ` and ` third ` are not supposed to be ranges , but you want exactly those indices -- a bit misleading , using consecutive indices , though .
You might then think you could do ` A [: , second , third ]` , but the numpy indices are broadcast , and broadcasting ` second ` and ` third ` ( two one-dimensional sequences ) ends up being the numpy equivalent of ` zip ` , so the result has shape ` ( 5 , 2 )` .
What you really want is to index with , in effect , the outer product of ` second ` and ` third ` .
The first method would also work with indices that are not equivalent to slices .
Diagonalize the matrix with ` numpy `' s ` eig ` or ` eigh ` ( in our case the latter works fine since the matrix is Hermitian ) .
As a result we get two matrices : a diagonal matrix ` D ` , and the matrix ` U ` , with ` D ` consisting of eigenvalues of the original matrix on the diagonal , and ` U ` consists of the corresponding eigenvectors as columns ; so that the original matrix is given by ` U.T.dot ( D ) .dot ( U )` .
Exponentiate ` D ` ( this is now easy since ` D ` is diagonal ) .
We can actually diagonalize the matrix first with ` numpy `' s ` eig ` or ` eigh ` , and then take the exponential of the diagonal matrix .
@USER , pv .
Thanks , pv . and hpaulj , for running my code .
If your system is ill-conditioned , the extra round-off error in the gradient might cause failure to converge .
not sure how to check , but probably the newest ones since I installed them through ubuntu's software center .
I think what you're looking for here isn't ` where ` , which will return you an array of elements from one of two different arrays depending on the condition , but ` argmax ` , which returns you the index of the maximum value or , for a 2D array , the indices of the maximum value of each row or column .
I installed Python 2.7.8 as my default python , then installed the SciPy stack according to the site instructions .
Does it make a difference if in the pyx , i define ' cdef double [: ] zeros = np.zeros ( 360 )' . then i make aggloan = zeros ?
You could use ` reshape ` while specifying Fortran order ( first index changes the fastest ): #CODE
My first reading was that OP wants arbitrary striding rather than a plain reshape ; but this may infact be what he is looking for .
TypeError : list indices must be integers , not tuple
` np.where() ` always returns a ` tuple ` of indices .
The flow chart is like : individual feature extraction , pool , PoolAggregator , concatenate to form the whole feature list from poolAggregator using ` np.concatenate `
Also , post the complete stack trace your stripped-down code produces .
The stack trace refers to file ` featureExtract.py ` , but it appears you are editing file ` featureExtractor.py ` .
That stack trace makes no sense .
Then find the indices of the rows that fall within the first bin , the second bin ... etc .
Find the indices of all rows that have a zero as a value , and the indices of all rows that have a one as a value .
2- Suppose that I knew that column 0 has only real-values , then I know how to get the indices of the rows like this : ` np.nonzero ( X [: , 0 ] 0.1 )` , however this is not what I need because it's missing the second condition i.e something like this : ` np.nonzero ( X [: , 0 ] 0 and X [: , 0 ] 0.2 )` of course this doesn't work , so I don't know to incorporate the second condition here .
If I understand , you're saying that ` X ` is a floating point array , but that some of the columns contain only 0.0 or 1.0 , and you want to find the indices of these columns .
You could use the set of boolean indices directly , e.g. ` x [: , bool_cols ]` , or you could return the indices of the boolean columns using ` np.where ` .
However , these indices would also include any boolean columns .
Either make sure the file is in that directory , or provide an absolute path in your code .
column max with list of matrices
If the 2d arrays are all the same size , you can convert the list into a 3d array , and choose your ` max ` over one or more of the dimensions .
You can first stack the arrays vertically and then take the maximum of each column : #CODE
If you click ` log in ` , it should prompt you to create a new account that's linked to your SE network profile
+1 for use of ` flatten() ` as it is the most similar to MATLAB's linear indexing EG : ` a (: )` , but I think that @USER use of ` order= ' F '` is better than transpose because it explicitly changes from NumPy's _row-order_ to MATLAB's ( and Fortran's ) _column-order_ , which is a fundamental difference in MATLAB and NumPy which new users should understand
BTW : transpose the array using ` a.T.flatten() ` is an alternate to changing the order using ` order= ' F '`
Consequently , if you want to mimic MATLAB's column-order arrays you will end up using ` reshape() ` a lot , but you could get exactly the same results with simpler ( IMHO more Pythonic ) code if you transpose your paradigm to row-order .
You can use the " Fortran " ` order ` option to e.g. ` reshape ` : #CODE
Using flatten unfortunately puts the NumPy array into the transpose of what it should be , where using the Fortran ordering seems to preserve this .
Initially I thought I would make an array containing all the indices I want to loop through , using #CODE
You could reshape ` a ` to compress the 1st ` d ` dimensions into one : #CODE
Normalize a multiple data histogram
I have several arrays that I'm plotting a histogram of , like so : #CODE
I'm looking to normalize them to the total number of elements , so that the histogram of ` y ` will be visibly taller than that of ` x ` .
Yes , you can compute the histogram with numpy and renormalise it .
Now , to plot the histogram : #CODE
sum it all together
Whether or not it gives you results which are good enough for the outer iteration is an interesting question .
I think making some sort of exp ( log ) transformation on epsilon and beta would help the bounding issue , but I haven't figured out how to yet .
You might try to replace the Gaussians by a home-brewn version ( it is only one exp , one square , one subtraction , one division , cannot take much time ) .
Lists are fast if you're doing lots of ` append ` s , but slow for most other operations , and they don't support vectorization .
Is there an efficient way to remove Nones from numpy arrays and resize the array to its new size ?
Or do you want to remove the None's , then resize the array ?
I want to remove the Nones and resize the array .
I have an array ` traced_descIDs ` containing object IDs and I want to identify which items are not unique in this array .
Then , for each unique duplicate ( careful ) ID , I need to identify which indices of ` traced_descIDs ` are associated with it .
And to get the items and indices : #CODE
Getting the indices is a little more involved , but pretty straightforward : #CODE
I used polyfit to find a fitline of a data set , but now I need to find the natural log of that fitline function and plot it .
If you have a ` list ` of z-values , you can use ` map ` to perform some function to each value , in this case ` log ` ( which is ` ln `) .
The indices of these rows / columns are stored in a list ` z ` .
So in this example , suppose that my indices are : #CODE
Define boudned ufunc for log
I am trying to define a function ` log_bounded ` which is the same as numpy's ` log ` for positive input , but which gives a big negative number for nonpositive input .
More advanced alternative : ( handle scalars as well ; save unnecessary ` log ` computations ) #CODE
You might as well spare yourself the expensive ' log ' operation for the ' verylow ' entries : ` out = np.empty_like ( x , dtype=float ); mask = x <= 0 ; out [ mask ] = verylow ; mask = ~mask ; out [ mask ] = np.log ( x [ mask ]); return out `
Then use ` pd.cut ` to bin the data in the same way as you were doing in your histogram : #CODE
I'm not sure I follow you -- I don't have explicit indices i and j if I'm vectorizing .
If you're feeling really fancy , you could create ` yx ` as a 3D array , and broadcast all of the operations over it : #CODE
Extract elements of a 2d array with indices from another 2d array
The ind array is the same length as data , and contains indices like [ 4 , 3 , 0 , 6 ] .
How can I create another array with shape == ( n , 4 ) containing the elements from data specified by the indices from ind ?
Unfortunately , it is expressed in 2-D indices .
The first line above converts these into ` indices ` of the 1-D raveled ` data ` .
The 2-D indices represented by ` ind ` is converted to ind ` indices ` has the indices
What is the difference between relative tolerance ( rtol ) and absolute tolerance ( atol ) in allclose .
absolute ( a - b ) = ( atol + rtol * absolute ( b ))
Then take a view of ` arr ` and reshape to make it two-dimensional : #CODE
First , I open a bunch of pictures and I append them togheter ( converting to np array )
You can use ` reshape ` : #CODE
it will have a shape of ( 1 , 3 ) , which you can transpose to get the result you want :
The reason you can't transpose ` y ` is because it's initialized as a 1-D array .
I think these functions aren't as well known as they should be , and I find them much easier , more flexible , and safer than manually fiddling with reshape or array dimensions .
To answer your question , you should use ` np.atleast_2d ` to convert your array to a 2-D array , then transpose it .
The other way to quickly do it without worrying about ` y ` is to transpose ` x ` then transpose the result back .
Just make sure keys are unique ( and same dtype ! ) or else create one that is .
According the ` csr ` documentation , the component arrays are ` data , indices , indptr ` .
The ` coo ` arrays , ` data , row , col ` are easier to understand , but the ` csr ` ones are preferred for most math operations .
The CSR matrix is represented using three numpy arrays ( data , indices and indptr ) that are accessible .
The log says
match the broadcast shape ( 1 , 3 )
That is unwanted , because then your ` dot ` does not work .
There are several ways to fix the problem , but possibly the easiest to read is to reshape ` x ` for the calculation to be a 3-element vector ( 1D array ): #CODE
Possibly a cleaner solution would be to define ` w ` as a ( 1 , 3 ) 2D array , as well , and then transpose ` w ` for the ` dot ` .
What's wrong with the normal div / mod operations ?
As KeillRandor suggests you could use parameters to find only the outer contour , but in that scenario you might miss other shapes within that contour ( if for example a circle within a square is allowed ) .
The elements of ` x ` are indices of the rows of ` A ` .
numpy number array to strings with trailing zeros removed
question : is my method of converting a numpy array of numbers to a numpy array of strings with specific number of decimal places AND trailing zeros removed the ' best ' way ?
Both of the functions ' rstrip ' and ' mod ' are numpy functions which means this is fast but is there a way to accomplish this with ONE built in numpy function ?
( ie . does ' mod ' have an option that I couldn't find ? ) It would save the overhead of returning copies twice which for very large arrays is slow-ish .
@USER because that would not remove trailing zeros .
The reason I want to remove the zeros is it will make my files smaller .
Speed-wise , the additional operation to remove trailing zeros is not a bid deal so I consider it worth it for the gain of having smaller files .
Finding c so that sum ( x+c ) over positives = K
For a given positive value of ` K ` , I would like to find the offset ` c ` that makes the sum of positive elements of the array ` y = x + c ` equal to ` K ` .
If you have a -1 in the array and the offset is 2 , does the -1+2 count towards the sum ?
How about binary search to determine which elements of ` x + c ` are going to contribute to the sum , followed by solving the linear equation ?
The running time of this code is O ( n log n ) , but only O ( log n ) work is done in Python .
If the next element plus ` c ` is positive , it should be included in the sum , so ` c ` gets smaller .
The running time is obviously ` O ( n log n )` because it is dominated by the initial sort .
Install build-deps headers and static libraries ( ` sudo apt-get install libblas-dev liblapack-dev gfortran `) , maybe there are some others but these are the ones I remember .
Eigen.shape = ( 100 , 100 , 100 , 10 ) , where the first three indices are over the vector vec_k , and the third is the band index ' n ' .
I would like to find the fastest way to give order to the second file based on ` X ` and ` Y ` positions in the first file and append extra properties to the original file .
The reason why I claim that it is the transformation that is slowing things down is because I tested the code ( listed below ) after i changed ` transformedValue = f ( float ( values [ 0 ]))` into ` transformedValue = 1000.00 ` and that took the time required down from 1 min to 10 seconds .
You mean , split the string on ' : ' and then append ' h ' ?
The closest thing I could find is ` theano.tensor.diag ` , which allows me to construct a symbolic matrix from its diagonal : #CODE
You could use the ` theano.tensor.triu ` and add the result to its transpose , then subtract the diagonal .
This approach would require a full matrix to be passed , but would ignore everything below the diagonal and symmetrize using the upper triangle .
In order not to deal with a multidimensional output , we can e.g. look at the gradient of the sum of the matrix entries #CODE
This reflects the fact that the upper triangular entries figure doubly in the sum .
I don't see how that can go through a gradient .
The gradient will probably be in matrix shape , but only have non-zero coefs on the upper part , so I don't think this will incur any extra instability .
I added a gradient calculation .
Definition : df.A.where ( self , cond , other=nan , inplace=False ,
are from self where cond is True and otherwise are from other .
I would like to compute the sum of the elements along a line that starts approximately from the center of the matrix .
For each step I want to have the sum of the elements , thus the output should be a numpy array of 10 elements .
Once I have " d array " , I want to sum all the elements of " data array " which are located at the same degrees with respect to the origin , i.e. along 180 , along 165 , along 161 and so on till zero degrees .
The output should be an array containing degree and the sum of element for that degree , i.e. out = array ([[ 180 , sum along 180 ] , [ 165 , sum along 165 ] ,... [ 0 , sum along 0 ]]) .
I am trying to install the SciPy stack located at scipy ( dot ) org / stackspec ( dot ) html [ I am only allowed 2 links ; trying to use them wisely ] .
http ( colon ) // sourceforge ( dot ) net / projects / numpy / files
So , assuming the size of the array is x_ , y_ , z_ in the three axes , the valid indices will have the form like [ 1 :( x_-1 ) , ( 1 : y_-1 ) , ( 1 : z_1 )] .
In ym case I could simply use numpy indices .
You create another list of indices you don't want : ` inds = [ 1 , 3 , 6 ]` .
Whilst the matrix is initially the same for all users , it must be unique for each user as it changes according to each user's set of answers .
Let us call it data and it can have an arbitrary shape and I am trying to generate some indices array as follows : #CODE
I would like basically to create an array of indices .
This creates an ndarray with indices starting from 0 to ( n-1 ) along each of the axes of my input array .
You would need to ` clip ` subsets of the arrays .
You'll need to either clip in-place or assign the values back to the array .
However , when I dump the csv to file , I see nan and not zero .
I am trying to get some array indices with python .
I was wondering if there was a more elegant and hopefully efficient way to generate these indices ?
So every time the outer loop runs $p$ will be a 2D array of array indexes .
( vector field is the minus gradient of the potential field . )
@USER - Use ` numpy.gradient ` to get the gradient of the potential field .
( On a side note , you have an error in the code to evaluate your equation . It's missing a negative inside the ` exp ` . ): #CODE
Next , we'll need to calculate the gradient ( this is a simple finite-difference , as opposed to analytically calculating the derivative of the function above ): #CODE
A better way to visualize this might be with an image plot with black gradient arrows overlayed : #CODE
It won't have any visual effect , in this case , but the absolute values of ` dx ` and ` dy ` are completely wrong , as you pointed out .
replace ` X2 [ i ] [ j ]` with ` ( X2 [ i ] [ j ] - mean [ X1 [: , i ]]) / std [ X1 [: , i ]]`
It plots data points based on their indices , do you need this plot ?
I am multiplying two n-d arrays using some set of indices that I have obtained using numpy mesh grid .
So , I have two n-d arrays called current and previous and I have a set if indices p and q and I have something as follows : #CODE
Now , I have a another n-d array filled with only 0s and 1s and what I want to do is make sure this multiplication only happens in the indices where this mask is set to 1 .
This ensures that it zeros out the regions I do not want and they do not contribute to the sum .
So , the indices are generated using mesh grid , so all the computation with this one line .
p and q are arrays of indices .
This is quite easy : ` .sum ( 1 )` calulate the sum row wise .
When calculate ` sum ` , ` True ` is ` 1 ` and ` False ` is ` 0 ` .
If it is ... how do you sum a mask ?
You can read the whole file content at once , ` split ` the string at spaces and line breaks and ` join ` the pieces in the required order : every tenth element in one line , then a new line and so on : #CODE
You can do this using matrix to store the norms and find the indices that attend your conditions , without for loops ...
` w ` is a matrix of norms applied to the differences of the points broadcast together .
` y ` and ` z ` are both arrays with the third coordinates of the points , shaped so that they broadcast together into the same shape as ` w ` .
replacing quotes inside a pandas df to count unique terms
I am trying to obtain a unique list of firms and pass to a python list , so I do the following : #CODE
Bo Of A M L and others are repeated , so my code is not choosing the unique terms .
Is there are way to create a new pandas or numpy array that contains the unique terms that are comma separated in a string ?
@USER ` concatenate ` is a numpy method not a ` ndarray ` method
The next step is necessary to create your unique list of firms , and this is another reason why I think the other solutions have not worked : #CODE
You are likely using ` numpy.sum ` instead of the built-in ` sum ` , a side effect of ` from numy import * ` .
When given a generator expression , ` numpy.sum ` falls back to calling ` sum ` , which promotes integers to longs .
However , the [ source code ] ( #URL ) has a weird special case for generators ( and not other iterators or iterables ) that uses the Python built-in sum .
@USER Thanks for pointing out that ` numpy.sum ` falls back to the built-in ` sum ` when given a generator .
Clobbering built-in functions like ` sum ` is * evil* .
@USER This would still clobber builtins ` sum ` , ` all ` , and ` any ` .
I need to multiply each element of y by every element of X , and then sum up .
Something like ` [ sum ( X [ 0 ] *y ) sum ( X [ 1 ] *y ) sum ( X [ n ] *y )]`
If you're multiplying each element of ` y ` by every element of ` X ` , just multiply all the elements of ` X ` together first , then use multiply the array ` y ` by this number and sum : #CODE
Edit : the array you specify can be obtained by multiplying the array ` X ` by the sum of the elements of ` y ` : #CODE
I want a 1d array like ` [ sum ( X [ 1 ] *y ) , sum ( X [ 2 ] *y ) , sum ( X [ 3 ] *y )]` and so on .
( You can ` append ` to them , but you're creating a new copy and deleting the old one each time . ) If you have something that needs to constantly change in size , use a ` list ` .
I am not sure about the code given under the gradient descent section .
Its under gradient descent section here : #URL
Each time ' train ' is called , Theano will compute the gradient of the ' cost ' w.r.t.
Can you create a numpy array with all unique values in it ?
Is there a way to ensure every value is unique ?
I have created a basic function to generate a list and populate a unique integer .
You seem to be asking for 2500 unique random integers between 0 and 100 .
Do you need random numbers , or just unique ?
If just unique , maybe ` np.random.permutation ( np.arange ( N ))` ?
The most convenient way to get a unique random sample from a set is probably ` np.random.choice ` with ` replace=False ` .
If all you're looking for is a random permutation of the integers between 1 and the number of elements in your array , you could also use ` np.random.permutation ` like this : #CODE
This should lead to me getting the correct size with allowed unique values in each column / row location in the matrix .
Well , if you just want a random permutation of the elements in the ( inclusive ) range ` [ 1 , col_x * col_y ]` , you might as well just use ` ( np.random.permutation ( col_x * col_y ) + 1 ) .reshape ( col_x , col_y )` , as I think @USER was hinting .
where I'm assuming that we're using ` s ` , which is a ` Series ` , to hold the indices .
The dot product is like numpy dot product , it works FINE , ` printing _dotproduct ( x1 , u )` give a valid answer .
What I'd like to do is take an input integer array , and expand its data into indices ( e.g. , [ 2 , 1 ] -> [ 2 , 2 , 1 ]) .
This method is called within a nested for loop that I'd like to squeeze additional performance out of .
In addition I would make a ` prange ` instead of a simple ` range ` iterator for the outer loop .
` _dotproduct ` is simply finding the dot product of the two matrix .
mean is a d by 1 vector so it's multiplication by it's transpose is a d by d matrix .
Using Cython with a scipy sparse matrix is a pain , unless you are familiar with the particular format ( e.g. in the case the ` csr ` indices ) .
its not dot product .. specifically i need bit-wise so I am doing element-wise operation ; #CODE
I'm not sure what your code is doing ( string join and concatenate are not happily mixed with numpy ) , but I believe that in Win8 you are still limited to 4GB per process .
Your loop halves ` n ` each time through , and since log ( 255589106 , 2 ) ~= 27 , you could end up looping about 27 times .
and I wanted to actually concatenate ( c_mat , C );
Update : With the updated description of the problem it seems that the first column contains start indices and the second column has the end indices .
But then I've heard that in Numpy arrays , ' append ' is discouraged as it's not as efficient .
What I read from the question is that he is concerned about efficient loading ** into ** memory ( talking about append , etc . ) .
My main concern is the ' append ' operation on a large numpy array .
Now , what I want is the sum all rows in a , where each row is weighted by the column values in b .
But I have the feeling that if I knew how to broadcast the two correctly as 3-dimensional arrays I could get the result I want in one go .
Now you only need to concatenate the result and your original array .
I have data for which as in the example below Sub is unique but Que is not .
In the case where there are multiple Que matched with the same Sub I want to choose the Que with the max Cov .
The following code does almost what I want , It returns the max Cov for each Sub .
@USER Would it be possible to append and add the ( z , x , y ) to the end of an empty list ( to start with ) every time the while-loop cycles through ?
Then , ` make_median_image ` would return an ` ndarray ` and you could append them together in the while loop to make a nice convenient ( and space-efficient ) big ndarray .
Moreover , the problem is exactly identified : There should be one unique solution .
I wanted to ask if SVM and Naive Bayes consider the input value of first and last names as independent values or there is some relation between numbers ? in other words , is it important that 5 is greater than 2 , or the numbers are just going to be considered as unique values regardless of their arithmetic value .
You can then iterate over the numpy array directly ( rather than iterating over it's index ) but you have to transpose the array ( using ` y.transpose() `) otherwise for each iteration you'll get the x-value for each 1000 random numbers .
I want to append the x numpy array to the existing one to add a new column so I can output some information to that column for each row .
You could use numpy's hstack or vstack append column ( s ) or row ( s ) to your array .
I used ` zeros ` here , so the string fields will start out with the value `''` .
The example below iterates over a ` lil_matrix ` and calculates the sum for each row .
Note that the ` csr_matrix.sum() ` over the columns is almost one order of magnitude faster than the dense sum .
The ` sparse ` ` sum ` is a dot product with the appropriate vector of ones .
With your sample matrix , the ` csr ` sum is 5x faster than the dense sum .
The ` lil ` sum is slow because of the time it takes to convert it to ` csr ` .
It returns the indices of the array in partially sorted order , such that every ` kth ` item is in its final sorted position .
if you can , try to use a color map which can be produced by simple arithmetic operations form the data ( for example , grayscale , grayscale with gamma , etc . ) , then you can make step 5 faster
My problem is that the instances are identified by unique 17-digit integers .
which I'll dump to the console in ipython using ` !
how to vectorize a matrix sum in a for loop using numpy ?
At least in this case , ` einsum ` is faster than ` sum ` .
If I wanted to find the indices where the elements were greater than 1 , I would do : #CODE
If you want , you can then ` argwhere ` that to get indices : #CODE
python pandas flatten a dataframe to a list
I want to flatten the df so it is one continuous list like so :
Maybe use stack ?
@USER official guide of Scipy stack giving links to unofficial windows binaries by Christoph Gohlke , Lol .
or , you can have a tuple of unique elements instead of set object : #CODE
That being said , from a dump of your file : #CODE
Of course I can think of a normal manual process to do this ( iterate through each column , find indices which are outliers , delete row , proceed to other column ) , but I've always found that Numpy contains some quick nifty tricks to accomplish statistical tasks like these .
sum the table along axis ( sum of each row )
create a new table where there are only the rows where the outlier sum is 0
indices is then a boolean array of the same shape as x and contains .
so you can just index any other array ( with appropriate shape ) with ` indices `
and you will get the value of that array in the ` indices ` positions .
Probably need to post your code and the stack trace in order to get better help .
I am trying to concatenate two numpy arrays , but I got this error .
You need to put the arrays you want to concatenate into a sequence ( usually a tuple or list ) in the argument .
As far as I know , this API is shared by all of numpy's concatenation functions : ` concatenate ` , ` hstack ` , ` vstack ` , ` dstack ` , and ` column_stack ` all take a single main argument that should be some sequence of arrays .
But this means that ` concatenate ` is interpreting ` allValues ` as a sequence of arrays to concatenate .
The second argument is taken as the second ( optional ) argument of ` concatenate ` , which is the axis to concatenate on .
How do I get rid of the two ones before it ?
In that case , it should be as simple as a ` reshape ` command .
I'll mark it as the answer in a few min ..
There is the method called ` squeeze ` which does just what you want :
Because when you pass ` old_a ` you are passing the reference to the outer list and not the copy .
I can't think of a way of having two unique copies of the same data without cloning it ...
However , the ` outer ` function seems to flatten my array of vectors before subtraction and then reshapes it .
An ( almost ) pure Python way to do a " pair-wise outer subtraction " of vectors ` r ` would be as follows : #CODE
Finally you can ` reshape ` it as usual .
( The ``` -1 ``` tells ``` reshape ``` to compute that dimension itself . )
Slice a broadcast object ?
I have a 2-dimensional array that represents a mask of a 3-dimensional array , and can be broadcast as such .
Arrays should be constructed using ` array ` , ` zeros ` or ` empty ` ( refer
Storing debug log for failure in / Users / me / Library / Logs / pip.log
If you have only a limited number of elements to index , you might want to store the indices as a list of indices .
store indices of non-zeros
( Of course , the " per dimension " can be avoided by using flat indices . ) The boolean approach takes 1 byte per element , so if you have more than 1 / 8 of the elements with a ` True ` in the boolean table , it is the more space-saving solution .
This is actually quite interesting : Using boolean indices is worst when half of the elements are ` True ` .
The use of list indices behaves as expected .
However , it seems that performance-wise list indices are very good in most cases .
By the way , I cannot find a way to plot an histogram of dates in numpy / pandas
if your distribution is from a histogram or non-parametric then I can post a recipe how you sample from these kind of distributions .
you do not need to use tile : #CODE
Instead , NumPy will broadcast the arrays for you : #CODE
When I apply qr function from numpy.linalg ( there is also a version of this function in scipy.linalg , which seems to be the same ) , it returns matrix Q with m x m dimensions , and R with m x n dimensions , even when the rank of matrix A is less than m .
Is it possible to identify the independent columns of A through this R matrix returned by function qr in numpy.linalg ; scipy.linalg ?
Check for diagonal elements of R that are non-zero : #CODE
Multiple lists of indices for one array in numpy
In normal situations a list with integers can be used as indices for an array .
Instead of one list of indices , I have several , with different lenghts , an I want to get ` arr [ l ]` for each sublist in my list of indices .
If there are many tiny slices then you may want to concatenate them .
One option is to concatenate them all , do the slicing and then redistribute into lists .
Also possible is : ` indices = np.cumsum ( map ( len , lists ))` , ` np.split ( extracted , indices [: -1 ])` .
IndexError : arrays used as indices must be of integer ( or boolean ) type `
This will not work exactly because your x and y indices can go negative -- you might need to add an offset in this case .
About the negative indices I don't really mind that they go negative if I could re-label them from 0-9 and 0-255 I would be perfectly okay with that .
We can now rearrange the data using reshape : #CODE
This just throws the information in the first two indices away .
My Python seems to align the ` longdouble ` items at 16 byte borders ( i.e. there is always 6 bytes of zeros per each element ) , but the C compiler may use 10 / 12 / 16 byte alignment .
concatenate with the zero-length array can also be done directly with vstack if you reshape : ` S = np.vstack (( S [: , None ] , X ))`
I changed it back to vstack , in that case ( although you could change the input shapes to concatenate or reshape afterwards ) .
sum elements of a list in numpy
I want to sum the elements of each array and have an output array that looks like this :
` map ( sum , sections )` ?
algorithm used inside for reshape of matrix ( 2d array ) in numpy
basic link where numpy implemented reshape is
but how it reshape algorithm i didn't understand ,
what is a optimised way to reshape big matrix ( 2D array ) ?
Take a look at the ` .__array_interface__ ` property of an array , before and after ` reshape ` .
Take a look at the numpy ` clip ` function / method ( #URL )
where ` exp ` comes from the namespace of the module ( s ) you chose .
` lambdify ( x , exp ( x ) , [ ' numpy ' , ' sympy '])` is roughly equivalent to #CODE
Pandas broadcast with numpy scalars
Pandas objects ( for example a ` DataFrame `) can broadcast operations with python scalars .
` row ` and ` col ` are integer indices , ` data ` has the same data type as the equivalent dense matrix .
` data ` and ` indices ` are the same as with ` coo ` , but ` indptr ` has a value for each row plus 1 .
An empty row , for example , requires a value in ` indptr ` , but none in ` data ` or ` indices ` .
This allows you to specify a relative / absolute tolerance for your comparison ` ( absolute ( a - b ) = ( atol + rtol * absolute ( b )))`
` sqrt ( x )` when ` x 0 `
See the man page for ` atan2 ` and ` log ` , for instance .
For ` fft ` , for instance , you're liable to get ` NaN ` s if your input values are around ` 1e1010 ` or larger and a silent loss of precision if your input values are around ` 1e-1010 ` or smaller .
Apart from truly ridiculously scaled inputs , though , you're quite safe with ` fft ` .
Don't forget the obvious ones ...
Find largest index of nonzero in python
and it returned all 12000 indices in y .
There is probably a solution where you can find the indices of each dimension and use ` itertools.product ` to generate all of the possible combinations of indices etc . etc ., but is there not a nice pythonic way of doing this ?
You're basically applying sum along the first axis to reduce the dimensionality of the input back down to 2D .
It looks like ` data ` is a ` BinTableHDU ` , not an ` ImageHDU ` , which is why using multiple indices failed .
I've read all over stack exchange how it's bad to call python script from other scripts and even then passing of variables is difficult .
But if you weight the latitudes by the associated ` temp ` value , then instead of a count you get the sum of the temps .
If you divide the sum of temps by the bin count , you get the average temp in each bin : #CODE
What I want is for all items under " value " where " month " is e.g. 1 in this case to count how many times each unique " value " occurs .
Convert this into group numbers , by shifting backwards once , filling the first value ( group 0 ) , converting to integers , and then taking a cumulative sum .
Assuming everything has the same indices , this should work as expected and be pretty quick .
How do I separate out unique rows in a list that has both a datetime and float column ?
I'm relatively new to Python , and I am having trouble separating out unique rows from a data set that I had recently converted into lists .
Then when I recombined the data into a list I tried to separate out the unique rows of data .
You could write your own unique function .
What's is the problem using np.any ? or the data types for the converted list are not the right ones ?
An array of arrays tends to lead to confusion anything you broadcast over it will generally treat each sub-array as just a Python object , not do anything numpily .
Here is the code that crashes when the sum is not 1 .
Can you show us the code that crashes when the sum is not 1 ?
When I run ` np.random.choice ` , I only get that error if the sum of probabilities is off by -5 orders of relative magnitude .
It's perfectly happy with a sum of ` 1+ 3e-17 ` ; it's only when you above ` 1+ 1e-5 ` that it complains .
abarnert , that's a simple for loop outside this function that prints out the sum of each distribution stored as a returned value from this function .
that's a simple for loop outside this function that prints out the sum of each distribution stored as a returned value from this function
So , some of your frequency distributions sum to ` 0 ` .
Presumably your code that builds the distributions have some edge case that either returns an empty distribution , or one that's all zeros .
Some letters do not appear at all , therefore their distributions sum to 0 causing the error when sampled .
Therefore , I have to produce the array ' b ' from array ' a ' by reshaping / rearranging its elements using indices .
I suspect you get a tuple of DataFrames , which is probably not what you want ; you might want to use [ concatenate ] ( #URL ) first .
I'm obviously new so don't think I'm formatting everything properly within the stack overflow site .
append a list at the end of each row of 2D array
I want to append a list / 1d array ( b ) at the end of each row of a 2d array ( a )
Numpy max function fails when using Decimal values that include NaN
Here the ` shift ` is saying how many elements to roll the data by ( Positive values to the right , negative to the left ) .
Given the specification , we want to roll it to the right by half the length of the array along the second dimension ( ` axis=1 `) .
Starting with the typical way to apply ` np.unique ` to 2D arrays and have it return unique pairs : #CODE
The absolute fastest , especially if your matrix is very sparse , is almost certainly going to be to use a CSC format and do the following : #CODE
How to perform a rolling sum along a matrix axis ?
How to perform a rolling cumulative sum of ` X ` along the rows axis with lag ` H ` ?
This of course uses convolve , but the question , as stated , is a convolution operation .
You are actually missing one last row in your rolling sum , this would be the correct output : #CODE
` einsum ` is a bit faster than ` sum ( 0 )` .
When you slide your window by one position , the optimal algorithm subtracts from the last windowed sum the item going out of the left side of the window , and adds the one coming in from the right .
If you prefer to use column-major indexing ( x , y , c ) and don't mind the potential performance penalty , then use ` numpy.transpose ` to permute the indices : #CODE
We recommend simply learning to reverse the usual order of indices when accessing elements of an array .
` healpy ` uses the HEALPix pixelization , so a * map * is a 1D array , where indices correspond to pixels .
In python what is the most efficient way to get the sum of values in a 2d numpy array ?
Is there a way to get the sum of all x , y coordinates in a frame that is 1 ) most efficient 2 ) most pythonic ?
it seems ` numpy ` create copy of matrices when trying to compute the dot product and it causes me some memory issues .
it is mentioned there : ` Although C is only 40 by 40 , inspecting the memory usage during the operation of dot will indicate that a copy is being made .
The reason is that the dot product uses underlying BLAS operations which depend on the matrices being stored in contiguous C order `
If you're " dot " -ting the transpose of a matrix with itself ( I can confirm this issue ) , or using numpy < 1.8 , there's a chance you run into memory issues .
I now want to get the top 10 elements of one matrix and increase the value on the same indices on the other matrix .
The reason I have to go this way is that a.data and b.data do not necessarily have the same number of elements and hence the indices would differ .
As far as I know the nonzero procedure is not elegant as I have to allocate two new arrays and I am very tough on memory already .
It indicates where each row starts in the ` data ` and ` indices ` arrays .
But beware , there are some warnings about indices may not be sorted .
Look at the code for ` nonzero ` .
I assume that if you have matrices A and B you want to set B [ i , j ] += 1 if A [ i , j ] is within the first 10 nonzero entries of A ( in CSR format ) .
I haven't benchmarked this but I suppose your option is 10 * O ( nnzs ) at worst , when you are adding 10 new nonzero values , whereas the DOK version should need O ( nnzs ) to build the matrix , then O ( 1 ) for each insertion and , finally , O ( nnzs ) to convert it back to CSR ( assuming this is needed ) .
where ` [ 2662 , 22789 , 5932 ]` are the indices corresponding to the three closest points given in ` xyCenters ` .
You can use these indices to get your ` ra ` and ` dec ` values very efficiently using ` np.take() ` : #CODE
Now ` misclassified ` is an array of indices into ` X_test ` .
a == max ( a )
calculate the expression max ( a ) n-times or just one ?
You probably want [ ` numpy.amax `] ( #URL ) rather than plain Python [ ` max `] ( #URL ) .
It only evaluates ` max ` once .
It computes ` max ( a )` once , then it compares the ( scalar ) result against each ( scalar ) element in a , and creates a bool-array for the result .
I can do the opposite function with ` np.split ` , but ` np.concatenate ` does not seem to work the way I need it to , and there is no such `' join '` function like there is in pandas .
` - ValueError : shape mismatch : objects cannot be broadcast to a single shape `
The ultimate goal is to find membership of one group in another using something like ` numpy.in1D ` and then use the indices for that membership to select more data to match up 2 huge data sets .
Add an axis so that ` a ` can be broadcast onto ` b ` and test for equivalancy #CODE
So how can I find the indices for elements in foo that equal some value without having to loop through it ?
The code you posted transforms the signal to freq . domain , takes the absolute value and then transforms it back to time domain .
Anyway , you can either map / broadcast the Pandas ` Timestamp.strftime ` function over it , or use that ` to_pydatetime ` function that you found and then map the Python ` datetime.strftime ` function over the resulting array .
Given a ` YYYYMMDD ` string , to convert that to an integer , you just call ` int ` on it or , if it's in a Pandas / Numpy array , you map or broadcast ` int ` over it , or , even more simply , just cast its dtype .
debug performance diff of Same code on nearly same cpu / ram
This returns a tuple : an array of the unique entries of ` myarr ` and an array of the indexes of the first occurrence of that entry .
I will also create one with only ones which will be used to check which values are already taken by the polygon .
I would like to convert my 1-D arrays of array objects in a numpy matrix to perform the sum of its elements over the rows .
First I would like to sum the double values ( if they exist ) in the different arrays , this is the case of the first array ( 1+0 ) and the last array ( 1+0 ) , to have a final 1D arrays of array with all single values : #CODE
The idea is to sum up the first two rows of the first column of the new matrix , the second two rows of the first column until the end , for all columns .
The final output should be made of 4 arrays ( after the sum ): #CODE
Anyhow , you can use ` map ` to apply ` sum ` to all the sub-lists in the rows .
Now , you can use , e.g. ` matrix [ 0:2 , :] ` to get the first two rows of that matrix , ` transpose ` them and use ` map ` and ` sum ` to get the sums of the columns .
Wouldn't be possible to iterate over the matrix and sum two rows by two with a final 4X8 matrix ?
Do you prefer a for loop over those map / sum / zip constructs , or do you want to create all four sums in one loop ?
I thought you wanted to have four separate sum variables .
I'm programatically aware of the exact indices of the rows to be removed .
You can use ` arr.base.resize ` to truncate or enlarge the array , then ` arr.flush() ` to save the change to disk : #CODE
Thanks a lot for that , can you think of any way of doing that automatically for all function likely to be used on vectors ( e.g. if ` T ` is a ` Float ` , one may apply ` sin ( TimeSeries )` and be guaranteed to a new ` TimeSeries `) ?
Taking a tensor product in python numpy without performing a sum
I tend to use ` np.einsum ` for these problems , which makes it easy to specify what should happen in terms of the indices : #CODE
I wish to extract a large number of rows and columns -- the indices of which I know in advance , though it will in fact be all rows and columns that are not all-zero -- to get a new square matrix ( approx 10,000 x 10,000 ) .
If you have to pass in arbitrary indices ( i.e. , not a rectangular slice ) , you're not going to get max speed .
When I try ` A [ np.ix_ ( indices , indices )]` using the latest numpy 1.9 release candidate , I get the error ` ValueError : Cross index must be 1 dimensional ` .
@USER : Yes , something like ` r = A.take ( indices , axis=0 ) .take ( indices , axis=1 )` is faster than indexing with ` [ ]` ( using numpy 1.8.2 ) .
I notice that depending on how many indices you choose , the indexing can even be faster than the multiplication .
How to truncate a numpy array for values greater than a specified value ?
Similar to this Matlab question , I am wondering how to truncate a numpy array by cutting off the values greater than a certain threshold value .
Bonus : I actually have two arrays I would like to truncate based on the values in one of the arrays .
I would like to truncate ` b ` in the same way that ` a ` gets truncated , so that the result would be #CODE
including a negative number in the log sum of exponents , in python
But now I have to append a ` -e^ax ` , and because it's negative , I can't simply append ` ax ` to the end of ` myarray ` .
I also can't append ` -ax ` because that's just wrong , it would mean that I'm adding ` 1 / e^ax ` , instead of ` -e^ax ` .
Is there any direct way to append this so that I can still use ` logsumexp() ` ?
Alright , example : I need to take the log of the sum of ` e^0 + e^1 + e^3 - e^2 ` .
So the sum is about ` 16.41476 ` .
Then the log of this is about ` 2.79818 ...
Does the input matrix always contain only zeros and ones ?
In my experience , one of the more enjoyable parts of using the Python scientific stack is coming up with efficient ways of doing computations , using the fast primitives provided in creative ways .
Similarly , try ` 2 == sqrt ( 2 ) *sqrt ( 2 )` ...
These two functions have built-in absolute and relative tolerances ( you can also give then as parameter ) that are use to compare two values .
I am able to generate binomial samples from an array of probabilities in a desired size ( output should be the same size of input probabilities array shape ) using the following lines of code #CODE
Is it possible to do the same with multinomial ?
It is possible to generate multinomial samples for a single row .
How could one generalize this to get output same as binomial ( output sample shape = input probabilities shape ?
It would be nice if we have generalized version of multinomial .
I am able to get a histogram from a Pandas dataframe to appear fine .
` norm.pdf ( x )` computes the PDF of the standard normal disttribution , with mean 0 and std . dev .
` vstack ` stacks the elements of the input vertically , and the transpose is needed to get the right dimensions ( very fast operation ) .
You'd obviously find the root of x by setting ` y=0 ` however , now you have one variable which I imagine is what you mean by k , which you specify , leaving a sum rather than a formula .
Ok , since I only plotted absolute values of FFT , I guess I lost some informations .
Then I plotted real part , imaginary part and absolute values for both signals :
abs ( fft ) : frequencies + amplitudes
real ( fft ) : ?
imaginary ( fft ) : ?
For each frequency bin , the magnitude ` sqrt ( re^2 + im^2 )` tells you the amplitude of the component at the corresponding frequency .
You can convert the signal 1 , which consists of a product of three cos functions to a sum of four cos functions .
This makes the difference to function 2 which is a sum of four sine functions .
A cos function is an even function cos ( -x ) == cos ( x ) .
That is the reason why the plot of the imaginary part of the fft of function 1 contains only values close to zero ( 1e-15 ) .
A sine function is an odd function sin ( -x ) == -sin ( x ) .
That is the reason why the plot of the real part of the fft of function 2 contains only values close to zero ( 1e-15 ) .
You are right , I didn't not submit the good example ( sin / cos ) .
You are not going to be able to train on a string , you have to encode it either yourself or use [ one hot encoder ] ( #URL ) , also why are you training on ip addresses ?
You could convert it to it's 32-bit address value , this would preserve the ip address I suppose but really the ip address is an identity and probably meaningless so I think you need to encode it , as suggested either map to int yourself or use one hot encoder .
As you can see , the values are not unique and a single x value can have several y values .
This is because ` exp ` tends to 0 but doesn't actually reach 0 .
I have a bigger problem now where I'm getting ValueError : could not broadcast input array from shape ( 3 , 2 , 3 ) into shape ( 3 , 0 , 3 ) in my code now that I've rolled it into my full main function .
op_table [ i , j , k ]= sin ( xi ) ?
Please make this question better by including a * complete * example , especially including what values of X , A and gamma you passed to your function .
In MATLAB , Simon Lucey's code initializes ` L ` to be a zero matrix such that ` L = zeros ( size ( X )); ` .
I'm trying to efficiently compute a running sum , with exponential decay , of each column of a Pandas DataFrame .
I'm not sure how to calculate the rolling sum ( with decay ) without iterating through the dataframe , since I need to know yesterday's score to calculate today's score .
` np.linspace ` generates a dataset of size ` len ( dataset )` and calculates how many times each row is multiplied by ` exp ( - 0.05 )` for the current row .
Optimal way to append to numpy array
I have a numpy array and I can simply append an item to it using append , like this : #CODE
But is this the quickest way to append to an array ?
If you do need to append on-the fly , it's probably better to try to not do this one element at a time , instead appending as few times as possible to avoid generating many copies over and over again .
I have a 2-D NumPy array and a set of indices the size of which is the first dimension of the NumPy array .
This question Find unique rows in numpy.array looked very promising but somehow it did not work for me .
Starting with the peak , go in both directions as long as the diff does not change ( You can use ` np.diff ` two times for this ) .
You'll have to reshape to 2d : the first dimension the independent axis , and the rest for all the other parameters .
Is it possible to simply sum up the entire array using all elements except the first one ?
Note also that ` sum ( alpha )` is calling the Python builtin function ` sum ` .
That's not the fastest way to sum items in a NumPy array .
` alpha [ 1 :] .sum() ` calls the NumPy array method ` sum ` which is much faster .
I wasn't actually using sum though , I just put it up to demonstrate any array calculation .
concatenate and remove duplicate element from arrays
I compute an array of indices in each iteration of a loop and then I want to remove the duplicate elements and concatenate the computed array to the previous one .
you can concatenate arrays first with ` numpy.concatenate ` then use ` np.unique ` #CODE
Is it possible to convert an array of indices to an array of ones and zeros , given the range ?
I need the answer : How was the cos ( u ) and sin ( v ) etc chosen ?
Good night guys , i have a question , im trying to concatenate strings into an array i have a .txt , that is divided in 3 gruoups " .I 1 " " .I 2 " " .I 3 " my code generates an array of the total number of " .I n " in this case an array of 3 , and my objective is to store all the lines after " .I x " in the x position of the array example ....
my code is just saving the last line before the next " .I " instead of saving all the lines , i knew that this was going to happend but the problem is that i dont know how to concatenate or push or append the lines in to the same position of the array
i Found an easier solution , instead of worrying for concatenations in the array , i created simple temporal variable called " doc " i use this variable to concatenate the lines and then when they are all concatenated i just save the variable in the array position that i want , and then i clean the variable this is how my code ended #CODE
Internally ` diff ` uses that same slice difference .
Since the output has a different size than the input , I don't see the need for an inplace difference ( especially if the diff step is larger than 1 ) .
` out = log ( sum ( exp ( threshold if a - a_max < threshold else a - a_max ) , axis=0 ))
For a multiset , if the distribution is very overdense , changing it into an array of unique values and counts instead of an array of duplicate values can speed things up .
Furthermore , if I run ` sqrt ( 0.93 * ( mat1 [ x , y ] **2 ) + 0.07 * ( row [ 1 ] [ x+1 ] **2 ))` in console I get ` 0.1613 ` , if I run ` zeromatrix [ x , y ]` I get ` 0.158 ` ( x=4 , y=4 ) .
` out = log ( sum ( exp ( threshold if a - a_max threshold else a - a_max ) , axis=0 )) ValueError : The truth value of an array with more than one element is ambiguous .
is the same as ` max ( a - a_max , threshold )` .
` out = log ( sum ( exp ( np.minimum ( a - a_max , threshold )) , axis=0 ))
If you just want the indices themselves , call ` list ` with the returned generator , and you should be able to use that with numpy's index arrays stuff .
I have a 2D numpy array filled with 0 an 1 , with an absorbing boundary condition ( all the outer elements are 0 ) , for example : #CODE
the array element [ i ] [ j ] is isolated if all its neighbours are zeros , and its neighbours are the elements : #CODE
The for loop iterates from 1 to L-1 because all the outer elements are 0 .
RequestId is unique .
And calculate averages or create a histogram with the resulting dataset .
I might need to perform a JOIN-type operation between the two tables so that I can correlate and analyze data from both datasets .
sounds like you should store the data with pytables , and use pandas to do the join and file I / O .
For this , I want to create a callable that generates elevation for a given longitude and latitude as a weighted sum of real spherical harmonics .
Given a large sparse csr_matrix ` A ` and a numpy array ` B ` , what is the fastest way to construct a numpy matrix ` C ` , such that ` C [ i , j ] = sum ( A [ k , j ])` for all ` k ` where ` B [ k ] == i ` ?
Then if you browse down there is a sum function where the axis can be specified .
The thing is , it is typically not possible to build the ` data ` , ` indices ` and ` indptr ` arryas in a manner that is more efficient than what the ` data , ( rows , cols )` constructor does .
I am trying to plot the frequency on the x-axis and the depth on the y-axis of a histogram .
In addition , I'm plotting a histogram to show this , and I would like the depth to be on the y-axis ( so that the plot is a bit more intuitive to look at )
Currently I have code that sorts the data into bins ( I don't know the size ) and then plots a histogram , but with the depth on the x-axis .
My Buffer inits with an numpy array filled with zeros : ` np.zeros (( lengthOfSlices , numberOfTrackedPoints ))`
That means I can't broadcast the array as the shape is not correct .
Is there a numPythonic way to initialize the array with zeros and store vectorwise afterwards ?
You need to reshape the array based on the lenght of the frame .
Instead of loading each dataset into a different variable , you could create a list of all datasets you want to load , load them into a list , and then sum them up .
Assign to one target , then address the two indices in the tuple : #CODE
Using ` np.arange ( len ( data ) , 0 , -1 )` places the higher index first ( for tied rows ) , so that when you reveres the indices , the lower index is first .
I have a huge array and I want to calculate dot product with a small array .
So 8GB for the first ePrime , at least the same again for a , and perhaps some unseen intermediate ones too .
A possible solution might be using a sparse matrix and the sparse matrix dot operator .
I believe once you do a calculation like dot , you'll have a dense matrix again .
You can use the ` nonzero ` function - #CODE
The problem is that the ` .coef ` attribute of the ` Polynomial ` class returns a set of coefficients that are somehow normalized or changed and I cant see how .
The class provides a ` __call__ ` , which makes ` pol ( 0 )` work , but as far as I can tell there's no documentation for the callable ( see Polynomial docs ) .
Most importantly , the ` Polynomial ` class orders coefficients differently from ` numpy.polyfit ` and ` numpy.polyval ` .
In ` Polynomial ` , as you described , the highest order coefficient is last in the list / array .
The code snippet below illustrates how to evaluate the polynomial represented by your ` Polynomial ` object at an arbitrary set of x-values , and also shows that in order to get the same behavior out of ` numpy.polyval ` , you have to reverse the order of the coefficients using ` coef [: : -1 ]` .
I think there is a difference between the ` Polynomial ` class and ` numpy.polyfit ` / ` numpy.polyval ` , in terms of fitting , since the former allows you to specify ranges over which the residual errors are computed .
The Polynomial coefficients are for scaled and offset polynomials for improved numerical stability .
My current solution determines a bounding box and then references items in the array within that box using their indices , which is slow with numpy .
Start with an array of zeros : #CODE
For points near the border , you would make a copy of ` mask ` and set rows / cols outside the image to zero before setting ` indices ` .
I managed to truncate it a little bit , does this solve your issue ?
I have two lists from which I need to find the indices associated with unique pairs ( all the SO posts I could find are only interested in the pairs themselves ) .
I zipped the lists to create a list of tuples , which then ` set() ` and ` np.unique() ` successfully pare down to only the unique pairs , but what I want is the indices into the original list .
The documentation for ` unique ` indicates that it will return those if ` return_inverse=True ` .
Is there some way to make ` unique ` give me the indices that I want in the first case ( which would be something like ` [[ 6 , 7 ] , [ 0 , 1 ] , [ 4 , 5 ] , [ 2 , 3 ]]`) ?
I need the indices to operate on other values from similar lists .
If the input isn't an array and ` return_inverse ` and ` return_index ` are not present , the routine delegates to Python built-ins to find unique elements .
I need to get a new matrix generated by selecting a subset of columns from another matrix , given a list ( or tuple ) of column indices .
Without customization of this parallelization , I'd have to request the max amount of CPUs on each machine in the queueing system .
You can select multiple indices using a list or a tuple .
That didn't work well because in the ` plt.imshow() ` call the sum overflowed the range 0 ..
Roughly speaking , adding ( or subtracting ) a constant to the RGB value of all your pixels changes the image brightness , multiplying your pixels by a constant changes the image contrast , and raising your pixels to a constant ( positive ) power changes the image gamma .
That's not a problem for gamma adjustment , or contrast adjustment ( assuming the constant is in the 0.0 .. 1.0 range ) , but it can be a problem for brightness modification .
Sort numpy.array rows by indices
I have 2D ` numpy.array ` and a tuple of indices : #CODE
How can I sort array's rows by the indices ?
I'm trying to get a cumulative sum that changes according to the product of another variable and the lagged value of the sum ( sounds a bit like math gibberish , I know .. please bear with me )
Now , we need to multiply the ` xx ` by the lagged value of ` n ` , iteratively , and then take the cumulative sum of this value : #CODE
I'm using numpy.histogram2d to generate a 2D histogram without passing a value for the ( optional ) ` bins ` argument , which means ` numpy ` obtains it automatically .
My question is : when I pass no ` bins ` argument , what rule is this function using to calculate the number of bins used in the histogram returned ?
For me personally , I've used the histogram feature for about 20 items but also for a few million giving 4 to 1000+ bins .
How would this translate to my case ?
Create as many repeats of your base ` n ` range sequence as you will need , offset each by the right amount , flatten and discard extra items : #CODE
Instead of using the colon ` : ` to refer to any row / column , I just opened up the loop into two loops to explicitly refer to the indices in each dimension of the array : #CODE
Numpy : given the nonzero indices of a matrix how to extract the elements into a submatrix
I know that numpy.nonzero ( a ) gives you the indices of the elements that are non-zero , but how can I use this to extract a submatrix that contains the elements of the matrix at those indices .
Your example output and your description are not exactly the same - do you want to extract all the nonzero elements of a matrix , or do you want to get a matrix representing the smallest rectangular region of the original matrix that contains all the nonzero elements ?
It seems like you're looking to find the smallest region of your matrix that contains all the nonzero elements .
If space is a big issue , you could either compress the pickled results or create your own file format .
` scipy.io.savemat ` saves arrays ( including sparse ones ) in a MATLAB compatible format ( 4 & 5 versions ) .
To view what's stored under the ' ones ' key , you would do : #CODE
Is there a way to " compress " an array in python so as to keep the same range but simply decrease the number of elements to a given value ?
So I need a way to compress the array while keeping the -1 to 1 mapping .
I can probably restrict it to the case where ` numpy.linspace ` is always generating the array I need to compress . what are the simpler ways in that case ?
Note that ` exp ( 20 ) = 4.85e8 ` and ` exp ( 3 ) = 20.1 ` , and since the first number is too large to conveniently express directly and the second one can be expressed directly , you have the scientific notation in the first and not ( explicitely ) in the second .
Series.where ( cond , other=nan , inplace=False , axis=None , level=None ,
self and whose corresponding entries are from self where cond is True
But , if all you're trying to do is replace rows of all zeros for specific set of columns with ` NA ` , you could do this instead #CODE
which I then want to flatten to : #CODE
Unfortunately , the things I try all either fail to flatten the matrix or flatten it too much or permute the entries so that some columns are empty .
There are a lot of other questions similar to this one , but the things suggested in them haven't worked ( or maybe I missed the ones that work ) .
But if you want to make the ` reshape ` approach work , first rearrange the rows in the tensor : #CODE
I am guessing there should be some numpy reshape method which can easily achieve this , rather than writing the three for loops which I am trying to avoid .
Use numpy reshape #URL e.g. #CODE
sum of [ 0.0 , 0.7 , 0.2 , 0.1 ] do not equal to 1
Use Decimal for accuracy ` from decimal import Decimal \n sum ([ Decimal ( ' 0.0 ') , Decimal ( ' 0.7 ') , Decimal ( ' 0.2 ') , Decimal ( ' 0.1 ')]) = Decimal ( ' 1.0 ')`
Are you confusing " integral over the range " with " sum " ?
I want to essentially subtract B from A and obtain indices of elements in A which are present in B .
Several usual commands like nonzero , remove , filter , etc seem unusable for ND arrays .
You need to get the data , indices and matrix size of the matrix in Scipy and use them to create a sparse matrix in Matlab .
Practically , it computes the distance between a vector in the first configuration and any other vector in the second , checks for a threshold value ` a ` , and returns the total sum after a loop over all the positions .
If you need to calculate the range dynamically , find min and max pressure , then find an appropriate grid step so that the number of isobars is reasonable .
I used the following imports in my files ( not all in the same but in different ones ): #CODE
Interestingly , if I were to do something like ` import numpy.sinh ` I get ` ImportError : No module named sinh ` and it is only when I do ` import numpy.numpy.sinh ` that I get ` ImportError : No module named numpy.sinh ` .
` import numpy.sinh ` won't work since ` sinh ` is no module .
But when i change the argument of fft to my data set and plot it i get extremely odd results , it appears the scaling for the frequency may be off .
When i do an fft on the whole thing it just has a huge spike at zero and nothing else
When i put these lists of data into the fft example it just has a huge spike at zero
The important thing about fft is that it can only be applied to data in which the timestamp is uniform .
In this case , you can directly use the fft functions #CODE
Most people will like to look at the magnitude and phase of the fft .
Then plot each of the ` ys ` and the total ` y ` and get the fft of each component .
If the signal you are trying to analyze is the first one you have ever taken the fft of then you will always feel that you are doing something wrong ...
Another way , is to visualize the data in log scale :
I want to store x2 ( which ever gives the min Euclidean distance ) in some variable so that i can use it later .
And let we have array of indices in the each ROW .
You could reshape them , but for that you need to figure out what their shape is .
you could concatenate the pieces in 2 steps to make ` blank ` directly #CODE
Often it is possible to assemble a large array of one concat followed by a reshape .
and i try reshape but as you can see , the values are not well positioned
I managed to calculate and plot the cumulative sum with ` matplotlib ` with the following code snippet : #CODE
In blue , you can see the curve depicting the cumulative sum , in other colors the parameters I would like to obtain .
And those 2 are the ones characterizing the curve that is best matching my data .
And the documentation on curve_fit says those parameters contain the " Optimal values for the parameters so that the sum of the squared error is minimized " and the covariance of the previous parameter .
num.save ( ' max ' , CXY [ i , j ])
How would I plot a function f ( x , y ) ( technically a field ) as a colour gradient in Pylab ?
I want the plot to be done over an XY graph , where ` f ( x , y )` is represented as a colour gradient .
It is the users ' responsibility to keep the aliases unique within a package .
I want to append this array to another numpy array in the 2D direction .
and I want to append it to something of the form #CODE
My issue comes from the fact that in the first run through the loop , I don't have anything to append to yet because first array read in is the first row of the 2D array .
I can create an empty numpy array , but then I can't append to it in the way I want .
Then append my arrays with ` np.append ( z , x ) .
Then , because all the arrays I read in are of the same size , after the loop I can simply resize with ` np.resize ( n , m )' and get what I'm after .
Alternatively , you can create a regular list , append a lot of arrays to that list , and in the end generate your desired array from the list using either ` numpy.array ( list_of_arrays )` or , for more control , ` numpy.vstack ( list_of_arrays )` .
This is the end of the stack trace : #CODE
Run in the debugger or , better , a nice graphical visualizer ( I don't know of any that work with ` numpy ` , but that doesn't mean they don't exist ) , or just add a ` print ` call before and after each step you don't understand to dump out what they're doing .
Python loadtxt and genfromtxt truncate strings
I have a 2-column array mixed type array that I need to read in and reshape into a data cube .
and after that I decide to use ` sum ` from the standard library .
( 1 ) IPython does not import numpy's sum by default -- unless you're working in a legacy ` pylab ` mode , in which case you shouldn't .
( 2 ) ` sum ` isn't a great example because using ` sum ( something , [ ])` to concatenate lists will show quadratic behaviour and so should generally be avoided .
This can be otherwise thought of as taking the max value for each element of the two arrays .
You want to broadcast the whole ` if ` / ` else ` over the array .
First , notice that your ` mymax ` function will do exactly the same as Python's built-in ` max ` , for 2 values : #CODE
That's the only tricky bit about numpy : most things just work the obvious way you'd expect , but every once in a while you want to do something that's obviously iterative in your hear , and you have to figure out how to translate it into something element-wise .
I have the following code that reads data from a CSV file and creates a 2D histogram : #CODE
Each pixel in the histogram contains the probability of lightning for a given range of temperatures at two different frequencies on the x and y axis ( ` min_85 ` on the x axis and ` min_37 ` on the y axis ) .
I am trying to reference the probability of lightning from the histogram based on a wide range of temperatures that vary on an individual basis for any given storm .
Each storm has a ` min_85 ` and ` min_37 ` that corresponds to a probability from the 2D histogram .
Is there a more efficient way to reference the probability from the histogram based on the given ` min_85 ` and ` min_37 ` ?
I have a separate file with the ` min_85 ` and ` min_37 ` data for a large amount of storms , I just need to assign the corresponding probability of lightning from the histogram to each one .
It sounds like all you need to do is turn the ` min_85 ` and ` min_37 ` values into indices .
But do make sure that the resulting indices are valid before you extract them .
I am trying to define a function that returns elements of a multi-dimensional variable according to a nested list of indices .
where ` l ` is a list of indices #CODE
vectorized matrix power and matrix dot using numpy
I would like to the same for dot multiplications : #CODE
I doubt that , in the scheme of things , having the outer loop written in Python is that much of a performance drag .
` np.einsum ( ' ijk , ikl -> ijl ' , mat_list1 , mat_list2 )` is a vectorized ` dot ` , but with these relative dimensions , it does not save time .
Agree with @USER - the outer loops are relatively minimal .
Do you want the sum to be divided by the number of rows , or by the number of non-missing values ( for each column ) ?
Update : I needed to transpose the numpy array so that ` rate_df ` was correctly oriented .
@USER ah , you need to transpose the array ( my rate_df was different from yours ) .
You probably were working with my original data and that's why the transpose was needed .
It checks that the absolute difference for all terms is small .
So if it can broadcast the arrays to do the math , it can compare the arrays .
I get what I want ( a symbolic sum over ` u ` as a function of ` w `) .
Is there a way to get the sum I want ?
Sounds like you just need to convert the indices to the original ` N `' s coordinates : #CODE
Obviously ` exp ` from ` numpy ` doesn't work ( I get an error ` AttributeError : exp `) .
To do an elementwise exp on a qobj you can operate on the underlying sparse data .
Basically my aim is to write ` rho2 ` by having an operator ` exp ( i pi Jx )` act on the ket ` rho1 ` .
numpy concatenate two matrices .
The ID arrays contain repeated combinations , for which I need to sum the 4th array , and remove the repeating element from all 4 arrays .
You can use unique and sum as reptilicus suggested to do the following #CODE
To get started , we can stack the ID vectors into a matrix such that each ID is a row of three values : #CODE
Now , we just need to find the indices of repeated rows .
This part is borrowed from the ` np.unique ` source code , and finds the unique indices as well as the " inverse index " mapping : #CODE
Finally , sum over the unique indices : #CODE
Also there may be slightly faster options to find unique rows in a 2D array , there was plenty of discussion [ here ] ( #URL ) .
Theano get unique values in a tensor
You can use convolve to compute sum_seq in function ` validate ` as follows #CODE
rearranging rows in a big numpy array zeros some rows .
` 84122 * 7966 ` is a little under ` 2**30 ` , which to me makes it smell like 32 bit pointers / indices being used in a 64 bit system .
I suspect it may be a manifestation of this bug , where you can see that setting a large array to a value silently fails and outputs zeros .
However , if I drop the width down to 54122 , the code works as expected ( doesn't output zeros in rows > 7966 ) .
Is there a bitwise or | that works on every element , similar to sum , or np.mean ?
I'm a bit confused because if you normalize an array of ints between 0 and 1 you'll just have an array of zeros and one .
I'm curious why you are subtracting the min , in affect losing the offset information .
( It would not be hard to roll my own , but I prefer to use built-in tools if they exist . )
I think the OP will need to roll his own test function .
The error I'm getting is " IndexError : arrays used as indices must be of integer ( or boolean ) type " #CODE
You can get the shape of the array and then iterate through the range of indices instead : #CODE
it returns the shape of the array ( i.e. the max index of each dimension in the array ) .
FWIW , exp (8 .04812689598e -13 ) ~= 1.00000000000080481268959832386 , and exp (8 .04812689598e13 ) ~= 4.0192352016871412017421425E +34952571005812 , calculated using mpmath .
But this is typically not a problem because a constant will automatically broadcast to the correct shape in any operation that you use it in with an array .
Changing ` data = data [: , :] ` to ` data = data [: , 4 :] ` or changing the numbers to positive ones does make the script run .
The absolute value 2.3e-99 is not credible since it is floating point arithmetic after all .
The outer product of an array [ a , b ; c , d ] with itself can be described as a 4x4 array of ' strings ' of length 2 .
This is an example for just one outer product .
The goal is to handle k successive outer products , that is the 4x4 matrix can be multiplied again by [ a , b ; c , d ] and so on .
I am afraid this doesn't generalize to higher number of outer products ?
almost , except that the ( 16 , 4 ) array should be square , just like in outer products , so it should be reshaped into ( 8 , 8 ) array .
I can reshape the ` ( 16 , 4 )` into a ` ( 4 , 4 , 4 )` that makes sense , but not a ` (8 , 8) ` .
Thanks but this isn't quite an outer product , the answer should be [ ' aa ' , ' ab ' , ' ba ' , ' bb '] , [ ' ac ' , ' ad ' , ' bc ' , ' bd '] , [ ' ca ' , ' cb ' , ' da ' , ' db '] , [ ' cc ' , ' cd ' , ' dc ' , ' dd ']] .
So basically reshape each line of the ( 4 , 4 , 4 ) array into a 2x2 square which goes into an 8x8 array .
` kron ` rearranges the ` outer ` elements by reshaping it to ` ( 2 , 2 , 2 , 2 )` , and then concatenating twice on ` axis=1 ` .
I recently came across a great SO post in which a user suggests that ` numpy.sum ` is faster than Python's ` sum ` when it comes to dealing with NumPy arrays .
The reason ` np.sum ` is faster than ` sum ` is that ` sum ` is implemented to " naively " iterate over the iterable ( in this case , the numpy array ) , calling elements ' ` __add__ ` operator ( which imposes a significant overhead ) , while numpy's implementation of ` sum ` is optimized , e.g. taking advantage of the fact it knows the type ( dtype ) of the elements , and that they are contiguous in memory .
The ` sum ` operation , however , is not overridable , so numpy provides an alternative optimized version of it .
I will just expand slightly on ` sum ` vs .
built-in ` sum ` will go through an array , take the elements one-by-one and convert them each to a Python object before adding them together as Python objects .
` np.sum ` will sum the array using an optimized loop in native code , without any conversion of the individual values ( as shx2 points out , this crucially requires homogeneity and contiguity of the array contents )
` sum ( list )` is a lot faster than ` sum ( array.array )` .
And the fact that different ones overflow each time shouldn't make it that much harder to debug especially given that it apparently usually happens in a single place , just not always .
print indices ( vals , [[ 0 , 1 ]])
One reason you might want to do this is to concatenate the sum as a row : #CODE
Fitting to Poisson histogram
I am trying to fit a curve over the histogram of a Poisson distribution that looks like this
Two things : 1 ) You don't need to write your own histogram function , just use [ ` np.histogram `] ( #URL ) and 2 ) Never fit a curve to a histogram if you have the actual data , do a fit to the data itself using [ ` scipy.stats `] ( #URL )
An even better possibility would be not to use a histogram at all
However , if you are fitting to poissonian data , scientifically / statistically you'll be better off to fit to the sample itself , not the histogram !
I'm trying to sum some values in a list so i loaded the .dat file that contains the values , but the only way Python makes the sum of the data is by separate it with ' , ' .
You have to tell ` loadtxt ` what delimiter is actually being used , once the data is imported from the file it will be in an array , which you can certainly ` sum ` .
If I dont use the comma-separated the sum is 0 .
I assume you want to sum the numbers that look a person's height ( in meters ) .
As you can see the terms are only separated with blanks , If I put the commas then the sum will give the result i need
I wonder if your sum is 0 because ` altura ` is empty .
Therefore , the reshape operation needs to make a copy here .
I would also like to know the indices and size of this box with respect to the original array .
The method below fills one diagonal at a time : #CODE
This kind of matrix is called a Toeplitz matrix or constant diagonal matrix .
What is the simplest way to dump this column copy of excel data into a python numpy array ?
The data sets are often of different lengths and I cannot flatten them together .
You can just use ` concat ` and pass param ` axis=1 ` , to append the arrays as columns : #CODE
You can specify how many of the nearest points to your station you want to extract the indices for .
Once you have the list of indices for surrounding points you can interpolate the values to your station .
It can either return the indices of the entries that meet this condition , or the entries themselves - as long as I do this within a reasonable amount of time ~minutes .
Interestingly , it freezes even with a single process Pool in my computer ( on the dot product of two ` [ 20000 , 36 ]` dim matrices , of course one is transposed ) and it works on my remote server without any flaw .
Can you post the parts of the code that launch / join your subprocesses , the functions your pools are executing , and any shared or global objects ?
You can use the diff function to calculate each of your numerical dy , the diff function to calculate your numerical dt , and then element divide these arrays to determine f ' ( t ) at each point .
In Python , how can I append an ndarray to self in a function
How to accumulate values in numpy array by column ?
numpy glossary says the sum along axis argument ` axis=1 ` sums over rows : " we can sum each row of an array , in which case we operate along columns , or axis 1 " .
I don't think ` accumulate ` is what you're looking for as it provides a running operation , so , using ` add ` it would look like : #CODE
@USER : Just to be clear , ` sum ` as used here is the numpy version and not the python version .
If you want the running totals , you could use ` add.accumulate ` or the cumulative sum ` cumsum ` , they're equivalent ( at least in output ) .
But what does the reshape ( -1 , 1 ) mean ?
You might also want to change dtype definitions wherever zeros ( ..., Float ) are used .
These are important if you are going to manually move poles and zeros in the plot .
With larger arrays , I get an error ` Exception RuntimeError : RuntimeError ( ' cannot join current thread ' , ) in ignored ` .
But it does not produce a runtime error like my test of the map version : ` Exception RuntimeError : RuntimeError ( ' cannot join current thread ' , ) in Finalize object , dead ignored `
So once I get the indices from numpy.argsort ( energies ) , how do I rearrange only the rows of states using them ?
once you have the indices which sort the array use ` np.take() ` which showed to be faster than fancy indexing ...
What is the most efficient way to reshape ` data ` to ` fencepost ` with ` numpy ` ?
It appears that ` operator() ` cannot be used when the object is stack allocated #CODE
After looking through the C++ compiler errors of examples like yours a little more , what appears to be happening is that Cython has a bug when overloading ` operator() ` for a stack allocated object .
Now , if you want to do this for a stack allocated object , you can change the Cython interface as follows :
Maybe you could add something to your answer about stack allocated objects ( if you can confirm that that is the issue ) please ?
It seems to be a bug that only applies to stack allocated objects .
However , I need to use a subindex to look up the indices for one dimension .
Find all indices into the data array and
Translate the column indices according to some other array , subindex .
The code below therefore generates indices for all array positions ( using ` np.indices `) , and reshapes it to ` ( ..., 2 )` -- a 2-D list of coordinates representing each position in the array .
For each coordinate , ` ( i , j )` , we then translate the column coordinate ` j ` using the subindex array provided , and then use that translated index as the new column index .
With numpy , it is not necessary to do that in a for-loop -- we can simply pass in all the indices at once : #CODE
are just the indices for the " y-axis " , the first dimension of ` data ` .
Ah great , I hadn't encountered broadcasting indices before .
Depending on the required output shape you might not need to do the reshape via ` [: , None ]` .
A more compact version uses the dot product between the random matrix and a vector of decimal powers : #CODE
If you want to compute a lot of basepairs , this way is about 5 times faster ( 3 seconds vs 17 seconds for 10**8 random bases ) than the one-liner that first generates random numbers and then takes the dot product .
If d is larger than 8 or 9 , then bases will be sufficiently long that you probably would be better off going with the other version using the dot product .
In your first case , numpy will treat the array of length n as row indices , so you'll get the expected result : #CODE
If you flatten the boolean mask like : #CODE
I have a matrix written in this format inside a log file : #CODE
So I decided to parse the log file this way : #CODE
is the log file produced by some application controlled by you ?
Alternatively , you could read your file's lines into a list , and just operate using indices of i , i+1 .
Just ` strip ` each line , ` join ` them together , and write to a new file or list .
I have a rather big matrix ( 500000 * 24 ) as an ndarray and I want to multiply its cell with the corresponding column min .
This will multiply with the minimum of all the array I want to multiply with the columns min
In your example , you did this after the fact by dividing ` cdf ` with ` sum ( Px )` ( I assume the comma in that line was meant to be a slash ) .
You can implement rejection sampling to give you a sample at a time by creating some number of trial x , y , z uniformly sampled : ` Uxyz = rand ( 3 , N )` and returning the first ` Uxyz [: , n ]` where ` 0.001 *PDF ( Uxyz [ 0 , n ] , Uxyz [ 1 , n ] , Uxyz [ 2 , n ]) rand ( 1 )` .
The classic example is when the PDF looks like ` exp ( -a*x**2 - b*y**2 - c*z**2 )` .
Is there a way to use the population std calculation ( ddof=0 ) with the groupby statement ?
The records I am using are not ( not the example table above ) are not samples , so I am only interested in population std deviations .
Using memory views was MUCH faster and quite easy to use , but I suspect I can squeeze even more speedup from using c-arrays .
While it can be convenient sometimes it replaces certain built-in functions like ` sum ` with numpy's versions which behave very differently ( i.e. can give the opposite results ) in some circumstances .
I guess the lesson is to not generally assume numpy will translate a dataframe or series the way you think it will .
In the above case 26 small letters , 26 capital letters and 10 digits equal to 62 total letters and let us take permutation and combinations ,
So I need to save every unique random number into nosql db .
62 letters gives more unique random numbers compare to 36 letters .
I want to broadcast a function f over a vectors so that the result is a matrix P where P [ i , j ] = f ( v [ i ] , v [ j ]) .
The reason why I want to control the number of processes sent to each node is that some of the instructions inside of ` foo ` can be multithreaded ( like ` dot ` which would also be linked to the MKL libraries ) .
where mean , cov are already defined .
this will be an error " TypeError : string indices must be integers , not tuple " .
The permutation vector needs to be interpreted in sequence .
Read elements with given indices from an array with arbitrary number of axis
The element selection is specified as an array , where each row contains the indices for one element .
Using ` where ` to find the indices of these 2 values : #CODE
I tried your example and got a ValueError : shape-mismatch for sum
Ones and zeros make for confusing examples for multiplication since is ` 1 ` : just ` 1 ` , or maybe ` 1*1 ` , or maybe ` 1*1 + 0*1 ` , etc ..
It can be simplified by removing the sqrt but does not improve speed alot .
And yes , total min distance when adding distance between all three points .
@USER That's why his second solution returns ` min_pos ` , that's just the indices in each array of the minimum point .
( IndexError : too many indices )
I was able to split out the SHAPE @USER and SHAPE @USER directly in my call line ... locxyarray = arcpy.da.FeatureClassToNumPyArray ( " Points " , [ ' SHAPE @USER ' , ' SHAPE @USER ' , ' PrimeKey ']) But the resulting array still has too many indices to be able to calculate the mean of the X or Y column ...
@USER you can use ` np.where ( locxyarray [ ' SHAPE @USER '] <= np.mean ( locxyarray [ ' SHAPE @USER ']))` to again get the indices of elements less or equal to ` mean ` value .
For large numbers of particles and modes , I'm looking for an efficient way to obtain these indices , and I figured using numpy arrays were quite efficient , but I have just hit this block and have not found a solution to it .
` pattern ` is broadcast , so ` a == pattern ` has the same size as ` a ` .
` np.where() ` is only used to return the indices where we have ` True ` values ...
Generating all possible combinations of a zeros and b ones
Is there an efficient way to generate a list ( or an array ) of all possible combinations of say 2 ones and 8 zeros ?
then concatenate the saved objects whit this code : #CODE
this is an example that concatenate ' a ' to a ` numpy ` zero array : #CODE
I have a 3x4 array ; I convert each element to a string using ` str ( i )` , and use ` join ` to concatenate the strings of a row into one longer string .
Of course it could be refined by elaborating on the ' int ' to ' string ' formatting ( ie . fixed width ) , maybe adding delimiters in the ' join ' , etc .
Also , the different functions sometimes had slight differences in precision , so some of the them didn't pass the equality tests but if you diff them , they are really close .
A change similar to this was recently added to ` np.unique ` , for the case where ` return_inverse ` indices are requested , see here .
In your case , you'd need to transpose ` a ` first , to make it an array of shape ` ( m , n )` , i.e. an iterable of length ` m ` : #CODE
Usually , because of NumPy's ability to broadcast arrays , it is not necessary to iterate over the columns of an array one-by-one .
For example , if ` a ` has shape ` ( n , m )` and ` b ` has shape ` ( m , )` then you can add ` a+b ` and ` b ` will broadcast itself to shape ` ( n , m )` automatically .
Often ( but not necessarily in your case ) , you want a relative error , along the lines of ` abs ( arr_f - a ) / max ( abs ( arr_f ) , abs ( a ))` and a value for the tolerance of perhaps ` 1E-5 ` or ` 1E-6 ` .
An alternative solution that also returns the relevant indices is as follows : #CODE
This means that the values residing in indices 27 and 28 of ` arr_f ` are within the desired range , and indeed : #CODE
It's losing me a bit to be honest , in particular what it's doing when dealing with the two arrays that are dotted ( dot producted ? ) together , but even despite that , the general goal of what this part of the code is doing is a little unclear to me .
` d ` is the matrix ( dot ) product of a 4x3 matrix with a 2x3 ( or after transpose , 3x2 ) array , resulting in a 4x2 .
` p ` is just that dot product normalized by ` v2 ` .
( I get most of the Python data stack on Ubuntu with ` sudo apt-get install python-statsmodels ` . ) For people with Windows who come to our Python meetup group and who have trouble installing things , we typically recommend getting the Python data stack distribution , Anaconda , from Continuum Analytics which will include Numpy .
Python , numpy , einsum multiply a stack of matrices
I'm curious if there is a way to multiply a stack of a stack of matrices .
Its basically a 500 length stack of ( 201 , 2 , 2 ) matrices where for each of the 500 , I want to multiply the adjacent matrices using einsum and get another ( 201 , 2 , 2 ) matrix .
If I know the first dimension of the array ( which I don't always ) then I could hard code a string for the einsum indices , but probably limited by the alphabet no ?
If I don't know the indices I would have to dynamically generate an index string , and again I'm limited by the alphabet .
The reason for all the transposes is that ` f2py ` implicitly transposes all the arrays , and we need to transpose them manually to tell it that our fortran code expects things to be transposed .
numpy 1.9.0 : ValueError : probabilities do not sum to 1
The sum of the PDF probabilities will not sum to exactly 1 . given the small deviations that appear when using very small floats .
python lambda , numpy help to find sum of time
Also how to sum the ' 1's and ' 0's and then subtract them .
I would say that's often true , but i really don't like the way that ` numpy ` reuses ` np.dot() ` for matrix multiplication , and sum products as well as just the dot product .
How does dot obscure what is actually happening ?
Looking at the code for ` np.full ` - it creates an ` empty ` , and uses ` copyto ` broadcast the value to array .
When i am trying to read them and plot specific column against log of 10 power .
Order the product names by the sum of the costs
Of course I could separate real and imaginary parts and sum them separately , but is there a built-in ( thus faster ) function already doing it ?
Essentially I want the code to go through the list of indices I have and find the index of the lowest value .
then concatenate the saved objects whit this code : #CODE
How to transpose ( stack ) arbitrary columns in Dataframe ?
Now I want to transpose ( stack ) columns ' 2010 ' , ' 2011 ' and ' 2012 ' into rows to be able to get : #CODE
By using ` df.stack() ` pandas " stacks " all columns into rows , while I want to stack just those pointed .
So my qestion is how to transpose arbitrary columns to rows in pandas Dataframe ?
How can I " flatten " this 2D array from this data structure ?
Use ` np.vstack ` to stack a list / array / whatever of rows : #CODE
When I use column_stack to concatenate numpy arrays the dtype gets converted ( usually to float ) #CODE
You can stack two arrays with ` numpy.lib.recfunctions ` method and preserve the type with it : #CODE
I need to calculate the mean along axis=0 , but before I can do that , I have to roll the data along axis 2 , until it is ' in the right place ' .
And my 3D-arrays ` data ` of shape ( 1000 , 800 , 1024 ) that I need to roll according to ` pos ` .
In C one can write code which is doing this pretty simple so I wonder if I can kind of , ' tell ' the numpy mean() or sum() routines they should start at different indices and ' roll around ' at the end of the 1D-subarray .
Any chance you can roll the 1024-long array when you read it from the circular buffer , before putting it in ` data ` ?
in numpy it will be something like : ` x = rand ( H.shape ); H [ x > =p ]= 0 `
I have a 1GB log file ( .txt ) in the following format , #CODE
If so , you won't need to keep the entire log in memory .
Have you tested how long it takes if you remove those ` append ` lines and just read the lines and ` findall ` ?
I tried things like calling ` mean ` or ` median ` on the summary table , but I end up with a Series rather than a row I can concatenate to the summary table .
The summary rows I want are sort of like pivot_table margins , but the aggregation function is not ` sum ` .
I don't see ` polyfit ` returns any metrics ( i.e. min square error value ) .
That does not apply for reshape ( not a.reshape ) - that is a function you've imported from numpy nodule in ` from numpy import * ` .
I'm looking for the best way to repeat ` a [ i ]` exactly ` s [ i ]` times and then have a flatten array in the form of ` b = [ 0.1 , 0.1 , 0.1 , 0.2 , 0.2 , 0.2 , 0.3 , 0.3 , 0.3 , 0.3 , ... ]` .
I searched around and find out about ` repeat ` , ` tile ` and ` column_stack ` which can be used nicely to repeat each element ` n ` times but I wanted to repeat each of them different times .
The ` -- ffree-line-length-none ` flag is forwarded to the compiler , and will instruct gfortran to never truncate the lines in the source code .
you can use ` sum ` function for sum the numbers before ` v [ i ]` : #CODE
But it does not work with a function like f ( v [ i ]) = v [ i-1 ] + v [ i ] + v [ i+1 ] , since accumulate needs only function with two arguments .
counting the unique items in a numpy array : why is scipy.stats.itemfreq so slow ?
I'm trying to count the unique values in a numpy array .
Why is the mean larger than the max in this array ?
Also , where are your ` min ` , ` mean ` , and ` max ` functions coming from ?
Look at pv .
This is so I can treat them as scalars and dot this with another array , like so : #CODE
I am trying to change the gamma value of of a video widget ( Phonon.VideoWidget ) displayed in a QGraphicsView via a proxy .
Phonon.VideoWidget itself has options for brightness , contrast , and even hue that work great but no options for gamma correction strangely .
I started with something simpler , a single QPixmap ( I take the QImage from it . Since QGraphicsEffect seems to work primarily with QPixmaps in its virtual draw function I figure this is a good place to start ) and gamma correcting that .
The way I do it is I create a list of gamma converted values from 0-255 based on the gamma value entered .
I don't know how to work with C++ and compiling for PySide / Python and I'm not sure if even if I could that I can even translate this to C++ especially when it comes to the idea of dealing with pointers and sharing between Python and C++ .
@USER using KDTree is [ ` O ( n log n )`] ( #URL )
Starting with your data , the first thing is to put in a complete format , where the missing values are filled by zeros , then transform it to arrays and create the ` KDTree ` ( here I am using a ` cKDTree ` which showed to be faster ): #CODE
where ` ind ` contains the indices of ` a1 ` that will be closest to each point in ` a2 ` , and ` dist ` their respective distances .
@USER check the edit ... now the unique keys are found automatically ...
You first reshape the array to ` nx1 ` .
When you subtract a 1D array , they are both broadcast to ` nxn ` : #CODE
sorry , missed the " they are both broadcast to nxn " .
How to store numpy.array in sqlite3 to take benefit of the sum function ?
I am trying to use sqlite3 to compute the average of a numpy.array and I would like to take advantage of the sum function .
but unfortunately the request output does not give me the sum of the arrays .
Edit2 : in other words what I am trying to do is to redefine the sum function to the particular kind of data I am using .
That's what was done to compress / uncompress the numpy.array .
find maximum value in array with excluded indices
I also have a list of indices that I don't want to include when looking for the maximum value .
I'm trying to fit a histogram with some data in it using ` scipy.optimize.curve_fit ` .
That is , for ID4-ID7 , column B is filled with ones ( given the initial 1 in column A @ ID3 ) .
Next , from ID10-ID14 is filled with ones ( since column A @ ID9 =1 ) .
This is tricky , I can't think of way to do this without some iteration , you could get the indices of these markers using something like ` mask = df.loc [( df [ ' A '] .shift() == 1 ) | ( df [ ' A '] == -1 )]` then collapse this again using ` mask.loc [( mask [ ' A '] == -1 ) | ( mask [ ' A '] .shift ( -1 ) ! = -1 )]` which should then display the start and end indices and then iterate over or pull the indices into a list of tuple pairs where the pair has beg , end and set these to 1 .
But that seems quite vague to me ; in the above case I don't understand how numpy decides about the final size of the output array based on the order of indices ( which apparently changes ) .
It doesn't say how the forcing takes place when one puts the index on the right side of arrow or how the sizes should change when the order of the indices change on the left side of the arrow ...
Here the repetition of the indices in all parts , including output , is interpreted as element by element multiplication .
Again no summation because the same indices appear on left and right sides .
In effect an outer product .
On the left side of the statement , repeated indices indicate which dimensions are multiplied .
The goal is the create an ` nditer ` with which it does a sum of products calculation .
By way of example , consider a generalization of the ` dot ` , one that multiplies the last dimension of ` A ` with the 2nd to the last of ` B ` .
ik ` , ellipsis plus the non-summing indices .
That is , I can't tell it to fill in indices for ` A ` , followed by different ones for ` B ` .
` ji ` means , effect use the transpose of ` b ` .
` ij ` are the row and col indices of ` A ` .
` jk ` are the row and col indices of ` B ` .
Or should I try to log a bug ?
Mapping values from a joint histogram back into the image spaces
I computed the joint histogram between them and found a few interesting clusters for which I want to map back the values in image space to locate where this corresponds .
Your histogram might be easier to think of as a crossplot .
How to compress numpy array for mongodb
Any suggestions on how to compress the numpy 2d array , or save it in a more memory efficient manner ?
I'm trying to implement a discrete bayes filter ( i.e. histogram filter ) for robot localization as described in ' Probabilistic Robotics ' by Thrun , Burgard , and Fox .
I'm looping over indices of my probability density map which seems very inefficient , but I don't see how to vectorize this algorithm correctly .
I'm keeping a cumulative sum at position i , j .
MATLAB also gamma corrects the images where OpenCV doesn't .
Recall that ` KL ( p || q ) = \int p ( x ) log ( p ( x ) / q ( x )) dx = E_p [ log ( p ( x ) / q ( x ))` .
( ` mean ( log ( p ( x ) / q ( x ))) = mean ( log ( p ( x )) - log ( q ( x ))) = mean ( log ( p ( x ))) - mean ( log ( q ( x )))` is somewhat cheaper computationally .
( ` log_mix_X ` / ` log_mix_Y ` are actually the log of twice the mixture densities ; pulling that out of the mean operation saves some flops . )
This link describes how to do so on a 3d dataset , using svd .
I tried it on a joint histogram image , so a 256x256 np array with dtype float64 .
By subtraction 1 , these are the indices of our array ` b ` #CODE
EDIT : If your array is 2D , one way would be to use the ` in1d ` method and then reshape : #CODE
I think this would take ` A^T * B ` , but I'm not sure ( it's taking the transpose of one of them right ? ) .
Repeated subscripts labels in one operand take the diagonal .
This means that ` np.einsum ( ' ij ' , a )` doesn t affect a 2D array , while ` np.einsum ( ' ji ' , a )` takes its transpose .
To take the trace along the first and last axes , you can do ` np.einsum ( ' i ... i ' , a )` , or to do a matrix-matrix product with the left-most indices instead of rightmost , you can do ` np.einsum ( ' ij ..., jk ... - ik ... ' , a , b )` .
Thus , taking the diagonal as ` np.einsum ( ' ii- i ' , a )` produces a view .
( ` b ` will be broadcast along ( ? ) the first axis ) #CODE
` j ` is absent from the right-hand-side so we sum over ` j ` which is the second axis of the 3x3x3 array #CODE
Finally , the indices are ( alphabetically ) reversed on the right-hand-side so we transpose .
Your calculation , takes a ' dot ' ( sum of products ) of a ( 2 , 3 ) with a ( 3 , 4 ) to produce a ( 4 , 2 ) array .
` 0 + 4 + 16 = 20 ` , ` 9 + 28 + 55 = 92 ` , etc ; Sum on ` j ` and transpose to get the earlier result : #CODE
sum this new array along particular axes ; and then maybe
transpose the axes of the new array in a particular order .
There's a good chance that ` einsum ` will help us do this faster and more memory-efficiently that combinations of the NumPy functions like ` multiply ` , ` sum ` and ` transpose ` will allow .
We will multiply ` A ` and ` B ` element-wise and then sum along the rows of the new array .
So here , the indexing operation on ` A ` lines up the first axes of the two arrays so that the multiplication can be broadcast .
By omitting the label , we're telling ` einsum ` to sum along this axis .
To explain the dot product , here are two new arrays : #CODE
We will compute the dot product using ` np.einsum ( ' ij , jk- ik ' , A , B )` .
Summing axis ` j ` gives the expected dot product , shown on the right .
numpy multiplication : can not broadcast
Use the function ` np.dot ` or the ` dot ` method for matrix multiplication .
I managed to use resize ( 1420,820 ) to get it to work on my screen but I was wondering it was possible to use a keyword e.g. " maximized " to do it on any screen size .
Cartesian product to get set of indices to point to unique elements in NumPy array
Whats a good way to get combinations of indices that points to unique elements in array .
I can use ` argsort ` in combination with splitting the elements by frequency to then use something like ` itertools.product ` to get all sets of indices I want .
This does get you the indices , but possibly not in the format you want : #CODE
@USER : I don't understand that suggestion ; ` a [ a == x ]` gives the * values * from ` a ` , not their indices .
First , extract the inverse indices and counts of each unique item .
In the simple case of a 1D matrix , one can simply shuffle over all indices with non-NaN values using the Fisher Yates algorithm : #CODE
I would like to extend this algorithm to handle large multidimensional arrays without reshape ( which triggers a copy for more complicated cases not considered here ) .
` np.ndindex() ` is used to run throught the different multidimensional indices , excluding the one belonging to the axis you want to shuffle
In short : I used a pivot table to transpose my matrix and then I was able to plot the single cols automaticly , at example 2 here .
You need to repeat the indices you supply to ` np.insert() ` : #CODE
My results with the mock data with the sin functions suggested in the comments below are here :
Here's a histogram of the times ( code below ) .
A simple ` a.dot ( b )` returns the dot product for every pair of the first axis ( so it returns an array of shape ` ( K , N , K , N )` .
No , as explained , ` a.dot ( b )` returns an array that is of shape ` ( K , N , K , N )` - that is , the dot product for every pair of the first axes .
In stack i found this command , but i can t apply a second condition to get my specific array ...
Have you even looked at the [ getting numpy ] ( #URL ) and [ installing the stack ] ( #URL ) docs ?
In norm.ppf ( probability , mean , standard deviation ) so 10 is mean and 5 is std .
Presumably , ` norm ` is ` scipy.stats.norm ` .
Explanation for people who didn't get , lts = norm.ppf ( np.random.random ( 4000 ) , 10 , 5 ) .astype ( ' int ') will create 4000 random variable and calculate norm .
If ` A ` and ` B ` are two arrays corresponding to two orderings of the same ( distinct ) elements , there is a unique indexing array ` P ` such that ` A [ P ]` is equal to ` B ` .
Fast 1D convolution with finite filter and sum of dirac deltas in python
Moreover , since n b , it still holds that O ( d*n ) is much less than O ( b * log b ) for fft based convolution .
Convolutions with large box filters can be speed up using a cumulative sum of the signal :
Convolution using the cumulative sum : #CODE
Neat , why do you have to concatenate 0 to the array a ?
And when I check my log file , this is the line I get : #CODE
Also , you can always aggregate all the data and do the histogram only afterwards , in one go .
Seriously what @USER said : averaging ` N ` histograms of ` N ` different datasets is essentially finding a single histogram of all ` N ` datasets concatenated together ( up to a factor of ` N ` for ` meanFreq ` ; ` meanBins ` will be the same ) .
So just store the eigenvalues in a single array and histogram them at the end .
Also , don't generate the histogram twice just to get the two outputs !
If you insist on computing 100 histograms and then averaging them , I would add a check in the loop to make sure all eigenvalues are strictly inside the ` ( b [ 0 ] , b [ -1 ])` interval , because if you have some huge eigenvalues , the right-most bin is going to get unnaturally saturated and skew your final averaged histogram ; in that case , the program should quit and restart with more bins .
Is there a way to normalize the histogram so that the total eigenvalue frequency is 1 ?
A good example of vectorised speed-up can be seen in cases , where calculation strategy can be parallelised / broadcast along 1D , 2D or even 3D structure of the native numpy array .
Another set of speed-up figures may be seen on cases , where vectorisation potential goes on max .
First I would try to compute only half of the diagonal , starting the inner loop in the ` i ` point and avoiding the calculation of previous alignments : #CODE
If you need to find the maximum from this list of maximums , you can use the built-in function ` max ` .
Just load the max values for all arrays into a list and find the biggest value in the list .
The reason for the difference in the outcome of the different approaches is an accumulated rounding error that is greater during the sum divide computation .
We shall do it by invoking the sum divide approach twice , but each time with a different precision : #CODE
This improves the precision of the sum divide approach , but doesn't affect that of the rolling mean approach : #CODE
I was able to get the more precise value in my previous environment by doing the incremental update to cumulative mean instead of taking a batch sum and divide .
Instantiate a matrix with x zeros and the rest ones
I would like to be able to quickly instantiate a matrix where the first few ( variable number of ) cells in a row are 0 , and the rest are ones .
I have instantiated the matrix first as all ones : #CODE
Then imagine we have an array that announces how many leading zeros there are : #CODE
Obviously this can be done in the opposite way as well , but I'd consider the approach where 1 is a default value , and zeros would be replaced .
First , how to create a row with ` m ` zeros and ` n-m ` ones ?
As @USER points out in his comment , this is basically an outer product of the arguments , using the ` = ` operation .
To add some explanation , the pattern ` a [ np.newaxis , :] op b [: , np.newaxis ]` is a general way to perform an outer operation on numpy arrays due to [ broadcasting ] ( #URL ) .
np.arange followed by reshape
I often follow ` arange ` with a ` reshape ` to generate test arrays , e.g. #CODE
Numpy histogram output to a file
For the below dot product code I'm getting fairly different results between the GPU and CPU versions .
As a data point , you can implement a very simple dot product in raw Python and see how much that differs from both the OpenCL result and from Numpy .
One explanation for this could be that Numpy's ` dot ` function probably makes use of fused multiply-add ( FMA ) operations , which will change how intermediate results are rounded .
MATLAB : 1.4725e +04 ( 3.8913e-14 diff )
Specifically , I want to get the average and sum ` amount ` s by tuples of [ ` origin ` and ` type `] .
Also , to start with , rather than ` pd.Series.sum ` - just use `' sum '` - the code should take a faster path .
Likewise , when constructing arrays , each element is considered a row , so after construction , we need to transpose it so the lists we feed the array construct are transformed to columns .
Thanks Roger , the function I wanted to use was a bit more complex than just taking the sum , so I might end up using the list comprehension solution .
according to the example , the append is working fine since it's appending . what do you expect to happen ?
To solve this problem you can just transpose your data using ` .T ` , i.e. #CODE
When i have obtained the 2 masked arrays I want to combine it ( sum the 2 results ) in a same dimension array ... but the masks " annihilate " the results of the other part array .
In other words , i would like to stop the effect of mask when i sum the 2 results .
Do you want to completely get rid of the mask once you sum the results ( and after that ) ?
Yes i would like to transform my masked values in 0 in order to sum the 2 arrays , it could be the easiest way couldn't it ?
I have an algorithm that iterates through all of the nonzero values of a matrix which looks something like this : #CODE
Therefore , I do not want to iterate through all the nonzero triplets ` ( row , col , val )` in the matrix for which ` vec [ row ] !
However , the code runs slowly in the case when all values of ` vec ` are nonzero , and that is a case I am not allowed to ignore ( for example , if ` vec= np.ones ( len ( matrix.data ))` .
The original code now still runs about 20 percent faster than the modified code if ` vec ` has no nonzero elements , but if ` vec ` is sparse , this code quicker by 2 to 3 orders of magnitude .
I involves to stack the arrays before performing the t-test : #CODE
I have a 2D NumPy array ` ( N , D )` where each row has a unique index ( a non-negative integer ) .
The indices of the rows are increasing .
For example , the indices of my ` ( 4 , D )` array could be ` ( 10 , 20 , 21 , 30 )` .
Select rows from their indices : ` arr [ 21 ]` instead of ` arr [ 2 ]` ( i.e. absolute rather than relative indexing )
Convert an arbitrary list of absolute indices to relative indices , and conversely .
Creating a class deriving from ` ndarray ` and overriding indexing such that the absolute indices are used .
And , when you say the indices could be ` ( 10 , 20 , 21 , 30 )` , do you mean that is the shape of your array ?
I forgot to say : my arrays are quite big ( N can have 100 million elements , D may be between 1 and 1000 ) , and I may need to convert millions of indices between relative and absolute coordinates .
Simply check if the number if unique items in the array are 1 : #CODE
This might be faster than identifying unique rows for larger arrays because it avoids the large number of comparisons needed .
Note : #URL is about FINDING the unique row , OP is about TESTING if the rows are all unique .
Finding Unique rows would essentially be the same thing as seeing if each row is unique .
You can calculate the correlation matrix and ask if only the diagonal elements are ` 1 ` : #CODE
Fiter out the unique rows and then test if the resultant is same as ` M ` is an overkill : #CODE
grayscale gradient with skimage or numpy
I have to create a linear grayscale gradient , with black shade on top and white shade at the bottom .
I've found on scikit the code for a color linear gradient that goes horizontally instead of vertically here : #URL .
Let's say we wanted to create a 100x100 gradient image .
First , we use ` np.linspace ` to construct the gradient values , 100 values between 0 and 1 .
We then repeat this vector 100 times in the vertical direction ( using ` np.tile `) to form the gradient image .
At this stage , the image is a gradient from left to right , so we use the transpose to flip it to be oriented up-down .
Maybe it is me not understanding how ` tile ` works ...
You'll need to make ` a ` a 1x8x1 array and then call ` tile ` .
As the documentation for ` tile ` notes :
Elegant expression for row-wise dot product of two matrices
I have two 2-d numpy arrays with the same dimensions , A and B , and am trying to calculate the row-wise dot product of them .
Is there another way to do this so that numpy is doing the row-wise dot product in one step rather than two ?
[ Note that " matrix " is a technical word in numpy-speak , and differs from " array " : if A and B are arrays , not matrices , then ` A*B ` is an elementwise product , not a dot product . ]
Basically , you'd need to wrap this particular use of ` einsum ` with this particular string into a stand alone helper function whose name an documentation made it clear that it was this unusual row-wise dot product .
So you could say , ` x = np.eval ( ' g = sin ( y ); x = cos ( g ); ')` and it would look up ` y ` in the local namespace , apply the functions , and return whatever ` x ` got bound to within the string context back into the namespace as the variable named ` x ` .
Then suppose someone accidentally typed ` sign ` instead of ` sin ` .
And maybe you didn't know they wanted ` sin ` instead of ` sign ` .
` sum ( stats.norm.pdf ( 50 , loc=arange ( 0 , 100 , 0.01 ) , scale = 1 )) * 0.01 ` -> 1
For example , I know I can do the max by : #CODE
How can I find the indices in a numpy array that meet multiple conditions ?
I want to find all ` [ row , col ]` indices in ` scores ` where the value is :
Sign on results of fft
The result that you're expecting probably corresponds to ` data = zeros ( 20 ); data [ -1 ] = 10 ; data [: 2 ] = 10 ` , i.e. a source signal that's symmetric about index 0 ( the DFT assumes the source is periodic ) .
Consider the signal component at the mth frequency : ` s ( m , n ) = A_m * cos ( pi*m / 10*n )` .
Delay this by 10 samples : ` s ( m , n-10 ) = A_m * cos ( pi*m / 10*n - pi*m )` .
So the mth frequency component is shifted by ` pi*m ` radians , i.e. multiply the DFT vector by ` exp ( 1j*pi*m ) == [ 1 , -1 , 1 , -1 , ... ]` .
If you do this , you'll find a pretty wide sinc , as would be expeceted for a short duration pulse on so few samples .
first is input Real domain signal ( Im=0 ) single finite nonzero width pulse
Centering on index 0 gives the sinc the OP is probably expecting , instead of the ` ( -1 ) **n ` modulated result , but the coefficients aren't all positive .
It's a sinc !
This scaling results in the real-valued sinc you're expecting .
Is there a way to slice the array below without having to define the row indices i.e. not having to write ` range ( len ( X ))` ?
You take the diagonal elements of ` X [: , L ]` using ` diag ` ( or ` diagonal ` ): #CODE
And you want the diagonal elements :
This is the one I was trying to work out earlier but didn't get the transpose right so it didn't work .
But when I run it I get the error : " could not broadcast input array from shape ( 2 , 2 ) into shape ( 2 )" .
ValueError : could not broadcast input array from shape ( 2 , 2 ) into shape ( 2 )
And then use this to filter out the unwanted ( ` False `) values from ` A ` , reshape , and then rebind to the variable name ` A ` : #CODE
This should be quicker than using ` concatenate ` because it avoids copying arrays for each iteration of the Python ` for ` loop .
There are however simpler ways of doing the same thing if your filters are within the matrix , notably through ` numpy argsort ` and ` numpy roll ` over an axis .
First you roll axes until your axes until you've ordered your filters as first columns , then you sort on them and slice the array vertically to get the rest of the matrix .
An alternative implementation would be to transform the indexing matrix into a string matrix , sum row-wise , get an argsort over the now unique indexing column and split as above .
For conveniece , it might be more interesting to first roll the indexing matrix until they are all in the beginning of the matrix , so that the sorting done above is clear .
Given an ` N x M x D ` matrix ` A ` and a matrix ` I ` that contains a list of indices .
I have to fill a zeros matrix ` ACopy ` with the sum of element of ` A ` according to the indeces found in ` I ` ( see code ) .
How to broadcast a numpy 2D-array to a 6D-array
Average Value is a 2D-array , I need the ` fn() ` to generate a 6D-array , i.e. to broadcast the 2D-result over the other 4-dimensions
For some reason , when I used the line of code below I get the Error ` RuntimeWarning : divide by zero encountered in log print np.log ( 69 / 74 )` .
If the argument array ` A ` is not a vector then I can transpose it nicely using ` A.T ` however if ` A ` is a row vector this will NOT turn ` A ` into a column vector .
Is there a way to transpose an array independently of its shape ?
The dot Product of a column vector with a scalar is a column vector ( yeahh ! ) .
The dot Product of a column vector with a 1 element numpy array is a row vector ( nayyy ) .
Now you can see that when you transpose these two , ` a ` stays the same , but ` b ` becomes a column vector : #CODE
Quick tip : You can avoid having to use a dot ` .
Now map 1 to a new matrix if the element is max , else 0 : #CODE
I've been running into a few problems using cv to display images from numpy matrices when I transpose them .
` onesT ` uses the Fortran order because the transpose of a 2-d array is implemented in numpy by simply swapping the " strides " for each dimension , without actually copying the array of values in memory .
( This makes the transpose operation very efficient . )
But if I use inner it returns not inner product but like outer #CODE
Why , what on earth inner , and dot for 1-D array ?
` = sum ( a [ i0 ,..., ir-1 , :] *b [ j0 ,..., js-1 , :]) `
I'm trying to use a for-loop to stack 6 different images one on top of another to create a 3D stack.I ' m new to Python ... and I am not able to figure this out .
How can I create the stack and how can I access each image in the stack later ?
01 / 01 / 2012 w_avg = 0.5 * ( 60 sum ( 60,80,100 )) + .75 * ( 80 sum ( 60,80,100 )) + 1.0 * ( 100 / sum ( 60,80,100 ))
01 / 02 / 2012 w_avg = 0.5 * ( 100 sum ( 100 , 80 )) + 1.0 * ( 80 sum ( 100 , 80 ))
Now the sum of this column is the desired : #CODE
If you're only interested in the overall total , you could also separately sum the real and imaginary parts and combine them in the end .
You could decode then you can access the keys without using ` b " 5 "` etc ..
If you really want strings just use the ` decode ` method I showed you
If you were to choose one of the following three ways of initializing an array with zeros which one would you choose and why ?
" full " and assignment are identical , though of course I like to mention that I prefer ` a [ ... ] = 0 ` instead of ` a [: ]` . zeros nowadays tells the kernel to zero the memory .
Optimize NumPy sum of matrices iterated through every element
Accounting for errors when creating a histogram
I'd like to generate a 2D histogram of these ` N ` points , accounting not only for the weights but also for the errors , which would cause each point to be spread possibly among many bins if the error values are large enough ( assuming a standard Gaussian distribution for the errors , although other distributions could perhaps be considered ) .
Unlike the standard normal case you assumed ( which corresponds to all e_x and e_y being equal to 1.0 ) , you have covariance matrices where the diagonal can have distinct values .
However , the resulting distribution is not a normal distribution - you'll see , after running the example , how the histogram doesn't resemble a Gaussian at all , but instead a group of them .
To create a histogram out of this set of distributions , one way I see is by generating random samples out of each point using numpy.random.multivariate_normal .
As @USER S has pointed out , ` pairtime [ pair ]` contains only the sum of the times , and not the complete series .
Since you will calculate many statistics from the time series , I believe a better approach would be to keep the whole time series instead of just the sum as @USER S did in his answer .
This gives me the sum of total pairtime .
Your main problem is you're passing in a single floating point into the " median " function ( pairtime [ pair ] contains the sum of the 3rd column values for the given c1 , c2 pair ) .
RuntimeWarning : overflow encountered in exp in computing the logistic function
You should compute the logistic function using either ` scipy.special.expit ` , which in recent enough SciPy is more stable than your solution ( although earlier versions got it wrong ) , or by reducing it to ` tanh ` : #CODE
I have implemented this gradient descent in Numpy : #CODE
ValueError : Shape of passed values is ( 4 , 35047 ) , indices imply ( 1 ,
plotting a histogram of a numpy array by timestamp
I have managed to plot a histogram of the whole second column with ` pyplot.hist ( array [: , 1 ]); pyplot.show() ` .
But what I really want to do , is to bin ` array [: , 1 ]` by day ( as derived by the unix timestamps in array [: , 0 ]) , and plot these as a stacked histogram , with each ( colored ) stack representing a day .
( Note , I come from a Fortran background where the indices start at 1 as well , and after a short amount of time , the differences are no big deal ... )
Where array is a numpy is a 512x 512,512 numpy array of zeros and ones .
I really want the ` ndarray ` functionality , like dot product , matrix operations and all the other numpy extras .
What's the ` dot ` product involving your ' point ' ?
I need to compute the transpose of the matrix
I need to compute the dot product with a x_dimensional vector
I was reading several posts on stack overflow and the rest of the internet ;) I found PyTables , which isn't really made for matrix computations ... etc ..
( EDIT : It turns out this first step -- flattening ` A ` -- is not necessary . As @USER points out in his answer , ` np.insert ` will flatten the array by default . )
` np.ravel_multi_index ` is used to convert the desired 2-d positions into the indices into the flattened array .
Now just reshape that to get the final result : #CODE
I have just started experimenting with cython and as a first exercise I created the following ( re ) implementation of a function computing the sin for each element of an array .
Note that the point of this exercise is to not to rewrite a faster sin function but rather to create some cython wrappers for some of our internal tools but that's another issue for later ...
If cython is still slower then numpy probably uses a faster sin function than the standard c one ( you can get much faster sin approximations , try googling it if interested ) .
While I'm rebuilding some packages in the background , none of the current ones are python modules , so I can only blame the ` -- pylab inline ` switch for my ipython , which I just removed .
I like this method because the " logspace " function name makes it clear that I'm going for a range with log ( as opposed to linear ) spacing .
and let's say I want to resort the last dimension according to the following indices of the old array : #CODE
Numpy : vector of indices and value
I guess I just reshape the result ?
This sequence produces a ( 3 , n*m ) array with the indices and values #CODE
FWIW if time still ends up a bottleneck you can broadcast ` searchsorted ` : ` numpy.searchsorted ( odds , const*odds )` .
If I get you right : Having a list [ start , end ) , you want to find all indices y
First , you'll need to convert your indices into pandas ` date_range ` format and then use the custom offset functions available to series / dataframes indexed with that class .
It appears from the log message : #CODE
There are tools like GParted which can resize partitions without re-partitioning , but it can take a long time to complete for a large hard drive .
If you have a Pandas Series , use it's ` unique ` method instead of ` set ` to find unique values : #CODE
@USER I obtained incorrect results because I am comparing the results with the correct ones obtained with a for loop .
Your for-loop example fails for me , because concatenate gives 6 rows but A2 only expects 5 rows .
The next epiphany was that when you're inserting into ` linear ` , you need to offset the indexes in each row because as you insert elements the subsequent ones are moved back .
There might be some combination of dimensions where it is better , but not the ones that I've tried .
Suppose ` arrayA ` and ` arrayB ` have been initialized , but ` arrayB ` has some zeros .
This is usually done , by keeping the linear colormap and some tricks ( either by plotting ` log ( array )` and replacing the labels ` x ` on the colorbar by ` 10^x ` , or by explicitly changing the normalization behavior of the plot command ) .
You don't want to compute the inner product ` dot ` , but the outer product ` outer ` .
It might get rather complicated , since the handy function ` outer ` always flattens the input arrays ...
You can use something like ` sympy.sin ( 2 ) .evalf ( 100 )` to get 100 digits of sin ( 2 ) for instance .
You need the [ 0 ] after the ` np.where ` since ` np.where ` outputs a tuple of array ( s ) ( array ([ ]) , ) containing indices , but you just want the array of indices for ` np.take ` .
But you can notice that the elements are ravelled , count ones in each mask to
It should be very easy to implement in C , but slow to implement in Python because this is a case where we do not want to broadcast , so it would require an explicit loop .
For every sample , get the sum of spend within the interval .
I would like re-sample my original dataframe at the top of the post on the locations defined by ` my_new_timestamps ` by getting the sum of ` money_spent ` .
` resample ( ' 2.5S ' , how= ' sum ' , label= ' right ')`
It would also sample on ` 2.5s ` intervals that are different from the ones I want ( those defined by ` my_new_timestamps `) .
pls post what you are expecting , as I suppose `` df.resample ( ' 2500L ' , how= ' sum ')`` doesn't work for you ?
the final output would be a stacked bar chart , with each segment of the stack ( on the ordinate ) representing the count() of values in each bin , and the abscissa showing time values ( e.g. days ) .
I tried to transpose the matrix , but that led to nowhere .
If you are grouping by both columns , a natural way to compute the histogram would be to use ` numpy.histogram2d ` or , to plot it , [ ` plt.hist2d `] ( #URL ) .
I have written this script in python using NumPy to create an array of arrays by line but am getting " TypeError : list indices must be integers , not tuple "
Numpy : increment elements of an array given the indices required to increment
I have been following this question : Increment given indices in a matrix , which addresses producing an array with increments at indices given by 2 dimensional coordinates .
I'm not 100% sure if this is what you wanted ; another way to make the code run is to specify ` mode= ' clip '` or ` mode= ' wrap '` in ` ravel_multi_index() ` , which does something a bit different and I'm guessing is less correct .
The extra dimension is added so that the comparison to ` np.arange ( A.max() +1 )` will broadcast to each element , giving a result with shape ` ( 3 , 4 , A.max() +1 )` .
Then sum .
IIUC , you could use ` dot ` , I think : #CODE
You can sometimes squeeze out a bit more performance by dropping to bare numpy ( although then you have to be more careful to make sure that your indices are aligned ): #CODE
In terms of color , in order for the colormap to be scaled properly it is probably best in this case to set your limits , ` vmin ` and ` vmax ` because when automatically set , it will use the min and max of Z , but in this case , they are both ` nan ` , so you could use ` np.nanmin ` and ` np.nanmax ` .
I tried max ( Z ) and I get this : " ValueError : The truth value of an array with more than one element is ambiguous .
But max ( Z ) is not returning NaN for me .
What code did you use to see that max ( Z ) = NaN ?
( a ) the problem there is that you are using the builtin ` max ` on a ` ndarray ` , what you want to use is either ` np.max ( Z )` or ` Z.max() ` .
Ah , sorry , originally I was playing around with ` norm ` , but ended up not needing it .
It would be useful if you wanted log scaling of the colors for example , ` norm= colors.LogNorm() ` .
The ` transpose ` operation returns a view of the array , which means that no new array is allocated .
I have a number of arrays that I wish to broadcast into a single array using addition , which I know can be simply done such that : #CODE
Using a loop I therefore append the arrays I do have into a list , so that I end up with something like : #CODE
Why not just use ` sum ` ?
But if they are all guaranteed to be ndarrays , then yes , absolutely , ` sum ` is the best option .
I've also realised why this meant it would continually roll , it needed to reach 10 points in the bank before it can bank any points , this is obviously an impossible condition to satisfy , so it would keep rolling forever .
I know that I can reshape the array to a 100 x 2 array of grid points : #CODE
You're pretty close already ; instead of printing True , you could just append the points to a list .
For example , if I create a zeros array : #CODE
` numpy.savetxt ` has a ` fmt ` argument that defaults to `' % .18e '` , which formats each of your zeros as ` 0.000000000000000000e +00 ` .
It's also very important , that all zeros in this file all need 1 or 2 bytes , depending on the encoding .
outer product of vector with each column of matrix as : #CODE
I need sum each of the matrix here i'e first 40 columns with another next 40 column and so on .....
A bit of reshaping is all you need to be able to ` sum ` over the axis : #CODE
The idea of a gufunc is that it can broadcast over the arguments , but suppose I want to pass in two different scalars [ 3 , 4 ] -- the output shapes can't be ( 2 , n , 3 ) , ( 2 , 3 , m ) and ( 2 , n , 4 ) , ( 2 , 4 , m ) simultaneously .
It's because you have a dot instead of a comma , leaving off the other three : #CODE
Generate an empty dataframe with all zeros #CODE
The map is there because I needed to broadcast the df row into the new dataframe at several rows ( not just once ) .
Error : ` ValueError : operands could not be broadcast together with shapes ( 3 , ) ( 3 , 5 )` .
I got again the same error : ` ValueError : operands could not be broadcast together with shapes ( 3 , ) ( 3 , 5 )`
Filter and sum different columns in a numpy array
I have a large numpy array ` data ` that I wish to filter by one column ` [: , 8 ] = radius ` and get the sum of a different column ` [: , 7 ]`
So just slice with those and take the sum .
I find it helps to write the ` einsum ` version first -- after you see the indices you can often recognize that there's a simpler version .
Also note that since ( AB ) T = B T A T , and therefore ( by transposing both sides ) (( AB ) T ) T = AB = ( B T A T ) T you can make a similar statement about the rows of the first matrix being batch- ( left- ) multiplied by the transpose of the second matrix , with the result vectors being the rows of the matrix product .
it is like we are computing the scalar dot product of the columns of the two matrix .
You can do simple multiplication first and then sum over axis=0 : #CODE
` abs ( x )` gives the absolute value of a number which is always positive .
In addition , it's not unlikely that the elements of a row or column are indeed shuffled , but you'll end up with ones and zeros in the exact same place anyway ( because there's only a limited number of unique permutations ) .
The statistics should include things like mean , std , min , max , etc .
This is wrong . some should be integers , but I can't figure out how to change only the numeric ones .
The dot product in the example does not seem to benefit from it .
Or would that basically require implementing the outer loop in Cython ?
The reason I use many threads is that I implemented an outer function in Python , and then started optimizing subfunctions in Cython ( so the outer function creates those threads ) .
However I could implement the outer function in Cython if that would have clear advantages .
@USER there is a bit advantage of using the outer function in Cython if you are going to call multiple times a function like that ` dot() ` example IF you declare it as t ` cdef ` function ... in this way it has the overhead of a C function call , avoiding all the overhead that comes with Python's API that you carry when using a ` def ` function ...
Let's say I have a list of random variables , and I use the histogram function like this : #CODE
1 ) How do I efficiently normalize this histogram to become a list representations of a probability mass function ?
The intention is that at run time x will be a simple vector of indices of the form #CODE
So to index self.W I could either update Theano or loop through all of my indices adding them to self.output each at a time .
You could use ` np.savetxt ` if you flatten the array and include the array shape in the header so its the first line ) .
That way you know how to reshape the array when you open it in some other program : #CODE
To import the data and reshape it , you could do the following , for example : #CODE
There has been a [ similar question ] ( #URL ) on stack overflow before .
Output : O is an mxn matrix where O [ i , j ] = R [ i , j ] if ( i , j ) is a local max and O [ i , j ] = 0 otherwise .
for any fixed ( i , j ) ` sum ( B [ i , j , :]) == A [ i.j ]` , that is , ` B [ i , j , :] ` contains ` A [ i , j ]` ` 1s ` in it
Just stack ` y ` vertically and subtract ` x ` .
This works because ` y ` now has shape ` ( 3 , 1 )` and so can be broadcast with your ` x ` which has shape ` ( 3 , 10 )` .
` x ` and ` y ` can be broadcast together if each trailing dimension ( i.e. starting from the end of the shape tuple ) of ` x ` is equal to the trailing dimension of ` y ` , or if one of the compared dimensions is 1 .
outer ' ` i ` ' and ' ` j ` ' for loops , leaving the inner ' ` k ` ' and ' ` p ` ' loops .
Also please confirm if the indentation levels of ** ` p ` ** & ** ` q ` ** loops is correct to be just a serial code-chunk inside the outer ** ` k ` **-loop .
That's matrix multiplication of the matrix C by the matrix with diagonal v .
How to install SciPy stack with pip and homebrew ?
I'm trying to install the basic scipy stack ( numpy , scipy , matplotlib , pandas , sympy , ipython , nose ) into a virtualenv ; currently , I'm using OSX Mountain Lion .
After activating the virtualenv in which the packages are going to be installed , first pip install numpy ; this should work as expected and numpy should be installed ( note that numpy should be installed first since a lot of the other packages in the scipy stack depend on numpy ) .
Points for which the dot product is positive fall on one side of the line , negatives fall on the other .
The function returns the sum of the integers in
the diagonal positions from top left to bottom right .
If the array is indeed an ` numpy.array ` , then getting the diagonal is trivial .
The diagonal from a list of lists shouldn't be hard either .
Starting with a multiline string , parsing , selecting the diagonal , and summing can be done in one line : #CODE
and sum the diagonal #CODE
The array has its own function to get the diagonal and sum #CODE
You need concrete numbers as weights , so you could use very large ones .
There is also + / -nan for not allowed calculations like 1 / 0 or log ( -5 ) .
Now , since I am a bloody beginner and have never ever subclassed np.ndarray and fear I might dump many hours into this , just to learn later , that the whole approach was not very smart , my question is :
So you can reshape your array to be Mx2 , and slice using ` np.choose ` : #CODE
( Note that I'm using string labels instead of numeric ones . I'd also advise you standardize the feature values , since they have rather different ranges . )
too many indices when debugging in Spyder
-> vmax = max ( voltage [: , 1 ] .tolist() )
IndexError : ' too many indices '
` voltage ` is a one-dimensional array , so ` voltage [: , 1 ]` will give the ' too many indices ' error .
I propose a solution using array indices .
I already moved B initialization outside the function and my function just only update the value inside B in the random indices .
However , what I need is a string containing all the elements in the list linked by ' ; ' , not the list itself , so it seems like I have to sum all the elements in asString with another iteration ?
You didn't specify what grid_code was , but a better way to figure out what all of the ids are would be to use ` numpy.unique ( new_array [: , 0 ])` which will give you a list of the unique values in the first column .
SciPy KDE gradient
However , I would now like to obtain the gradient of the KDE at a particular point .
Is there any way such that I get the result direct in the form of ` 10 x 1 ` without need to transpose it ?
Try [ scipy.interpolate.RectBivariateSpline ] ( #URL ) and specify a nonzero smoothing ` s ` parameter .
The ` usecolumns ` to specify which ones to use .
Converting to greyscale , then looking at a histogram
Creating a histogram ( 256 bins ) of either the B W data or the Hue band ( ` np.histogram() `)
Count the non-empty bins in the histogram , if less than 100 the vote is ' map ' .
Get the peaks of the histogram ( ` signal.argrelmax ( hist , order=20 )`)
I want to group by column ` A ` , and find the row ( for each unique A ) with the maximum entry in ` C ` , so that I can store that ` row.A , row.B , row.C ` into a dictionary elsewhere .
The simpler thing to do is just find all the primes up to ` n ` , and throw out the ones below ` x ` .
They'd be a bit complicated to work into this algorithm ( you'd probably want to build the array in sections , then drop each section that's entirely ` x ` as you go , then stack all the remaining sections ); it would be far easier to use a different implementation of the algorithm that isn't designed for speed and simplicity over space
no , no , what you need is the [ offset sieve of Eratosthenes ] ( #URL ) - sieve up to the ` sqrt ` of ` n ` , while sieving an offset segment from ` x ` to ` n ` too .
A *** lot *** less work than what you're proposing , when ` sqrt ( n )` << ` x ` .
Which you can't , except by finding them all and dropping the ones below X .
It already works up to the sqrt of n , all that's needed is to split the sieve array in two .
@USER : Either ` x <= sqrt ( n )` , in which case enumerating all primes up to ` sqrt ( n )` includes all primes up to ` x ` , or ` x > sqrt ( n )` , in which case enumerating all primes up to ` sqrt ( n )` and discarding the ones below ` x ` is the exact thing I suggested .
The proper modification for case x > sqrt ( n ) is to split the array in two .
I have a numpy array which I need to filter and perform a sum on .
Need to return the sum of column ` 7 ` where column ` 0 == ptype ` AND column ` 8 == radius ` .
The answer is a sum , which is the reduce step .
You definitely can use ` meshgrid ` and ` scatter ` to plot the fft result , but remember that ` fft_z ` is a complex number so you might want to choose between ` real ` and ` imag ` parts of your fft result .
I now get ` gradient ([ 1 , 2 , 4 ])= [ 1 ., 1.5 , 2 . ]` .
The problem is that my matrix has only integers and I can't use a ` float ` because the first column in the matrix are integer " keys " that refer to node ids .
If it's not possible to solve , then how could I prevent numpy from doing that to the ids ?
1 ) Is there any method that at the same time we transpose the arrays these are saved individually as consecutive columns ?
2 ) Or maybe is there anyway to append columns to a text file ( given a certain number of rows and columns to skip )
my answer would be similar to JoshAdels's , the point is to write like in Fortran when using Numba , just reminding that the indices here start at 0 :)
Someone ( without having the full stack trace , it's hard to tell who , either scikit or Numpy ) is trying to treat a Numpy array as a string ( `" FOO " .lower() ` returns `" foo "`) .
@USER , just added full stack trace to the question !
Note : In my previous comment , I assume that you fill ` mealarray ` from the beginning , with no indices containing ` None ` between indices containing words .
I'm trying to take an array on numpy , add a line of code , append it to the array , and then reshape the entire array back to the translated r .
We can conclude that the call to reshape is unnecessary because the array , as created , already has the specified number of rows and columns .
Let's consider ` reshape ` : #CODE
As you noted , with ` reshape ` , the " total size of new array must be unchanged " .
The solution is to use ` resize ` .
Here , we demonstrate ` resize ` on array ` a ` : #CODE
Is there a better way in ` numpy ` to tile an array a non-integer number of times ?
For these index arrays to create a 2-d result , we have to reshape the first one into a column : #CODE
When arrays are used as indices , they broadcast .
Here's what the broadcast indices look like : #CODE
but it is slower for small arrays ( much faster for large ones though ) .
It was also able to " compress " my array and imdecode was able to recover it with decent accuracy
In general , the methods on Numpy arrays are restricted to only the most basic operations ( ` max ` , ` min ` , etc . ) .
If you don't care about shape , you could flatten the array before giving it to kurtosis .
Also , is it normal , that my values are a bit " off " the real ones .
But I'll keep an eye on Cython :-)
You can lexsort on the row entries , which will give you the indices for traversing the rows in sorted order , making the search O ( n ) rather than O ( n^2 ) .
Python : How to generate a vector with nonzero entries at random positions ?
I want to generate a vector using Numpy that is k-sparse , i.e. it has n entries of which k are nonzero .
The positions of the nonzero entries are chosen randomly , and the entries themselves are chosen from a Gaussian distribution with zero mean and unit variance .
My current approach is to generate a random list of k integers between 0 and 256 , initialize a vector full of zeros , and then use a for loop to choose a random value and replace the entries of the vector with those values , like so : #CODE
Use ` random.sample ( xrange ( n ) , k )` to generate ` k ` random indices .
As I said above once I normalised the data ( ie 0 mean 1 std dev ) then this doesn't happen .
It has nothing to do with stack size : I increased the stacksize via $ulimit -s 16000 and also split up the calculation to require only a 10th of the memory ( well below what we have available , yet the problem persists .
I need to perform the dot product of two matrices in numpy .
Is there a way to calculate the dot product between this and a usual 2d matrix without having to explicitly create the above matrix ?
When you do the dot product of two arrays , you are computing the dot product of every row of the first array with every column of the second .
If your matrix-that-fits-in-a-vector ( MTFIAV ) is the first operand of the dot product , it is easy to see that you can factor the repeating value from the dot product , and do a single multiplication with the result of summing up every column of the second array : #CODE
If your MTFIAV is the second operand , it isn't hard to see that the result will also be a MTFIAV , and that its vector representation can be obtained computing the dot product of the first operand with the vector : #CODE
and you want a dot product with matrix m : #CODE
This paper : #URL ( don't ask me why it is reversed ... ) , describes an O ( n log ( n )) algorithm .
to compute the minium distance between the points of your set you create a matrix containing every distance then take it's min : you can certainly do better
` d , e , f ` are indices of points that are " interesting " ( lengths of ` d , e , f ` are the same and around 900 )
The mean is taken over all the points in ` C ` with the indices ` d , e , f+b [ i ]`
You sum over means ,... why ?
improving even more , by prematurely taking the elements that will form the diagonal : #CODE
And you can combine the summation and multiplication into a dot product : #CODE
From that point , ` mean ` and ` dot ` will get you to the destination : #CODE
Now I want to index " a " three times with the indices in " idx " to obtain an object as follows : #CODE
If idx is all zeros , ` take ` will produce an array full of ` 5 ` while it should be ` [[ 5 ... ] , [ 10 ... ] , [ 5 ... ] , [ 10 ... ] , [ 5 ... ] , [ 10 ... ]]`
Note that the output of ` in1d() ` is a flattened array , needing to be reshaped such that ` where() ` will return the correct indices .
I am sure that I must be able to just do this argsort once outside of the function , when the spatial distance window is originally set up , and then include those sort indices in the masking , to get the first howManyToCalculate indices without having to re-do the sort .
Then the numbers in sorted_dist_idcs_to_incl don't correspond any more with the indices of the trimmed array slices .
In this situation the indices reflect the " full " window and thus can't be used to select cells from the bounded window .
We still need to do the ` argsort ` to get the ranks at the outset ( rather than just partitioning based on distance ) in order to get the stability of the mergesort , and thus get the " right one " when distance is not unique .
` w1 = array ( dot ( matrix ( C ) .I , R - 0.03 ) / sum ( dot ( matrix ( C ) .I , R - 0.03 ))) [ 0 ]`
Without seeing the rest of your code , the most likely candidate is ` sum ` .
In vanilla python ( as when your script runs ) , ` sum ` is the python built-in function , which doesn't know about numpy arrays .
In IPython's Pylab mode ( as at Canopy's ipython prompt ) , which implicitly starts with ` from numpy import * ` ( terribly confusing , and one reason that the IPython team is now discouraging use of their Pylab mode , which I would guess Canopy will follow before long ) , ` sum ` is the numpy function , which behaves quite differently .
Look up both of these ` sum ` functions , and try using ` numpy.sum ` instead of ` sum ` in your script .
If it's not ` sum ` , then I suggest that you narrow it down to the simplest possible case ( just a few lines ) , and then it should jump out at you ; or if not , you can post it here and it will jump out at someone else .
@USER the thing is that ` np.load ( ' file.npz ')` is not loading the file into memory until you do ` a = npzfile [ ' key ']` , and therefore it creates a copy everytime you call ` __getitem__() ` ( it reads from disk and dumps into memory )
Differences between matrix multiplication and array dot
If I do dot product then , #CODE
Whether dot product and matrix multiplication are same here ?
Basically what I need is an outer product where the numbers instead of being multiplied are put in an array , i.e. : #CODE
You could also take the ` transpose ` of the meshgrid : #CODE
Let's say we create a boolean array wich marks the unique values on ` b ` : #CODE
I was not returning the indices of the unique elements , as I intended
if the sequence is [ 20 , 20 , 40 , 20 ] which indices should be returned ?
` diff ` returns [ 0 , 2 , 3 ] , ` unique ` returns [ 0 , 2 ]
If your b is sorted , using ` diff ` should be a bit faster than using ` unique ` : #CODE
@USER True , but you'd also lose proper stack traces ( without a lot of scaffolding to preserve them , somewhat defeating the purpose of inlineing ) , and Python is loathe to lose proper stack traces .
I had writted a script using NumPy's ` fft ` function , where I was padding my input array to the nearest power of 2 to get a faster FFT .
My input file was a bit more complex than this so I just replaced all ' = ' and ' ; ' with ' \t ' and then did usecols to pick out the ones I was interested in .
numpy : fancy indexing " unexpected behaviour " -- fancy indexing seems to give results that are the " transpose " of what would be expected intuitively
This behaviour makes more intuitive sense when using index arrays ( i.e. ` t [ arr ]` where arr [ 0 ] are the axis 0 indices , arr [ 1 ] the axis 1 indices , and so on ) .
I.e. mask out all bits except the last two and get the indices where it's all-zero .
Your eample should be more minimal , I had to replace ` phi ` by ` t ` , and add ` from numpy import exp , linspace , interp , pi `
I want to sample only some elements of a vector from a sum of gaussians that is given by their means and covariance matrices .
( A ) how do I sample from the sum of gaussians , ( B ) how do I sample only part of the vector .
For diagonal covariance matrix , one can just use the covariance submarine and mean subvector that has dimensions of missing data .
For covariance with off-diagonal elements , the mean and std dev of a sampling gaussian will need to be changed .
The bad days are eliminated , and the good ones are kept .
I believe part of the problem I am having is replacing what is in " domlist " with the new greatest ordered pair and returning that as the answer , i.e. I don't want to append " domlist " with the new greatest ordered pair until I remove the old greatest ordered pair from domlist .
Just use max : #CODE
No worries , I was not fully clear on what you considered pairs but max and filtering with a list comp will work
But the extra work is restricted to ` LOAD_FAST ` and ` STORE_FAST ` , which are just moves from the top of stack ( ` TOS `) to / from variables .
A quick diff of the two shows the same , plus the possibility the clearer function requires more byte codes .
My task is to create a method that returns 2 one dimensional arrays of indexes that select a tile from the image .
We can thus ask what pixels the tile at the ith row and the jth column span .
The function should return two 1-dimensional arrays , the first providing the row coordinates of the given tile , and the second providing the column coordinates .
I am getting a ValueError while trying to run the Polynomial Regression example : #CODE
The indices for truncating the arrays were found with some logic and trial and error .
In principle , the arrays ` indptr ` , ` indices ` and ` data ` keep the same , so I only want to change the dimensions of the matrix .
Is there a quick way to reshape my ` csr_matrix ` without copying everything in it ?
What you want to do isn't really what numpy or scipy understand as a reshape .
But for your particular case , you can create a new CSR matrix reusing the ` data ` , ` indices ` and ` indptr ` from your original one , without copying them : #CODE
next construct similarly ` indices2 ` and ` tmp2 ` , the indices of the remnant are the negation of the ` or ` ing of the first two indices .
Excluding overlaps can be obtained by ` hl = max ( lr , hl )` .
I tried using ` numpy.where ` but it's not working as it looks for equality at each of the four indices .
EDIT : I do also need to store the indices ` ( i , j )` which satisfy the condition
Use ` np.where ` on that array to get the indices ` ( i , j )` .
This will result in ` indices ` containing : #CODE
I need to store numpy arrays in a varbinary ( max ) field of an MS SQL database .
I have a signal in frequency domain.Then I took numpy.fft.ifft of signal.I got time domain signal.Again I took fft of same time signal properly I'm not getting negative and positive frequencies ( Plot 3 in Figure ) .
The reason you only see one peak in ` fd2 ` is because it is a transform of ` exp ( 4j* np.pi *tt )` rather than ` cos ( 4* np.pi *tt )` .
: Thanks frank.If I'm doing one more time of ifft and fft of signal_fd.I got two peaks which is same as singl_fd.Why fd2 transform exp() .. instead of cos() here .
For ` signal_fd ` and ` signal_fd2 ` you get roundoff error only , and for ` inv_td2 ` you get sqrt ( 2 ) .
You want to append it to the Python site packages .
/ Library / Python / 2.7 / site-packages Here you can append the framework manually into the Python folder .
You can combine ` map ` , ` join ` , ` zip ` and ` iter ` to make it work #CODE
You can use ` numpy.resize ` on ` a ` first and then add ` b `' s items at the required indices using ` numpy.insert ` on the re-sized array : #CODE
At any rate , given your 12 final positions , you just calculate the distance of each one from ` ( 0 , 0 )` , then square that , sum them all , and divide by 12 .
So , you can either read those 12 final positions into a 12x2 array and vectorize the mean square distance , or just accumulate them in a list and do it manually .
PIP Install Numpy throws an error " ascii codec can't decode byte 0xe2 "
So essentially the bar plot ( or histogram , if you can call it that ) should show that 32pts occurs thrice , 35pts occurs 5 times and 42pts occurs 4 times .
But not able to plot it as a graph ( something like a histogram ) ... that is the problem .
Cannot use SciPy's non-linear least square with function containing sum
Multipy matrix by its transpose in constant memory
And here is the output mat .
So far I managed to load it as a list in a really fast way ( 1 min ): #CODE
Using np.loadtxt freezes my computer and takes several minutes to load ( ~ 10 min ) .
Now , a previously-working program is giving me the following stack trace : #CODE
I have 140 arrays that are each 734 x 1468 , I need the mean , max , min , std calculated for each field .
At this stage " all you need " is to reinsert the values " no_zeros " on an array of zeroes with appropriate shape , on the indices given in " index " .
One way is to flatten the index array and recalculate the indices , so that they match a flattened arr array .
numpy create 2D mask from list of indices [ + then draw from masked array ]
I have a 2-D array of values and need to mask certain elements of that array ( with indices taken from a list of ~ 100k tuple-pairs ) before drawing random samples from the remaining elements without replacement .
In other words , convolve the delta-function response with a circular aperture / kernel ( in this case ) response at each position .
For example ( see numpy create 2D mask from list of indices [ + then draw from masked array ] ): #CODE
( Follows on from numpy create 2D mask from list of indices [ + then draw from masked array ] )
` , using very high indices , which are simply ignored ...
No matter if the array A is 1D or 2D because I can reshape it .
Also , you are not iterating over indices , but elements in your arrays .
That means you probably want to substract one from the indices 1 to 9 in accessing B .
Also , the row index goes first in Python , so you probably have to swap you B indices .
Two lines of code , please notice that you have to pass the transpose of your ` c ` array .
a scalar or an array of length ` k =2 ` giving you the indices
Should you need the indices of the closest ( s ) point in the mesh matrices , it simply a matter of using ` divmod ` .
You have two indices for one dimension .
I have a numpy array for an image and am trying to dump it into the libsvm format of ` LABEL I0 : V0 I1 : V1 I2 : V2 ..
Numpy : Fastest way of computing diagonal for each row of a 2d array
Given a 2d Numpy array , I would like to be able to compute the diagonal for each row in the fastest way possible , I'm right now using a list comprehension but I'm wondering if it can be vectorised somehow ?
There may be a good reason that you need all of those diagonal matrices , but for large ` n ` you'll be wasting a whole bunch of space .
I don't know what you're doing that would make diagonal matrices preferable over a vector containing the same information , but you might want to see if there is a way to do it with the compact representation of your data .
Sure , so in the end if all you need is multiplication of a matrix by a diagonal matrix , that's the same as broadcasting multiplication of a vector by a matrix .
What I want to do is be able to take each [ float , float , ... ] entry from the DataFrame and report new values / variables based off of it to new columns appended to the same DataFrame , such as calculate the standard deviation of each ' values ' list or the new variable ' avg = sum ( values ) / num ' and store the results in df_new [ ' stddev '] or df_new [ ' avg '] .
Then the values you are looking for could be computed with ` df.groupby ([ ' ID ']) [ ' val '] .agg ([ ' mean ' , ' std '])` .
I tried concatenate , append , vstack but nothing work , I always end with this message : #CODE
I give an example of a possible result , but I use random data so I can't guess when I will get an empty array and I parse some data so I add them one data by one data , that's why I talked about concatenate or append .
Simply you can use ` np.append() ` But note that you need to pass the indices ina list like ` [[ 1 ] , [ 1 , 2 ] , [ ]]` : #CODE
How does the performance change if I pass the actual row , for example , ` pool.apply_async ( sum , ( x [ i ] , ))` and change the function sum() accordingly .
If I rewrite the program with joblib package , ` results = joblib.Parallel ( n_jobs=-1 , max_nbytes=1e6 ) ( joblib.delayed ( sum ) ( i ) for i in xrange ( len ( x )))` does each processor / worker has its own copy of x ?
Your sum function should be summing the actual values ( a numpy array of the actual ints for that function ) , not a copy of the entire numpy array accessed by index .
As I understand , I need to normalize the fft result by the length of the data sample .
normalize the fft result by the length of the data sample #CODE
So this would be my max scale that I would use to calibrate my calculation ?
Now to try to bypass this I eventually just calculated the average manually ` sum ( volumetric_power ) / len ( volumetric_power )` and I was able to get past the error .
You could index ` a ` by getting the indices of the rows and cols where ` c ` is nonzero .
Now index ` a ` with ` c2.row ` and ` c2.col ` to get the values from ` a ` at the positions where ` c ` is nonzero : #CODE
yes , in this case looks like a sin curve , but the solution is more complicated .
Because it is cumulative sum and there are negative numbers under the curve the plot goes wrong .
Cleanly tile numpy array of images stored in a flattened 1D format
I want to tile these images with a certain number of images per row .
I'm a bit of a novice at Numpy and I was wondering if there was a cleaner way to tile the flattened data in a way without all the manual padding and conditional concatenation .
You can do this using ` np.newaxis ` and the ` T ` transpose method .
In the above case , trying a couple of tests showed that ` reshape ` created a view of ` a ` ( no data copied ): #CODE
So all I want to do is extract the VelocityField data from the clip , preferably with coordinate locations for each data point .
EDIT : If you want to remove the outer loop as well , we need to pull ` otime ` out : #CODE
This means that it has identified your data as categorical ( are they , e.g. a finite number of string labels ? ) and provides one column for each unique label , placing a 1 if it is present in a given sample and a 0 otherwise .
Since the values of some of my attributes in the original dataset are strings , each unique string in each attribute is changed to another attribute with something like a boolean ( 1 , 0 ) value for presence or absence in each instance within the dataset .
I looked into ` reshape ` - but that assumes no missing values .
( Note that I've taken the more Pythonic approach of putting the 0-based indices in ` row ` and ` col `) .
@USER without adding the ` [: : -1 ]` at the end , index of ` mask [: : -1 ] .cumsum() > 0 ` does not align with the dataframe . the warning basically says that pandas will do ` reindex ` to align the indices . see [ here ] ( #URL ) for some examples of alignment .
How do I get the indexes of unique row for a specified column in a two dimensional array
How would I find the index of the rows where the values in one or more specified columns are unique ?
If I specify a column as a " mask " how would I find the unique rows using that column as a mask ?
because if you were to use column 0 as the criteria for uniqueness rows 0 and 1 would be in the same " unique group " and rows 2 and 3 would be in another " unique group " because they have the same value in column 0 .
because using column 1 as the criteria for uniqueness would result in rows 0 and 2 and rows 1 and 3 being in their own separate unique groups because they have the same values in column 1
I also want to be able to get the unique rows with respect to more than one column So if I wanted the unique rows with respect to column 0 AND 1 ( now both column 0 and 1 are the mask ) I would want this return ....
because when you use both columns as your uniqueness criteria there are four unique rows .
So if I use column 0 as my mask I would want [ 0 , 1 ] and [ 2 , 3 ] as my return values because these are the rows that are unique using column 0 as the criteria for uniqueness .
Do you want the first value , the last value , the min value , the max value ?
I see , you want the partitioned groups of indices to be returned , not the orginal row values .
You can probably use ` NP.unique ` and the ` return_inverse ` parameter to get the indices of the unique elements .
You would then have to group them using the unique array .
2 ) If the values aren't sorted , do you need globally unique values to be grouped or just adjacent unique values ?
I do need the indexes for all globally unique values ( all rows ) with respect to the column ( s )
Notice that this returns the actual values , not the indices .
Specifically , I need to create a column ' E ' that is equal to column ' B ' minus the rolling min of column ' C ' , in function : #CODE
I don't know if this gives you what you want though ... your rolling min doesn't give you the numbers you specify in your desired output ( maybe you just want min() in that function ? )
Whenver I use it , I include comments like the ones above , to make it easier on the reader to understand what is going on .
Each ` if ` or ` elif ` statement in your " scalar-version " can translate to a call to ` choose ` , so you'd need to use ` choose ` twice .
The question comes from when I was playing with the exponential distribution f ( x ) = { 0 :( x < 0 ); exp ( -x ): ( otherwise ) } which is piecewise .
Make this new array of indices compatible with ` A ` by adding a new axis ( ` axis=2 `) .
Call this new array of indices ` x ` .
@USER I did it by adding a dimension to each matrix and replacing ` hstack ` with ` concatenate ` .
It can be assumed that all elements of each array are unique .
For large ` n ` , the probability that a random permutation of a list has no fixed points is roughly 1 / e , so it's safe to keep reshuffling until you get lucky .
I'm trying to translate code for k Nearest Neighbors from R to Python .
You're using ` reshape ` in the correct manner .
How can I append the values I receive in this array to my original data frame ?
? Alternatively , since I'm taking linear combinations of RGB values , could I create some kind of 3D linear filter and convolve it with my image by doing simple elementwise multiplications in FFT space ?
If you just want to have a list of coordinates you can use reshape : #CODE
the comma separates the indices you want to take from each dimension .
How to append or concatenate ' n ' numpy arrays ?
I want to append 10 numpy arrays one after another .
There is also vstack if you want to concatenate along the vertical axis .
I was looking for a way to directly ( using python functions ) get the matrix having all zeros and ones .
Perhaps one that doesn't require me to roll the axes ?
' , ones (( 1 , 2 )) , ones (( 1 , 3 )))`
' , ones (( 1 , 2 )) , ones (( 1 , 3 )))` .
'` doesn't work here because that implies repeated axes ( to the extent possible ) , where as you want unique axes ( except for the 1st ) .
Another option with ` einsum ` is to reshape the arrays , reducing the ' remaining ' dimensions down to one .
I add an approach using reshape .
Instead of plotting elements of the matrices with respect to t ( the last 2 lines of my code ) , how can I plot Eigenvalues ( absolute values ) ( lets say , the abs of eigenvalues of matrix A as a function of t ) ?
Sum of absolute values of polynomials with python numpy
I'd like to do the same with ` Lambda ` so that I can find its maximum using the built in function of the derivative ( find its zeros and so on and so forth ) .
The problem is that it is a sum of ` abs ( polynomials )` .
Take the absolute value of each polynomial and then add them together .
Unfortunately this is a function which sums the absolute values of the coefficients ..
My attempt ( but I believe now it is not possible using the built in functions ) is , for instance , if A=0 and B=x the sum of the absolute values should be absolute value of x ( which isn't a polynomial so , after searching for so long , I think I'll build a class to manage this sort of things ) .
You just need to add another dimension on to ` I ` , such that you can broadcast ` r ` properly : #CODE
Find all indices of numpy vector whose value is in a given set
My input is a pair of vectors , one large vector ` v ` and a smaller vector of indices ` e ` .
What I want is to find all the indices ` i ` for which ` v [ i ]` is equal to one of the values ` v [ e [ 0 ]] , v [ e [ 1 ]] ,...
The singularity shouldn't matter , since there is only one zero eigenvalue , thus the corresponding eigenvector is unique ( up to sign and length ) .
@USER n I used the dictionaries to not get lost in indices and to get rid of the look-up step if I store the index information seperately .
It * will * however take up more space than storing a ` numpy.array ` and a list of X / Y indices .
Horizontally stack all parts together : The first column of the original ` data ` , the date and time information as well as the last columns of ` data ` .
Get the indices of N highest values in an ndarray
Considering an histogram of shape 100x100x100 , I would like to find the 2 highest values a and b , and their indices ( a1 , a2 , a3 ) and ( b1 , b2 , b3 ) , such as : #CODE
I understand that one normally uses np.argmax to retrieve the value indices , but in that case : #CODE
You can use ` numpy.argpartition ` on flattened version of array first to get the indices of top ` k ` items , and then you can convert those 1D indices as per the array's shape using ` numpy.unravel_index ` : #CODE
To get the indices of the ` N ` largest sets you could use the ` nlargest ` function from the ` heapq ` module : #CODE
yes only small integers from 0 to 100 max .
After that ` np.where ` gives those indices which hold a True value , so ` len ( np.where ( ... ))` gives the total mismatches .
So ` popt ` , according to the documentation , returns * " Optimal values for the parameters so that the sum of the squared error of f ( xdata , popt ) - ydata is minimized " .
Notice the { built-in method dot } difference from 0.035s / call to 16.058s / call , 450 times slower !!
The results are 0.153s / call for the { built-in method dot } still 100 times faster then Linux !!
Notice the { built-in method dot } difference from 0.035s / call to 16.058s / call , 450 times slower !!
That is 401^3 iterations * 4 array accesses * 3 indices per access * 2 checks per index .
` numpy.delete ( dataset [ ListifoIndex ] , axis = 0 )` , where ` [ ListifoIndex ]` corresponds to the indices of the selected items to remove .
Apparently the ` numpy.delete ` metod creates a copy of my database for each of the indices ( 16x1.2GB ) , which exceeds the amount of memory that I have on my computer .
As you indicated above , in my case it is much better to use only the indices to create a new dataset .
actually it is float64 , but it's a decimal , and i want to ultimately take those values and compare to ints in a database ( they are ids of some sort )
One common pitfall with ` dtypes ` which I should mention is the ` pd.merge ` operation , which will silently refuse to join when the keys used has different ` dtypes ` , for example ` int ` vs ` object ` even if the ` object ` only contains ` int ` s .
Now , plots with dot symbols no longer render .
Creating a dot plot with seaborn imported gives a blank set of axes : #CODE
This fix will translate your sinusoids half of the range to the right , so they won't be split anymore .
One solution would be to remove zeros in self.squared_euclidean_distances into a small number or replace infinite numbers in T.sum ( c / self.squared_euclidean_distances ) to zero .
Unfortunately , the length of my input array is not an exact multiple of the row length I want ( ie . ` a = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ]`) , so I can't call ` reshape ` .
Then reshape to 2D , call transpose , and reshape to 1D .
The simplest way I can think of is not to try and use ` reshape ` with methods such as ` ravel ( ' F ')` , but just to concatenate sliced views of your array .
Alternatively , use ` as_strided ` to reshape .
Luckily I need to remove more rows than add new ones , so in theory this could all be done in-place .
Call ` .resize() ` on the matrix to resize it in memory
The result of the last line is not an array with zeros .
To add up several arrays , you have to either do it directly , or use a sum function .
I need to plot them as a kind of histogram / heatmap on a 2d plane with the axes ` x ` and ` y ` and a color indicating ` z ` .
where ` 0 : max ( Y )` is the interval and ` 800 ` a step count .
Did you want a 10x4 matrix with each row replaced by 4 copies of the row's sum or something ?
the column sum of ` B ` .
If you think of the operation in matrix multiplication terms , you can't multiply a 10x5 matrix with a 10x1 matrix ; if you think of it in NumPy terms as the multidimensional dot product , you can't multiply a ` ( 10 , 5 )` array with a ` ( 10 , )` array .
It's true that you can extend the dot product to specifically the domain of MxN matrices vs .
But that's not the definition used by either the paper's standard matrix multiplication notation or NumPy's ` dot ` function .
Well , note that the operation you're trying to do is commutative , so swapping the order of operands is perfectly legal and if you do that , then it does happen to correspond to the general dot product .
I'm also not sure why you're using dot product in the first place .
the thing is that the equations do not mention that i should transpose the inv ( A ) but i should use it like this .
Or maybe , as I explained in the answer , they're relying on the fact that the dot product of a matrix and a vector is unambiguous ( unlike the general N-dimensional case ) .
At any rate , I'm not going to read a 200-page thesis to try to figure out where it explains what you're trying to do so I can try to guess what you may be missing in your attempt to translate it in some way you haven't explained .
I was going to propose ` reshape ` and then ` mean ` , but that would be the same as the accepted answer to [ this question ] ( #URL ) .
Subtract transpose from a matrix but keep the original diagonal
Is there a pythonic way ( i.e. avoid nested loops ) to subtract mat ( A , B ) from mat ( B , A ); mat ( A , C ) from mat ( C , A ) and so on ....?
The diagonal elements can be left alone .
To put the diagonal back , you could do #CODE
It means that somewhere , something is trying to dump a numpy array using the ` json ` module .
bounded sum or difference of arrays in numpy
If I restrict the typ ( i.e. uint8 ) any exeeding sum produces an overflow ( i.e. start from zero again ) and any exeeding difference an underflow ( i.e. start from 255 again ) .
For these riddles with indices I recommend ` x= np.array ([[[ i*100+j*10+k for k in range ( 3 )] for j in range ( 3 )] for i in range ( 3 )])` as ` x [ i , j , k ]` -> ` ijk ` in terms of the corresponding 3 digits number .
` vertices [ 0 ] [ 0 ] , [ vertices [ 0 ] [ 0 ] , [ vertices [ 0 ] [ 0 ]` are all same = 0 , i.e. why different results than previous ones .
Makes sense , each element ( tuple ) in the outer array indicates the indices along a particular axis , right ?
If , as it happens , you have the indices of a vertex in a tuple and a list of vertices to denote , say , a polyline , for me the simplest thing to do is ` polyline_values = X [ zip ( *vertices )]`
My experience with their distribution is that the SciPy stack is almost always guaranteed to work , regardless of OS .
e^ ( cos ( x )) f ( sin ( x )) dcos ( x )
In this example the variable of integration is dx , but now I want to change the variable of integration to a function , such as x**2 ( I know this can be solved analytically , but I want to apply it so a function that is not analytically solvable . ) , or cos ( x ) .
EDIT : of course , you could always sample your functions on a mesh of ` x ` , use interpolation to construct an implicit function , and integrate that [ in your example : sample ` f ( x )` and ` cos ( x )` on a mesh of ` x ` , interpolate ` f ` vs ` x ` and ` x ` vs ` cos ( x )`] .
To get precompiled packages for Windows , have a look at Gohlke's Unofficial Windows Binaries or use a distribution like Winpython ( just works ) or Anaconda ( more complex ) which provide an entire preconfigured environment with lots of packages from the scientific python stack .
Here , you could also replace you min defaults with ` None ` too .
I have tried doing a diff in both directions , but this doubles up .
Is there a 2D diff function ?
You could convert ` inliers ` into column matrix ( e.g. ` inliers.reshape ( -1 , 1 )` or ` inliers [: , np.newaxis ]` , so it has shape ( m , 1 )) and index ` mat ` with that in the first column : #CODE
and find the max value on that array and from there , find the plot that gave that R^2 value thus I will find a particular n .
If I plot y vs x and got an R^2 value of 1 ( max value ) , it means that they're correlated with each other .
To do it with pure numpy , you'll need to get the sum of squared residuals from numpy's ` lstsq ` and then perform the formulaic calculations for R-squared manually .
The final step is to call ` max ` on a sequence of the key-value pairs in ` r2s ` , and ` key ` tells max that it is the second element ( the R-squared ) by which elements are compared .
When I try to resize the image using skimage's rescale method like this : #CODE
Amazingly , the random generate I wrote only goes up to 255 unique combinations .
Use vectorized functions which will process all elements of va and vb at once using Numpy's ` outer ` functions ...
If you can express it in terms of numpy ufuncs , then use ` outer ` method : #CODE
The problem is that it sum up x+y , so it would say that [ 1 , 3 ] is the closest one and it could be true in some cases but not all and in this specific case [ 3 , 2 ] is closer .
Note that if ` arr ` is very large and you need to compute the point in ` arr ` closest to many other points , it is more efficient to use a KDTree since once you have the data in the KDTree ( which takes O ( n log n ) time ) , searching for the nearest point requires only O ( log n ) time .
Using ` arr [ norm ( ... ) .argmin() ]` requires O ( n ) time .
Is there a way I can still get the indices of the original array to use them in ` do_something() ` ?
Since my arrays are two-dimensional , ` len ( pixels )` is 2 , so your code does not work directly , but I can get the actual indices using
In order to do calculations , I have a set of arrays : " sub " array ( as you can see below ) , and I want to reshape it in an array as given by " test " array : #CODE
Thanks for help ;) First i want to say ( for some errors ) when i play on the size of subarrays or the array to decompose , i do reshape and some subarrays are not well placed ...
Here's an alternative way to swap , slice and stack your array into shape : #CODE
Edit : Or instead , squeeze , slice and stack : #CODE
This can be done using a ` reshape / swapaxes ` trick : #CODE
Getting different answers with MATLAB and Python norm functions
Python is returning the Frobenius norm .
By default , ` norm ` gives the 2-norm ( ` norm ( R , 2 )`) .
Matlab default for matrix norm is the 2-norm while scipy and numpy's default to the Frobenius norm for matrices .
Specifying the norm explicitly should fix it for you
@USER Yes , but the answers still show how to convert a numpy array to a mat .
I have a pandas DataFrame with log data : #CODE
Numpy Construct a new matrix without rowi , columnj keeping row , column indices
There is a cell at the position ( i , j ) .. and I want to construct a new matrix with size ( m-1 ) * ( n-1 ) where the new matrix doesn't include the row and column the cell reside with the original indices kept .
However , in the new matrix , I lost the original column and row indices ...
Names , if any , are in separate arrays or lists - either ones that ` pandas ` creates , or ones you create and maintain .
It may need to be transposed depending on the shape of the gradient , but essentially you have stated the correct way to go and it should work using a shared variable .
You should ` argsort() ` only that column along which you want to sort the matrix and use the generated indices to sort the matrix .
How to fix " ValueError : need at least one array to concatenate " error
All I did was look at the error message and guess at why ` concatenate ` raised .
However , if you use e.g. ridge regression or OLS ( maybe not useful in your case ) , then the solution is closed form obtainable by matrix multiplication and everything can be done in one reshape and matrix multiplication , pushing the multi-target aspect of your problem down to C-level treatment .
EDIT : Given an array of points p , of the shape ( 2 , P ) , this is how you would find out which of these points are underneath diagonal n : #CODE
So , this returns a single value ([ 50 , 2 ]) which is a problem since I will later be evaluating whether a particular point is above or below the appropriate point on the diagonal .
To do this , I need every value that falls on that diagonal and not just the first value .
If you just want the values along the diagonal , why dont you just create a 1D list ?
You no longer need the 2d array and then call the diagonal .
So the issue here is that calling the diagonal returns a single value , not every value on the diagonal which is what I need
First , the distance on the circumference ` D ( 0 , theta )` is equal to ` D ( 0 , -theta )` , hence we can compare the absolute values of the angles .
If I use the above test on the absolute values of the angles to be tested , everything
` nzz ` as first approximation is the length of ` data ` ( not accounting for zeros and duplicate coordinates ) .
If P is within the circle ( i.e. the sqrt of the sum of squares of its coordinates is less than the circle radius ) and P is also between the line segment's end points , then the line segment intersects the circle .
I'm pretty sure you can get a phrase like " the square root of the sum of the squares of its coordinates " .
Summation of every row , column and diagonal in a 3x3 matrix numpy
I winner is found if the summation of ANY row , column , or diagonal is equal to 3 .
This should continue until the summation of any row , column , or diagonal is 3 .
EDIT : Basically , how do you find out the summation of every row , column , and diagonal to check if ANY of them are equal to 3 .
The Winner is determined by the summation of ANY row , column , or diagonal .
I could implement what you said but it wouldn't help me understand how to find the summation of every row , column and diagonal to check if any of them are equal to 3 .
In your case , to find the sum of the columns you could do #CODE
to calculate the sum of the leftmost column of xPlayer and #CODE
calculates the sum across the rows in each column and #CODE
calculates the sum across the columns in each row .
You could even calculate the diagonals without manually specifying the indices , so #CODE
for the top left-bottom right diagonal and #CODE
for the other diagonal .
Any non-ASCII character in the log causes PIP to crash with UnicodeDecodeError .
I don't know if there's any built in function in the NumPy / SciPy stack , but the link should explain the steps in more detail .
If I reshape in python I use this : #CODE
If I use ` reshape ( z , 4 , x )` it would give #CODE
Also is there a way to do reshape without specifying the second dimension like ` reshape ( z , x )` or if the secondary dimension is more ambiguous ?
` reshape ( z , 4 , x )` would seem like what you want , no ?
what you want is the transpose of your first attempt , which is #CODE
So if one of the python libraries I use has ` reshape ` in python I guess it would be difficult to switch it over to julia without using Arrayview ?
" Also is there a way to do reshape without specifying the second
dimension like reshape ( z , x ) or if the secondary dimension is more
the answer is not exactly , because it'd be ambiguous : ` reshape ` can make 3D , 4D , ..., tensors so its not clear what is expected .
Just to check whether the error is in ` min ` , did you try using ` np.min() ` or ` np.amin() ` ?
` min ` takes an iterable for an argument and compares each element to the other , each comparison results in a boolean value .
Iterating over a 1-d ` numpy ` array produces individual elements - ` min ` works for a 1-d numpy array .
` min ` won't won't work for 2-d arrays because it is comparing arrays and - ` The truth value of an array with more than one element is ambiguous ` .
You simply need to concatenate two tuples : #CODE
You need to append the tuple : #CODE
How to compute both max and argmax ?
Is there a way to get max and argmax by one stroke ?
I am trying to append a date-time field ( datetime64 ) to an existing recarray - without much success .
I can create the datetime field , but when I attempt to append it to the record array I get the error :
So this append function constructs a masked array ( ` ma `) , and checks the ' fill_value ' for the appended ' dtype ' .
Here's a simple , do-it-yourself append : #CODE
Only the zeros in position 3 and 9 are of interest to me .
Here's a slightly modified Series ` s ` ; we still want the zeros at indexes 3 and 8 : #CODE
` t ` is a series with all the nines and zeros in ` s ` .
We can retrieve the relevant indices just as before : #CODE
I'm attempting to encode ( VP8 codec ) and write a video using FFMPEG ( version 2.3.3 ) and Python .
However , I get these diagonal green stripe artifacts after I've finished encoding my video and I cannot find why .
The first index is the time index , and the other two indices are the 3x3 matrix .
( Or maybe you want the transpose , but if so , you should be able to figure that out from here . )
The ` print X ` shows the nonzero values of this matrix , in the ` ( row , col ) value ` format .
Your ` X ` is at least a ` ( 2,100 3454 )` matrix , but mostly zeros .
Slice subarray from numpy array by list of indices
I have a 2D numpy array ` input_array ` and two lists of indices ( ` x_coords ` and ` y_coords `) .
x_coords and y_coords are pre-checked and the ones on the border I leave out .
Here is a method that use array broadcast : #CODE
I'm using the skimage transform module's resize method .
I'm trying it like this : ` candidate = resize ( np.ascontiguousarray ( img [ candidate_box [ 0 ]: candidate_box [ 2 ] , candidate_box [ 1 ]: candidate_box [ 3 ]]) , ( 50,100 ))`
@USER Yes , if ` resize ` requires the array to be C contiguous , that seems like a good place to use it .
I have input file that has about 8 columns and all i have to do it find min and max value for every column .
The output is supposed to be the max value in col [ 2 ] i.e. the column 3 from input file .
You said you don't want to resize the lists so you'll probably just have to iterate both lists using a while loop and keeping track of indices for each array .
pairwise by indices , discarding extra length
make copies of indices in a2 to pad its length
ValueError : operands could not be broadcast together with shapes ( 700,600 ) ( 600 , 700 , 3 )
Basically , ` joblib.dump ` can optionally compress an array , which it either stores to disk with ` numpy.save ` , or ( for compression ) stores a zip-file .
I'm going to answer your primary question , and leave the others ( performance of transpose , etc . ) out .
' operands could not be broadcast together with shapes ( 80 , ) ( 5 , )' .
d^2y / dt^2 + c * dy / dt + sin ( y ) = a * cos ( wt ) #CODE
where vMax0 and vMin0 are just vectorized function max ( x , 0 ) and min ( x , 0 ) .
@USER Basically , I want the sum of w's components to be close to 1 ( < 0.01 ) , and the positive components has sum less than or equal to 1 , and negative components has sum greater than or equal to -1 .
The first constraint for sum makes ` - 0.01 = sum ( w ) = 0.01 ` which is not " close to 1 " .
Now the absolute difference of the sum to one is no greater than 0.01 :)
sorry that was a typo -- I meant sum ( w ) should be close to 0 :(
ValueError : too many boolean indices for a n=600 array ( float )
+I modified the example to show how to manage indexes in case u have diff sizes ( Check UPDATE )
I have a simple question about the ` fix ` and ` floor ` functions in ` numpy ` .
The obvious reason to follow this rule is that even a very small difference between two floats registers as an absolute difference , so numerical error can cause set-like operations to produce unexpected results .
I have no idea what the standard specifies regarding signed zeros in any of the four rounding rules it defines .
It's also worth noting that the two zeros should compare to equal , so #CODE
If you want to keep the numbers as floating point but have all the zeros the same , you could use : #CODE
This is why your method is not working , since it's comparing the binary representation of the numbers , which is different for the two zeros .
But if you want unique floats , you should be able to do this too , though you need to do it a bit more carefully .
After resolving this issue I was able to write a function for python to find the unique rows in a numpy array with the option of accepting precision ( number of decimals ) .
The next thing I would like to achive is to strip the line by " name " .
This can be done directly with reshape and transpose : #CODE
Efficiently get indices of histogram bins in Python
How do I extract the indices of each bin to efficiently perform my calculation using the bins values ?
Is there a way to obtain directly , in one go , the indices of the elements belonging to every bin ?
One way to achieve what I need is to use a code like the following ( see e.g. THIS related answer ) , where I digitize my values and then have a j-loop selecting digitized indices equal to j like below #CODE
Is there a way to obtain directly , in one go , the indices of the elements belonging to every bin ?
The above IDL implementation is about 10 times faster than the Numpy one , due to the fact that the indices of the bins do not have to be selected for every bin .
I assume that the binning , done in the example with ` digitize ` , cannot be changed .
This is an elegant way to keep the indices selection outside the for loop as needed .
I have changed the solution , so that now ` searchsorted ` is used instead of ` unique ` .
If you don't really need the indices , but you just want to do some processing on the bins : #CODE
In that case it seems indices are actually needed .
@USER : Indeed , for that case you'll need to get indices of some kind .
@USER : In terms of rewriting stuff for Scipy I think it would be better if ` numpy.histogram ` would have the option to return indices just like in your IDL example .
How to return indices of values between two numbers in numpy array
I would like to return the indices of all the values in a python numpy array that are between two values .
The problem is that ` foo ` is sometimes an empty array , and the ` max ` function ( understandably ) throws : #CODE
All I want is sum of all values across all arrays like sum ( my_array ) = some float number
You could use ` np.ix_ ` to build the appropriate indices : #CODE
Mapping joint histogram values
I have asked a similar question ( Mapping values from a joint histogram back into the image spaces ) , but I've realized that any points on the JH that has value greater than 1 would NOT be mapped back as the amount of pixels in the image space .
Here is my ( failed ) attempt to map back the indices .
However if you map values of 1 in the joint histogram using the method linked above , you get something like 3662 values , but this method you get 6507 , almost double !
Concisely stack a constant with a Numpy array ?
Is there a concise way to stack a constant with an arbitrary-length ` ndarray ` ?
Here I applied the indices to return the list of SELECTED tuples : #CODE
In Python you can do this using either ` operator.itemgetter ` or loop over indices in a list comprehension and fetch the item at that index : #CODE
Just append this to your code : zil = griddata (( x , y ) , z , ( xi.reshape ( 1 , -1 ) , yi.reshape ( -1 , 1 )) , method= ' linear ') ; zi [ np.isnan ( zil )] = np.nan ; zi = np.ma.masked_invalid ( zi )
OTOH if you use a 300x300 ` zi ` matrix ( the actual dimensions depend on the max spatial frequencies of features that you want to represent ) matplotlib can interpolate the colors in " missing pixels " .
Here we are going to use the integers from ` A ` as key of the outer dict and and for each inner dict we'll use the words from B as key , and their value will be their count .
Now we need to loop through the outer dict and call ` tuple ( .iteritems() )` on the outer list to get the desired output : #CODE
Or , more specifically , ` max ` is an alias for the ` amax ` function .
You can mimic this behavior with a simple function to flatten a list : #CODE
and the built-in ` max ` function : #CODE
If anyone is running an older version though , simply replace ` yield from ` with a normal for-loop : ` for i in flatten ( item ): yield i ` .
No reason to break it for ` min ` .
For flat ( one-dimensional ) arrays , the Python ` max ` and ` np.max ` do the same thing and could be exchanged : #CODE
For arrays with more than one dimension , ` max ` won't work : #CODE
( You can also find the maximum along particular axes , and so on . ) The Python ` max ` cannot do this .
So far my best idea ( assuming there is no equivalent to numpy.loadtxt ) would be to load line by line into an array then just reshape it afterwards , but I want to make this as efficient as possible and that seems like a slow and not clean way of importing it .
The actual ` argmin / max ` is performed by the functions defined [ here ] ( #URL ) .
Now I stack the three masked arrays , which returns an array with shape ` ( 480 , 640 , 3 )` .
Mask array entries when column index is greater than a certain cutoff that is unique to each row
You can use broadcast : #CODE
In other words , i would like to concatenate values in time with " slices " for each positions ...
why do you need this dot next to each number ?
why is there a dot ` .
How to plot histogram with data prepared in Pandas DataFrame ?
I just want to plot a histogram with 1-24 as bins and corresponding activity as value ( height ) .
What's more , I have tried to decode the first element of data string in a wxImage instance that load the same image directly .
( it should ) sets every value of IAM to 0 where the corresponding absolute theta value is bigger than 90 OR IAM is smaller than zero .
I have tried profiling lapack's svd algorithm ( gesdd ) called via ublas lapack bindings and python's numpy ( numpy svd .
It should be noted that according to the documentation numpy uses the same lapack routine gesdd for calculation of the svd .
I was not linking against the right ( i.e. the ones that I built myself ) lapack / atlas .
I used CMake to auto-detect the locations of the libraries , but , unfortunately , it found the ones installed through fink ( not sure how because they were not on the include path and I did not even know that I installed them at some point - must have been part of some other installation ... ) .
For a simple example , one could think of doing sum ( idx ) .
+k_{ p-1 }+k_p = m ` , instead of calculating all possible combinations in the cartesian product and filter afterwards , ` break ` the loop each time the sum already goes beyond your limit ` m ` , since adding a bigger integer instead in the next iteration will also pass over the limit .
We want to threshold every of the two arrays separetely in the ` c ` with a min value ( e.g. the value 3 ) and at the end to have : ` d =[[ 4 , 5 ] , [ 4 , 5 , 6 , 7 , 8 , 9 ]]` .
Replacing ` c [ i ]` with ` c [ i ] [ 0 ]` will fix your code , and also you cannot assign anything to an empty list so replace ` d [ p ]` with an append call : #CODE
Averaging adjacent values corresponds to multiplying with a ` sinc ` type function in fourier space , which has infinite support and doesn't decrease well , whereas gaussian smoothing , despite its infinite support , is at least rapidly decreasing .
You can use ` np.einsum ` to calculate the dot products and create the matrix of the desired shape : #CODE
I thought that separating them into a couple of individual ones , would disenable skipping replies to some of them .
I ran into a memory problem when trying to use ` .reshape ` on a numpy array and figured if I could somehow reshape the array in place that would be great .
I realised that I could reshape arrays by simply changing the ` .shape ` value .
Unfortunately when I tried using ` .shape ` I again got a memory error which has me thinking that it doesn't reshape in place .
I added my code and how the matrix I want to reshape is created in case that is important .
Apparently the transpose seems to be important as it changes the arrays from C-contiguous to F-contiguous , and the resulting multiplication in above case is contiguous while in the one below it is not .
When I run your code , it allocates the expected 762 MB array in ` rand() ` but memory usage doesn't change on the subsequent ` reshape ` and ` shape ` lines .
@USER If the array is not contiguous it cannot be reshaped in-place , see the comments in the answer to [ reshape an array in numpy ] ( #URL )
I've noticed that slicing in numpy.ndarray gives me a 1D array , thus data [ 0 , :] .T , data [ 1 , :] are both 3-elements array , and dot product is just the summation of element-wise product .
Then , I find I could use numpy.matrix to transform data into matrix scenario , and then perform the slicing and dot product , this works .
So , my question is , for my case , what is the best way to perform dot product on slicing ?
To get what you're looking for ( an outer product ) , you'll need to pass arrays with the shapes ` ( 3 , 1 )` and ` ( 1 , 3 )` .
thanks , I should mean the dot product of 3*1 matrix and 1*3 matrix , I've re-edit my question .
yes , but it does not matter because ` distance_function ` , only calculates distance for unique pairs that are not already in the cache , and caches the distances for later ...
You can translate your code to vectorized Numpy using arrays of indices : #CODE
= j ` will evaluate to , it's faster to just use a diagonal array .
I want to repeatedly sum over varying dimensions of a numpy ndarray eg .
The argument to ` reduce ` is ` numpy `' s ` sum ` function , a list with the dimensions to sum over , and the ` numpy ` array ` A ` as the initial element to reduce .
The function gets as parameter the ( partially summed ) ` numpy ` array and the dimension to sum next .
Numpy's ` sum ` accepts a tuple for the ` axis ` argument : #CODE
and I would like to get a new array with size n_b , which contains array a and zeros ( or any other 1-d array of length n_b ) on certain rows and columns with indices , e.g. #CODE
Mostly these expressions are of the kind exp = f ( x , y , z ) for example
f ( x , y , z ) = sin ( x ) *cos ( y ) *sin ( z ) .
Keep smallest value for each unique ID with arcpy / numpy
The MSLINK is not unique , because of a spatial join .
What I want to achieve is to keep only the features in the shapefile that have a unique MSLINK and the smallest DIAMETER value , together with the corresponding values in the other fields .
The fastest approach depends on a ) if you have a lot of ( MSLINK ) repeated rows , then the fastest would be inserting just the ones you need in a new layer .
And for alternative b ) I would just fetch 3 fields , a unique ID , MSLINK and the diameter .
Then make a delete list ( here you only need the unique ids ) .
The code below first creates a statistics table with all unique ' nMSLINK ' values , and its corresponding minimum ' DIAMETER ' value .
After this iteration , I use the python join function to create an sql string that looks something like this : #CODE
The sql selects rows where nMSLINK values are not unique and where DIAMETER values are not the minimum .
Because all of the features are categorical , I'm using a generator to iterate over the file and the hashing trick to one hot encode everything : #CODE
where ` data ` is just a list of lists of ones mirroring the structure of ` positions ` and ` positions ` would correspond to your feature indices .
As you can see , the attributes ` l.rows ` and ` l.data ` are real lists here , so you can append data as it comes .
The first and third index slots of the 4-d array act as the indices that select one of the 2x2 blocks .
I want to randomly produce an ` array ` of ` n ` ones and ` m ` zeros .
produce the ones array ( ` np.ones `)
produce the zeros array ( ` np.zeros `)
Do you want an array of _exactly_ ` n ` ones and ` m ` zeros , or an array of ` n+m ` elements that _on average_ will have ` n ` ones and ` m ` zeros ?
exactly n ones and m zeros
Your solution looks perfectly fine and Pythonic to me , if you want exact numbers of ones and zeros .
I'd make an array of n ones and m zeros as #CODE
Creating lists only to convert them to arrays later is slow for large sizes , better to use ` np.ones ` and ` np.zeros ` and ` concatenate ` them together , exactly as the OP suggested in his question .
( I don't know enough about ` numpy ` to comment on the difference between ` concatenate ` and ` hstack ` ; they seem to produce the same results here . )
Why use ` permutation ` to make a shuffled copy instead of shuffling it in-place ?
I just noticed that ` permutation ` seemed to be the ` numpy ` " equivalent " of pure Python ` sorted ` ( or , rather , of ` random.shuffled ` , the hypothetical counterpart to ` random.shuffle ` . )
As for the difference between ` concatenate ` and ` hstack ` : if you don't pass an ` axis ` argument , and you've got 1D arrays , there's no difference at all ; it's just a matter of which one you find more readable for a given problem .
I think I would have chosen ` concatenate ` here , but since the OP chose ` hstack ` , I figured better to stick with that .
If I have an k mxn matrices , and I stack them together , should the result kxmxn or mxnxk ?
How to create a column with the max / min value from two independent series ?
I need to create a df.column with the max of two other series ( these series are not in the dataframe , but they do share the same index ) .
I need to solve a linear programming problem in python and have only libraries from numpy stack which essentially includes scipy.optimize since as far as I understand linear program is a constrained optimization problem is it justified to use " SLSQP " from scipy.optimize.Or does it lead to grossly inefficient solution ?
Thanks for your reply , yes it is a standard LP problem.Its a pruning problem.You have two vectors v1 , v2 and you have a parameter vector ' x ' .Now , find the point ' x ' such that the difference in dot product x.v1-x.v2 is minimum.The constraints being sum ( x )= 1 and x [ i ] > =0 for all i .
Adding ` print ( x )` to ` func ` indeed shows that the solver jumps directly to ` [ 2.5 , 2.5 ]` on the first iteration --- this is a point where the gradient of the constrained objective function is zero .
Why do I get error trying to cast np.array ( some_list ) ValueError : could not broadcast input array
I need it because in the next part I will sum up this large np.array with some delta_array that has the same shape .
Like matrices sum
// It's matrices of weights for gradient descent for machine learning .
Since there's no way to ' stack ' these , it creates a 2 element array containing these 2 .
Due to an overlap in dimensions ( 4 rows ) it appears to be trying to join them into one array , possibly ` ( 2 , 4 ,? )` , but isn't quite succeeding .
I would like to " concatenate " results of detections of
So ( 2 , 0 ) and ( 2 , 2 ) are redondant and i would like to supress it and to gather all in an unique array ... as :
I could concatenate for each specific time #CODE
and I would like to reshape the results in an array whose shape is ( 3 , 2 ) in order to get access easily to my results ( because with the shape ( 3 , 1 , 1 ) , i dont manage to access to values ) .But perhaps for this part , i should ask in another question ...
Similarly , if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity ( accept for Hispanic ) .
If all ` transpose ( a )` does is call ` a.transpose ` then how do we look up ` a.transpose ` ?
All I see is referral to another transpose function .
It's effectively saying " If object ` a ` already has a ` transpose ` method , then leave it alone ; otherwise , use ` _wrapit ` to wrap the object ` a ` in an ` ndarray ` object " .
As the ` ndarray ` class has a ` transpose ` method , forcing the Python object into that class gives the object access to the method .
so there are other ` transpose ` methods floating around numpy , depending on the type of my object ?
You can use ` data ` and ` indices ` as : #CODE
Internally the ` csr ` matrix stores its data in ` data ` , ` indices ` , ` indptr ` , which is convenient for calculation , but a bit obscure .
The ` lil ` format stores the data as 2 lists of lists , one with the data ( all 1s in this example ) , and the other with the row indices .
and a one dimensional list of indices , #CODE
Flipping zeroes and ones in one-dimensional NumPy array
I have a one-dimensional NumPy array that consists of zeroes and ones like so : #CODE
I'd like a quick way to just " flip " the values such that zeroes become ones , and ones become zeroes , resulting in a NumPy array like this : #CODE
I am trying to use the ` FuncAnimation ` of Matplotlib to animate the display of one dot per frame of animation .
( In other words , are you potentially looking for a list of indices on each axis , or only a start / stop / step slice ? )
Are all / most of the values in the matrix unique IE : They only appear once in matrix B ?
The more unique the values the better the improvement you can make over subgraph isomorphism ( SI ) .
If all values are unique , then you can just do a reverse look-up on each value to determine it's row / column pair , union the list of rows and columns ( separately ) .
Of course , the less unique the values , the more false positives you get that need checking and the closer you get to SI and the less simple things get .
Python Pandas drop columns based on max value of column
I want to remove all columns that have a max value less than x .
I've looked at df.max() to get the max values of the column .
Indexing offset diagonal of 4 dimensional array
A trivial answer would be to use ` tile ` and ` repeat ` on the index and data arrays , but I believe there is an easier solution that I am missing .
Numpy maximum ( arrays ) -- how to determine the array each max value came from
How do I determine the year that each max value came from ?
If your temperature arrays are already 2D for some reason , then you can use ` np.dstack ` to stack in depth instead .
combine types : sum , subtraction , median and average .
After this I can get median , mean , average , sum etc of this array .
I use transpose so that every image ( e.g. ar1 ) is a column and every row is a pixel #CODE
Actually it works only if you reshape your matrix after the process .
The values are distributed into two peaks , as shown by the histogram below .
Note the strikethro comment doesn't work , it's the histogram which has zero values , not the data , oops !
Rejecting special values in a 3D array given the indices to reject
So rather than working with indices of elements you want to mask in the array , you should work just with the ( boolean ) mask .
The solution by using reshape , certainly not the more efficient but it works : #CODE
Creating a permutation , with restrictions
I have a list ( actually an array ) of elements , and I want to generate a permutation of that list .
I want to generate a permutation that obeys these restrictions .
Numbers are not unique .
About 1.4 *10^7 rows , 2*10^6 unique values in X and a similar number in Y .
Running the restricted permutation algorithm above ( implemented in numpy instead of pandas for added speed ) takes 7 seconds for just 10^3 rows .
Would it be enough to filter out those values after using the ` permutation ` function ?
It could be that the permutation process matches , e.g. all the 2s to each other .
Then grabbing only those values where X == Y and permuting them wouldn't solve anything ; the resulting permutation is the same set of 2s as before .
However , if feasible , you might create EVERY permutation then remove the ones that don't match your criteria .
It seems like you can do the permutation , then break the permuted dataframe into two subsets :
On my Mac , a simple permutation takes 5.3 seconds .
How would we modify this if we can't enumerate every permutation due to memory limits ?
That way no chance of n=1 and almost no chance of ending up with all values the same ( assuming a large number of unique values , which I think you have )
The " flat arrays " you see in the bottom left are the result after the ` reshape ` and ` transpose operations ` .
When numpy is asked to subtract those arrays it'll broadcast both to become the same shape , by placing repeated copies of itself along the missing axis ( axis ` -1 ` or ` 2 ` in this case ) .
+1 : googling " flatten " only works if you know to use the word " flatten " , but if you know that , you already know the answer .
For example , in many constructions , one gets items one at a time and doesn't know ahead of time the size of the resulting output array , and in this case it can make sense to build a list using ` append ` and then convert it to a numpy array to take an FFT .
An interesting point is that ` flatten ` returns a copy , ` ravel ` ( and ` reshape `) only does so if necessary .
Im new to grabbing unique values but here is the code #CODE
This gives you the unique corridor numbers in list .
Because you pasted 4 rows of ` [ ' 0 ' ' 1 ' ' C ']` one after another , I couldn't understand if you wanted 1 set of unique corridor each time you enter file or something else .
Also if you want unique values , use ` set.add ` instead of appending to a list .
dtype.num A unique number for each of the 21 different built-in types .
Each row represents a different physical object's properties , so I would say hstack is what I need for my specific example , but thank you for including the different ways to ' stack ' the array for others to use .
You can do this ` O ( N )` time if you store all the unique two item tuples from ` a ` in a set first : #CODE
Conjugate transpose operator " .H " in numpy
However , there is no similar way to get the conjugate transpose .
And yes , I also want to insert all the zeros .
For what I want , it should also produce the zeros .
How about using ` nonzero ` to identify which elements are not zero ?
` nonzero ` functions the same for both dense and ` csr ` arrays .
When ` s ` is a scalar , or list , this produces an array that can be broadcast to fit ` x ` .
You need to pass ` s ` through a function that turns it into an iterable that can be broadcast .
According to my limited understanding of the situation , this should create a temporary NumPy array with the same size as ` a ` which holds all the values from ` s ` ( and many zeros ) and which is then assigned to ` a ` .
` find ` then performs some extra functions to extract the indices .
These indices are then used to do what is known as " fancy indexing " in numpy .
Perhaps you could append to your question more info about the type of these surfaces .
These are per-GROUP ( e.g. that's what the indices represent ) .
Then you need to select the indices where the original breakpoints happen .
IOW . seems you wanted to calculate diff betwee 4 and 2 ( and 3 is in that group ) .
I did find this related SO question ( Building and updating a sparse matrix in python using scipy ) , but the example assumes you know the max COL , ROW sizes , which I don't , so that data type doesn't seem appropriate .
Part of my problem is that I don't reliably know what the max COL value will be .
I could write a script that finds out the max value for a given data file , but thought there might be some existing scipy sparse matrix datatype that doesn't require me to specify that .
Or ( mis ) use numpy's histogram function : #CODE
Now I can only think of this approach : use ` scipy.ndimage.filters.correlate ` and pass in weights with zeros and one 1 to get four matrices , each containing the neighbor for each cell in a direction , like passing in ` weight = [[ 0 , 0 , 0 ] , [ 1 , 0 , 0 ] , [ 1 , 1 ]]` and I get ` a [ i , j ] = x [ i-1 , j ]` , and with other weights I can get ` b [ i , j ] = x [ i , j-1 ]` , ` c [ i , j ] = x [ i+1 , j ]` , ` d [ i , j ] = x [ i , j+1 ]` .
Find out the max rainfall
the smaller array is broadcast across the larger array so that they
numpy too many indices for array error
` too many indices for array , `
The ` too many indices ` error means you are treating it like a 2d array when it is actually just 1d .
too many indices for array
Don't access ` myData ` with 2 indices .
I have the indices of the these nan elements saved in a list called ` nanind ` .
Be sure to print an informative error message , like `" Unexpected sqrt value , input somevar= " +str ( somevar ) + " , sqrt was NaN "`
Each column of a matrix should sum to 1 .
In MATLAB I would write for a matrix ` mat ` #CODE
As a side note , in Matlab you'd better use the [ more efficient approach ] ( #URL ) ` mat = bsxfun ( @USER , mat , sum ( mat , 1 ))`
The rules are described in the [ broadcasting ] ( #URL ) page : when doing an operation on two arrays of different sizes , numpy tries to reshape the arrays so that the operation is well-defined .
In ` numpy ` math operations normally operated element by element , where as in ` matlab ` you have to append the dot ` .
Recent versions of ` octave ` also ' broadcast ' , so ` mat .
/ sum ( mat )` works .
` np.copyto ` ( is documented to ) and ` np.where ` ( appears to ) broadcast the shape of the mask to match ` array ` .
In contrast , ` array [ mask ] = 0 ` does not broadcast ` mask ` .
Because the question was eventually answered to your satisfaction , I won't roll back your edit , but this is not the way things are done on StackOverflow .
Basically the problem is that you should change your indices arrays ` my ` and ` mx ` to get the output you desire .
or explicitly assign zeros ( again , fancy-indexing , this time using a mask ): #CODE
Does any one know how to get unique elements row wise in a matrix .
Do you need to retain the order of the unique elements ?
It's not very efficient , because moving all zeros into a row's end can't be very efficient .
Note that ` using_complex ` does not return the unique values in the same order as ` rowWiseUnique ` ; per the comments underneath the question , sorting the values is not required .
The trick is to add a unique imaginary number to each row .
I was looking at ` np.unique ` documentation and in ver 1.9 there is an additional argument ` return_counts ` that returns the counts of each unique value .
Lists are used for picking item at particular indices , i.e index arrays : #CODE
P is a diagonal sparse matrix .
If ` P ` is diagonal , then only store its diagonal as a column vector ( shape ` ( N , 1 )`) and compute ` P * B ` .
Another option is to use ` einsum ` : ` np.einsum ( ' ij , j , jk ' , A , P , B )` assuming ` P ` is just the diagonal as a 1-d array .
Since I have large ndata and kdata above approach becomes a memory bound problem so next bet would be the dot product with nested for loop over ndata and kdata as follows : #CODE
I am new to the stack so please be gentle with me :) .
For a clarity purpose I only mentioned the dot product in the inner loop .
However , in reality I am doing several matrix algebra including inverse , transpose , a couple of more time dot products etc .
Looks like ` numpy.random.multivariate_normal ` can do this , but I don't quite understand what the ` cov ` parameter is supposed to be .
Later in the page , in the examples section , a sample ` cov ` value is given : #CODE
If someone could clarify what the ` cov ` should be or suggest another way to generate points in two-dimensional space given a mean and standard deviation using python I would appreciate it .
How can I vertically concatenate 1x2 arrays in numpy ?
How can I concatenate 1x2 arrays produced in another function ?
The code I tried for concatenate is #CODE
All I want to do is concatenate / join / append these xa values vertically .
All I want to do is the concatenate the xa 1*2 arrays ...
I used A.append ( xa ) and this is what I get : AttributeError : ' NoneType ' object has no attribute ' append '
If you disapprove , you can roll it back ( or just say so ) .
However , since Pandas is rather general ( Pandas doesn't care that ` x ` and ` y ` are grid indices ) , I feel that this may not be the optimal solution : having a solution that knows that the input and output are already on a ( 2D ) grid seems more efficient .
I have , however , not been able to find one ; ` np.digitize ` comes closest , but that's only 1 dimensional , and still requires a loop in Python to access the indices and average or median over the data .
Some copying or duplicates of either some additional indices or array values will be required , since the original array needs to be restored .
Reassign a column-shuffled copy of ` X ` to the relevant indices of ` Y ` using the mask on ` Y `
I have a complicated architecture and I am calculating gradients wrt to diff parameters , but I am getting following strange error , what is this indicative of ?
` np.unique `' s ` return_inverse=True ` to obtain unique labels which are integers .
` uniq ` holds the unique values in ` int_colors ` and ` int_keys ` .
Get the cumulative sum of the sorted list
If you ask the question with the numpy tag , then despite you rrestrictions , the useful answer is making a list of indices ...
You need to build the slices for yourself as indices and then use them : #CODE
Numpy sum every n columns of matrix
I'd like to sum every n columns of a matrix .
Here's one way ; first reshape ` x ` to a 3D array and then sum over the last axis : #CODE
which in this example returns a 50*3 matrix filled with ones .
If you reshape ` A ` slightly , you can add the two matrices together and concatenate the entries in the first axis of the resulting array : #CODE
A slightly quicker alternative to ` concatenate ` is to use ` reshape ` : #CODE
Isn't concatenate quite an expensive operation though ?
@USER - that's true , it can be ; you can use ` reshape ` instead ( I've added it to the answer )
This example , from the excellent matploblib documentation , exactly solves your problem , except that you may want to use an ad hoc solution for the secondary axes ' ticks ( I mean , the ones with percentages ) .
` numpy.trace ` seems to only return main diagonals , and ` numpy.diagonal ` doesn't seem to help out with secondary diagonal either .
I don't know of any NumPy functions that calculate the secondary diagonal explicitly , but this method should be relatively efficient .
Why does transpose make the array non-contiguous ?
Finally , why can't we flatten the Fortran contiguous array by assigning a new shape ?
The transposed ones are not #CODE
But the shape of the transpose cannot be changed .
You could also experiment with adding a ` order= ' F '` parameter to the ` copy ` and ` reshape ` commands .
You can use the answer to this other question of yours to get the counts of the unique items .
Your " stack " has the " shape " ` ( 30000 , 30000 , 4 , 11 )` .
Well , yes , that's another problem ; lists don't have an ` add ` method , only sets do ; lists have ` append ` ( and a few other things , like ` insert `) .
I want to join them into a single array or dataframe .
My logic is to join into a single array by df= np.concontenate (( a , b , c ))
I am using Python to make a visualization generator for prime factors , but I'm having trouble linking the colors with the unique prime factors of a given number .
All that is good , now I want to link some unique colors to the unique primes in the DataFrame .
An all Pandas solution would be to make the color map a pandas dataframe and then join it to the primes dataframe : #CODE
As a side note , it's often clearer , simpler , and more efficient to use ` zeros (( 2 , 1 ))` instead of constructing a 2x1 list of lists of zeros to pass to ` array ` ( especially when you get a lot bigger than 2x1 ) .
So , numbers on the diagonal are the things I wanted , but what about the rest ?
sorry @USER , one reshape too far ;) fixed it
As I understand your question , you are trying to do a column-wise parallel dot product .
It's ` dot ` that doesn't work the way you expect -- it does matrix multiplication .
So you get the following results ( depending on how you transpose ): #CODE
The diagonal of the second result is what you're looking for , but the full matrix multiplication does too much extra work .
Here's how I usually get just the diagonal , roughly speaking : #CODE
In short , you actually want not to broadcast in this particular case .
dot product of subarrays without for loop
this is just basic example , for some test cos in orginal data i had
If I am getting your code right , you want to perform 49998 dot products of a 3x3 matrix with a 3 vector , right ?
This value will be assigned to ` indices ` .
Using parentheses , you can think of it as ` indices = ( data [: , 1 ] == 1 )` .
So if ` data [: , 1 ]` was something like ` [ 1 , 2 , 3 , 2 , 1 ]` , the result of ` data [: , 1 ] == 1 ` would be ` [ True , False , False , False , True ]` , and this is the value that would be assigned to ` indices ` .
I don't want to say anything , but if I define ` data ` as a list if integers ( ` data = [ 1 , 2 , 3 , 4 , 5 ]`) and try ` data [: , 1 ] == 1 ` , I get a ` TypeError : list indices must be integers , not tuple ` .
A floor version using numba is here : #CODE
new code : import pandas as pd from pandas import DataFrame , read_csv import numpy as np from pandasql import sqldf pysqldf=lambda q : sqldf ( q , globals() ) rolup = pysqldf ( u " select MasterUserId , DeviceUsed , hcluster , count ( MasterUserId ) as Tot_Rec , sum ( Visits ) , sum ( PV ) , sum ( TimeSpent ) from clstrd_data group by MasterUserId , DeviceUsed , hcluster ; ")
pysqldf=lambda q : sqldf ( q , globals() ) rolup = pysqldf ( u " select MasterUserId , DeviceUsed , hcluster , count ( MasterUserId ) as Tot_Rec , sum ( Visits ) , sum ( PV ) , sum ( TimeSpent ) from clstrd_data group by MasterUserId , DeviceUsed , hcluster ; ")
The idea I'm trying now is to compare the button image with the suspected area in the screenshot and compute the Manhattan norm , if the images are too different the score will be high and low if they are pretty similar .
I was wondering if there was some way of doing it without having to reassign ; that is , with the vector being called with indices in the expression , but this is also a good answer , so I will give it a point for now and wait a bit before declaring it the accepted answer .
` IndexError : too many indices ` at ` x1 = x [ 0 , :] ` .
Python : Too many indices
Now , when I run it , I get a " too many indices " error .
` where ` handles the looping and indexing internally , just like adding two numpy arrays without specifying the indices .
to Mr E i understand that where handles looping and indexing internally without needing to specify indices , but however how would you compare rows without having some sort of index to show which one you re selecting like what i did in my snippet ?
What you should do is write a small piece of code that produces the result you want - do it with loops and indices the " normal " way .
Now I have two arrays X and Y of shape sx + ( 3 , ) and sy + ( 10 , ) , where the sx and sy are two shapes that can be broadcast together ( i.e. either sx == sy , or when an axis differs , one of the two has length 1 , in which case it will be repeated ) .
First off you are allocating the memory on the stack WITHIN ` linspace ` with #CODE
In your example , things are missing : minor ones like ` import numpy as np ` and major ones ( how does file.txt look like ) .
in the specific matrix case you need only search indices that are nearby .
All other solutions coming to my mind are rather pointing to absolute maxima ( like e.g. histogramming ) ...
However , I've faced another problem discussed in the link PIP Install Numpy throws an error ascii codec cant decode byte 0xe2
In ` core / _methods.py ` , ` sum ` is defined as a call to ` add.reduce ` .
` min ` and ` max ` similarly .
Keyword parameters become positional ones .
In regular ` numpy ` the ` sum ` call is : #CODE
Apparently someone was trying to squeeze out a bit of performance by minimizing keyword parameters .
or , if ` dog ` is a NumPy array , you could call its ` min ` method : #CODE
Putting ` axis=1 ` in the call to ` dog.min ` tells NumPy to take the min over the ` axis=1 ` direction , thus eliminating the axis of length 2 .
I have this list ` [ 1268857 , 384269 , 72468 , 161 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]` , in which I compute a log calculation on each value .
So I first remove the zeros and then I get this list : ` [ 14.05362705319161 , 12.859098107153008 , 11.190900364095901 , 5.0814043649844631 ]` .
You can read more about ` logical_and ` and ` nonzero ` .
You can unpack the transpose of the array in order to use the columns for your function arguments : #CODE
NumPy arrays will be unpacked along the first dimension , hence the need to transpose the array .
Theano gradient of sparse matrix mulitplication
\edit This diff fix in Theano fix this crash : #CODE
needs to append to the existing $PATH .
Does anyone know a good way to generate a new permutation of 3.6 million ( very long ) rows of numpy data ?
Right now , my best idea is to create a permutation of paired indices ` ( c , r )` into the data , where ` c ` choses a chunk and ` r ` choses a row from that chunk .
Is there some principled way to shuffle random pairs of chunks together until you get a permutation that's statistically independent from the starting permutation ?
Create an index of the data in the form of a ` dict ` , mapping each unique record ID ( 0 through n - 1 ) to some means of finding the data again .
Grab the data at that location and append it to the output file .
This process produces a permutation as random as a normal shuffling process would .
It's also able to compress the data down to ~650M -- nearly a 90% deflation .
The easiest way is to use ` nonzero ` .
If this does not hold for your problem , the length of ` indices ` can be checked and in case of length ` 0 ` you may want to terminate with an appropriate error .
As usual , answers that attack _the problem_ rather than _the question_ are the better ones .
If the matrices are fresh and independent there isn't much you can save because the only way to compute the mean is to actually sum the numbers .
If however the matrices are obtained from partial views of a single fixed dataset ( e.g. you're computing a moving average ) the you can use a sum table .
Btw . the formula I actually have to translate is the first one on slide 14 in this [ pdf ] ( #URL ) .
@USER Thanks for the code , but this is not exactly what is needed , e.g. ` cosdist ` is normally a matrix , so that ` sum ( terms )` would not suffice ( easy fix though ) .
The question is how to append rows to an initially empty array and your code doesn't address that .
Both ` vstack ` and ` append ` end up using ` concatenate ` .
I've tried reading through the Merge , join , and concatenate - documentation , but can't get my head around how to do this in Pandas .
` m [ np.triu_indices_from ( m , k=1 )]` returns the values above the diagonal of ` m ` and assigns them to the values values above the diagonal of the transpose of ` m ` .
The idea was that in a given photon's row , the indices from the starting layer through and including the ending layer would be set to 1 , with all others remaining 0 .
A little trick I kind of like for this kind of thing involves the ` accumulate ` method of the ` logical_xor ` ufunc : #CODE
I want to find the lag between the center of the two peaks ( not just their max ) .
The only ready-to-use installers available out there are the ones you have mentioned , they use Intel Fortran compiler and these are clearly unofficial binaries .
Sometimes there might be a numpy function alternative for sin , cos but not for all function like the quad function in scipy.integrate .
Sometimes there might be a numpy function alternative for sin , cos but not for all functions like functions in scipy .
If I have the upper triangular portion of a matrix , offset above the diagonal , stored as a linear array , how can the ` ( i , j )` indices of a matrix element be extracted from the linear index of the array ?
The sequence 0 , 2 , 5 , 9 ( indices of diagonal elements ) corresponds to triangular numbers ( after subtracting 1 ): a [ n - i , n + 1 - i ] = Ti - 1 .
The function ` triu_indices ` returns a tuple of arrays picking out the indices in an upper triangle of an nxn array ; one entry for the x-index , one entry for the y-index .
I want to shift the image by an X and Y offset and want the rest of the frame padded with zeros .
I have seen discussions about the ' roll ' function but that only works in 1 axis .
Shouldn't the first parameter for ones , shape , have two elements ?
Fastest way of calculating mean values for each indices
I have to write a file for each unique value of the reference_array computing mean values where the unique value are in the given array .
In contrast to the solution of the OP , finding the unique values and calculating the mean values is done in one loop .
` unique ` is a dictionary where the key is one reference value and the value is a pair of the sum and count of all the given values which have the same reference value .
After the loop , not only all unique reference values are put into the dictionary ` unique ` but also all given elements are sorted to their reference value as sum and count , which can easily used to calculate the mean value in a second step .
sorry there was writing mistake in my question CORRECT ONE is : index = reference_array == unique Does your code still works ?
this was a obvious mistake , I even don't need unique for my solution .
how large are your arrays , and how many unique values are there ?
If you get these arrays into memory , you should have no problem with memory with the second example , the ` unique ` -table uses only a few hundred megabytes .
The unique value of reference array are 1 to 1600000 .
THere is no problem until reading out both the two arrays , but later on computing the mean of unique indices , the computer really hangs out .
You can get the whole thing to work in numpy using ` unique ` and ` bincount ` .
Since numpy's ` unique ` uses sorting , it is going to have linearithmic complexity , but it typically beats pure Python code using dictionaries , despite the linear complexity .
Finally , compute your group calculation , using ufuncs and their ` reduceat ` method , e.g. for the group ` max ` : #CODE
It is not a very numerically stable approach , but you can compute the standard deviation from the sum of the values in each group , ` sx = np.bincount ( inv , weights= given_array.ravel() )` and the sum of their squares , ` sx2 = np.bincount ( inv , weights= given_array.ravel() **2 )` , see e.g. [ this answer to a similar question ] ( #URL ) for details .
See the edit for max .
numpy resampling reshape data
Here is what I used to convolve that data
According to the documentation for numpy.convolve this will return a result of size max ( M , N ) , where M and N are the size of the two input vectors .
Replace ` xx = zeros ( len ( L ) , dtype=float32 )
You have a ` j ` in the failing expression that receives the wrong value from the outer context .
Change ` exp ( j ...
` in your ` Emag ` to ` exp ( 1j ...
How does numpy order array slice indices ?
It switched the indices around , even though it did not when I sliced the last time !
I could of course transpose the matrix after the operation but this seems very counter-intuitive .
[ The docs ] ( #URL ) for advanced indexing ( using arrays as indices ) acknowledge that the behavior " can be somewhat mind-boggling to understand " .
Suffice it to say that using an array instead of a slice ( i.e. , ` [ 0 , 1 ]` instead of ` : 2 `) triggers a different type of indexing behavior , and combining different sorts of indices ( single integers , slices , arrays ) can quickly take you into strange territory .
That is , you step through lists-as-indices for each dimension in parallel , with length-one lists and scalars being broadcast to match the longest .
After stepping through the indices , that would evaluate to ` array ([ extract [ 0 , slice ( None ) , 0 ] , extract [ 0 , slice ( None ) , 1 ])` .
So to conclude I think it is a side-effect of the broadcasting that is done to make all the dimensions have an index list of the same length , which leads to ` : ` being broadcast too .
If I understand correctly then the ` 0 ` in my code will be treated as a one-item list , so the two ` lists ` will be broadcast together ( which gives a 1x2 array ) , and thus the index moves to the front .
And the whole idea of using broadcasting at all is not slicing as such but indexing , where I could put arbitrary arrays in the indices and get all sorts of permutations of the original array's content -- that seems to make sense , although it would seem more consequent to drop the index in first place for ` [: , : , [ 0 , 1 ]]` , as well .
Which means I should consequently use separate indices ( ` [ ... ] [ ... ]`) to make my intention unambiguous -- thank you !
and then append but I can't figure out how to append correctly , so it looks the same .
I gather that one or both indices in the expression should be long .
You can also build a list of indices with ` [ random.randrange ( n ) for i in range ( k )]` .
Getting the indices of the top k numbers by value in a numpy array ?
You can find the max by : #CODE
But for the indices of the 2nd and 3rd top values , how can you find those ?
The only option I can think of is to set 0 for the value stored at the max index , and run the process again .
You need to flat your array first , then sort it and use ` unique ` to refuse from duplicated entry .
and for get the indices you can use ` argsort ` : #CODE
That will get me the values , but how do I get the indices of those values then ?
To pair the indices into coordinates , we can use ` zip ` : #CODE
The initial guess could be ` [ np.mean ( Kp4 ) , np.mean ( xk ) , 5* ( max ( xk ) -min ( xk )) / len ( xk )]` , to have a general starting point .
Like in the scalar case , you can specify the relative tolerance ` rtol ` and the absolute tolerance ` atol ` as keyword arguments .
But note that unlike NumPy's ` allclose ` ( and the previous solution of this answer below ) , array- ` isapprox ` calculates a norm first , and then decides for the resulting value .
( Appearently , checking ` isapprox ` pointwise is wrong . ) By default , the Frobenius norm ( ` vecnorm ` ) is used , but you can override this behavior using the ` norm ` keyword argument .
because it doesn't initialize values , only maps memory . use ` zeros ` instead .
Your map matrices are wrong , to get the result you want they need to be like these ( since , when you put values into b , you are checking if m [ k , j , i ] ! = -1 and you want the last columns to be 0 , not the first ones ) #CODE
As in the 2dim case , you could alrenatively use fancy-assignment ( again , axis=0 is being broadcast ): #CODE
shape mismatch : value array of shape ( 1000 , ) could not be broadcast to indexing result of shape ( 1000,3,255,255 )
The only meaningful way to do this here ( if option ( 1 ) is not feasable ) , is to simply ignore ( 1 / n ) / sqrt ( 1 / n1*n2 ) .
Since this number is smaller than one , the estimated correlation coefficients will be larger ( in absolute value ) than in ( 2 ) , but will remain between -1 , 1 .
You can convert all ` nan ` values to zeros using ` np.nan_to_num() ` and then proceed further .
Convert ` nan ` values to zeros : #CODE
I posted an answer on doing this kind of dot product [ here ] ( #URL ) , may be helpful to you .
or if you want to use ` np.dot ` you have to flatten your ` X ` and ` Y ` arrays and combine them to appropriate shape as follows : #CODE
I need to translate a matlab code to python numpy code
Then print the min , max of the dictionary entries for the specific counts .
I would like to log all intermediate steps it takes .
This works because flattening an array returns entries in order of the fasted-varying index , and for C-style indexing , the last indices vary the fastest .
I have a .txt file that contains the ids of interest that are present in the HDF5 file and wish to output the corresponding data of those rows - all corresponding data are numerals .
I wrote the following code but the output only contains the ids ( single column ) .
What is the output of ` type ( hdf5_file [ ' observation '] [ ' ids ']` ?
Also , could you dump the output of ` dir ( hdf5_file [ ' observation '] [ ' ids ']` ?
G = sum ( w [ i ] *f ([ i ]))
I have a list that contains all the radii of a circle and I'm trying to compute the diameter of the circle with the diagonal radii .
yes :) diameter is just 2*radii but in this case will be the diagonal radii .
What is a ` diagonal radii ` ?
But you may want to consider also changing the outer loop to a NumPy loop , by using a 2D array instead of a list of arrays : #CODE
I only want to sum the radii which has a value of > 0 .
How to efficiently sum numpy arrays after multiply them ?
And what if the formula ** S_i = sum ( U_j * U_j.transpose ) * K_i ** where i is not necessarily equals to n but far less than n , is there any change on the difference of time efficiency between using ` np.dot ` and ` np.einsum ` ?
( only positive numbers are allowed as indices ) For now I just add to x and Y to make it positive .
In the plot when I hover the mouse over the graphics , the max value is shown at x=12 and y=7 .
You could use the ` indices ` routine : #CODE
NumPy has built in ufuncs for all the basic arithmetic operations and some more interesting ones too .
It will return a series with indices as " ItemID " and value as the count of " ItemID " .
Numpy array , insert alternate rows of zeros
I am trying to modify a NumPy array by adding a row of zeros after each row .
If you take the dot product of two one-dimensional arrays you will get a scalar .
I don't know well numpy , and have to realize that W [: , np.newaxis ] ( or W [: , None ]) gives a diagonal matrix .
I.e. array ([[ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ]]) means a 5x5 diagonal matrix with these values on the diagonal .
You need to turn W into a column array to broadcast this multiplication correctly over A .
I found another approach ( using W as a diagonal matrix , and matricial products ) : #CODE
But it looks like it's used for effectively removing those entries from the ` max ` computation in the last line .
If Python doesn't have " infinity " ( I don't know Python ) and if ` dist ` is really a distance ( hence nonnegative ) , you could use any negative value instead of ` -inf ` to achieve the same effect , namely remove those entries from the ` max ` computation .
For instance if I want to find the the maximum value in a function ( and have forgotten the command max ) .
I'm a python beginner and I'm just trying to translate an idea into python code .
Are they over a small set of unique integer values ?
It's simply the dot product : 3 .
I also have a question : why the dot product * ( aka as scalar product ) * is simpler to compute than the length of intersect between the 2 vectors ?
We use the sparse dot product here because it is vectorized , while numpy's intersection function is vectorized only for 1d arrays .
For calculations with 2 terms , ` einsum ` often does as well as ` dot ` , and better if it saves transposing etc .
It can be boosted by telling the function the fact that values in the vectors are unique .
` np.in1d ` and ` np.intersect1d ` will both sort the concatenation of the unique values in your two arrays .
You can use ` np.take ( ..., mode= ' clip ')` to return maximum for such values ( and it will fail the equality test ) .
Fancy array indexing into different column indices of each row
The number of entries ` s ` is the same for each row , but their indices vary .
As we need to access 3 columns on each row we need to repeat each row's index 3 times , we can do this using ` numpy.repeat ` , now using the repeated row indices and flattened version of ` col_ix ` we can perform multi-dimensional indexing on ` x ` : #CODE
A second possibility would be done in df.values and probably use some functional programming in numpy to broadcast values .. as referenced here but I am not such a great numpy maestro .
Further reading about ` resize ` and some caveats here .
The ` *stack ` commands all use ` concatenate ` .
You are specifying the axis from which to delete the given index ( or indices ) .
You select different rows of ` z ` by varying the first index of ` z ` ( i.e. by selecting indices along axis 0 ): #CODE
Or you could construct a discontinuous list of indices , e.g. ` [ 0 , 1 , 4 ]` #CODE
To flatten the list , i do the following : #CODE
I think you are trying to flatten your np array into a single list .
So you could flatten a list of list like this ::
Say we have a 5x5 numpy array of ones : #CODE
To flatten the array : #CODE
We can directly find the min , max and average of ` a ` #CODE
The same goes for max and average : #CODE
column ' mod ' and return the values in column ' n ' that correspond to the column ' m ' hits .
( since 101,201 and 501 all have close hits in column ' mod ') .
np.isclose returns a boolean indexer : for each value of the iterable , there's a ` True ` or ` False ` value corresponding to the value ` m ` being within ` atol ` of each value of ` df [ " mod "]` .
This doesn't answer the question , because your comparison between ` m ` and ` mod ` does only compare ` m [ i ]` vs .
` mod [ i ]` .
` mod [ j ]` for all ` i ` in ` range ( len ( m ))` and ` j ` in ` range ( len ( mod ))` .
As we care only about ' mod ' values that have adjacent ' m's , aggregate over axis=0 : #CODE
You have indices of next elements , but they may point beyond the original
Often ` einsum ` is as good as ` dot ` .
` dot ` just has tighter code for a specific combination of dimensions .
` dot ( b , a )` produces a ( 2 , 2 , 4 ) array .
` diagonal ` selects 2 of those ' rows ' , and transpose to clean up .
For the ` a , a ` case we have to do something similar - roll the axes of one array so the last dimension lines up with the 2nd to last of the other , do the ` dot ` , and then cleanup with ` diagonal ` and ` transpose ` : #CODE
` tensordot ` is another way of taking a ` dot ` over selected axes .
The corner coordinates and the ` min ` and ` max ` functions should be all you need .
This type of intersection is easily done by the " min of the maxes " and " max of the mins " idea .
Using the max and min approach is just an easy way of what would otherwise be a complicated set of conditionals to determine the relative positions .
That is , read ` max ( a.xmin , b.xmin )` as the " righmost left corner " , etc .
You don't even need to reshape : #CODE
And if you know the shape already , you can reshape that : #CODE
your post links to pss12lons instead of pss25lons , so the reshape isn't working
Python : Creating a 2D histogram from a numpy matrix
I want to create a 2D histogram using this data .
If you have not only the 2D histogram matrix but also the underlying ` x ` , ` y ` data , then you could make a scatter plot of the ` x ` , ` y ` points and color each point according to its binned count value in the 2D histogram matrix : #CODE
I would like to append the results run by run , row by row to a human-readable file .
The use of ` nonzero ( in1d() )` to find the matches looks good , but I haven't explored alternatives .
Since there are a lot more rows ( users ) than subreddits , it might be worth collecting , for each subreddit , a list of user ids .
At.A is A , and det ( A )= 0 --> singular .
fft of numpy and octave different on transpose
when i do an fft from numpy.fft i get following result : #CODE
but if i transpose the list in octave and python i get : #CODE
I also tried to reshape in python but this results in : #CODE
The `'` in octave is returning the complex conjugate transpose , not the transpose , ` .
In NumPy , the transpose of a 1D array is the same 1D array .
That's why ` fft ( np.array ([ 1+ 0.5j , 3+0j , 2+0j , 8+3j ]) .transpose() )` returns a 1D array .
You could take the FFT first , and then reshape .
To make a 1D array 2-dimensional you could use ` reshape ` to obtain a column-like array of shape ( 4 , 1 ) , or use ` np.atleast_2d ` followed by ` transpose ` : #CODE
Instead , reshape the array to ( 1 , 4 ): #CODE
Again the FFT is taken over the last axis , returns an array of shape ( 1 , 4 ) , which you can then transpose to get the desired result : #CODE
Also , The ` matrix ` class ( unlike the ndarray class ) has a ` H ` property which returns the complex conjugate transpose .
If you first reshape the input to ( 4 , 1 ) , use ` axis=0 ` in the call to ` fft ` .
the array ` b1 = np.array ([ True , True , True , False ])` causes a ' ValueError shape mismatch : objects cannot be broadcast to a single shape '
are there only ones and zeros ?
You can use ` np.where ` to return a tuple of arrays of x and y indices where a given condition holds in an array .
Calculate min of rows ignoring NaN values
Numpy append to an empty array
So when you append ` x = np.append ( x , 1 )` , the value ` 1 ` get's appended to your array ( which already contains 0 ) i.e. it now contains 0 and 1
Since you have no values in the empty array , when you append ` x= np.append ( x , 1 )` the value ` 1 ` get's appended and the length of ` x ` becomes 1 ( i.e. it now contains only 1 )
So whatever you are going to append further to ` x ` will contain ` 0 ` along with the appended value
You can skip the ` reshape ` step with the matrix class as all matrices are inherently 2D .
numpy row pair sum of squared row wise differences without for loops ( only api calls )
For computer language only folk this would translate as :
Numpy correlate
I've two signals stored in a 1D array ` a ` and ` b ` that I'd like to correlate .
You can use ` np.ravel ` , or the method ` a.ravel() ` , to " flatten " the array into a 1D array with shape ` ( 10501 , )` .
Since I then want to use the results from this calculation later on , e.g. to FFT , this creates a major problem , since the fft routine can't handle NANs in the input .
What does it mean when numpy.shape returns a partially empty shape and why can't I concatenate it with another array ?
Beyond just the answer to the concatenate question , what is the fundamental difference between an array with shape ( 400 , 1 ) and one with shape ( 400 , ) ?
Functions that automatically change the shape of the array such as ` vstack ` and ` hstack ` still call concatenate , but with additional python overhead ( e.g. vstack source ) .
Is it possible to vectorize the above so that I lose the outer loop over the elements of ` actualrgn ` ?
I.e. something like ` I = actualrgn > = a1 && actualrgn < a2 ` , but not that , cos I already tried and it didn't work ...
Roughly speaking a sparse matrix with ` N ` nonzero terms will store the data in 3 arrays ` ( N , )` long ( 2 of those will be int dtype ) .
So as I mentioned it is strange that with numpy I can calculate a dense matrix of 4096 x 4096 , but here I can't exceed a sparse matrix with 55496 nonzero elements .
Numpy Broadcast indices from shape
I want a function that takes those shapes and gives me an iterator returning the indices from these arrays with these shapes that would be broadcast together , and the indices in the resulting output array .
Nice , succinct , but it requires that the arrays be instantiated in order to get the indices .
But , the solution depends on the creation of dummy arrays ( in1_ix , ind2_ix ) , even though it only needs their shape information to come up with the indices .
if the diagonally opposed element are equal but opposed in sign , keep the absolute value of an element and zero the diagonally opposed value
if it is not the case , then one of the two element is 0 ( but we don't know which one ) , so take the absolute value of both .
Once this is done transpose the lower triangle of the matrix and add it to the upper triangle .
Note that ` triu ` and ` tril ` are the same size as ` a ` , but filled with zeros outside the triangle .
Define the triangle you want to modify , and transpose the other .
Whats the best way to plot a histogram of this data with minute bins and 10-min bins ?
[ GCC 4.2.1 ( Apple Inc . build 5666 ) ( dot 3 )] on darwin
I assume that with object-arrays , Numpy won't use the objects methods to compare , but the ids like you assume .
@USER : I can demonstrate that this is not the case : an array with ` dtype ` of ` object ` containing strings uses string contents -- and not just ids -- to establish equality .
@USER : Try two different strings with identical contents but different ids .
A histogram of random samples of the distribution :
You initial ` onsite ` with ` empty ` , so the off diagonal values can be anything .
Note that the matrix only has only non-zero elements at the diagonal .
Here is the code to do that , the basic idea is to ` diff ( membership )` , and get the index of rising edge and falling edge : #CODE
Speeding up selecting sets of 3 nodes that form triangles with given min and max side length
` triangles ` will list all the indices within the points that meet your specifications .
That is take three random points , ( check min and max distances , ) calculate the radius of the corresponding circle , compare to previous calculated radius , repeat .
You would have to figure in some kind of stopping criteria , like max number of iterations , or no better radius found over the last x iterations , or radii growth rate below x percent , or some combination of those .
Then I want to calculate the laplacian matrix for ` W ` ( the symmetric one ) , according to the formula ` L = D - W ` , ( where ` D ` is the diagonal matrix whose diagonals are sums over either cols or rows of ` W `) , according to definition of ` L ` , I need to check whether it's positive semi-definite ( PSD ) or not .
But I still wonder is there a better way to check PSD or is there any specific relationship between eigenvalues of a symmetric matrix and its triangular ones ?
Is the original array already sparse ( plenty of zeros ) , or are those just a product of ` tril ` ?
I tested your code using ` %timeit ` and it's much , much faster than mine indeed , and this morning I've also looked up many references , which show there's no special relationship between eigenvalues of a symmetric matrix and its triangular ones , except for the ` trace ( L )` is equal , so seemingly there is no need to convert a no sparse symmetric matrix to sparse format .
how to construct diagonal array using a 2d array in numpy ?
Using ` np.diag ` I'm able to construct a 2-D array where a 1-D array is input is returned on the diagonal .
This question may be relevant ; it also shows a quicker ( but slightly more verbose ) way to derive the desired diagonal matrix from the 2D array .
Append numpy array with diff dimensions
But when I try to append another record with the same format , I get an error : #CODE
I am working with 3 numpy array ` A ` , ` B ` , ` C ` ( this are ones array to semplify ): #CODE
I think you misunderstood , the matrices I'm referring to are examples ( now I edited my question in order to clarify it ) , I'd like to insert in A all elements from B , according to C but A is a generic matrix , not a ones matrix .
You could reshape your arrays to 3d , and use the solution ( s ) from the previous question : #CODE
An alternative to your ` concatenate ` would be : #CODE
What the formula does is compute a few partial sums so that the sum of these sums is the volume sum .
Of course , with all ones there could two errors that cancel each other out , but this wouldn't happen everywhere so it's a reasonable test .
For something more robust , you could modify ` polyfit ` to accept its own ` ddof ` parameter , perhaps in lieu of the boolean that ` cov ` currently accepts .
A quick final note about the calculation of ` cov ` : If you look at the wikipedia page on least squares regression , you'll see that the simplified formula for the covariance of the coefficients is ` inv ( dot ( dot ( X , W ) , X ))` , which has a corresponding line in the numpy code -- at least roughly speaking .
The numpy code also does some scaling ( which I understand ; it's part of a strategy to minimize numerical error ) and multiplies the result by the norm of the residuals ( which I don't understand ; I can only guess that it's part of another version of the covariance formula ) .
and ` mat ` is now : #CODE
roll as suggested by abarnert would work , but creates two copies , which isn't ideal from a performance perspective .
Unable to retrieve required indices from multiple NumPy arrays
I have to know the index of the last array ( d ) where the elements of d are smaller than 20 , but those indices of d should be located in the region where elements of array ( a ) are 1 ; and the elements of array ( b ) and ( c ) are not 1 .
I then digitize the Frequency values into these bins and use : #CODE
I then Eradicate any negative frequency values and the corresponding fft values using : #CODE
In my speedtests , ` np.trunc ` is still slower , but looking at the [ source ] ( #URL ) , this is probably because it is implemented in terms of ceil and floor , and not a simple cast .
With SSE4.1 it's possible to do round , floor , ceil , and trunc from double to double using : #CODE
SSE4.1 can round / trunc / floor / ceil from float to float or double to double efficiently .
But I would like to do so when using the fmin_bfgs function which requires the gradient .
I even tried returning the gradient and the cost in two different functions , but got a ` vstack ` error of some sort ( if that matters / helps at all ) .
There are several ways of using bfgs minimzation with an explicit gradient .
First , no gradient information : #CODE
Now , with gradient , you can either have a callable which returns the function and the gradient : #CODE
Alternatively , you can set ` jac ` to a callable , which should have the same signature as the cost function and return the gradient : #CODE
I took you advice on reproducing something simple , so I had the gradient function spit out what the first gradient should be .
[ 0 . ]]))` before a flatten method , and ` ([ 0 ] , array ([ 0 ., 0 . ]))` after a flatten method .
Best break your question into separate ones : there's clearly one about minimization itself , and a separate one about the cost function .
I'm trying to use both counts and tfidf as features for a multinomial NB model .
I have a CSV containing weather data like max and min temperatures , precipitation , longitude and latitude of the weather stations etc .
Finding the max or min is easy :
How can I find the position of where the min or max is located , so I can find the latitude and longitude ?
Error : List indices must be int
How could I find both and their indices ?
You can find the index number of the ` min ` and then use this to find the elements present in the same position on the longitude and latitude lists .
Without actually seeing your data it is diffult to say how to find location of max and min in your particular case , but in general , you can search for the locations as follows .
Boolean in a numpy sum
2 Does it .. sum a boolean ?
So to answer to your question " Does it .. sum a boolean ?
Simple example for sum : #CODE
Only sum the numbers that are 3 or higher ?
How do you know 212 is the sum of all numbers that are 3 or higher ?
Its not the sum of all numbers that are 3 or higher .
it is the amount of such numbers , not sum of their values .
` np.sum ( dice == 6 , 0 )` is the same as ` np.sum ( dice == 6 , axis=0 )` , meaning the sum is taken along the ` ndice ` axis , so now you have a 1 x 1000 array .
Taking the sum of this 1 x 1000 array then gives you the number of runs where this is true .
So say if dice [ 1 , 0 ] and [ 2 , 0 ] are 6 , and the rest are different , [ 0 ] in the new sum array will be 12 ?
The call to ` flatten ` can be removed since ` arr ` is already one dimensional .
I don't know of a built-in function to do it , but you can write one using the math package to specify approximate indices like this : #CODE
Get Row and Column Names ( argmax ) for max entry in pandas dataframe
df.idxmax() returns max along an axis ( row or columns ) , but I want arg_max ( df ) over the full dataframe , which returns a tuple ( row , column ) .
I preprocess the correlation matrix to consider its absolute values and set the diagonal elements to -1 .
For ` numpy ` arrays you could ` reshape ` the array into a multidimensional array such that the continuguous parts are all at the start of a row and select the first portions of the rows : #CODE
( There are many ways to reshape and flatten the result ; read the ` numpy ` documentation if you are interested ) .
From this last idea you could also do something to avoid ` reshape ` ing the numpy array : #CODE
However I cannot think of a simple and efficient way of building the ` arr ` array of indices , so I doubt it improves performance .
Then I want to sum up the contribution of all pairs [ i , j ] for a fixed i and call this c [ i ] ( and repeat this procedure for all particles i ) .
Typically the number of relevant pairs is much smaller than N^2 , so having an ( N , N ) -dimensional array C with the relevant information at the positions [ i , j ] and a lot of zeros elsewhere is fairly quick with numpy , but also very inefficient in terms of memory usage .
Sparse matrices don't store zeros , so you wont have memory problems .
If you do not want all of those extra zeros , you could do something like : #CODE
And you could later assign these unique values by doing something like : #CODE
Great , the first part with the zeros is ( almost ) exactly what I need !
I just need to make sure that ci is of dimension N by appending zeros after the last non-zero element ( but that might not have been obvious from my question ) .
As an added problem , I'm not actually doing a sum when I aggregate , I'm doing a custom function that needs all the numbers at once ( doing it by rows and then columns wasn't working )
{ roughly - my_function = ( take sum of elements , if any were negative remove those from sum and then mulitply the whole thing by 1 /( 1+sum of negative elements )) }
As an added problem , I'm not actually doing a sum when i aggregate , I'm doing a custom function that needs all the numbers at once ( doing it by rows and then columns wasn't working )
Is there an easy way to index a numpy multidimensional array along the last dimension , using an array of indices ?
Let's assume I have an array of indices ` b ` , of shape ` ( 10 , 10 )` so that the result would be ` c [ i , j ] = a [ i , j , b [ i , j ]]` .
The ` choose ` option looks nice , but it is not general enough as ` choose ` does not work for indices larger than 32 ( a current bug in numpy , see [ discussion on increasing ` NPY_MAXARGS `] ( #URL )) .
@USER I tried this with unequal row and column sizes and it didn't work ( ` IndexError : shape mismatch : indexing arrays could not be broadcast together `) .
` glob.glob ` will operate in the current working directory , so ou might want to use the absolute path instead : #CODE
I want a way to quantify this matrix so I though of using the L1 Matrix norm ( numpy.linalg.norm ( flow , 1 )) Which throws a improper dimensions to norm error .
I'm thinking about getting around this by calculating the euclidean norm of every vector and then finding the L1 norm of a matrix with the distances of the vectors .
What would be the fastest way to iterate through a numpy matrix with vectors as entries , norm them and then take the L1 norm ?
As of numpy version 1.9 , ` norm ` takes an ` axis ` argument .
Numpy gradient of big 3D array consumes all memory
I'm using numpy to compute the gradient of a big 3D array with possibly arbitrary dimensions #CODE
Is anything better to do than this brute-force approach of using the numpy gradient ?
I am looking for the absolute error , not a goodness .
The residuals are the absolute error for each point in the fit .
` residuals ` then is the sum of least squares .
Then , the diagonal elements of ` cov ` are the variances of the coefficients in z , i.e. ` np.sqrt ( np.diag ( cov ))` gives you the standard deviations of the coefficients .
You can use the standard deviations to estimate the probability that the absolute error exceeds a certain value , e.g. by inserting the standard deviations in the uncertainty propagation calculation .
I know I could reshape but that looks dirty to me .
In the pandas std class , it will calculate standard dev by using the mean of column D .
Is that correct , or did you mean ` sum ` ?
First instinct is to just reshape the array as desired : #CODE
But now that the flattened array reads in the manner that you expect , you can safely reshape it to give you the desired result .
The bulk of the time in my method is consumed by ` mgrid ` itself , so you can't even get out of it by avoiding ` rollaxis ` and ` reshape ` .
I just realized that another way of achieving the ` rollaxis ` + ` reshape ` effect , but losing out on numpy-ness in the process , is to use ` zip ( a [ 0 ] .flatten() , a [ 1 ] .flatten() , a [ 2 ] .flatten() )` .
I have been trying to compute cumulative sum over 3 axis for a large 3D dataset .
Hey Ed , this gives me " ValueError : operands could not be broadcast together with shapes ( 4692 , ) ( 2 , ) "
I'm leaving off the details of determining which term is a slice , its dimension , and the relevant reshape .
If indices were generated by something other than ` ix_ ` , reshaping this slice could be more difficult .
This is the closer I can get to your specs , I haven't been able to devise a solution that can compute the correct indices without knowing ` A ` ( or , more precisely , its shape ... ) .
In more general contexts you have to know how ` j0 , j1 , j2 ` broadcast with each other , but when they are generated by ` ix_ ` , the relationship is simple .
returns ` 1 ` , making the sum not commutative !
Or wasn't the sum of bools intended in vanillas bool either ?
I think hstack and your concatenate method are basically equivalent .
` concatenate ` would work : #CODE
` sum ( right [ ones ] == 0 )`
replacing ` sum ` by ` np.sum ` helps indeed .
Should I transpose all my data when I read it in and then transpose again when I output ?
What is a little off in your examples is your use of ` x ` for indices .
If I would use numpy.zeros , then the first stack are 0's , and I want it to be completely empty before vstacking .
If you have the memory , it would be faster to append the rows to a list , and then simply call ` np.array ` ** once ** ( after the loop completes ) to turn the list of rows into a 2D array .
The formula should be like this : ` d = sqrt ( (( x2 - x1 ) * size ) **2 + (( y2 - y1 ) * size ) **2 )`
In this last matrix , you will find ( above the diagonal ) , the distance between each marked point in ` a ` and another coordinate .
The Euclidean distance between those two ` sqrt ( 2 ) ~ 1.4 ` .
So remember , ` pdist ` interprets its first argument as a stack of coordinates in n-dimensional space , ` n ` being the number of elements in the tuple of each node .
@USER , the way you've described the problem , you made it seem like each " clump " was a single marker , a single element ( 1 ) surrounded by zeros .
I would like to plot a 2D histogram that includes both positive and negative numbers .
I know you can specify colour levels in plt.contour and plt.contourf but I can't find a way to plot the 2D histogram using blocks .
Add ` vmin ` and ` vmax ` parameters with equal absolute values #CODE
numpy array to permutation matrix
I would like to turn it into a numpy array with tuples of each 1:1 permutation .
MPI for python broadcast
I want to work out ` T2 ` in the root process then broadcast it to all the other processes .
it did work out ` T2 ` in the root but it didn't broadcast it to the others they are all ` 0 ` .
In MATLAB and numpy , you can index a vector by an array of indices and get a result of the same shape out , e.g. #CODE
However , in R , indexing a vector by an array of indices returns a vector : #CODE
Or maybe the docs were just written with Python 2.6 instead of 3.4 , so they get the old not-quite-right rounding-for-printout rules instead of the newer , more accurate ones .
( The old rules truncate the partially-significant digit at the end ; the new rules give you the shortest decimal string that evaluates to the exact same bits . )
For a suggestion on how to save a sparse matrix to a text file , see my answer to a different question here : How to format in numpy savetxt such that zeros are saved only as 0
How to set array into float32 data type ( need to be compatible with append ) ?
So I found that if I use array ([ ] , numpy.float32 ) , then with append commands , it will say ' numpy.ndarray ' object has no attribute ' append ' .
There is a ` np.append ` function , but it is just a cover for ` concatenate ` .
' numpy.ndarray ' object has no attribute ' append '
-> You are trying to call append() on a numpy array but the type ndarray has no append method .
This creates a new array with all values ( old and appended ones ) .
I have a 1D array and I would like to compute sums over sliding windows of indices , here is an example code : #CODE
transpose row to column in python
I am a newbie in python and I would like to transpose a CSV-formated file .
I need to transpose the class variable into columns , i.e. #CODE
I know that it's possible to transpose a matrix completely with ` zip() ` in Python but I just want to transpose the ` class ` -column .
You want to pivot , not transpose the data : #CODE
I've been browsing the source and it seems the ` tri ` functions have had some very substantial improvements relatively recently .
parent method to append vector to attribute of derived class
Specifically , np.linalg.norm ( sparse_vector ) fails ( see Get norm of numpy sparse matrix rows ) .
When I give 1-cosine_similarity , the diagonal values which should be zero , are not zero .
You want the square root of the sum of the squares of the three dimensional velocities : #CODE
Using the norm is the best solution here .
In many occasions , I need the function to broadcast the operation , so that if both ` a , b ` are 1D arrays , the result will be a 2D array .
ufuncs have an ` outer ` method - your function ` do_sth ( a , b )` seems to be exactly ` np.add.outer ( a , b )` for example .
They might use functions like ` np.atleast_2d ` to ensure there are enough dimensions , and ` .reshape ( -1 , 1 , 1 )` to compress excess dimensions .
They might use functions like np.atleast_2d to ensure there are enough dimensions , and .reshape ( -1 , 1 , 1 ) to compress excess dimensions .
` np.tensordot ` is an example of one that performs axes transpose plus reshape on the inputs so it can apply the compiled ` np.dot ` .
numpy digitize with datetime64
As you can see , there is a 1 to 1 correlation down the diagonal from top left of the array to bottom right .
I am developing a fft visualization option , and I am running into an overflow error when I fft my data sets . here is my exact problem :
I only get this error when I run the -f fft option .
One point to make about your specific examples is that NumPy and SciPy support vectorization ( you can pass a sequence to ` numpy.sin() ` and create a sequence of values , ` sin ( x )` .
But as for why there isn't just one common way to do these things , I think that's just the bazaar-like nature of open source software libraries : these packages have bubbled up as the most popular and widely-used ones for the purpose ; there hasn't been any top-down , cathedral-like planning behind it all .
how to perform histogram on numpy array whose dtype is object using histogramdd ?
I want to perform histogram on a ` ( N , 3 ) numpy array ` , whose three dimensions represent longitude , latitude and time-stamp correspondingly , like this : #CODE
I know it was the datetime object causing the error , but I want to know how to correct this error or how to perform histogram on numpy ndarray whose ` dtype = object ` ?
While this will give you a histogram , you may also need to re-interpret the floats as dates .
You can use the ` tile ` function to do this efficiently .
Alternatively , if you are summing a series ` ( -1 ) ^k a_k ` and you have already computed ` a_k ` , as a numpy array , you can sum ` a_k [: : 2 ]` and ` a_k [ 1 :: 2 ]` and take the difference .
The reshape is in there so that numpy will apply broadcasting , more here also .
This is the join between the arrays .
Assuming the values in the first columns are all perfectly equal , the insertion indices returned have two nice properties .
Second , they can be used as indices into ` a ` to create a new array using fancy indexing .
Get the indices of the matches .
Use the indices to create the result from ` a ` #CODE
Elementwise division , disregarding zeros
It's a python question : let's say I have an ` m+1 ` -dimensional numpy array ` a ` consisting of non-negative numbers , and I would like to obtain an array ` b ` of the same size where the last coordinates are normalized so that they sum up to 1 , or zero in case all of them were zeros .
yields ` Y == X ` ( because along the last axis the sum is already one or zero . )
It is slightly easier to sum over the leftmost ( zeroth ) axis .
No , it works on arbitrarily dimensioned data , the sum is on the last axis .
If you wanted to create your example array , you'd do best to use a reshape command on a 1d array : ` np.arange ( 1 , 7 ) .reshape (( 3 , 2 ))` .
for all choices of indices .
Alternatively , you could be looking for ` tile ` instead : #URL
So , basically , I'm looking for a way in pandas to start at the mode and return a new series that of value counts that includes 68.2 % of the sum of the value_counts .
This is the order they would be added based on an algorithm that walks the value count on each side of the mode and includes the higher of the two until the sum of the counts is > than 14.3 .
I am trying to create a histogram .
I have several numpy arrays that I would like to multiply ( using ` dot ` , so matrix multiplication ) .
But I feel like there's a usage of ` dot ` or ` tensordot ` or something that would do this in one line really easily .
I just can't make sense of the ` dot ` and ` tensordot ` functions for > 2 dimensions like this .
` einsum ` performs a sum of products .
Since matrix multiplication is a sum of products , any matrix multiplication can be expressed using ` einsum ` .
It means that the sum #CODE
Since this sum can be computed for each ` i ` and ` j ` , the result is an array with subscripts ` i ` and ` j ` .
If there's always a column of zeros and ones , why bother evaluating ?
For example , for ' a ' , I need ` #URL I could get headers and row data separately , however , I'm unable to concatenate them in the desired way .
Pandas : how to broadcast values by specific column rather than index when doing arithmetic operation ?
In the other words , when doing the operation columns ' b ' plus ' d ' , it will automatically broadcast just like ' a ' is the index .
numpy is arriving at the value given ( ` 2 ** 36 `) by reducing the shift modulo the length of the integer , 64 bits ( 100 mod 64 == 36 ) .
The NPY format is not simply a dump of the array's data to a file .
Can anyone explain me why does this happen ? and what do i do if i have keep the frame from getting changed when i write a composite clip into a video ?
These structured arrays work similar to the typical n-dimensional arrays that you're used to but that are homogeneous ( i.e. all elements are of the same type ) , but you can't group numerical indices within the same set of square brackets ( so no ` data [: 3 , ' f1 ']` , but ` data [: 3 ] [ ' f1 ']` or ` data [ ' f1 '] [: 3 ]` are fine ) .
If you do not care about speed so much you could save your results in an ` int16 ` first and then clip it and write it back to an ` uint8 ` array .
And if it turned out you did need that speed , what would be the alternatives , adding a c-type with a check on each basic operation to floor or max when it would otherwise wrap ?
Difference between ` diff ` and ` ediff1d ` is ediff1d will flatten before comparing : #CODE
( The question was " What's the difference between ` ediff1d ` and ` diff ` ? " ; I deleted the question just before it was answered . ) Thanks .
The ` sum ` of an array is much faster than for a dataset .
How to get lists of indices to unique values efficiently ?
Is there a built-in method that would help me achieve the following efficiently : given an array , I need a list of arrays , each with indices to a different unique value of the array ?
I am aware that ` pandas.Series.groupby() ` can do this , but it may no be efficient to create a dict when there are over 10^5 unique integers .
fyi , pandas.Series objects also have a ` unique ` method .
Also , if you want to make sure that the indices for each value are sorted , I think you will need to use a stable sort , e.g. ` np.argsort ( unq_inv , kind= ' mergesort ')`
First , we get arrays of the ` ( i , j )` indices where the data is nonzero : #CODE
Here ` ix ` is a list of random , unique indices , 20% the length of ` i ` and ` j ` ( the length of ` i ` and ` j ` is the number of nonzero entries ) .
To recover the indices , we do ` i [ ix ]` and ` j [ ix ]` , so we can then select 20% of the nonzero entries of ` x ` by writing : #CODE
You can get rid of this using ` strip ` .
How to do a sum of sums of the square of sum of sums ?
I have a sum of sums that I want to speed up .
Note : S_{indices} : is the sum over those indices
Also , I have thought of trying to expand the square but won't that be a killer as I would need to sum over x , y , k , l , k , l instead of x , y , k , l ?
I tried to simplify the sum I'm actually after but messed up and the sum above allows for the excellent answer provided by @USER .
I've edited the code above to demonstrate the sum which is below :
For the square sum , we have : #CODE
Similar reductions occur when continuing the sum over ` x ` and ` y ` : #CODE
Update : I just realized that perhaps for the square sum you wanted to compute
So when we sum over the over variables we get : #CODE
we see that the total is just the sum of all of the matrix elements of A times the sum of all of the matrix elements of B .
Given a symmetric matrix ` A ` and a positive definite matrix ` B ` , we can find the eigenvalues and generalized eigenvectors with scipy's ` scipy.linalg.eigh ` , or matlab's ` eig ` .
eigenvalue problem is to find nonsingular P and diagonal D such that #CODE
The diagonal of D holds the generalized eigenvalues , and the columns of P
The " reverse " problem can be stated as : given nonsingular P and diagonal D ,
Each adjacency matrix is about 4000 x 4000 ( square matrix where the diagonal means the object is paired with itself ) so it's big but not too big .
@USER You might want to put a print statement in that outer for-loop you are talking about .
Also , does each object belong to a unique pair ?
Instead of looking for absolute peaks , try looking for inflection points .
@USER : Without seeing your code , or what " doesn't like it " means , the only ideas I have are the ones that are already written in the docs , and the docs are written more clearly than anything I can cram into an SO comment , so not really , no .
And I want to have rows unique by third column .
You could try a combination of bincount , nonzero and in1d #CODE
How to sum of squares of sum with memory limitations ?
How to do a sum of sums of the square of sum of sums ?
The solution below presents the 3 different methods to do the simple sum of sums and 4 different methods to do the sum of squares .
sum of sums 3 methods - for loop , JIT for loops , einsum ( none run into memory problems )
sum of square of sums 4 methods - for loop , JIT for loops , expanded einsum , intermediate einsum
You could sum over one index first and then continue the multiplication .
Leveraging what each does best ( numexpr - multiplication , numpy - dot / tensordot , summation ) you can still improve over the fun3 more than 20 times .
Also , the final thing to note is that if you're doing it this way you might as well use ` numexpr ` . for example replace ` np.sum ( Fv*Fy*B , axis=-1 )` with ` ne.execute ( ' sum ( Fv*Fy*B , 3 )')` this way it doesn't store the intermediate result and is multithreaded
Also , please note that in python you have the indices from 0 , so you should use cols -1 instead of cols and rows -1 instead of rows
To do this , I've written code modelling an asymmetric triangle and implemented numpy's fft .
At 0 the abs of the fft is 0 .
then you could use ` reshape ` to make it 2-dimensional with 3 columns : #CODE
From my searching all I've been able to find so far are ways to append ` newcol ` to the end of ` x ` to make it the last column , or ` x ` to ` newcol ` to make it the first column .
slicing numpy arrays by combining indices and expression masks
What I don't know is how to efficiently get a mask from a given list of indices ; the brute force approach in my SSCCE below ( at ( A )) #CODE
You can just use a list of indices directly to index your array .
You just need to use ` append ` .
` append ` adds them to the end of the list , which is exactly what you want .
If you create an empty list ` old_items_data = [ ]` and try to access indices like ` old_items_data [ 0 ]` you will get index out of bounds errors .
Where each row is a user and each column is a histogram bin value .
You could use ` groupby ` to group by User , ` value_counts ` to compute a histogram , and ` unstack ` to reshape the result : #CODE
As others answered , numpy has built in vectorized versions of common functions , including + , / , and exp .
What I did , was to calculate the autocorrelation coefficient for each ` k ` value from the range of ` |-k| ; k ` where ` k ` belongs to ` N+ sum { 0} ` .
I want to select some values for all these 2D arrays along their axes ( condition on two reference axes ( ` x ` and ` y ` below ) , and then perform some operation ( e.g. , mean , sum , ... ) on the resulting succession of " smaller " 2D arrays .
Regarding Step2 , which one is the best option , are there any other , more efficient ones , and why is calculation ` C2 ` not working ?
@USER : also , if speed is really an issue for you , you might want to try things like ` take ` , ` compress ` , etc ( eg , see #URL ) But these things generally only give a small factor of improvement ( 1x-3x ) and one should always consider whether it's necessary to spend chasing these .
Unfortunately , won't find a python implementation of ` correlate ` in ` numpy `' s code base , but if you are familiar with ` C ` and ` python `' s extension modules , you can have find the relevant code here .
You can get the same result by filling ` v ` with zeros : #CODE
You get the same result by filling ` v ` with zeros from the right and the left , until ` len ( v ') == len ( a ) + len ( v ) + 1 ` .
For example ( note ` e-05 ` on the ` zeros ` call ): #CODE
But then strangely writing to an array created with ` zeros ` is noticeably slower than an array created with ` zeros_like ` : #CODE
My guess is ` zeros ` is using some CPU trick and not actually writing to the memory to allocate it .
` zeros ` uses ` memset ` ; ` zeros_like ` seems to effectively do a ` fill ` which does a ton of nonsense .
` zeros ` does not use memset .
My guess is that ` zeros ` , ` ones ` , ` empty ` are all early compiled creations .
` zeros_like ` was written with more of an eye toward easy programming maintenance ( reusing ` empty_like `) than for speed .
#URL - source for ` zeros ` and ` empty ` .
This simple test with the ` memory_profile ` package supports the claim that ` zeros ` and ` empty ` allocate memory ' on-the-fly ' , while ` zeros_like ` allocates everything up front : #CODE
Strange that ` zeros_like ` doesn't just call through to ` zeros ` .
` zeros ` obtains memory from the operating system so that the OS zeroes it when it is first used .
` zeros_like ` on the other hand fills the alloced memory with zeros by itself .
Both ways require about same amount of work --- it's just that with ` zeros_like ` the zeroing is done upfront , whereas ` zeros ` ends up doing it on the fly .
Most of the packages of the scientific python stack do not support ` python3.2 ` anymore .
Is it possible to sum over multiple axis in numexpr ?
How to sum of squares of sum with memory limitations ?
How to do a sum of sums of the square of sum of sums ?
You can of course reshape the result of the multiplication but I doubt it will be faster than np.sum over two axes .
I have tried using for loops to append data to new lists , but that is a very slow process .
With ` dtype= ' u1 '` the array should contain 1 byte ints rather than the normal 4 byte ones .
We'll reshape it to give rows of four elements each : #CODE
You can give ` reshape ` more than two dimensions , too , so say ` x ` was ` 5x5 ` RGBA image in a 100-element 1-d array , you could do ` y = x.reshape ( 5 , 5 , 4 )` , so that ` y [ 0 ] [ 0 ]` gives the four channels of the ( 0 , 0 ) pixel , ` y [ 0 ] [ 1 ]` contains the four channels of the ( 0 , 1 ) pixel , and so on .
How is able to keep only the lines and not the whole spectrum of each histogram ?
You can use , for example , Kernel density estimation to " smooth " out the histogram .
In this case you place a bunch of gaussians centered at your data points -- the sum is reflective of the underlying probability distribution .
Finding min and max and other functions , pandas series
However ` min ( x , 5 )` works , but in order to get the same behavior with the series I must do ` ss.apply ( lambda y : min ( y , 6 ))` and this is a pain for the following reason :
Try numpy's min function ( you might need to feed the args as a list .
I used fsolve to find the zeros of an example sinus function , and worked great .
Not full stack traces or such , but just something to give an idea of why it might not have worked .
Floats as array indices don't make sense .
Which is , as stated , how to find the intersection between a data set and a function , or two data sets , as we can easily define an array full of zeros .
-- edit -- an example for computing an intersection of sin and cos from 20 samples ( I've used cubic spline interpolation , as piecewise linear gives warnings about the smoothness ): #CODE
That's because I missed the later a which makes a to build zeros (( n , 1 )) .
The problem is that ` zeros ` function of numpy generate a structure like ` [[ 0 . 0 . 0 . 0 . ... ]]`
` zeros ` function return a multidimensional array .
When executing ` theta = zeros (( 1 , n ))` with n = 10 will return #CODE
However , this shuffles the row indices and so each column has the same ( random ) ordering .
This method can be useful if you have a very large array which you don't want to copy because the permutation of each column is done in place .
This produces a random permutation of each column's indices .
Note that this has sub-optimal asymptotic time complexity , since the sort takes time ` O ( n m log m )` for an array of size ` m x n ` .
Finally , there is also a ` nonzero ` method that does the same thing as ` where ` in this case .
Using the transpose trick now : #CODE
If you'd prefer the format of this output array to be grouped by column rather than row , ( that is , ` [ column , row ]`) , just use the method on the transpose of the array : #CODE
Yes , you're right , using the transpose is better than ` argsort ` here .
I'm new to Python ( coming from MATLAB ) , is there a more generic statement that I could use to ' roll up ' all of the columns ?
I am having trouble with my indices in a nested for loop .
How can I effectively call out both indices ?
It points to the line containing rhs [ j , i ] = alp [ j ] * sin ( theta [ j ]) * c [ j ] * a [ j ] / ( 4 * span )
What you have posted above says ` rhs [ j , 0 ] = alp [ j ] * sin ( theta [ j ]) * c [ j ] * a [ j ] / ( 4 * span )` ...
Using vectorised operations to check a condition creates a Boolean array that you can compress and cast to produce your desired output of ones .
Writing a 3D numpy array to files including its indices
how do I write it to a file so it list the indices and the array value
Would that work with the np.savetext to write it to a file ? and if so can I strip out the brackets ?
The sum of two lists is a new list , with the items concatenated .
The ` strides ` attribute contains the necessary information how to map multidimensional indices to the flat indices in the memory .
You could use reshape in combination with swapaxes #CODE
Trying to strip b ' ' from my Numpy array
Writing to a ' wb ' file removes that outer layer of ` b ''` , leaving the inner for the bytestring .
`' numpy.void ' object has no attribute ' decode '`
Using ` np.vstack ` and transpose ( similar to your method ): #CODE
Using ` np.concatenate ` and ` reshape ` : #CODE
You can however , find a least-squares solution that is a " good-fit " -- though as mentioned is no unique solution to the matrix given since some rows are linearly dependent on each other .
hey , that was a great answer . now i understand the problem and i see how dump i was . thanks for this input
Or do I have to ( gulp ! ) just use a one dimensional array and just reshape it at the end ??
Or do I have to ( gulp ! ) just use a one dimensional array and just reshape it at the end ?
Numpy is very lenient about what kinds of arrays can be multiplied using ` dot ` :
it is a sum product over the last axis of ` a ` and the second-to-last of ` b `
I'm trying to run something in a for loop and analyze a 2D histogram of some data where the bin vectors do not change .
Can you post the full stack traceback ?
This code is to implement a log-likehood gradient to estimate coefficients .
Where ` z ` is the dot product of the weights with the x values .
Since they're of ` matrix ` type ( as it seems from the stack trace ) , the ` * ` operator translates to ` dot ` .
Perhaps you forgot to transpose the first matrix or something like that .
There are differences between them , in particular `` * `` translates to elementwise multiplication for arrays and to `` dot `` for matrices .
In your question you can see in the stack trace that `` dot `` gets called , so I assumed you had `` matrix `` objects .
I changed the flawed line to ` s += lhs * np.float64 ( rhs )` hoping it'll work , and it does work until it throws : ` ValueError : non-broadcastable output operand with shape ( 18209 ) doesn't match the broadcast shape ( 1,182 09 )` .
Use ` numpy.any ` with ` axis=0 ` ( to flatten along the first axis , i.e. flatten along the rows ): #CODE
Gives you a generator with the values , which you can reshape and print as you need .
since power and sum operate elementwise so the expression is OK as it is .
Disclaimer : this answer is built on the one by pv ., and is just a little more than a comment on her / his answer .
You can resize the array after taking the mean : #CODE
Now tile that resulting array into the shape of the original #CODE
In order to correctly satisfy the condition that duplicate values of the means appear in the direction they were derived , it's necessary to ` reshape ` the mean array to a shape which is broadcastable with the original array .
or alternatively , use ` .transform ` to obtain values based on ` clients ` grouping and then sum for each ` clients ` and ` odd1 ` grouping : #CODE
If your ` x ` values are uncorrelated , then the uncertainty of the higher-frequency portions of the fft will be meaningless ( infinite uncertainty ) .
In fact , depending on the what the values and uncertainties are in ` x ` , the uncertainty of fft may be meaningless .
The fft depends on the differences between adjacent values .
then a little bit of algebra shows that the variance of X_k is the sum
it is the sum of the variances of the measurements in ` x ` .
all ( approximately ) the same as the sum of the measurement variances .
It's still giving ` IndexError : too many indices for array `
I'm getting ` IndexError : too many indices for array ` for your answer and also @USER method .
The Plural of index is " indices " , not " indexes " !
I have changed it from max to min , therefor some still say max , but i dont think that matters as long as " y [: idx ] .argmin() " is min ?
Also , ` min_idx = y [ 2 :] .argmin() ` gives you the min index with respect to ` y [ 2 :] ` .
So the min index with respect to ` y ` would be ` 2+min_idx ` .
Does ` FGrad2 ` need some sort of diagonal component ?
You can linearize the problem ( fast , unique solution ) , but any noise in the " tails " of the distribution will cause issues .
We could write a wrapper to " translate " , but let's just re-write the ` gauss2d ` function instead : #CODE
This is optional ( the default is all ones , if I recall correctly ) , but you're likely to have problems converging if 1 , 1 is not particularly close to the " true " center of the gaussian curve .
The ` ones ` need a ` dtype=int ` parameter .
You might not even need the ` ones ` in that line .
The use of ` ones ` instead of ` zeros ` also suggests that .
How to calculate the mean of a stack of arrays ?
my stack is something like this #CODE
A stack of 2D arrays would be a 3D array , not three separate variables .
Did you want to truncate it , or round it , or did you want the median or median-of-modes or something else rather than the mean ?
Well , first , you don't have a stack of 2D arrays , you have three separate variables .
In your updated question , that seems to be what you want ; in your original question I'm not sure if you wanted to truncate or round , or if you wanted the median or something else rather than the mean , or anything else , but those are all easy ( add ` dtype=int64 ` to the call , call ` .round() ` on the result , call ` median ` instead of ` mean ` , etc . ) .
We know about concatenate but we would like to use a more general approach .
Less datapoints means that you either pick any random datapoint , of perform some operation : here you got many possibilities : get the median or average for a given day / week / month , or maybe max / min values , whatever you want , really .
I put a break point on the write call and verified that pngImage has nonzero values .
How to select specific column indices from a matrix ?
I have a matrix and a list of column indices that I want to select from the matrix for each row .
Notice we transpose ` my_matrix_2 ` for this to work : #CODE
Please post your code and the stack trace of the error .
Inplace permutation of a numpy arrray
I have a quite large numpy array of one dimension for which I would like to apply some sorting on a slice inplace and also retrieve the permutation vector for other processing .
However , the ndarray.sort() ( which is an inplace operation ) method does not return this vector and I may use the ndarray.argsort() method to get the permutation vector and use it to permute the slice .
This type of question will likely accumulate down votes , since it is not well suited for stackoverflow .
So ` ( 442 , )` can broadcast to ` ( 1 , 442 )` .
And axes of length 1 can broadcast to any value .
I want to sum the elements of ` b ` according to the indices in ` a ` .
The ` i ` th element of the matrix I want will be the sum of all values ` b [ j ]` such that ` a [ j ]= =i ` .
I have some very large arrays that I need to sum like this over multiple dimensions , so it would NOT be convenient to to have to perform explicit loops .
@USER I believe another way to put it is that OP wants to group the elements in ` b ` using the their positionally corresponding elements in ` a ` as unique identifier , and then compute the sum of the elements in ` b ` per unique identifier .
Assuming the indices in ` a ` are always 0-based and a continuous sequence .
The ` zeros ` function will use ` calloc ` which zeros any allocated memory before it is first accessed .
I created a simple min / max function that calculates the min and max of a 2D float32 array and compared it to running ` numpy.min ( a ) , numpy.max ( a )` .
What happens if you flip your inner and outer loops ?
First to avoid confusion it is never a good idea to use the build in function names min and max as variable names so call the fmin and fmax .
I did not do the ` cdef DTYPE_t max ` because I'm not really sure what you meant by that .
It looks like NumPy uses SSE instructions where available for ` min ` and ` max ` , which means they can likely take advantage of your hardware to a much greater extent than Cython can achieve .
Here's the source code for NumPy's ` min ` and ` max ` reduction implementations in SSE : #URL .
Numpy , given norm returns possible cartesian coordinates
Above I am assuming that with module of a vector you mean an L2 norm .
If you use ` rand ( 3 )` then you are selecting a point on a unit cube and then projecting that onto a sphere .
a remark : doing it like this you have that ` phi ` is truly random , and ` cos ( phi )` and ` sin ( phi )` are functions of a random variable , so that the coordinates are not technically as random as ` phi ` .
Then you could run ` diff ` on the log files generated by the different machines and see where in the code things diverge .
If your matrix is ill conditioned , then the inv will be largely numerical noise .
into a tuple of grid indices , then use these to get the pixel coordinates as
Also notice that I've updated ` get_win_pixel_coords() ` to deal with cases where ` shiftSize ` is not ` None ` ( i.e the windows don't perfectly tile the image with no overlap ) .
No , if the windows tile the image without overlap ( i.e. ` shiftSize=None ` , which I've assumed so far ) , then if you made the grid dimensions equal to the pixel dimensions of the image , every window would just contain a single pixel !
If you want the coordinates of every pixel in the window , relative to the image , another trick you could use is to construct arrays containing the row and column indices of every pixel in the image , then apply your sliding window to these : #CODE
On the contrary , I want the program to give me each window's coordinates , relative to the absolute original image dimensions .
To do so I first calculate an initial transform based on points I have selected and then I attempt to transform the image corners and translate them to get a final optimum transform .
In other words each row has trailing zeros at both ends , while the actual data is between the trailing zeros .
However , I need the number of trailing zeros to be equal at both ends or in other words what I call the data ( values between the trailing zeros ) to be centred at the middle of the row .
subtract ` d_m-d ` from ` n_m ` and place ` x ` at this position in the row of zeros of length ` n `
I need to reshape it so that all the first rows in every matrix is group together in a matrix then all the second rows , etc .
( a few simple tests confirm this - ` repeat ` is 2x faster for small arrays , slightly faster for large ones ) .
I try to convert matlab code to numpy and figured out that numpy has a different result with the std function .
The default behaviour of MATLAB's ` std ` is to correct the bias for sample variance by dividing by ` N-1 ` .
I'll add that in Matlab , ` std ([ 1 3 4 6 ] , 1 )` is equivalent to NumPy's default ` np.std ([ 1 , 3 , 4 , 6 ])` .
Now by default , MATLABs ` std ` calculates the unbiased estimator with the correction term n-1 .
Have you tried using ` np.swapaxes ` to reshape to a ` ( 256 , 256 , 3 )` array ?
Since the transpose operator reverses the index , if the image is stored in ` RGB x rows x cols ` the transpose operator will yield ` cols x rows x RGB ` ( which is the rotated image and not the desired result ) .
The last one is what you might want in some cases , since it reorders the image ( ` rows x cols x RGB ` in the example ) instead of fully transpose it .
Where ` np.nonzero ` converts from a boolean array to an array , where all indices with True-values are listed .
Basic idea to deal with this issue is to append these indices to a list and delete them outside lope .
Array reshape not mapping correctly to numpy meshgrid
I have a long 121 element array where the data is stored in ascending order and I want to reshape to an 11x11 matrix and so I use the NumPy reshape command #CODE
The following code block shows how you reshape an array from 1x121 to 11x11 .
I also realize I can use ` transpose ` to put the indexes in any order I need , but I'd like to minimize that .
( ` dot ` acts on the last of one and the second-to-last of the other . )
A number of the builtin reduction methods have a ` keepdims ` parameter to facilitate broadcasting in uses like this ( e.g. ` sum ` , ` mean `) .
A number of the ' reduction ' methods ( e.g. ` sum ` , ` mean `) have a ` keepdims ` parameter to eliminate the need to add this ` newaxis ` back in .
Whether the process's deleted memory actually returns to the system stack is another matter .
The memory for ` x ` _is_ allocated with you call ` zeros ` ( should be ~76MB for 20e6 float32's ) , and when you call ` np.array ( x , copy=False , dtype= np.double )` a copy is being made because of the difference in dtype ( float32 is not float64 ) .
To get what you want , I'd use ` numpy.split `' s ability to provide custom indices .
I am sure that numpy is optimized to the max and I doubt I could be able to beat its performance but still factor of 10 slower means I am doing something wrong .
You should use the C ` sqrt ` function .
I believe np.sqrt has a lot of hearders and includes a lot of checking while C sqrt only does sqrt ...
You need to add axes to ` coeffs ` so it will broadcast in the dimension ( s ) you want .
The ` np.newaxis `' s allow the values of ` coeffs ` to line up with the first dimension of ` X ` and then they are broadcast across the remaining dimensions .
Following 1-liner does the trick using the powerful ` einstein sum ` , np.einsum() , function with the rich framework of subscript labels that allow for broadcasting of dimensions and controlling the output : #CODE
It takes a little getting used to but has many matrix-vector operations , like ` trace ` , ` diag ` , ` inner / outer product ` , ` element-wise multiplication ` , etc ., all lumped into this powerful formulation .
Offcourse you can manually broadcast the vector across extra dimensions using ` np.newaxis ` parameter and simple multiplication : #CODE
Tradeoff then is in the versatility that the ` einstein sum ` offers .
Here's the error : ` ValueError : operand 1 did not have enough dimensions to match the broadcasting , and couldn't be extended because einstein sum subscripts were specified at both the start and end `
From the [ docs ] ( #URL ) OneHotEncoder encodes an array of ints , are you trying just to encode the IdVista column only ?
Update max value to -1 in 2D array
This only considers the ` max ` in a per row basis .
Thanks @USER I've repaired the max-determination to be ` max ` of the whole collection instead of each row
As it stands , the expression for ` D ` takes the ` min ` of a 1-dimensional array ( assuming you fix that unbalanced parenthesis ) .
Python : append to numpy array
numpy append array to array
Numpy append to an empty array
I could , as a straight but time-consuming solution , define a new empty numpy array which I fill with the old data and the ones that should be append .
` append ` , ` concatenate ` , etc all create a new array , and fill it with data from the original arrays .
Your ` append ` action creates a new ` ( 1 , 2 , 3 )` array .
I was thinking about reading in parts of the stack and using scipy.ndimage_map_coordintes to get the interpolated pixel coordinates .
This just requires some attention when uncompressing because if the number of bits is not a multiple of 8 , some zeros are added as padding .
I could share npy dumps of sample data , if anyone was interested enough to see what I am working with .
If your coordinate grid is not rectilinear , so that the z-value for a given index changes for different x and y indices , then the approach you are using now is probably the best you can get without a fair bit of analysis of your particular problem .
The techniques above work fine if the ` data ` array is not monotonic or linear , but do you mean the values in ` grid ` are not monotonic for given x and y indices ?
Numpy : fast / easy way to get indices of array whose value is equal to another array ?
get an array ` unique_x ` of the unique values of ` x `
will fill the ` 100 ` by ` 100 ` array ` b ` with the sum of the squared " z " values of ` a ` , that is 1+4+9+ ...
I don't have realistic ` a ` and ` d_n ` arrays , but with simple test ones , this ` E.argmin ( -1 )` matches your ` b ` , with a 66x speedup .
Many functions in Numpy are " reduction " functions * , for example ` sum ` , ` any ` , ` std ` , etc .
* Strickly speaking a reduction function is of the form ` reduce ( operator , sequence )` so technically not ` std ` and ` argmin `
Matrix multiplication ( for example ) can be achieved by the ` dot ` function as ` a.dot ( b )` where ` a ` and ` b ` are two 2D numpy arrays ( not matrix ) .
However , I was wondering if there is more elegant way to take care of this permutation ?
I am using an algorithm that requires that each example has a matrix , say ` Xi ` which is ` ai x b ` , and that for each of the ` O ( n^2 )` pairs of examples , I find the difference between each row ` Xiu - Xjv ` , then sum the outer products ` sum_u sum_v np.outer ( Xiu - Xjv , Xiu - Xjv )` .
Unfortunately this inner double sum is fairly slow , and is causing the running time to spiral out of control on large datasets .
For each pair ` ( Xi , Xj )` I want to go through all the ` O ( ai * bi )` pairs of rows between the two matrices and find ` Xiu - Xjv ` , take the outer product of that with itself ` np.outer ( Xiu - Xjv , Xiu - Xjv )` , and finally sum all those outer products .
Simply enough , ( 0 , 0 ) and ( 1 , 1 ) are just 0 matrices , and ( 0 , 1 ) and ( 1 , 0 ) are both 4 matrices , so the final sum of all four outer products of pairs would be ` [[8 , 8] , [8 , 8]] ` .
No , the opposite of the squared Euclidean distance -- outer product .
The ids for a and b are different as one would expect for a copy .
See the curve below for the values of log ( p ) .
I don't think they have to sum up to 1 , but passing them through a Softmax layer will achieve both properties .
it's not exactly this sum , there are also weights
Yes , there are the weights but they are not the factor that constrains the value of p_i . log ( p ) < 0 for 0 1 log ( p ) > 0 and so -log ( p ) will increase as p becomes smaller .
That is why p needs to be in ( 0 , 1 ] . log ( 0 ) = -infinity so care should be taken not to have p=0 , but that is already implemented in the layer itself .
How to find the sum of number layer in ndarray
I need to find the sum of all layer .
1st find the sum of whole 10*10 array , then find the sum of inner 9*9 array .
Then subtraction is the result of sum of outer layer .
A loop will find all sum .
If you know how to get the sum of a ` ( 2n ) * ( 2n )` square in the middle , then it is very easy : take the sum of the ` ( 2n ) * ( 2n )` square , then subtract the sum of the ` ( 2n-2 ) * ( 2n-2 )` square inside it ; the difference is the sum of the elements on the border ( that is , are in the outer square but not the inner ): #CODE
While the general idea of subtracting inner squares from outer ones is probably the best approach , if you wanted performance none of the implementations presented would do too good , since they keep adding the same numbers over and over .
When you do e.g. ` sum ( a [ i , i : n-i ] + a [ n-i-1 , i : n-i ])` , numpy allocates a new ` n-2 *i ` item array and fills it with the elementwise sum of your two slices .
You should also use ` np.sum ` instead of the standard library's ` sum ` .
It's taking advantage of the fact that certain sparse formats implement slicing and indexing nearly as well as dense arrays ( ` mat [ #URL #URL ) .
The ` apply_along_axis ` is pure Python that you can look at and decode yourself .
add to ` my_list ` element-wise the sum of the first 4 values of ` harmonic ` after each has been scaled by ` var1 ` OR
append to ` my_list ` the first 4 values of ` harmonic ` after they have been scaled by ` var1 `
Then use ` np.append() ` to concatenate the newly formed list to your ` my_list ` .
You will get a 1000-number vector , each number being from ( 10 , 0.5 ) binomial distribution .
These two are binomial random number generator
Explanation of binomial :
A binomial random variable counts how often a particular event occurs in a fixed number of tries or trials .
Suppose , You wanna check how many times you will get six if you roll dice 10 times .
p = ( 1 / 6 ) # probability of getting six in each roll
I get a vector of zeros and ones , of length 1200 .
Actually , binomial is summation of bernoulli trails .
Each element of M is a different combination of ` sin ` and ` cos ` .
I want to know how to compute transpose ( x ) .M .X .
I how to take the transpose and compute transpose ( x ) .M .x ?
I want to know how to compute transpose ( x ) .M .X .
What is ` transpose ( x )` ?
That is , you want a dot produce of ` X ` with the 1st dim of ` M ` , and another dot product of ` X ` with the 2nd dim of ` M ` , without altering the last 2 dimensions .
That's probably faster for really large cases , but for modest ones , this #CODE
What is the best way WITHOUT explicit python loop ( actually , I want to implement this in theano where those loops are not going to work ) to build a ( N-by - #classes ) matrix whose ( i , j ) element will be the sum of distances from i-th point to its k-NN points with the class label ' j ' ?
If you don't apply a shift to ` l_val ` , then ` np.bincount ` will count the unique values in ` l_val ` , weighted by the corresponding amounts in ` K_val ` resulting in an array with just 3 bins ( since there are only 3 labels , 0 , 1 and 2 ) .
Since we want 3 bins * per row* , we need to give each row of ` l_val ` unique labels .
This expands ` l_val ` and ` labels ` into arrays that can be broadcast together .
Now all we have to do is sum over the columns .
It looks like indexing numpy record arrays with an array of indices is outrageously slow .
I used scipy's pdist with the correlation metric to construct a correlation matrix , but the values were not matching the ones I obtained from numpy's corrcoef .
scipy fmin operands could not be broadcast together with shapes
I am trying to compute the dot product of two numpy arrays sized respectively ( 162225 , 10000 ) and ( 10000 , 100 ) .
That together with how numpy creates temporary copies to do the dot operation will cause the error .
The extra copies is because numpy uses the underlying BLAS operations for dot which needs the matrices to be stored in contiguous C order
Check out these links if you want more discussions about improving dot performance
The reason you're getting a memory error is probably because numpy is trying to copy one or both arrays inside the call to ` dot ` .
This is encapsulated as a sub ref in the outer subroutine , so I don't have to pass variables to the subroutine .
Each atom is assigned a unique index beginning at zero .
Mapping is changed frequently , I permute all possible valid mappings and find the ones that satisfy the equate condition when row is equal to N .
How sparse is ` adj_matrix ` ( proportion of zeros ) ?
so please verify if I am on the right track with your code : 1 ) you are trying to * slide * through your ` array ` in the ` 3x3 sub-matrix ` fashion , 2 ) find the location of the ` max ` in this sub-matrix , and 3 ) append this value to a ` list ` until ` list ` is 100 elements long ?
Because if yes , it might be worth looking at the gradient image .
What the code does : ( a ) you iterate over every pixel in a 2D image ( b ) you perform the following operation : ( 1 ) you look at the 8-neighbourhood of the current pixel ( 2 ) you find the position of the maximum and save it in a list ( 3 ) you change the current pixel to the just found maximum ( 4 ) you go back to step ( 1 ) until the list is of length X ( 5 ) you throw away the list and start with the next pixel in the image // Interpretation in words : From each point of the image , you trace a line along the maximum gradient towards the next local maxima .
The first answer already shows how to access the indices of the maxima by comparing with the list .
See my answer for finding the indices .
If you're trying to generate random zeros and ones you can use :
but I get a dictionary with 0 in it , when I really need 0 to be assigned to each number -- so that when I concatenate shuffle the arrays I can still identify the ` arange() ` function that generated each value
Can you change the data structure of ``` newprobValues [ npv ]``` ?
That is , when I pull and delete a number from one of the a arrays in the stack , how would I also be able to get info on which array in the stack it was pulled from ?
Both your Sage objects and my test ones return a number when indexed ( ` a [ 0 ]`) , it just isn't wrapped in a ` numpy ` type ( i.e. Python ` int ` instead of ` np.int8 `) .
That is , they apply the distance calculation to the outer product of the input collections .
For any given distance , you can " roll your own " , but that defeats the purpose of a having a module such as ` scipy.spatial.distance ` .
To explain a bit more about what's going on , first we reshape the arrays such that they have a 2xN shape ( ` -1 ` is a placeholder that tells numpy to calculate the correct size along that axis automatically ): #CODE
So is not it possible to reduce the matix obtained from cdist into required ones .
Numpy : transpose result of advanced indexing
I need to sum it along the ` 0 ` dimension but in the real world the matrix is huge and I would prefer to be summing it along ` -1 ` dimension which is faster due to memory layout .
I have used ` line_profiler ` to see that the line which takes most of the time in my code is taking the sum of an array with respect to dimension ` 0 ` rather than ` -1 ` .
In other words , if the items you want to sum are not stored adjacent in memory , it will be slow no matter what axis of the resulting matrix they're in .
If you managed to get the transposed matrix , you would just move the slowdown from the sum operation to the transpose operation .
That is why I need the new array , which is always created by advanced indexing , to be transposed --- this way I will be able to sum along the fastest index .
I would like to sum it along the 0 dimension but in the real world the matrix is huge and I would prefer to be summing it along -1 dimension which is faster due to memory layout .
If the underlying array is row-major ( the default , i.e. ` X.flags.c_contiguous == True `) , then it may be slightly faster to sum it along the 0th dimension .
So if you have a ` ( 2 , 5 )` array , and you sum over the 0 dimension , you get a five-item array .
Since I want to conserve space , I'm wondering if it's possible to write that file already in compressed zip format without having to first write the binary file , then compress it and then delete it .
For example , if you transpose a matrix before interfacing it to your code , you'll get unexpected results !
You may or may not need to reshape it to 2D depending on what you're going to do with it .
An easy way reshape the array to 2D is using slicing and ` numpy.newaxis ` : #CODE
Your approach only differs from spicy in yours including a transposed delta whereas the spicy source code does not transpose the delta in the second appearance ...
In the scipy source , ` u ` and ` v ` are one-dimensional arrays , so taking the transpose ( in ` ndarray ` terms ) makes no difference .
Are you trying to broadcast the Mahalanobis distance calculation over pairs of vectors ?
The [ NumPy docs ] ( #URL ) have more detail but this call basically implements the matrix ( dot ) product by multiplying rows of A by columns of B but outputs ( ` -> `) only those elements that lie on the diagonal of the result .
` p2 = einsum ( ' nk , nk -> n ' , p1 , delta )` is the pairwise dot product of the rows of ` p1 ` and ` delta ` .
Also , if you aren't already , you should probably strip punctuation and reduce all words to lower-case .
I mean , among hundreds of thousand features select the most informative tens of thousand ones and therefore reduce the vector dimension ?
Otherwise , I suggest using neural networks with stochastic gradient descent instead .
Are you sure neural networks with stochastic gradient descent can get high accuracy ?
10 , 11 , 12 , 13 are the only ones you don't have in it
I tried squeezing both variables to produce a array of ( 3960 , 3960 , 64,128 ) so cov could work on these first two series of data ( the two 3960's ) of wind and temp , but this didn't work .
Therefore , it's easy to append data to a large dataset without having to create an entire new copy .
I can't easily share the data used here , but you could simulate it by an array of zeros of the same shape ( ` 621 , 4991 , 2600 )` and type ` np.uint8 ` .
I then use sphinx to translate that text into a .chm file for distribution to Windows clients .
Sum the absolute value of the differencing components to compose a descriptor for that image .
I am not creating the numpy array in the first place because I heard it's unpythonic and slow to append to numpy arrays .
Just use ` empty ` instead of ` zeros ` , and it should feel less ' wrong ' , since you are just allocating the data without unnecessarily zeroing it .
When dealing with a sparse matrix , ` s ` , avoid inequalities that include zero since a sparse matrix ( if you're using it appropriately ) should have a great many zeros and forming an array of all the locations which are zero would be huge .
But length of one line or sum of length of lines is not the filesize .
This creates an array of random values of the required shape and then sorts the indices in each column by their corresponding value .
This array of indices is then used to index ` a ` .
( The idea of using ` np.argsort ` to create an array of columns of permuted indices came from @USER ' s answer here . )
I imagine that for extremely large arrays , the disadvantage of an ` n log n ` solution like sorting might become apparent here .
Assuming you are ultimately intending to loop over multiple 1D input arrays , you might be able to cache your permutation indices and then just ` take ` rather than ` permute ` at the point of use .
This can work even if the length of the 1D arrays varies : you just need to discard the permutation indices that are too large .
Note that the above function gives ` ( k , n )` not ` ( n , k )` , this is because numpy defaults to rows being contiguous and we want the ` n ` -dimension to be contiguous - you could force Fortran-style if you wish , or just transpose the output ( which flips a flag inside the array rather than really moving data ) .
Which is about 70 times faster ... but that's in the simplest case where we don't call ` compress ` .
Then translate it into corresponding code done on NumPy arrays , making adjustments as necessary for the fact that we want to do the computations on a whole array full of rolling windows instead of just one window at a time , while keeping ` patch ` constant .
Extracting required indices from the array of tuples
Edit : To return the first column index of the maximum in each row , you just need to take the indices returned by ` np.unique ` from ` maximas [ 0 ]` and use them to index ` maximas [ 1 ]` .
from scipy import reshape , dot , outer
In which case my flatten solution should work fine .
I'm off to translate this into ` R ` :-)
To sum up , 2 questions :
How can I do when I write the " dot " after the word " weight " it complete me with the methods of the np.array type ?
You could use ` stack ` to pivot the DataFrame .
The default is to pad with zeros .
The DCTs are already calculated over rowwise moving windows , so we take a regular sum over those windows .
However we need to take a moving window sum over the columns , to arrive at the proper 2D window sums .
This avoids having to sum the exact same numbers over and over .
Unfortunately it doesn't work for the first window ( when ` p == 0 `) , so for that we have to take only ` cs [ q-1 ]` and stack it together with the other window sums .
Numpy reshape behaves unexpectedly
I found that reshape it into a one-dimensional matrix gives me a two-dimensional matrix .
Some of the arrays in the list are duplicates of earlier ones .
All the arrays have the same sum , but dot product might work ...
If your arrays are multidimensional , then first flatten them to a 1-by-whatever array , then use the same idea , and reshape them at the end : #CODE
At that point , I would stop to consider whether something like Cassandra , which can easily build database indices on top of large columns ( or multidimensional arrays ) of floating point ( or other ) data isn't the more sane approach .
You could reshape them to a long 1-by-whatever array , then use tuple , then reshape them back to their original shape at the end .
Then you can join this row by semicolon : #CODE
Now I want to find uncertainty of the fitted line , and tried to use ` cov ` argument , which returns 3x3 covariance matrix : #CODE
But I'm not sure how to calculate the uncertainty , which according my Google search should be calculated by squaring the diagonal of covariance matrix .
terms of the diagonal elements of the covariance matrix of the fit ,
The diagonal elements you are interested in are for example : #CODE
However , we need ` boolvec ` to be of a shape that can broadcast to the same shape as ` a ` and ` b ` .
You can index with ` boolvec ` if it's ` m ` -length , but ` np.where ` expects it to broadcast , which applies along the last axis .
so work with the norm of the vector ?
I am trying to sum the elements of all columns of a matrix y , and save those in a new matrix .
mat is a 70000 x 1063 sparse matrix #CODE
What are ` mat ` and ` ind ` ?
I also cleared up what mat and ind are doing .
Can you reproduce the problem by creating a different ` mat ` that doesn't depend on your csv file ?
` mat = pd.DataFrame ( ... put some explicit , self-contained data here ... )`
Let mat be a random sparse matrix with the shape 70000 x 1063 !
If ` y ` was array , then the ` matrix ` version of ` sum ` would not be called , and there'd be no attempt to call ` ._collapse ` .
Is there any possible other way to sum through the columns of a matrix ?
But it's a problem for ` x ` because it will hold the changed values until the outer ` for ` is reached again .
If on the other hand , you want changes in the inner ` while ` to control the iteration of ` i ` you need to use ` while ` in the outer loop as well : #CODE
Another way is to calculate the Kronecker product of ` img ` and an array of the appropriate shape filled with ones : #CODE
So in the example above , each value ` x ` in ` img ` is multiplied by a 2x2 array of ones to create a 2x2 array of ` x ` values .
Here is the error from the log : #CODE
Numpy : stack array by the last dimension
He's adding 1 to the main diagonal , in effect constructing a ` sparse.eye ( 1000 )` .
How to split / reshape a numpy array
Your desired result is not possible , your are missing an outer list : ` array ([[ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ]])` .
You can achieve this directly by using the ` reshape ` method .
Note that where possible , ` reshape ` will give you a view of the array .
They appear in a successive dot product .
You are right that at this point the dot between big some big ints and float16s could result in NaNs because the result array would still be float16 and hence unable to store big values .
Replace zeros in a numpy array with nan
How to replace zeros in a numpy array with nan ?
Here the ` outer ` method of the ` multiply ` ufunc is used to create the new 20x20 array .
But if you want to compute values mod 256 , it's probably easier to use Python's mod operator :
Eigenvectors computed with numpy's eigh and svd do not match
I wish to verify this equality with numpy by showing that the eigenvectors returned by ` eigh ` function are the same as those returned by ` svd ` function : #CODE
You can add a positive number to all the diagonal elements , i.e. make M2=M+a*I , where a is large enough to make M2 positive semidefnite .
It seems to be that the input element were too small that python turned them to be zeros , but indeed the division has its result .
` -x + log ( 1+exp ( -y+x )) = -x + 6.1441934777474324e-06 `
For the denominator you could proceed similarly but obtain that ` log ( 1+exp ( -z+k ))` is already rounded to ` 0 ` , so that the argument of the exponential function at the denominator is simply rounded to ` -z=-3000 ` .
I am trying to append data to a file using numpy's savetxt function .
I haven't examined it , but it might give you ideas beyond the ones I suggest below .
Note : it is possible to have two identical matrices , but with different indptr / indices / data arrays .
Find the indices in a multi dimensional numpy array
I would like to get the indices of ` [( 1 , 2 ) , 2 ]` ( 0 and 3 should be the output )
That is , find the locations equal to ` [( 1 , 2 ) , 2 ]` , turn the 2D result into 1D using AND , and do ` where ` to convert the boolean mask to numeric indices .
` where ` just gives the indices of the True values in this boolean array #CODE
Usually object dtype arrays appear in complex dtypes , ones with several fields .
If your data is not so big it might be easier to read all and then just select the required indices from your data : #CODE
How to get rid of this ` array() ` and make the elements of outer tuple just coma separated ?
it's done by ( Z-min ( Z )) /( max ( Z ) -min ( Z )) .
In my case min = 2 , max = 8 , so ([ 2 , 5 , 8 ] - [ 2 , 2 , 2 ]) / 6 = [ 0 , 0.5 , 1 . ]
Your equation represents a system of equations , where each element of ` v0 ` is expressed as a sum of the respective elements in the arrays ` v1 , v2 , v3 , v4 , v5 ` .
Is this because of some zeros stuck somewhere in division ?
Check that there are not zeros or NaNs in the data for an entire line and that it looks as you expect .
) and integer or boolean arrays are valid indices
What I want , is a 40x40 matrix ` mat ` , where the indices are the entries in the second and third columns .
The first entry mat [ 0 , 0 ] = data [ 0 , 0 ] is easy , but the problem is that the list is not sorted and that the entries in the second an third columns are floats so I can't reference them in the slice .
If I understood your question then I guess you want to sort your array based on indices .
I'll use a smaller data set , and create a 10x10 array ` mat ` .
If an index ( i , j ) is not in the CSV file , ` mat [ i , j ]` will be 0 .
Create the array ` mat ` : #CODE
Assign the data to ` mat ` : #CODE
Efficient outer product in python
If calling a single , typically highly optimized , numpy function is too slow , reconsider whether you can avoid calculating a full outer product .
But its timing is the same ballpark as the others , somewhere between ` outer ` and ` einsum ` .
You say " I have to do this operation several times " , so it is possible that you can reuse the array that holds the outer product , instead of allocating a new one each time .
So you get a nice performance boost if you can reuse the array that holds the outer product .
Other programming languages ( most compiled ones , at least ) , use stack for this purpose .
I had to resize all the images first before taking out the HoG features , so that the arrays could all be the same size .
Additionally , if you don't want an array full of zeros at the rear ( x-axis ) , you could do : #CODE
Anyway , in case is just a mistake in the example , to perform what you want you can either ` clamp / clip ` or ` normalize ` the values of the resulting operations :
You can find np.clip which limits the values of an array to a ` min ` and ` max ` values : #CODE
how to solve diff .
You don't need the append .
The header is copied ( hence the different ids ) , but it will be the same data in the same memory location .. hence the word " copy " is confusing .
I like @USER ' s answer , but I wanted to point out that this is not unique to numpy arrays .
I tend to work with boolean arrays rather than indices where possible to avoid this issue .
will return 5 entries , corresponding to indices [ -1 , -11 , -21 , -41 , -71 ] i.e. 255 , 245 , 235 , 215 and 185 in your case
You can then ' roll ' through your values by translating in texture coordinate space .
Personally I'd go with ' roll ' , ' shift ' or ' wrap ' .
Do you really want this ' roll ' ?
For nonstandard ' roll ' like this it is unlikely that there's anything faster .
For what it's worth , the core of ` roll ` is : #CODE
Your particular ' roll ' could be reproduced with a similar ' take ' #CODE
` is faster than ` roll ` because it does not create a new array ; instead it just overwrites part of the existing one .
For large ones your overwriting assignment is faster .
I'd also suggest looking at using the transpose , and rolling on the ` axis=0 ` .
after the shift left the value at the far right end is overwritten with a brand new value that comes off the threaded Queue so that type of roll is fine !
@USER Based on the wording of your question I had no idea you were inserting new values into the array as well as shifting the existing ones .
I know I can use ` rand ` or any other C RNG ( e.g. ` gsl `) ; is there a more standard way ?
If I am using the full ` scipy ` stack , why should I use a different RNG ?
There are a variety of Cython files included throughout the scipy stack that aren't exposed as public interfaces .
It complains that x is 3-dimensional array and I am not sure how to tell it that it needs to broadcast over the first two dimensions .
In the example , it works because the number of observations and parameters is the same but one can have more observations than the parameters , in which case the reshape should be something like : sol = result [ 0 ] .T .reshape ( x.shape [ 0 : -1 ] , ( 2 , )) .
Getting indices of elements that are in another list in numpy
I have two numpy arrays and I'd like to get the indices of all elements in the first array that are in the second array .
You have created a numpy array using an array , ` vector ` , that is on the stack .
Producing slices of dot output without intermediates
If I stack the matrices into a ` n ` x 3 x 3 ` ndarray ` called ` R ` and the vectors into a 3 x ` n ` ` ndarray ` called ` v ` , I can obtain the stack of multiplied vectors via , #CODE
where ` S ( t , y )` is a sum equal to ` f ( a ( t-1 , 1 )) + f ( a ( t-1 , 2 )) + ...
For each unique number that is present in at least one of the sublists , let's maintain a list of indices of all sublists that contain this number .
This part is ` O ( n * log n )` time if we use sorting to find unique numbers or ` O ( n )` if we use a hash table , where ` n ` is the total number of elements in all sublists .
Let's create a graph where vertices are sublist indices and an edge is present if two indices appear together in at least one list of indices among all numbers .
We need create at most ` O ( n )` edges ( this part is slightly non-trivial : there is no need to create all edges explicitly , we can just add an edge from an element to the next one in each sublist for all unique elements due to transitivity ) .
I think you are looking for the outer product : #CODE
Yes also possible , ` outer ` [ is slightly faster ] ( #URL )
` y ` is a 1D array of the index of the minimum absolute value of each row of ` x ` .
I don't like the fact that I have to expand the dimension of a 2d Array and then append them together .
You are attempting to broadcast a 4-D array together with a 3-D array .
Your operation will probably work if your pad out / reshape your 3-D array ( which would no longer be 3-D I suppose ) to explicitly have the shape ( 96 , 96 , 55 , 1 ) .
Most likely you are trying to ` dot ` a ` ( n , )` with a ` ( 1 , m )` .
The reason it is a tuple is because ` nonzero ` gives you a tuple of indices for each dimension .
What is a pythonic way of finding maximum values and their indices for moving subarrays for numpy ndarray ?
I'd like to find maximum values and their indices in a moving subarray window with specified strides .
for max values and #CODE
for indices as output .
It makes finding the maximum values really easy , but getting the corresponding indices will be tricky .
There's also [ ` maximum_filter ` from Scipy ] ( #URL ) but again , the problem is the indices ..
So , I have 100 symmetric 3x3 matrices and I am only storing the unique components .
I found a better way by copying the sequence , shifting it by a different value until the window is covered , and splitting the different sequences with ` reshape ` .
So , to ` reshape ` , our steps for the two dimensions would be ` ( 1 , 3 )` .
I need to get the last four column data of a ` ndarray ` , most of time code ` arr [: , -4 :] ` is ok , but if the array just has one dimension , this will throw ` IndexError : too many indices ` .
numpy : find the count of each unique element in an array
In the for loop it then gives me the following error : IndexError : too many indices
Now the ` dtype ` of ` the_problem ` is not an object , and you can efficiently calculate for example the min as ` the_problem.min() ` .
I agree with iluengo that making a NumPy array of arrays is not taking advantage of NumPy's strengths because doing so requires the outer NumPy array to be of dtype ` object ` .
Say , I have a numpy array ` x = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]` and two variables , i.e ` min = 3 ` and ` max = 7 ` .
What would be the most efficient way to set ` x ` values to ` 0 ` when they go beyond the range specified by ` min ` and ` max ` variables , i.e ` 3 7 ` , so the final result would be ` x_after = [ 0 , 0 , 0 , 3 , 4 , 5 , 6 , 7 , 0 , 0 ]` ?
Well , I was expecting permutation according to index .
that creates a column array with all indices from ` 0 ` to the length-1 of the array in the first dimension , which is ` evals.shape [ 0 ]` .
You could use a lookup table instead of calling ` sin ` and ` cos ` .
As the results of ` sin ` and ` cos ` are zero , one , or minus one , and cyclic , you can look them up , modulo four : #CODE
Below is a solution that is using a stack and a while loop instead of the recursive calls .
But in Numpy ( n , 1 ) and ( n , ) coexist , and there are many functions for dimension handling alone : atleast , newaxis and different uses of reshape , but to me those functions are more of confusion than help .
MATLAB does not always preserve dimensions , ` x (: , : , 1 )` returns a 2d matrix , with an automatic ` squeeze ` on the final singleton dimension .
Well , you need to join the ` A [ lpha ]` , ` R [ ed ]` , ` G [ reen ]` , and ` B [ lue ]` bytes together to form a single integer value .
Numpy : Affect diagonal elements of matrix prior to 1.10
I would like to change diagonal elements from a 2d matrix .
Here is my use case : I want to recreate a matrix of the following form , which is very trivial using diagonal notation given that I have all the x , y , z as arrays .
Now you can intuitively access any diagonal : #CODE
On a 1000x1000 matrix , setting the first upper diagonal is around 100 times slower than specifying the indices directly .
Passing in a list of row indices and a list of column indices lets you access the locations directly ( and efficiently ) .
changes the array of zeros ` z ` to : #CODE
In general for a ` k x k ` array called ` z ` , you can set the ` i ` th upper diagonal with #CODE
and the ` i ` th lower diagonal with #CODE
You can write your own diagonal function to return of view of the diagonal you need .
It gives me an error : operands could not be broadcast together with shapes ( 10 , 3 ) ( 3 , 10 )" #CODE
File " / usr / lib64 / python2.7 / site-packages / numpy / core / numeric.py " , line 1746 , in indices
Second , append missing nans : #CODE
new values appear after numpy concatenate
Now I want to shift the data for another longitude coordinate , so I do split and concatenate : #CODE
So you can either call ` squeeze ` to produce a series or select the only column in the df in order for there to be no errors : #CODE
( Also , I prefer to use NumPy or other very efficient computational methods as there may be thousands of images in my stack . )
If I have 2 matrices ( of equal dimension ) and I stack them on top of each other , how can I find the covariance between each pixel on top and bottom ( and then plot that covariance in respective locations on a new matrix ) ?
The matrix R12 now contains the absolute correlation coefficients ( truncated to the range [ -1 , 1 ]) .
You can also use convolve from scipy.ndimage.filters , it is very fast , but it is offset by 1 compared to Matlab's conv2 .
How could I use the convolve function to do that ?
Previously , I had calculated the variance off all pixels ( at a point ) across a stack of images and plotted that as a matrix .
Fastest way to compute covariance on vectors that are cutting through a 3D image stack .
I want to store the covariance values in a matrix with the dimensions of one frame of the stack ( the width will decrease by one ) .
However it will be very slow and take a lot of memory if the values in ` input_1 ` are really large , i.e. they are not indices of an array , but say they are seconds or some other integer data that can take very large values .
It is much faster to use ` unique ` and ` bincount ` .
We use the first to extract the indices of the unique elements of ` input_1 ` , and then use ` bincount ` to count how often these indices appear in that same array , and weigh them ` 1 ` or ` 0 ` based on the value of the array ` input_2 ` ( ` True ` or ` False `) : #CODE
Below I report my original answer , using ` unique ` in a different manner .
It may be faster to go for another solution , using ` unique ` : #CODE
This has the drawback that it will require a lot of memory if the number of unique elements in ` input_1 ` with a ` True ` value in ` input_2 ` are many , because of the 2D array created and passed to ` where ` .
min=0 max < len ( input_1 ) might also be an option .
In this case , instead of generating two large matrices with the row and column indices , you can use a for loop on the rows of your array ( it's slower but not as slow as a double for loop ) #CODE
Memory Error in Python when trying to sum through Numpy Ndarray Object
I have a huge numpy ndarray ( called ` mat ` and of the shape 700000 x 6000 ) of which I want to sum through the columns and find the nonzero indices .
I want to sum through it like so : #CODE
Assuming ` y ` is an integer ` np.sum ( mat [: , y ] , axis=1 )` produces a ` ValueError ` because ` mat [: , y ]` has shape ` ( 7000000 , )` .
Are you doing ` mat [: , : y ]` instead ( slicing up to the ` y ` th column ) ?
See ajcr's comment , what you want to do is not feasible the way you try do do it because the notation ` mat [: , an_index ]` gives you back an array of dimensionality one , whose only axis is ` axis=0 `
Another problem is the nature of your array , if it is an array of floating point numbers the probability that the sum of 700,000 entries is exactly equal to zero is close to zero ...
There's a builtin to do this : ` clip ` #CODE
Some versions accept the ` max ` keyword , others don't .
` clip ` has a modest edge over ` minimum ` in my timings .
But when i trying to start this program on windows xp sp3 , i get error in log file with a content of : #CODE
In my example , calling the method ` array ( k , norm = True )` for different values of ` k ` will give resuslt as I shown below : #CODE
If you absolutely have to do concatenation , it's often faster to append to a normal Python ` list ` , then convert it to a numpy array at the end .
I'm trying to JSON encode a complex numpy array , and I've found a utility from astropy ( #URL ) for this purpose : #CODE
One key might also encode its shape ( though that is also deducible from the nesting of the list of lists ) .
But does handle general arrays , including ones with complex dtype .
` expected ` and ` dump ` prints are : #CODE
It would be better , I think , to encode some dictionary key to flag a ` numpy array ` , and another to flag a ` complex ` dtype .
This will encode / decode arrays that are nested to arbitrary depth in nested dictionaries , of any datatype .
If you transpose your data , that will not be true .
One of the methods should set the array of the class instance to the values of a list data attribute of the class instance and the other of the methods should append some list values to the array of the class instance .
I'm trying to fill a column ( signal ) in a dataframe conditional on another column ( diff ) in the dataframe being compared to 2 variables .
The single image is compiled from a stack of 400 images .
A more complicated operation is performed on this whole stack , but does not involve creating new images .
You can roll your own : #CODE
See this question for a similar problem , but keeping only the unique values in the merged array .
A has 9 unique components and x and b are 3D vectors .
The pseudo inverse of a vector x is the matrix inv ( t ( x ) *x ) *t ( x ) where t ( x ) is x transposed and * is matrix product .
There's also other stack overflow Q&A on this topic in python ... chances that this is a duplicate question are pretty high .
where ` A^T ` indicates the transpose of ` A ` .
append 2dim arrays to one array
Notice also that in ` numpy ` you don't append to arrays .
or you use one of the many helper functions ( ` dstack ` , ` hstack ` , ` concatenate ` described by the others )
Timing how fast this is , you find that both solutions are marginally faster to the fastest solution proposed by @USER , based on concatenate .
We want the order of the axes ` ( 0 , 1 , 2 )` changed to ` ( 2 , 0 , 1 )` , hence the need to transpose and swap the axes .
One way to join arrays is to use ` np.concatenate ` : #CODE
You can also concatenate the arrays side-by-side with ` np.concatenate (( a1 , b1 ) , axis=1 )` .
If you want to join in the arrays in a 3D array , you can use ` np.dstack (( a1 , b1 ))` : #CODE
The special directive `' 0 , 3 '` tells ` np.r_ ` to concatenate along ` axis=0 ` , and produce a result with at least 3 dimensions .
Or , alternatively , using ` concatenate ` and ` reshape ` is more readable and faster : #CODE
` concatenate / reshape ` is only more readable * to me * because I use them more often than ` r_ ` with special directives like `' 0 , 3 '` ( which I have to look up to remember how to use . )
Then I want replace the index 3000 by -2 into stack array .
I made a shorte example but there thousands of numbers I just want to reduce the number of values into my stack array #CODE
Python Numpy : operands could not be broadcast together with shapes
I am getting this error " operands could not be broadcast together with shapes " for this code #CODE
This is C++ but I'm sure you can translate #URL
That being said , to find the length of a table , ` t ` , using indices 1 ..
We can add a dummy tick column , pivot , and then use the " it's simply a dot product " observation from this question : #CODE
where the diagonal counts how many papers an author has written .
If you really want to obliterate the diagonal and above , we can do that too : #CODE
You have to copy ` X_test2 ` to this array , using , say , ` std :: memcpy ` or ` std :: copy ` in a loop over the rows .
Now I want to compare the results of these 3 specific classifiers against some specific indices of the overall results .
For example , I want to highlight the confusion matrix for the results for classifier ` C ` specifically from indices 91 to 180 of the overall results .
For classifier ` B ` I want to see the confusion matrix of the results from indices 1 to 90 .
I dont think concatenate does what I think ( or I'm doing it wrong ) .
You are trying to build such an array incrementally , starting with one field , and adding new ones with ` recfunctions.append_fields ` #CODE
Also the algorithm scales not in the size of arrays but in the number of arrays ; a harder problem will have more arrays , but not necessarily larger ones .
The solution you offer produces a unicode error saying " ( unicode error ) ' unicodeescape ' codec can't decode bytes in position 2-3 : truncated uXXXX escape " - and this was produced from the following code as per your suggestion : " import sys
Optimizing histogram distance metric for two matrices in python
I have two matrices ( A and B ) , each with a size of NxM , where N is the number of samples and M is the size of histogram bins .
Thus , each row represents a histogram for that particular sample .
If you have the RAM available , you could use broadcast the arrays and use numpy's efficient vectorization of the operations on those arrays .
A slower option ( but still faster than your original algorithm ) that uses less RAM than the previous option is simply to broadcast the rows of A into 2D arrays : #CODE
It's not relevant to your example , but it's also worth mentioning that if ` A ` and ` B ` each contain unique values then ` np.in1d ` can be sped up by setting ` assume_unique=True ` : #CODE
You might also be interested in ` np.intersect1d ` which returns an array of the unique values common to both arrays ( sorted by value ): #CODE
So assuming that two arrays are unique , we could use either np.in1d and np.intersect1d .
While there are numpy append and insert functions , in practice they construct new arrays from the old and new data .
/ opt / local / Library / Frameworks / Python.framework / Versions / 2.7 / lib / python2.7 / site-packages / numpy / core / fromnumeric.pyc in transpose ( a , axes )
536 return _wrapit ( a , ' transpose ' , axes )
--> 537 return transpose ( axes )
you cannot rely on just looking for the ` max ` of periodogram to find the dominant frequency , because the harmonics may fool you .
Comparing tuple ids gives a different result #CODE
Although many NumPy functions ( including ` sum `) have their own ` axis ` argument to specify which axis to use : #CODE
Many functions , among them ` sum ` , ` max ` , ` product ` , let you specify which axis ( axes ) you want to iterate over .
Your example , with ` sum ` , can be written as : #CODE
There are many other ` ufunc ` , and other iteration modes - ` accumulate ` , ` reduceat ` .
So if ` func ` is limited to operating on a 1D fiber ( unlike ` sum `) , then the ` ndindex ` approach is a good choice .
But I'm getting an error which tells me that the operands cannot be broadcast together .
Perhaps by changing all zeros to some arbitrary number and then nan-ing it out in the results ?
Ok , I shall edit the post immediately to add the data for x , y and redshift lists as these are the only relevent ones I suppose for my question .
Those are the ones related to my problem here .
The first ( outer ) loops over every datapoint in ` redfields ` .
The 2 that are visible , however , do not overlap with the bins set as their colors vary from the ones set in the bins in your code .
Is there an efficient way of flattening , saving indices and reconstructing to its original format ?
To an extent , you've answered your own question ; you need to modify ` flatten ` to provide retain information on the structure of the list and the shapes of the arrays within it .
Then you will have to slice up ` L_flat ` accordingly and ` reshape ` the arrays from each slice .
You're building a paradox : you want to flatten the object , but you don't want to flatten the object , retaining its structural information somewhere in the object .
So the pythonic way to do this is not to flatten the object , but writing a class that will have an ` __iter__ ` that allows you to sequentially ( ie . in a flat manner ) go through the elements of the underlying object .
The goal of the loop is to create an array of indices , showing which elements of a 1D array the elements of a 2D array are equal to .
I know that every number in the 1D array is unique , and that every element in the 2D array has a match , so this approach gives the correct result .
However , if I put the x and y arrays into the fill function ( ` plt.fill ( xarray , yarray , alpha= 0.01 )`) it views all the data as a single curve , so the alpha values do not stack when the curve lies on top of itself .
` indices = np.array ([ 0 , 0 , 0 , 1 , 1 , 1 ])` ?
I'm almost able to save it using numpy.savetxt() but I can't get the format string to left pad zeros thereby maintaining the right column width .
which is fine , however I need to be able to save this back in the same form that I got it ie . positions of each row need to be the same with the right padding of zeros to do so .
So as I mentioned the left padding of zeros is not occuring although I thought I specified this in the format string .
How to right align and pad number with max width using format specification mini language
Pad with zeros
which I am passing to the numpy.savetxt() function but it is not able to truncate the values to the specified width of 8 digits .
How to fold / accumulate a numpy matrix product ( dot ) ?
so I searched for a fold function , and all I found is an ` accumulate ` method on ` numpy.ufunc ` s .
If your matrices are large but the * number * of matrices is small , I might simply write a function with a loop and get on with your day ; it takes me ~200 us to take the dot product of two 100x100 float matrices and only 1 us to execute ` for i in range ( 100 ): pass ` , so it's entirely possible that the function overhead will be marginal .
The Python bosses have approved a new binary operator , ` @ ` , which might be used in ` numpy ` as an infix ` dot ` , allowing ` S0 @ Sx @ Sy @ Sz ` .
As food for thought , here are 3 ways of evaluating the 3 sequential dot products :
I think it has potential for being faster than ` repeat ( dot ... )` for lots of small arrays .
The key function for implementing the unique field as attribute behavior is ` __getattribute__ ` ( ` __getitem__ ` implements indexing ): #CODE
I imagine it could be modified to also return any * leftovers * - either the actual data or indices describing it .
` stride_tricks ` won't handle windows of different sizes ( e.g. 4x4 main ones , 4x2 , 2x4 and 2x2 boundary ones ) .
My goal is to resize the width of the bar chart , but the distance from origin must remain the same , which is 5.6 .
What does reshape ( -1 , 2 ) do ?
You are missing python-dev to compile python.h as the last line of log says .
I want to find max value for " masked " elements in each row .
Original array contains dot products between direction vectors and gradient vectors on " rays " pointing from the center outwards for given angles .
So i-th row of dot_product array contains dot products along the " ray " for i-th angle .
If the row intervals differ in size , I can use ` np.r_ ` to concatenate them .
It's not absolutely necessary , but it is a convenience when building up indices from multiple intervals and values .
You could ` clip ` ` idx ` #CODE
In other words , we'll pick out the indices from each row of ` arr ` as shown in an array ` idx ` : #CODE
I would have to transpose this array to use your solution .
I reshape it like this : #CODE
I would like to extract the local Fourier modes from a binary image ( ones and zeros ) , so if the image is , let's say , ( 1000,100 0 ) , I would like to take a Fourier transforms of windows of ( 30 , 30 ) .
Isn't this possible with scipy.ndimage.generic_filter , providing the fft function as filter function ?
I will go to read the references , I have an image of three layers ( imported from jpg image ) for which I want to extract the fft on sliding windows .
B is a 3D matrix . the indices that you specified ( 2x3x4 ) is exactly what is printed out . the outermost brackets have 2 elements , the middle brackets have 3 elements , and the innermost brackets have 4 elements .
I'm afraid I don't quite follow either ; if you loaded multiple text files into different frames and needed to concatenate , my answer might help .
There are additional examples in the documentation Pandas merge join and concatenate documentation
I was thinking you wanted to compose your additional columns into another frame then concatenate the old and new frames , but maybe that's not quite what you want ?
Furthermore , doing `` exp ( a*x ) - exb ( b*x )`` with `` a , b > 0 `` is numerically unstable for increasing `` x `` ( due to the limited number of digits of floats ) .
You could try `` exp ( d ) * ( exp ( a* x-d ) - exp ( b* x-d ))`` , choosing `` d `` that way that `` a* x-d `` and `` b* x-d `` will be close to zero .
The other option , if you want a weighted mean , is to concatenate the matrices , and then take the np.nanmean of that : #CODE
So when you append X you do not append the file , but a pointer to the file .
So the assignment of the zeros has to depend on the neighboring values of the array ?
You need to assign a unique integer to each unique string label .
I'm looking for a code that speed up a creation of permutation matrix .
You could do this by creating an ` ( n , m , m , ..., m )` array of indices for column 1 , column 2 , ..., column n using ` np.indices() ` , then reshaping the output into an ` ( n ** m , n )` array : #CODE
It seems that numpy detects an iterable in the assignment and tries to broadcast it .
@USER It's only ' wrong ' in the sense that numpy will try to broadcast when the argument on the RHS of the assignment is iterable .
I am trying to speed up the dtw ( x , y , dist=lambda x , y : norm ( x - y , ord=1 ))) at #URL by vectorizing it .
The indices was hard to get right .
D [ I+1 , J+1 ] = map ( norm , x [ I ] -y [ J ]) + np.minimum ( np.minimum ( D [ I , J ] , D [ I , J+1 ]) , D [ I+1 , J ]) ?
In general , a speed-up that scales linearly with the number of threads would be the absolute best-case scenario , assuming that :
So I take the transpose of the inputs matrix to preprocess my data : #CODE
If I would not transpose the matrix the dimensions would be even worse ( 1x752 ) .
You can use ` numpy.random.permutation ` to create a permuted list of ` indices ` , and then shuffle both ` X ` and ` Y ` using those ` indices ` : #CODE
For the i th value in the score array , find all other values that are larger than the i th value and note their indices
Then , in the i th row of the distance array , get all of the distances with the same indices as noted in step 1 . above and return the smallest distance
One could in theory make it a one-liner but it's already a bit challenging to read to the untrained eye .
I'll continue with the format from the last code block ( you'll need to swap a few indices if you're using the first form ): #CODE
To push everything into a single row , you can use ` reshape ` .
Removed the method as it was twice complex than the given ones .
` unique ` is actually ` O ( n log n )` , as it uses sorting to spot the repeated items .
So , starting at index 0 , you want to pick out the indices which mark a strictly increasing subsequence of the array ( and mask the other indices ) ?
Yes , the float default of ` tri ` is kind of unfortunate , but it is there for historical reasons : ` triu ` and ` tril ` used to work by multiplying by the matrix returned by ` tri ` , and most matrix operations happen with floating point types .
They now use ` where ` based on a boolean mask from ` tri ` , but you have to honor APIs for backwards compatibility , even when they don't make much sense anymore .
Print append values
@USER What is the need for taking log of a column in a data frame ??
Is there any specific rules for taking natural log ??
so basically log ([ 9000000 )] is equal to 2.718281828 ** 16 am I right ??
Because ` log ([ 9000000 )]= 16 ` then ` 2.718281828 ** 16 = 9000000 ` .
Using ` numpy ` you can take the diagonal of a column index , eg : #CODE
The result would be the the sum of the sizes is still the same but the individual sizes haven't been altered by more than the limit #CODE
Fourier transform and filtering frequencies with negative fft values
I'm trying to understand what do I get if I perform a Fourier transformation on a periodic signal and filter for frequencies which have negative fft values .
However if you want to do the inverse and compute the IFFT , you will need to feed the IFFT a conjugate symmetric negative half ( or upper half , above Fs / 2 ) of frequency data , or else your IFFT result will end up producing a complex result ( e.g. with non-zero imaginary ( sqrt ( -1 )) components , rarely what one want when dealing with base-band real data ) .
Thanks for the input , however , my question is not about the negative frequency values ( which are the mirror of the positive ones ) - in my example on the x axis , but about the negative ones in the y axis which come from the FFT - what do they mean ?
You should plot the absolute values of f_signal .
You can help us help you if add the entire [ stack trace ] ( #URL ) in the question .
i am reallz now to this ... can yoou tell me how to add a stack trace ?
To answer your question in the comments about tracebacks ( stack traces ): running the following #CODE
` float ` is probably not the best variable type to store node ids .
0-based node ids would make it a little simpler , too
Node ids should be integers .
First time posting so I'm kind of new to how stack overflow does things .
` translate ` is a method of string objects that used to replace particular characters with other particular characters .
Since you're only interested in the number of labels , try using a defaultdict with int instead of list , and just increment instead of append .
Each cluster center has a unique negative ( integer ) value .
More generally , let ` G ` be a sequence of indices that hop to the same cluster
I want to do fast subset-indexed ` dot ` and other linear operations .
For now on I'm using cython to code a fast column-indexed dot product through the cblas library .
This is far larger than doing ` dot ` on 500 columns .
@USER To add to hpaulj's comment , [ advanced indexing ] ( #URL ) ( e.g. with an array of integer indices ) always returns a copy of the data rather than a view , so you need to take into account the additional cost of allocating a new array when you do ` A [: , subset ]` , as well as the fact that you are indexing non-contiguous blocks of ` A ` .
And the default behavior of transpose is to reverse the order of the dimensions , which completes what you were after
But it is giving me a ` TypeError ` saying `" : can only concatenate tuple ( not " str ") to tuple "`
` ( p0 , )` is a tuple , and if ` args ` is also a tuple then the ` + ` operator would just join these two tuples together : #CODE
But if you're looking for 45-degree angles , have you tried Sobel and looking for where the gradient in the x- and y-direction is equal ?
I suspect that what's happening is that the kink represents a singularity in the gradient of E , which is forcing the solver to take smaller and smaller steps until the step size reaches the limits of floating point precision .
Then rolling ` mask ` two places up and two places left and using it on ` ones ` ...
There may be a more memory-efficient mathematical way to select the required indices .
I first tried incrementally deploying , as suggested in the linked answer , by adding pieces of the matplotlib package stack to my ` requirements.txt ` file in stages .
Then append : ` sys.path.append ( ' / opt / py26-selected-packages ')` .
Probably trivial to resolve , but requires looking into some log files after SSHing into the instance .
... and get the result , while not changing neither ` a ` nor ` b ` in-place , nor doing any superfluous resize operations .
1.5.1 here , but answers about newer ones are welcome too ...
It's merely about using the correct indices .
The log file shows :
You absolute legend , thank you .
Maybe i can combine this with pointwise maximum to achive norm = max ( 1.0 , np.linalg.norm ( var [ i , j ]))
@USER : Sorry , I missed the max bit at first now added to my answer .
install the packages of the SciPy stack is to download one of these
One intuitive way ( although inefficient ) would be to create an intermediate array of zeros , then fill in each row with the values in ` a ` , ` b ` , ` c ` , ... with the appropriate column shift .
Once you have this array you can just sum over the rows to get the result you want : #CODE
stack from Github in a virtualenv using Python2.7 :
That's a very convenient way of installing the scipy stack , especially for mac and windows users .
The matrix is initialized as 10 x 10 matrix of zeros .
I've been through lots of questions in stack overflow but still can't figure this out .
If you want to test multiple conditions then you can simply generate multiple sets of boolean indices , e.g. : #CODE
If i do not transpose ` a ` prior to argsort , I do get an array of shape ( 9 , 2 , 2 )
Numpy has a function that can find the indices of elements based on a condition .
indices : [ 2 , 3 ]
You can simply set indices = x > 2*y .
You will definitely have to reshape your input array , because cutting out " rows " from a 3D cube leaves a structure that cannot be properly addressed .
concatenate numpy arrays which are elements of a list
I want to row-wise concatenate a , b and c and get a numpy array with shape ( N_a+N_b+N_c , T ) .
` help ( ' concatenate '` has this signature : #CODE
And the default axis is the one you want to join .
So essentially , it should be faster to use concatenate here .
and instead returns ` ValueError : could not broadcast input array from shape ( 0 , 0 ) into shape ( 2 , 2 )` for ` pad=2 ` and I am not sure why .
How to vectorize the dot product between segments of a trajectory
Here is a function for the dot product between consecutive segments of a trajectory ( xy coordinates ) .
Unfortunately , the dot product produces a square matrix from which I have to take the diagonal : #CODE
In my frustration I wrote the dot product myself .
The bad part of ` func2 ` is that it does a lot of dot products that you don't
Whenever you are computing a sum of products , think about using ` np.einsum ` .
So for each ` i ` , the sum ` ( p1-p2 ) [ i , j ] * ( p4-p3 ) [ i , j ]` is computed , where the sum runs over all ` j ` .
I think the reason why ` func4 ` is beating ` einsum ` here is because the cost of setting of the loop in ` einsum ` for just 2 iterations is too expensive compared to just manually writing out the sum .
` einsum ` is a good tool for generalizing the ` dot ` product .
' ij , kj ' produces ` dot ( p1-p2 , ( p4-p3 ) .T )` ; the ' i ..., i ...
-> i ' does the diagonal - all in one step .
Fast python algorithm ( in numpy or pandas ? ) to find indices of array elements that match elements in another array
I am looking for a fast method to determine the cross-matching indices of two arrays , defined as follows .
The groupID entries of the groups array are unique , the groupID entries of the members array are not .
` einsum ` computes sums of products only , but you could shoehorn the cross-product into a sum of products by reversing the columns of ` tmp2 ` and changing the sign of the first column : #CODE
I think the reason why ` func2 ` is beating ` einsum ` here is because the cost of setting of the loop in ` einsum ` for just 2 iterations is too expensive compared to just manually writing out the sum , and the reversing and multiplying eat up some time as well .
how to make matrix into diagonal matrix in numpy ?
how can you make the first column , ` x [: , 0 ]` , into a diagonal matrix in numpy ? to get : #CODE
this gives a numpy array back with one element , not a diagonal matrix with zeros
There is a ` diagflat ` that ' Create a two-dimensional array with the flattened input as a diagonal .
Yeah , to automatically perform a broadcast of the ` max ` up ( for the reduced case where we were just trying to find out where a row had maximal group brightness ; or maybe an ` idxmax ` if we needed to preserve the location ) .
I changed the limitation of the integral into -inf to +inf by change of variable multiplying a different H ( t ) then I used this as my function to convolve with H ( t ) ( the one inside the integral ) , but the output plot is definitely not a exp function , neither I could find any mistakes in my code , please help , any hint or suggestions will be appreciated !
The lead-in is the only time when the convolution integral ( technically sum in our case ) increases ... thus after the lead-in is finished the convolution integral follows Exp [ -x ] ...
There is no way you can ever get exp ( -x ) as the general form because the equation for b ( t ) is given by 1 - R*exp ( -x ) ...
It can't mathematically follow an exp ( -x ) form .
You can get an exp ( -x ) form if you use b ( t ) = R*It1 - H ( t ) ... the code for that is here ( You might have to normalize depending on your needs ): #CODE
I used your method , but the plot b ( t ) is still not exp ( -Rt / L ) which is what it supposed to look like .
And the integral i mentioned in the beginning came from the derivation process , I already have the result b ( t ) = exp ( -Rt / L ) H ( t ) just by solving the differential equ , so if the plot from python is not like exp ( -x ) then i must have done the convolution in python wrong .
When we convolve ( discretely ) , we effectively flip one function around and slide it along the other one , multiplying corresponding values and then taking the sum .
So instead I looked up the debug log the console error told me to go to , at ` C :\ Users\jon\pip\ pip.log ` .
If it helps I can post the pip debug log .
My plan is to basically wrap ` complex_ode ` in a function performs a dot product between H ( t ) and y , and if that result equals 0 , then don't solve the ODE and just use the previous value of y .
I have already tried with " unique " but it sorts the values whereas i want the array as it is just with the counts beside every value .
You can do this using ` np.unique ` by returning both the item counts and the set of ' inverse ' indices that reconstruct the original array from the unique values ( in the example below , ` uvals [ idx ]` would give you back ` w `) .
You can use the inverse indices to index into the count values according to wherever the corresponding unique items occur within ` w ` : #CODE
I will use this " M " as a mask matrix for cancel out some values by element-wise multiplication ( exp : result=A . *M ) .
If you want a dense array output , you could just use two integer arrays to index the rows / cols of the nonzero elements : #CODE
Final note , it is probably a good idea to hard clip position to the bounding box with something like ` position [ xmax , 0 ] = limit_x ; position [ xmin , 0 ] = 0 ` .
However Numpy has two builtins we can use instead , np.triu lets us set all values below a diagonal to ` 0 ` and np.nonzero will give us the indices of non-zero elements in a matrix .
The goal here is to make two arrays ` diff ` and ` norm ` where ` diff [ i ] [ j ]` is a vector pointing from particle i to j ( so ` diff ` is a 3D array ) and ` norm [ i ] [ j ]` is the distance between particles i and j .
l.append ( max )
x [ max ]= rdotphi0
r=r- ( phi [: , max ] *rdotphi0 )
phi= scipy.delete ( phi , max , 1 )`
So you see , I need to append ' l ' using the original indices , but at the same time , I'm in a loop and have to work with new submatrices with deleted columns .
ValueError : operands could not be broadcast together with shapes ( 2,302 793,102 0 , 3 ) ( 302793 , )
I know how to extract the stack , but I don't know how to modify it and re-inject it before raising the exception .
I see your problem , in that case I would suggest using the ` limit ` parameter to strip out unneeded information about the traceback .
The only reason that ` A+B ` doesn't show any internal stack frames is that ` numpy.ndarray.__add__() ` happens to be implemented in C , so there are no Python stack frames after the one containing the ` A+B ` to show . numpy is not doing anything special to clean up the stack trace .
If I add ones instead of zero the residuals are calculated : #CODE
Additionally , If I calculate the sum of squared residuals in excel i get ` 9261214 ` if the intercept is set zero and ` 5478137 ` if ones are added to x .
I even get a very slightly different result for the sum of squared residuals in the ` ones ` case , from the sixth decimal place onwards : array ([ 42727293.12864096 ])
` lstsq ` is going to have a tough time fitting to that column of zeros : any value of the corresponding parameter ( presumably intercept ) will do .
So I have a given ( fixed ) set of column indices .
The row indices depend on them .
A diagonal solution : ` np.array ([ x [: : -1 , :] .diagonal ( i ) for i in [ -2 , -1 , 0 ]])` .
I was going to leave a ` diagonal ` based solution in the comments , but in time tests it proves to be faster than the ` strided ` one .
But the ` diagonal ` solution has the advantage that it might be easier to understand .
Time basically the same as for the smaller array , while the ` diagonal ` approach slows down with size .
Note that with the first method , the dot will end up on the top of an existing grid node , while ` plt.contour ` interpolates your data , and depending on the interpolation algorithm used , it may result in a somewhat different location .
Nearest neighbour searches on K-d trees are * O ( log ( N )) *
place smaller array in large array while ignoring zeros
i have a large array and want to place a smaller array at some offset and at the same time ignore zeros in the smaller array .
I'd try to remove the zeros from the smaller array first .
It sounds like you might want to use sparse matrices for this - are most of the values in ` output ` also zeros ?
at the beginning output is all zeros but the pixels are not and then later output is not zeros
The ( default ) initial guess , ` p0 ` , for ` curve_fit ` is an array of all ones :
But how do I know that I can append age to the groupby object like that ?
Sorry , I confusingly used the word " append " .
With ` tile ` , each row is a copy of the original row .
I have a 2D binary data representing a map of vegetation ( ones ark existence of vegetation , zeros for bare soil ) .
I understand that the ways to do it are fft or 2 points autocorrelation functions .
Is it right to take the fft of this data in this way ?
Creating an image mask with a linear gradient
Probably the easiest is to reshape the numpy array first , and then go about printing .
( I'm just approximating the notation , but hopefully the idea is clear . ) Actually , since ii goes up to k+1 , it will be a bit more complicated , but something with a cumulative sum or cumulative product may help .
Then it's just a matter of fiddling the indices correctly in the numpy function .
If you want additional speed , subsituting ` convolve ` with ` scipy.signal.fftconvolve ` may speed it up even more .
So what you need to do is to say that you want to find a value for , say , not just some abstract ` gamma ` ( which pipeline doesn't have at all ) , but ` gamma ` of pipeline's classifier , which is called in your case ` rbf_svm ` ( that also justifies the need for names ) .
You could either build it up from random integers , or take a random long and throw away the ones that don't fit .
Changing ` result._mask = cond ` to ` result.mask = cond ` might be all that is need to correct this issue .
That way ` masked_where ` will remain useful for unstructured numeric arrays , while preventing misapplication to structured ones .
I want to delete the elements of the ` z ` array where the gradient is almost 0 ( i.e. approx . the first 60 elements ) .
One option is to use numpy to compute the gradient explicitly ( it just uses a central difference scheme ) and then use numpy's boolean indexing feature ( aka index arrays ) to filter out indices with small values of the derivative : #CODE
Can the " small values of derivative " be small with respect to the sin curve ?
Efficient Parallel Sparse Matrix dot product in Scipy Python
The problem : This dot product is very slow because it uses just one cpu ( I have access to 64 of those cpus on my server ) .
Is there any more efficient way of doing this dot product ? or computing the pairwise similarity in Parallel ?
I could also divide A into n horizontal parts and use the parallel python package to run multiple multiplications and horizontally stack the results later .
I guess I could calculate the value for all rows and then delete the ones I don't want but this seems like a lot of wasted effort ( the NaNs are quite rare in the dataframe ) and , in some cases where ' condition ' equals 1 , the calculation cannot be made due to division by zero .
It also allows you to pass a tuple of indices easily , as below : #CODE
Why does this produce an array of zeros for q , instead of the actualy value or err which is 0.0159 ?
However , ( taking a stab in the dark ) , if ` q ` has an integer dtype , then ` q.fill ( 0.0159 )` would floor 0.0159 down to the nearest int and thus ` q ` would become filled with zeros : #CODE
Get indices of numpy 1d array where value is greater than previous element
I can find the indices where the element is greater than the previous ( the element to the left ) like this : #CODE
Filling multiple diagonal elements of a numpy 2D array
What is the best way to fill multiple diagonal elements ( but not all ) of a 2 dimensional numpy array .
I know ` numpy.fill_diagonal ` is the recommended way to fill all the diagonal elements .
If the array is large and the number of diagonal elements to be filled is also large , is there a better way than above .
Your expected result is ` [ 4.5 2.5 3.5 4.5 ]` which has only 4 elements , and you have 5 indices in B .
One issue that I still have is the method using ` bincount ` is that the indexing array ( B ) must contain at least once each of the indices of A otherwise you get a shape error ` ValueError : operands could not be broadcast together ` .
unfortunately if you want a unique random value for each item there have to be len ( items ) calls .
The number of items in your resulting sample ( ` n ` attempts each independently with probability ` p `) has a binomial distribution and thus can rapidly be randomly generated e.g with ` numpy ` : #CODE
If each item has a totally different ` p ` , this can't work ( if there are a few different values of ` p ` it can work by stratified sampling -- segment the population into uniform ones , each sub-population with a single value of ` p ` , and get samples from each of them independently , then union them ) .
Numpy , pandas , Matlab , R rolling sum inconsistency , with varying length of history
The problem is that fast rolling window implementations accumulate rounding errors , which do depend on the length of history outside the rolling window .
So when I do a 20 day moving sum for 1995.07.01 , the result will have more accumulated rounding error in the case of running the rolling sum operation on a timeseries since 1990 .
Then the result seem very similar to the original ' poly ' with degree of 3 .
I notice that it doesn't hold when you use ` reshape ` to insert the dimension -- at least for me , ` a.reshape ( 2 , 2 , 1 ) .flags [ ' C_CONTIGUOUS '] == True ` .
While ` newaxis ` introduces a ` 0 ` into ` strides ` , ` reshape ` does not ( its strides are either ' none ' or ( 8 , 8 , 4 )) .
His observation that ` reshape ` and slicing with ` newaxis ` produce different strides shows what causes this behavior -- answering the why question .
` reshape ` , on the other hand , needs to be able to produce arrays of arbitrary shape , so it respects contiguity requirements , and makes copies when it must to do so .
Note that row_id and column_id won't be unique in any of both cases .
But what is the whole hassle with the indices , and get the position of x / y data , et cetera ?
How to append a list as a row in pandas.DataFrame() ?
I am iteratively reading a log file and parsing / extracting data and would like to append that to a dataframe .
ValueError : Shape of passed values is ( 0 , 0 ) , indices imply ( 4 , 0 )
Also , My log file has data in the format #CODE
Please post data that reproduces your error , basically you need to return either a Series or DataFrame to append to your existing df , also this is horribly inefficient , what is the format of your data ?
My log file also has other type of lines in the csv that I am ignoring .
Extract the indices of whites surrounded by blacks
I want to extract the indices of the white ( ones ) patches if the patch is surrounded by blacks ( zeros ) in all four neighboring pixels .
I have 4 arrays ( all the same length ) which I am trying to stack together to create a new array , with each of the 4 arrays being a row .
` np.vstack ` takes a sequence of equal-length arrays to stack , one on top of the other , as long as they have compatible shapes .
ah i didn't realise i could stack multiple things , thanks .
It's ` concatenate ` that needs the extra brackets #CODE
I appreciate that there are builder functions and also standard interfaces to the scipy and numpy fft calls through pyfftw .
By first creating an instance of the fft_object and then using it globally , I have been able to get speeds as fast or slightly faster than numpy's fft call .
By this , I mean roll up your python calls to be as few as possible .
I've tried putting this two pieces together into a new function , which i can simply call by passing the filename as an argument and have the function dump the variables into the workspace .
For longer frames it'll be much faster than ` apply ` , but for smaller ones you won't be able to amortize the startup cost and soit might be a little slower .
( I have tried both dot product and matrix_vector_product , but at the end I find myself using np.einsum ): #CODE
I could make an argument for time on the first axis ( as you will find in most data files as you can perhaps more easily append a row to a file rather than a column ) , but would that answer your question ?
You are invoking the python builtin ` sum ` , rather than numpy's vectorized method ` sum ` : #CODE
It is my guess that calling the builtin ` sum ` on a numpy-array causes overhead by iterating over the array , rather than using vectorized routines .
numpy einsum to get axes permutation
What I understood in the documentation of np.einsum is that a permutation string , would give a permutation of the axis in a vector .
So writing `' kij '` labels the axes of the input matrix , not the output matrix , and this leads to the permutation of the axes that you observed .
So ( in my understanding ) the string represents the inverse of the permutation ...
I am puzzled by the behaviour of the ` numpy.where ` command with ` None ` values -- it does not seem to return the correct indices of
How can I use numpy.where to find indices equal to ` None ` ?
I am running into issues with performance using polyfit because it doesn't appear able to accept broadcast arrays .
I think that this is okay because it is not the absolute values of the independent vector ` x ` that are used , but the difference between them .
This has a well known closed form solution you can find in any statistics book , or produce your self by creating a 2x2 linear system of equation premultiplying both sides of the above equation by the transpose of ` [ X ]` .
Note : It works if I replace the dots by zeros .
I tested out your methods using an array with a known confidence interval . numpy.random.normal ( mu , std , size ) returns an array centered on mu with a standard deviation of std ( in the docs , this is defined as ` Standard deviation ( spread or width ) of the distribution . `) .
mean mu and std deviation sigma is #CODE
with mean mu and std deviation sigma is #CODE
Intuitively , these formulas make sense , since if you hold up a jar of jelly beans and ask a large number of people to guess the number of jelly beans , each individual may be off by a lot -- the same std deviation ` sigma ` -- but the average of the guesses will do a remarkably fine job of estimating the actual number and this is reflected by the standard deviation of the mean shrinking by a factor of ` 1 / sqrt ( N )` .
If a single draw has variance ` sigma**2 ` , then by the Bienaym formula , the sum of ` N ` uncorrelated draws has variance ` N*sigma**2 ` .
The mean is equal to the sum divided by N .
When you multiply a random variable ( like the sum ) by a constant , the variance is multiplied by the constant squared .
This is the origin of the ` sqrt ( N )` in the denominator .
sample mean and std deviation , #CODE
There's numerous posts and blogs talking about how to manipulate 2D arrays using append , vstack or concatenate , but I couldn't make it work in 3D .
numpy dot product and matrix product
I have implemented pointwise dot product , matrix multiplication , and matrix / vector multiplication as follows : #CODE
There is also a sublist method of specifying these indices .
By focusing on generalizing the ` einsum ` expressions , I missed the fact that what you are trying to reproduce is ` N ` small dot products .
Where as your problem is one of calculating many small dot products .
And without extra arguments that define axes , ` np.dot ` performs just 2 of the possible combinations , ones which can be expressed as : #CODE
An operator version of ` dot ` would face the same limitation - no extra parameters to specify how the axes are to be combined .
It may also be instructive to note what ` tensordot ` does to generalize ` dot ` : #CODE
It can perform a ` dot ` with summation over several axes .
But after the transposing and reshaping is done , the calculation becomes the standard ` dot ` with 2d arrays .
People have asking about doing multiple dot products for some time .
#URL adds that this kind of calculation can be done with ` * ` and sum , eg #CODE
On further testing , I think ` inner1d ` and ` matrix_multiply ` match your ` dot ` and ` matrix-matrix ` product cases , and the ` matrix-vector ` case if you add the ` [ ..., None ]` .
I think the ` numpy ` developers are less enthused about this PEP than the Python ones .
I know there's " savemat " and " loadmat " available from scipy , but I would like to migrate completely , i.e. , do not work with mat files but with numpy arrays .
Then I clear out the unnecessary variables and modules , leaving only the newly loaded ones .
What is the best way to concatenate column or row in numpy ?
If I want to concatenate a and b to make 3 x 10 array , i would do #CODE
If I want to concatenate a and b to make 6 x 5 array , i would do #CODE
` hstack ` and ` vstack ` both end up calling ` concatenate ` , which is a compiled function .
dot products of rows and columns
Surely in step one , what you describe as ' taking the dot product ' is just ab i.e. matrix multiplication ?
The dot product does the multiplication followed by a sum , producing a ( m , p ) array .
If you want the mean instead of sum , just divide by ` n ` , the number of items you are summing .
In reality the arrays I'm working with are 4d and 5d , and I'd like to avoid collapsing all of the beginning dimensions ( I also perform other operations on the array , like taking the sum over the 2nd dimension of the original 4d array which would become much more complicated if I stacked the entire array into only 2 dimensions ) .
Alternatively , to make the rest of the code readable , how much of a performance hit would I suffer by reshaping the array from 4d down to 2d for the dot product , then back to 4d after that step ?
I did find this blog post that links to a small library called ` fastdot ` that claims to do high-dimensional dot products more quickly , but I haven't used it personally so I can't say for sure how well ( or if ) it works .
You could try ` np.tensordot ` , as it does the reshaping and 2D dot under the hood .
This seems working , but know I cannot append multiple colors to numpy array.Somehow , after appending the dimension is reduced to 1 .
1-How to properly append color data without loosing its dimension ?
Repeated ` append ` is slow .
Or create one with all the new stuff , and ` concatenate ` it in the right dimension .
Just wrap it in ` np.array ` , and reshape to 3d .
You probably could get ` append ` to work , but it just does a step by step concatenate , which is slower .
In general if you need to append , do it with lists , and then convert to an array at the end .
` squeeze ` just manipulates the ` .shape ` ( and ` .strides `) attribute of the array .
So like ` reshape ` it is quite efficient .
Same goes for alternatives like ` ravel ` and ` flatten ` .
That is , if a weight of a column is ` 2 ` , the observation should count twice when computing mean , std etc .
Many statistical functions contain a parameters to append the computation weights , for example np.average :
Trivially , ` min ` and ` max ` shouldn't change , and computing ` len() ` shouldn't be too difficult either .
directly affecting the values works with ` mean , ` but not with ` std ` , if I recall correctly .
Say I have a range ` r= numpy.array ( range ( 1 , 6 ))` and I am calculating the cumulative sum using ` numpy.cumsum ( r )` .
If the array is very large , I would like the cumulative sum to break out before it starts calculating values that are redundant and will be thrown away later .
Depending on size of the array you are computing the cumulative sum for and how quickly you expect the target value to be reached it may be quicker to calculate the cumulative sum in steps .
Python append to array vs replace array of zeros
In Python , when calculating something in a for loop , it is possible to either append the solution from each iteration to a list , eg .
or create an array of zeros and replace each zero with the solution from each iteration , eg .
Where does the stack trace say the error is occurring ?
File " / usr / lib / python2.7 / dist-packages / numpy / linalg / linalg.py " , line 520 , in inv
A quick hack is to add a very small value to the diagonal of your matrix before inversion .
I get : ValueError : operands could not be broadcast together with shapes ( 4 , 4 ) ( 16 , 16 )
Take a look at the new stack trace .
Thankfully , ` numpy ` has a ` gradient ` method that does these difference calculations for us , taking care of the details of averaging previous and next slopes for each interior point and leaving each endpoint alone , etc .
Issue Converting Matlab Code to Python when trying to sum array
Numpy's sum function takes an additional argument ` axis ` , that defines over which axis of the array is summed .
If you only want to sum along columns , you need to tell ` sum ` to only sum over ` axis=0 ` .
To properly handle the borders , first pad the array with zeros : #CODE
I first generated a labelled array of unique IDs for each discrete region , calculated sizes for each ID , masked the size array to focus only on size == 1 blobs , then index the original array and set IDs with a size == 1 to 0 : #CODE
Now I tried to reshape ' data ' like this : #CODE
If you profile the ` improved_main ` , you'll see that the amount of calls to ` complicated_func ` has nearly decreased by a factor of 2 ( the diagonal still needs to be computed ) .
I've read that I can reshape these matrixes so that they shapes are the same , but it just seems odd that it works when the script is not run in ArcGIS .
What I cannot figure out how to do is how do I set the mask indices ( x , y ) to zero where the #CODE
Somehow I need to get the ( x , y ) pixel indices from this original image which meets this criteria but I am not sure how to do this .
Not only does the fft version rids you of the overhead of hstack , it also makes the calculation in n*log ( n ) complexity as opposed to n^2 in the case of correlation .
To correlate x with x circularly shifted by k , you could do #CODE
There's nothing there that says ' do some sort of outer or cross computation ' .
I wanted to prevent the window (/ sum ) from being redone in the loop and hopefully make it much faster so I came up with the following function which limits the sum to only the first and last elements of the rolling window : #CODE
Since running sum == moving average , possible duplicate : #URL
You're not taking the actual sum .
I think you can have a sum over a sliding window ( or a rolling window ) or a mean over a sliding window .
I propose to edit it to rolling sum , which seems to be closer to the right thing .
Just a nitpick , you have to take the actual ` sum ( running_sum ( M ))`
Slicing and updating non-contiguous indices in a DenseVector in Breeze
And you can't just literally translate from Python to Scala !
Set the elements with indices 1 , 2 , and 4 to the values 1 , 2 , 3 ?
` val indices = Array ( 1 , 2 , 4 ) val val_ = ( 1 , 2 , 3 ) indices.map ( i => v ( i ) += val_ ( i ))`
` ( indices zip val_ ) map { case ( i , x ) => v ( i ) = x} ` maybe ?
Something like : ids = mask > 1 m = 1 - m m [ ids ] = 0 .
How to sum two arrays in Python ?
Now I want to make a function so that , when I give two years , it will give me the sum of occurrences of those two years .
suppose i want the sum of occurrence between any two years
Note that if you wanted the sum of all occurences between those two years , you'd do something more like : #CODE
Vor ; your code only gives the sum of first and second year not the cumulative . like when i gave
But i need sum of the values between any two years .
s when i am running this code it gives me sum of two years not the the sum of all the values in between any two years
Are you sure which ` sin ` function you are using ?
I have retried the code making sure to explicitly use the numpy sin and I am still getting the same result .
For the moment , a quick and dirty hack might be to truncate the rows of the first layer : #CODE
Note that I'm using ` np.dstack ` to concatenate the layers along the third dimension .
I'm getting this error for all the examples I tried : ``` ValueError : Shape of passed values is ( 0 , 0 ) , indices imply ( 3 , 0 )``` Should I be iterating over the length of the original data set ?
To adapt to get all matching indices instead of one , use #CODE
No only one split into train and test , but after the svd transformed data .
In this case you aren't interested in ` .getnnz ` , which is the number of nonzero terms of a sparse matrix .
The problem comes when I declare the list output before the loop and by updating EulArr via a Tkinter window , append the arrays to the list .
I want this background cluster to absolute black again .
looks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func . however the pandas.rolling_quantile func returns diff results . reduce the row number to 5 , the problem will be gone ( all three methods return the same results ) . any thoughts ?
How to reshape 4d array to 2d array in numpy
i want to convert or reshape this into 2d array , the easiest way #CODE
Of course this just converts the input in the default order ( meaning you " unroll " the input array from inner to outer ); if you need special ordering , you should read up on slicing .
Also , this is not a valid input syntax for reshape it should be ` np.reshape ( input_4d , ( 1800 , 3 ))` or the more popular ` input_4d.reshape ( 1800 , 3 )` .
Extracting the indices of outliers in Linear Regression
How can I extract the indices of those outliers ?
A list is constructed from the indices of x , where the element at that index satisfies the condition described above .
The abs() function takes the absolute value .
but it didn't work work because ` indices ` doesn't give the indices of each row ...
Also , matrices should contain at least 80% zeros for sparse matrices to pay off .
I don't know if this is the problem , but you'll get that error if ` mat ` is an array with dtype ` object ` .
Just before you call ` eigvals ` , print ` type ( mat )` and , if it is a numpy array , print ` mat.dtype ` .
@USER , I have called type ( mat ) and the return was .
I'm aware that I can just call sort on the array ; however , I went to apply this same permutation over and over again .
This works for multirow ` xCoo ` with zeros .
python - simple way to join 2 arrays / lists based on common values
I have tried for a while but can't find a simple way to join 2 lists or arrays based only on common values .
Similar to an SQL inner join but with arrays / lists and not dict , or some other data type .
and append any data into eventlist , then I want send eventlist to called : #CODE
Numpy : 2d list min max is slow
Now I want to find min and max values : #CODE
( 1:15 min [ numpy ] vs 0:25 min [ native ])
The native code is quite optimized , yet I thought numpy could squeeze out a bit more .
The native implementation ( I assume it is something like calling ` min ` / ` max ` on the Python list ) only needs to loop over the data once .
Furthermore , it seems that numpy's min / max functions are surprisingly slow : #URL
I mean it's not even calling C code , I just calculate min and max of actually 2 lists ( sorry didn't mention ) and all 3 dimensions in one loop , which I can also reduce to n-1 by assign the first value of the list to min / max .
thanks pv . but I think this will only work for arrays of the same shape .
Many thanks pv .
For example , 1851 is four times and 1852 is 5 times , when i put the interval ( 1851,185 2 ) it will sum up and give out put as 9 .
I think what you are trying to do is use your mask to mask into the original df , print or get the sum , and additionally get the length .
i want to see the number of rows in between two years.FOr example , 1851 is four times and 1852 is 5 times , when i put the interval ( 1851,185 2 ) it will sum up and give out put as 9 .
indices of different elements #CODE
Anyway , in the case you mention , I suppose two indices are necessary .
What indices do you want returned ?
You want the index of the median in case of odd numbers and the indices of the two values from which you get the median , in the case of even numbers .
In the last case , the two returned indices would be ` [ 2 , 3 ]` .
Well the log of a fraction is negative , by the definition of a log .
That is the idea behind a log .
So your best bet is to display with a log scale so that it displays positive fractions .
Then you can set ` .ylabel() ` as " log scale " .
A way of selecting these values and maintain the shape you want ( more or less ) is to use ` np.ix_ ` to generate a tuple of indices .
Numpy : creating indices for a sparse matrix
So I need two numpy arrays one for the column indices and the other for the row indices .
Basically : step through d , make an array of ones of length of each item in d , multiply by its index in d , and then flatten all these into a vector .
For 1d arrays like this , ` hstack ` is the same as the default ` concatenate ` .
` np.nonzero ( x )` gives a tuple of the nonzero indices .
numpy newaxis does not always append new axis where I want to
Why doesn't ` np.newaxis ` always append the axis where I put it ?
One more question : will using the ` conda install ` command also install non-scientific stack modules ?
First off , to read " raw " data in from a file into a numpy array , use ` numpy.fromfile ` with the appropriate dtype and then reshape it .
If it was fortran ordered , we'd swap the number or rows and columns in the reshape and then transpose it .
The reason numpy exists is to translate these slow python loops and such into fast ` C ` code ( that we don't need to know exists ) .
It's simpler to read , but also likely to be way faster , because numpy has direct access to that data in the computer's memory , and can use fast ` C ` functions to find the sum , rather than using slow python functions .
Python checks that indices are in bounds mercilessly , and that's one of the notorious slowdowns in high-performance computing .
Just one thing , for me ` q [ i , 3 ] = sum ( indices )` didn't work because of the error ` TypeError : only length-1 arrays can be converted to Python scalars ` so I used ` q [ i , 3 ] = ( indices == 1 ) .sum() ` instead .
The ``` TypeError ``` is because the Python ``` sum ``` function can't deal with the array - when working with ``` numpy ``` arrays its best to stick with ``` numpy ``` functions .
Second function ( deriv ) finds the derivatives of eigenvalues with respect to delta .
The stack trace should tell you .
I can shortend ` pv ` s minimal example to : #CODE
Having looked over the man pages for ` numpy `' s ` eye ` and ` identity ` , I'd assumed that ` identity ` was a special case of ` eye ` , since it has less options ( e.g. ` eye ` can fill shifted diagonals , ` identity ` cannot ) , but could plausibly run more quickly .
What , then , is the advantage of using ` identity ` over ` eye ` ?
` identity ` just calls ` eye ` so there is no difference in how the arrays are constructed .
As you say , the main difference is that the diagonal can be off-set with ` eye ` whereas ` identity ` returns only the main diagonal filled with ones .
img_morph = morphology.binary_opening ( img_bw , ones (( 9 , 5 )) , iterations=2 ) [ NOK ]
Then , to count the number of bits , you can just sum up the values form the table , for instance : #CODE
In pandas , doing ` DataFrame.index [ 1 ] - DataFrame.index [ 0 ]` gives me the result I want the difference in time between the two indices I've picked out .
i would suggest looking into absolute and relative file paths .
gave the absolute path !
The question is how now I convert the 2D binary matrix or ones and zeros to a list of the coordinates of only the ones .
How does normalization of a histogram affects its bin error in Python ?
I created a histogram with the data , plotted its error bars , and fitted a Gaussian .
I did the normalization by dividing the histogram by the total number of counts .
Plotting histogram in python
But when I am plotting it I am getting a single line in figure instead of a histogram .
There is a text file for each day so ultimately I would like to stack the data in the text files in an array so that the times continue seamlessly ( eg . from 2012-01-01 23:59 : 55 to 2012-01-02 00:00 : 00 ) #CODE
When ` DataFrame ` constructor is called ( contained in ` rpy2 / robjects / vectors.py `) , this line try to encode the unicode string ( that contain special characters ) to an ascii string : #CODE
I need to call the conversion explicitly , so I tried : ` conversion.py2ro ( df )` or alternately ` pandas2ri.pandas2ri ( df )` but they result in : ` UnicodeEncodeError : ' ascii ' codec can't encode character u ' \xe0 ' in position 4 : ordinal not in range ( 12 8) ` .
The code is based on the discussion here : spectral density in python ; with differences --- a ) normalization of the fft , b ) based on numpy
The problem is that when compared with results on the same dataset processed with Matlab's cpsd , the absolute value of the python result is lower by a factor ~ 1.334 ( at all frequencies ) .
Create a numpy array ( 10x1 ) with zeros and fives
You can translate the OP's request ' number 5 in the first 3 elements and the other 7 elements with the number 0 ' with zip : ` np.repeat ( *zip ([ 5 , 3 ] , [ 0 , 7 ]))` -- which seems more natural to me ...
Truth be told , for my case I get by far best results with multinomial NB .
My favorites for text have been LinearSVC and SGDClassifier using either loss= ' modified_huber ' or loss= ' log ' .
I want to convert those data into a list where the ` i ` th element indicates the position of the nonzero element for the ` i ` th row .
Use ` np.argwhere ` to find the element indices : #CODE
( you can get just the column index by using : ` np.array_split ( indices , 2 , 1 ) [ 1 ]` for example )
If you know that you will only have a single ` 1 ` in your row , then you can transpose the original data frame so the indices of your columns from the original data frame become the row indices of the transposed data frame .
With that you can find the max value in each row and return an array of those values .
Your original data frame is not the best example for this solution because it is symmetrical and its transpose is the same as the original data frame .
Now to find the max of each row : #CODE
Which returns an array of values that represent the column indices of the original data frame that contain a 1 : #CODE
Numpy : use reshape or newaxis to add dimensions
Same shape , but reshape doesn't change the ` strides ` .
` reshape ` lets you specify the ` order ` .
` reshape ` may be faster because it is making fewer changes .
It would be interesting to see the absolute timings from your tests .
In addition , working purely with " relative " results ( as you did not post the absolute timings , and I did not have Numpy to hand at the time ) , I had indicated a few times that some important information may be lurking therein .
Figure 1 shows the absolute timings via timeit .
Looking at Figure 1 , with the absolute timings , it is clear the Numpy and Fortran are diverging .
I divided each ( absolute ) series with its own highest value ( i.e. at n_comp = 10k ) , to see how this " internal relative " performance unfolds ( those are referred to as the ?? 10k values , representing the timings for n_comp = 10,000 ) .
Thus , as n_comp increases and each method " stabilises " to a more or less linear mode , the curves flatten to show that their differences are growing linearly , and the relative ratios settle to an approximate constant ( ignoring memory contention , smp issues , etc . ) ... this is easier to see if n_comp is allowed > 10k , but the yellow line in my Apr 14 answer already show this for the Fortran-only s / r's .
I would even recommend @USER to delete the old ones and make one final answer and polish it to the shape of his preference .
IndexError : too many indices for array while plotting ROC curve with scikit-learn ?
Replace all zeros with the same value , or different values ?
Something like this would ensure you have unique elements in ` a ` , and also replace the ` 0 ` values with a unique value in ` range ( 1 , 11 )` .
It looks like you create a bag of words representation for each document , then join all the bag of words vectors into one vector , and then assign each element of the new vector to an array ?
For scientific stack python , you are hard pressed to beat an Anaconda python distribution .
Define the new row order as a list of indices , then define ` X_train ` using integer indexing : #CODE
circular numpy array indices
An alternative that you can use is the numpy ` roll ` function combined with indexing : #CODE
But when I construct my Phi matrix , I am getting all zeros .
I also seen to test out some different fft implementations , it'd be nice to get those a little faster .
This is probably related to the overhead in creating the fft class .
FYI , the following histogram shows how the really distribution looks like :
Can it be because of the many zeros in the initial table ?
Then calculate sum of squares of differences on the sequences and treat it as the distance .
Notice I passed in a list of indices that I wanted to take from the original array ...
The thing is to remove all conflicting ( understand : the ones that the SDK installer tries to install itself ) version of the Visual C++ redistributable .
The advantage of using established Python distribution installers like the ones I mentioned is that you can get a working copy running in minutes , in a process that is easily reproducible .
When you don't know what to guess , you can do a ` polyfit ( xdata , log ( ydata ) , 1 )` and some basic math to get an initial value , as shown by this answer to the question you linked .
Get indices of results from scipy.pdist ( myArray , metric= " jaccard ") to map back to original array ?
There is a module called ` scipy.spatial.distance.squareform ( y )` wherein it converts the condensed form 1-D matrix obtained from ` scipy.spatial.distance.pdist ( X , metric= ' jaccard ')` into a symmetric matrix so it would be relatively straightforward to obtain indices from there .
The errorsbars require the min value ( mean - standard dev ) and
max value ( mean + standard dev ) .
Is there a way to simplify the computation of the " min " / " max " columns , since it's such a common operation ?
groupby returns a hierarchically indexed dataframe , so I had put " mean " and " std " under " value " seems too complicated .
Fill scipy / numpy matrix based on indices and values
This uses an inverse indexing trick to reorder the columns and rows of an array of diagonal blocks into the desired matrix .
It seems to me that this does something quite similar to ` voxel_matrix [ np.ix_ ( voxels1 , voxels2 )] = 1 ` as suggested by pv . , except it does it all at once , instead of tracking each possible combination of nodes .
ind doesn't give consecutive rows but rather scattered ones .
But indexing with lists with unique values works fine #CODE
So if the indexing is right - unique values , and matching the array dimensions , it should work .
Consider the following solution to computing a within-group diff in Pandas : #CODE
Also , looking at [ this answer ] ( #URL ) from Jeff , I see that he applies ` transform ( Series.diff )` instead of just ` diff ` as in your code .
Add the diff column : #CODE
As far as installing without sudo , I have only ever managed to get openBlas working when i didn't have sudo . took the best part of a day ( cos i didn't know what i was doing , could do it faster now i think ) .
Using ` np.r_ ` to concatenate slices - not much of an improvement over your 1st .
The code for the ` permutation ` function is unchanged from version 1.8.1 to 1.9.1 .
The simplest way to input a raw Python integer is to append ` r ` to it : #CODE
But if I change the arguments to reshape to ( 200 , 200 ) , I recieve an error :
You can reshape 10000 points into 100x100 , you cannot reshape 10000 points into 200x200 .
@USER welcome to stack overflow .
They would need to be either ` ( 1 , N )` , ` ( N , 1 )` ( for the inner product ) or ` ( N , 1 )` , ` ( 1 , N )` ( for the outer product ) in order for matrix multiplication to work .
The methods for the ` WC_unit ` class require some numpy functions , like exp
I wrote some outer code in a script ` test.py ` located at .
Now , when I create an instance ` E1 ` of the class and run ` E1.update() ` , I get the error message ` global name ' exp ' is not defined ` .
Because you've already called ` from numpy import * ` within your IPython session , ` exp ` is defined as ` numpy.exp ` within the set of globals for the current ' module ' ( which , in this case , is just the IPython interactive namespace ) , so when you call ` exp() ` in ` WC_unit.update() ` ( or anywhere else within ` WC_class.py `) it will work fine .
However , you do not do a ` from numpy import * ` at the top of ` test.py ` , therefore when you import ` WC_unit ` into your script ` exp ` has not been defined within the scope of the current module ( which is now the ` test ` script ) .
Since the import fails , ` exp ` is still undefined and the ` WC_unit.update() ` method will raise the ` NameError ` you're seeing .
Suppose you had already defined your own function called ` exp ` .
When you do ` from numpy import * ` , you will be overwriting your own function called ` exp ` with ` numpy.exp ` , so when you later call ` exp ( y )` it might not do what you expect it to .
For example , this is exactly what happens to some of the built-in Python functions such as ` sum ` and ` all ` : #CODE
Can I somehow automatically broadcast a condition with np.where without having to create a matching 2d mask ?
` numpy.newaxis ` creates an axis of length one , the resulting array view has dimensions ( 6 , 1 ) and can be broadcast with the ` a ` arrray .
But I am getting error " list indices must be integers , not tuple " .
When I load the data in Python , the extra comma inside the quotes shifts all my column indices around , so my data is no longer a consistent structure .
You need to ` join ` them : ` ( ' ; ' .join ( x ) for x in reader ( f ))` .
Or replace the ones outside of quotes with a delimiter of your choice .
The ` join ` is required because ` reader ` produces a list of lists , while ` genfromtxt ` wants an iterable of strings ( it does its own ' split ') .
python : import numpy as np from outer code gets lost within my own user defined module
Now I can get around this by just importing numpy as np in top of the WC_class module , or even by doing ` from numpy import exp ` in test_class_WC and change the update() method to contain exp() instead of np.exp() ... but I'm not trying to do this because it's easy , I want to learn how all this namespace / module stuff works so I stop being a python idiot .
So when the stack trace says ` global name ' np ' is not defined ` , it's talking about it at a module level .
` TypeError : list indices must be integers , not str '`
numpy : ravel_multi_index increment different results from iterating over indices loop
I have an array of indices ( possible duplicates ) where I increment each these of indices in another 2D matrix by 1 .
and ending at ` max ( x )` : #CODE
` np.bincount ( raveled )` will be greater than the number of unique indices .
indices into ` acc.flat ` .
then reshape it to the same shape as ` acc ` .
` np.bincount ` , and use it to return both the unique indices and their corresponding
These can then be used to assign the correct counts to the correct unique locations within ` acc ` : #CODE
But I'm not quite sure I get the part about np.bincount being the length of np.bincount ( raveled ) will be greater than the number of unique indices .
I gave an example above : ` np.bincount ([ 1 , 1 , 3 , 3 , 3 , 4 ])` gives you the counts for indices 0 , 1 , 2 , 3 and 4 , whereas you only want the counts for the indices that actually occur at least once ( i.e. 1 , 3 and 4 ) .
@USER ` cpu_time ` will sum cpu_times of all threads , ` system_clock ` returns the elapsed wall clock time .
You need to decode the string as unicode rather than standard ASCII ( see here ): #CODE
You could also decode your input file as unicode like this : #CODE
I could loop over each unique values perform the min operation and store the results but I was wondering whether there is a faster and cleaner way to do it .
You need some sort of sort or groupby that can organize the indices once , and then give quick access in the loop .
First we sort the function indices , and then find the elements of the sorted array where each new index begins #CODE
Now there is no longer a need for blind lookups , since we know exactly which slice of the sorted array corresponds to each unique function .
` x [ a :] ` is equivalent to ` x [ a : None ]` -- so instead of breaking out the last call , couldn't you just convert ` unique_func_indices ` to a plain list and append a ` None ` value ?
Like yours , this splits a sequence of ` argsort ` indices into chunks , each of which corresponds to a function in ` func_table ` .
It then uses each chunk to select input and output indices for its corresponding function .
Then you can do the following : 1 . get unique values of the ` Y ` column , and 2 . for each unique value of ` Y ` , select the appropriate subset of data and plot ` Time ` v.s.
` append ` adds to the end of the array .
It's not generally the right tool to use to build multi-dimensional arrays incrementally as you're doing - you can add append to a specific access ( and so stack arrays ) but you need to ensure that both arrays are the same shape , and same size along that axis .
Note that for calculating the cross-spectral density , the order of the arguments is important , since ` Pyx ` is calculated ( after appropriate windowing and normalization ) as the average of ` fft ( y ) * conj ( fft ( x ))` over the individual windows .
Similarly , the PSD ` Pxx ` is calculated as averaging ` fft ( x ) * conj ( fft ( x ))` .
I forgot exactly why , but there is a good reason why you calculate it as the ratio between these two averages , instead of directly averaging ` fft ( y ) / fft ( x )` .
Since in that case , you will get the inverse transfer function , so the absolute value of the TF will be the negative when measured in dB .
I guess a proper ` csd ` is not yet implemented in Scipy , that is why I always used the ones from ` matplotlib.mlab ` .
Efficient way of computing dot product inside double sum in python3
I'm looking into how to compute as efficient as possible in python3 a dot product inside a double sum of the form : #CODE
I presume this is due to no big vectorizing advantage , only the short 3 dot 3 is np.dot , which is eaten up by starting N^2 of those in the loop .
and thus the dot product also becomes 0 , or splitting the sum up in two to achieve the same #CODE
for a given sum , yes .
Is it always the case that one element in ` x ` will be 1 and the other two zeros ?
IVlad did this already with his code by precomputing the dot product , and then only summing the 1-dim numbers .
Then I got rid of the ` zip ` s and ` sum ` : #CODE
Considering I'm using fewer ` exp ` calls than your initial code however , consider that maybe this is actually the more correct version , and your initial approach is the one with precision issues .
The same , except with less multiplications and a ` sum ` function call : #CODE
sum_p += cmath.exp ( -1j * ( sum ( r_p [ j ]) - sum ( r_p [ k ])))`
there is no sum needed ? as the generated inner product is already summed . ommitting this speeds this one already up by roughly factor 2
` , each ` * ` should represent a dot product with a scalar as a result .
Added another update with a ` sum ` call - a pretty obvious simplification but I thought I'd add it in anyway .
Precalculating the dot product before the sum makes hpaulj's numpy suggestion exactly the same fast : #CODE
However , I found one more thing that gives another ~50% increase , instead of computing the exponential twice , I take the complex conjugate inside the sum : #CODE
To support non-scalar outputs , you would have to roll ( har har ) your own version of ` rolling_apply ` .
Could you paste the full stack trace ?
The calibration signal noise can be averaged to a decent SNR , but the measured signal contains a systematic noise , which - even when averaged - displaces the mean value of the noise floor .
This is probably best solved using the normal equation , which will give you a fit that minimises the sum of squares error , which is what I think you are after .
Why can't I index an ndarray using a list of tuple indices like so ?
` numpy ` expects a list of row indices , followed by a list of column values .
Using your style of xy pairs of indices : #CODE
Each row and column in my transition state matrix is supposed to sum to 1 , however , I've found that rounding errors cause each row and column sum to only be approximately 1 .
If not , consider checking for ` T.T ` ( the transpose ) , because you need to make sure you're looking at the right state transitions : you need the left eigenvector of your stochastic matrix , but almost all out-of-the-box scientific packages ( numpy included ) default to computing the right eigenvectors ( this the same reason in textbooks and stuff you have to premultiply by row vectors instead of usual matrix-column multiplication when dealing with this stuff ) .
Maybe also ` sstate = sstate / sstate.sum() ` to make sure the probabilities sum to 1 despite roundoff .
` eig ` and things like it will compute the right eigenvectors , as in vectors ` v ` such that ` Av = ( lambda ) v ` for scalar ` lambda ` .
What you need though is the left eigenvector of ` A ` , so something that satisfies ` v.T *A = ( lambda ) v.T ` and this won't just be a transpose or conjugate of the right eigenvector .
` eig ` and things like it will compute the * right * eigenvectors , as in vectors ` v ` such that ` Av = ( lambda ) v ` for scalar ` lambda ` .
What you need though is the * left * eigenvector of ` A ` , so something that satisfies ` v.T *A = ( lambda ) v.T ` and this won't just be a transpose or conjugate of the right eigenvector ,
so look at the 1D array ( the first element in the 2-tuple ) returned from the call to eig ; this is the eigenvalue array , and as you can see it is not in descending order , you need to sort it manually and then apply the same ordering to the eigenvector array .
in addition , if the 2D array you are passing to eig is sparse , then use eig from scipy.sparse ; it's much faster , particularly so in your case because you only want one eigenvector .
If it's really a problem , after writing the file , you could seek backward one byte in the file ( ` seek ( -1 , 2 )`) and then truncate ...
I'm not really sure why it matters , or if there is a way to prevent it on the numpy side ( I didn't see anything in the docs ... ) but you can probably seek back in the file after writing and then truncate .
how to append numpy array of different dimension to already existing text file in python
But it seems i have to convert numpy array to string before i can append to text file and its hugely time consuming as array to string function for large matrices is really slow .
Hence can somebody please tell me how do i append numpy arrays directly into the text file without converting them to string or list
If you prefer to open and close the file each time , use the ' a ' , append mode .
Error using zeros
` pandas.read_csv ` is much more efficient than ` loadtxt ` , but you can also " roll your own " loadtxt-alike easily that will be much more memory-friendly .
A quick fix is to create normally distributed points at a fixed distance ( less than the std ) from the origin .
` savez ` is more complex to use if you just want to dump out a single array .
Then sum over the last axis ( columns ) , giving a ` N , )` array ( N element vector ) .
Finally get the main diagonal - again N values .
Do you know how to translate this to matlab ?
So i fill the batch with zeros and i overwrite them with predicted results .
` ValueError : could not broadcast input array from shape ( 55 , 1 , 96 , 96 ) into shape ( 64 , 1 , 96 , 96 )`
The last batch will have dummy zeros at the end , but thats ok :) #CODE
d is a unique index , and e is an arbitrary integer .
To obtain clean indices for mutating the sub-array , a convenience function is provided , ` np.ix_ ` .
If you want to get around this manually , without ` np.ix_ ` , you still can , but you must write down your indices to take advantage of NumPy's broadcasting rules .
What this means is you have to give NumPy a reason to believe that you want a 2x2 grid of indices instead of a 1x2 list of two specific points .
Hey i am trying to resize an image without stretching it but adding white pixels instead .
So instead i used numpy to add the extra pixels before the resize as arrays of [ float ( 255 )] .
Obtain indices of a array where 4 or more than 4 adjacent elements are 1
I want to obtain indices of array ( data ) where 4 or more than 4 adjacent elements are 1 : #CODE
Can adjacency be in any direction , e.g. vertical , diagonal , T-shaped ?
Function that works as append for numpy.array
If you have to use it a lot , you are probably better off converting to a ` list ` then append and when you are done convert back to ` np.array ` .
Note that append does not
Within the function the name ` ar ` got re-assigned to the ` None ` singleton object ( which is the return value of the list append method ) .
That is , for Python lists you would not outsource the append operation into another function .
Otherwise you end up with an array full of ones .
I believe you are looking for something that you won't get by using ` extraction_step ` as none of the patches in your array ` patches ` are all ones .
From the corrections to your post , I believe you might be looking for a way to detect where submatrices of shape ` ( 2 , 2 )` are all ones .
In that case , you're most likely interested in the staggered grid of that matrix that has a one in the center of each 2x2 submatrix whenever the 4 elements of that submatrix are all ones : #CODE
How can I achieve that matrix's rows are sorted after the size of the nonzero entries ?
Next , ` argsort ` sorts the indices of this array by their corresponding value , giving ` [ 3 , 1 , 2 , 0 ]` .
How to do histogram equalization for multiple grayscaled images stored in a NumPy array easily ?
It will return the indices into a sorted array in increasing order .
` indices ` now contains the indices of the input variables in order of decreasing feature importance .
It creates an array of the indicated shape filled with ones .
To create an N by 1 array of ones , use : #CODE
I guess your issue also includes horizontal concatenation of array ` data ` and ` ones ( N , 1 )` as done by ` [ data ones ( N , 1 )]` in Matlab .
So adding to the answers given by the others , you can use ` np.hstack ` to concatenate the two arrays .
` np.hstack ` will concatenate the two arrays ` data ` and ` np.ones ( N , 1 )` horizontally ( along axis=1 ) .
I think there's a misplaced ` encode ` , causing the ` tuple ` to split a string .
Out [ 34 ]: ( ' three ' , ' two ' , ' one ')` means in relation to my code , what would the first line translate to in my context ?
I want to filter the indices whose footprint ( 3 , 3 ) consists of 1s .
So this array ` dual ` now has a " 1 " ( True actually ) for every 3x3 submatrix that was full of ones .
Those submatrices are overlapping however , so you first want to set the patches all to 0 whenevery any of these 3x3 submatrices was not all ones ( that's the one-to-last line in the first code block : ` patches [ dual == False ] = 0 `) and then you can apply the ones again in each 3x3 submatrix that originally had all ones .
The alternatives are to correlate with a ` kernel = np.ones (( 3 , 3 ))` or to take multiple bitwise operations into account ( like in the other answer ) , but that last method becomes very difficult to write when the array's dimensions grow beyond simply ` ( 2 , 2 )` .
Hence the error : ` IndexError : too many indices for array ` .
Thank you , I am looking for code or sudo code to calculate the gradient using the given kernel .
Thank you , But I am looking to compute the gradient for the input image using a given kernel .
Join the 2 lists , and ask for the unique values ( sorted ): #CODE
or if the ` x ` might already be arrays , concatenate them first : #CODE
I was trying to use ` TfidfVectorizer() ` class of the ` scikit learn ` but the fit_transform method required to have list which means I have to take all my numpy array elements and append to a list .
Can you trace those values back up the calling stack ?
What is the power of your PCA ( sum of the first 2 eigen values ) ?
The power of your reduction is equal to the sum of the eigen values corresponding to the eigen vectors that you are using .
So , when you reduce to 2 dimensions , the power is the sum of the first 2 eigen values .
@USER go to the matplotlib gallery , run any of the examples , but first append ` plt.savefig ( ' test.png ')` to the script .
I can't find core / _dotblas.so under build / lib.linux-x86_64-2.7 / numpy / core but I get good results when testing the dot , multiplication , SVD and Eigendecomposition operations .
So I want to divide up the data into slices and then perform a histogram on each slice , then visualize the histogram using some colormap .
` np.log ( 0 )` gives the warning ` RuntimeWarning : divide by zero encountered in log ` and returns ` -inf ` ( using numpy 1.9.1 ) .
" data " is a sample of size 5000 , each of it is a 20X20 matrix stores greyscale value , I am using gradient descent for logistics regression , initial value for theta is np.ones() .
#URL RuntimeWarning : divide by zero encountered in log
I see now , the warning is all about np.log ( 0 ) .It ' s weird what's in log is 1.0 /( 1.0 +e^x ) , how should this function return 0 ...
I saw what the problem is , e^x sometimes goes to a large value and overflow happens ... causing the value within the log to be 0 !
Python program can not import dot parser
I have already tried the solution suggested for a similar question on stack overflow .
I still get the message " Couldn't import dot_parser , loading of dot files will not be possible " when I run my program though .
The gradient is altered .
I have to preserve the gradient .
Numpy : get 1D array as 2D array without reshape
Numpy : use reshape or newaxis to add dimensions
I'm trying to use os.path join to put the individual list entry at the end of the path specified in listdir earlier .
To sum up the solution process I'm trying to use so far :
` being the left edge of the histogram .
The problem I am running into is trying to rebin the data into a global histogram .
That is create a count vs . absolute time histogram for every sensor .
The bins of this new histogram are supposed to be 2 ms long .
Currently , I am using the following code to fill this global histogram : #CODE
I am looping over certain array indices repeatedly because they are split between absolute time bins .
Unrelatedly , it's not necessary in Python to create a list of zeros if you're just going to replace all the values in the list .
What I was hoping to do was populate the empty array , e , in a fashion which is more elegant than my list comprehension method AND / OR how to reshape the array properly .
I will experiment with your reshape option ( 3 , 4 , 3 , 2 ) , it never dawned on me that the last 3 , 2 wasn't enclosed in brackets and I have no idea why I thought 12 would be reshaped as ( 3 , 4 )
Many other universal functions in NumPy have an ` accumulate ` method to simulate looping through an array , applying the function to each element and collecting the returned values into an array of the same size .
It's very similar - ` accumulate ` stores the result of the operation on each element returning an array of the same length , whereas the unfunc's [ ` reduce `] ( #URL ) method just shows the final result ( collapsing the array ) .
So ` reduce ` would basically return the last element of what ` accumulate ` returns .
Now I want to sum them over the first dimension , but this is rather slow .
Is there a quicker way to sum an almost sparse nd array ?
It seems that reshaping does not do a lot of good for the sum , while transforming them into csr_matrix and back to numpy kills performance .
I am still thinking about using the indices directly ( below called ` rand_persons ` , ` rand_articles ` and ` rand_days ` , since also in my original problem , I make the big ndarray using these indices .
The original starting data structure is three arrays of ( i , j , k ) indices and one equally long array of values .
I made a numpy ndarray from this , which is fast , since it is sparse ( ~1 in 1,000,000 elements is nonzero ) .
You could reshape the array so it is 2d , do the sum , and then shape back #CODE
My guess is that your array will have to be very large , and very sparse , to beat the straight ` numpy ` sum .
If you have other reasons to use the sparse format it may be worth it , but simply to do this sum , no .
Since your data is already in sparse format ( indices and values ) , you can do the sum yourself .
Just create an array that is the size of the final summed array , and loop over the indices , summing the corresponding values into the right slots .
have you tried working with an array of indices : ` np.arange ( a.shape ( 1 ))` ?
I am able to print the respective binary values but not able to plot the square wave dynamically.In addition to this I am not able to dynamically resize the x axis in the plot window .
Pass the BoundaryNorm to the PolyCollection , ` poly ` .
Instead of the inverse Hessian H_k , L-BFGS maintains a history of the past m updates of the position x and gradient ?
If your intent is to count the elements of an array that are different from zero , you can use the ` numpy ` function ` sum ` .
Using ` sum ` , you can obtain the sum of all the elements in an array , or you can sum across a particular axis .
Now you are protesting : I don't want to sum the elements , I want to count the non-zero elements ...
` False ` is zero and ` True ` is one , at least when we sum it ...
or , if you want to sum over a column , i.e. , the index that changes is the 0-th #CODE
or sum across a row #CODE
So , My question is : How can I define a structured array with a string type first column and a float type 9726 other different unique columns ?!
Ashwini : when I try that I get ' TypeError : list indices must be integers , not tuple ' ?
due to broadcasting , you don't need to repeat duplicate indices , thus : #CODE
The whole point to using ` numpy ` arrays is to define fixed sized objects , ones can be traversed in a multidimensional way quickly .
Numpy built in elementwise append
I have an array 3D array of points onto which I would like to append a corresponding value in a flat array .
Is there a built in way to append elements from the second array to the corresponding positions in the first ?
ah i was missing the reshape .
But I tried lot of times , doing many kinds of reshape and transpose , they all either raise error saying not aligned or return a single value .
This is called an " outer product .
Can I easily obtain the absolute path to a python version managed by virtualenv ?
To reshape the array you could use : #CODE
Suppose you want to preserve the axes of length 2 , and reshape the other axes into a single axis of length 12 .
I am trying to append an array to a numpy recarray using the numpy.lib.recfunctions append_fields function .
Under the 2.7 , the full error stack is : #CODE
Convert it to an array and reshape the array : #CODE
You're getting an error because ` numpy ` needs indices passed in this way to have the same shape , or to be broadcastable to the same shape .
However , you might need to resize ` a ` to the right size before assigning .
Even so , the compiler will not immediately allocate all memory at the time you request the ` resize ` operation , so there won't be big useless memory allocations either !
Never use ` from pylab import * ` or ` from numpy import * ` since these imports would ** overwrite ** the builtin definition of ` sum ` with NumPy's ` sum ` function .
If that says ` numpy.core.fromnumeric ` , your ` sum ` is actually ` numpy.sum `
You can typically find the original python ` sum ` at : #CODE
Never use ` from pylab import * ` or ` from numpy import * ` since these imports would overwrite the builtin definition of sum with NumPy's sum function .
possible duplicate of [ reshape an array in numpy ] ( #URL )
You can use Numpy's reshape method and the ` -1 ` argument to reshape arrays of arbitrary length between the two forms you specify .
Simply join the strings and encode them using either a specified or an assumed text encoding in order to turn them into bytes .
The problem is that these equations are very long with nearly every permutation of A*B up to the 5th power and there are six of these equations .
I want to return an array which holds the unique index as well as the median value for objects with the same idx value .
For the idx array you can get the unique items using ` numpy.unique ` , and to get the corresponding values from the other array we can use ` numpy.diff ` with ` numpy.where ` to get indices where the items change .
Using these indices we can split the values array using ` numpy.array_split ` and then apply ` np.mean ` on its items : #CODE
This would make it ` O ( N log N )` in complexity , though we can still do this in ` O ( N ) ` time in pure Python , but the excessive for-loops may slow things down .
Get the unique " labels " in ` idx ` .
How to convert a column or row matrix to a diagonal matrix in Python ?
I have a row vector A , A = [ a1 a2 a3 ..... an ] and I would like to create a diagonal matrix , B = diag ( a1 , a2 , a3 , ....., an ) with the elements of this row vector .
I guess I could have some code count all the values above and below 50 to get the Y dimensions of the output axes and then make 2 output arrays of np.zeros ( Array.shape [ 0 ] , Yvalues ) and append row by row to that but I'm still not sure how that would work .
I need to sum up land area based on land class , and it would be delightful to print something like this : #CODE
To convert any field to binomial , we need to add a number of components to your feature vector ( header ) equal to the number of possible values in that field .
Python - While loop store min value
I have the sum of a ` csr_matrix ` over one dimension , which returns a 1 dimensional vector .
Notice that while ` a ` itself is a sparse matrix , the sum is a ` np.matrix ` .
All I can say is that from my experience getting Anaconda working correctly is likely worth any effort it might take : from your question it looks like it's going to take you a bit of time to get the paths right , but there are numerous packages in the scientific stack that are difficult to install and Anaconda makes most of this work dramatically easier .
I am trying to sum two series that have some matching indexes , but some that are unique .
I want to end up with a new series , which has the summed up aggregate of all indices : #CODE
notice index A is the sum of both a and b ( 0.2 + 0.2 ) , whereas B , C , and D are the original value .
The compiler chooses among a great many specific optimization approaches , and the standard -O*n * options say only which ones it * may * use ( in a compiler-specific manner ) .
This is will have something to do with different registers being available for the two variants due to the different number of function calls that imply spilling stuff onto the stack .
When you run long enough you will be able to single out instructions that are costing a lot of time , though the actually high sample counts will also then not match up directly with the slow instructions as the cpu may execute later independent instructions in parallel with the slow ones .
It instead means that the network interface directly retrieves its data from the array you are providing instead of making a copy of the data into its own buffers in the network stack .
You should only use them when you have clearly pinpointed the additional copy to the network stack to be a performance problem .
Regarding NumPy's ` ceil ` : This will not do what OP wants .
For getting the index of the minimum value , use amin instead of min + comparison
generally to interpret an fft you would plot 20*log ( abs ( fft ( x )) , this takes the magnitude of the complex numbers and puts it in a dB scale
Briefly , the absolute value of the complex number ( ` sqrt ( x.real **2 + x.imag **2 )` , or ` numpy.abs() `) is the amplitude .
More detailed , when you apply FFT to an array ` X ` ( which , say , contains a number of samples of a function ` X ( t )` at different values of ` t `) , you try to represent it as a sum of " plane waves " ` exp ( i w t )` ( where ` i ` is an imaginary unit , and ` w ` is a real-valued frequency ) with different values of ` w ` .
So , the amplitude ` r ` changes the absolute value of the term , and the phase ` p ` , well , shifts the phase .
A complex number has a norm , which corresponds to the amplitude .
` tan ( theta ) = y / x `
x + i*y = r * exp ( i*theta )
Actually , from the code in the OP it looks like we can only move up , right , down , or left - diagonal moves are not allowed .
@USER " diagonal moves are not allowed " - oh , you are possibly right .
Fast way to transpose np.arrays and write them to file
I'm trying to figure out the fastest way to loop over the groups and the channels and transpose them to " table " format .
or ` max_len = max ( len ( f ) for f in files )`
Note that Python offers a floor division operator too , in the form of the ` // ` operator and the ` __floordiv__ ` hook .
You might also want to avoid using ` from numpy import * ` since this overwrites common Python builtins like ` sum ` and ` any ` and ` all ` with the NumPy functions of the same names , leading to ( potentially ) surprising behavior .
Assuming that we have a large matrix A , and the indices of two matrix elements ( c1 , r1 ) , ( c2 , r2 ) that we want to swap : #CODE
You can easily vectorize the swap operation , by using arrays of indexes ( c1 , r1 , c2 , r2 ) instead of iterating over lists of scalar indices .
We can take a hybrid approach , by breaking the lists of indexes into chunks ( as few as possible ) , where each chunk only contains unique points , thus guaranteeing the order makes no difference .
Slicing here will be faster if you store the indices as a 2dim array ( N x 4 ) to begin with .
Now using the items in ` list_2 ` as indices assign -1 to ` arr ` and then find the items whose values are not equal to -1 or is equal to 0 and and assign the items of ` list_1 ` to it .
Bellow I show you an example with a ` sin ( x )` function because I'm sure you can figure out hwo to adapt it to your needs #CODE
this will call your ` sin ` function with various parameter ` b ` and will calculate it in different points ` x `
You select every second column and row , four times , starting with indices ( 0 , 0 ) , ( 0 , 1 ) , ( 1 , 0 ) and ( 1 , 1 ) , respectively .
The problem occurs when the sum of values is higher than 255 , resulting in a lower number .
Returns the indices that would sort an array .
Here's a working example for all non-complex data types that does this with ` std :: sort ` .
Why don't you just replace the ` NaN ` s in the dictionary before you encode it ?
As @USER points out , your hook ` dumps ( d , cls=NanConverter )` unfortunately won't work .
Another obvious solution would be ` dumps ( pd.DataFrame ( d ) .fillna ( None ))` , but Pandas issue 1972 notes that ` d.fillna ( None )` will have unpredictable behaviour :
Then I pick top N freqs and now I want to draw a signal of the sum of those frequencies using the following formula : #CODE
Do I have to use sin cos for the drawing ?
I found my mistakes .. first the correct way to calculate the waves is ( sum of those for topN freqs ) : #CODE
It can be useful in pseudo-spectral simulations ( where ` data ` is a 3d array of complex numbers ) but basically it applies a mask to a set of images , putting to zeros some elements for which ` where_dealiased ` is true .
To save some time there , you can remove the ` reshape ` , because ` find ` already returns linear indices : #CODE
Luis Mendo mentions that I can remove the ` reshape ` because ` find ` already returns linear indices .
I like it since the code is much cleaner but a ` reshape ` is anyway very cheap so it does not really improve the performance of the function : #CODE
Then ` x ` is printed as ` Polynomial ([ 1 ., 2 ., 3 . ] , [ -1 ., 1 . ] , [ -1 ., 1 . ])` which is to be honest ugly .
I also have an array of arrays or lists in which I have indices of labels I would like to access .
If only a few ` duplicates ` are longer , and you don't mind loosing the ' extra ' values , you could clip them : #CODE
How can I know to what size should I pad ( what is the max length of list / array inside my array ) ?
BUT I think I found a better answer - concatenate the duplicates , index , and then SPLIT the result into subarrays .
Instead of inv ( X'X ) X'Y , I am thinking to break up the regression into 8 parts .
Then my estimate would be inv ( X1'X1 + .. X8'X8 ) ( X1y1+ ... X8y 8) .
I am thinking to read in the entire dataset and dump it out to an organized new csv file .
Standard solutions like scipy.linalg.lstsq() use proper matrix decompositions instead off ` inv ( X'X ) X'Y ` .
You probably want to use gradient descent to compute your regression coefficients .
Or have I overlooked features of the ones above that make them usable ?
For instance , with f_signal=12 , f_sample=5 , it works fine , with both sin ( f_signal*t ) and sin ( f_alias*t ) hitting all sample points .
But with f_signal=14 , f_sample=5 , sin ( f_alias*t ) no longer hits all of those points .
say , ` a ` is a permutation of ` 0 ..
This produces a ` ( 10 , 10 )` array , to which we can apply ` where ` to get the indices .
Apply rolling mean function on data frames with duplicated indices in pandas
I have difficulty to use pd.rolling_mean function on the following data frame containing duplicated indices : #CODE
I would like to delete elements with same indices from several arrays .
But currently i got always zeros arrays as it doesnt happen ...
where ` i [ 2 * k ]` and ` i [ 2 * k + 1 ]` are the indices of the ` k `' th non-zero value ; but , then the result that is returned is : #CODE
This is a Python replica of the ` C ` nonzero function ( the ndim > 1 case ) .
@USER , I ended up using broadcasting with transpose .
So for example , suppose you want to extract the nonzero values from this array : #CODE
You would create an array of row indices ` R ` : #CODE
And an array of column indices ` C ` : #CODE
The only catch is that the resulting arrays must be broadcast able .
The first one ( ` a [ rr , c ]`) works because ` numpy ` can tell from the shape of ` c ` that it should broadcast .
What's happening here is that the row indices [[ 1 ] , [ 3 ]] have shape ( 2 , 1 ) , so they can be broadcast against the column indices [[ 1 , 3 ]] , which have shape ( 1 , 2 ) , resulting in shapes ( 2 , 2 ) arrays of row / column indices .
how to plot histogram of lottery numbers ?
I'm using ipython notebook to plot histogram of lottery numbers results .
Can't you use sum and length function , since you only have aces and zeros ?
gives you the number of ones .
gives the number of nonzero .
while executing loop the error is list indices must be intergers , not tuple
Experiment with something simpler like ` I = np.arange ( 10 )` and ` J = np.arange ( 10 )` , and look at ` I [: , None ] *J [ None , :] ` , or their sum , or difference , etc .
If one of them experiences an append operation , they all do .
You can inspect the ` id ` of the different elements of ` classes ` to verify that indeed they have distinct object ids .
If you want to go between to known indices , then we will use #CODE
But my question is I do not know how to append value from one matrix to another .
You can use variables as slice indices e.g. ` a [: , i : j ]` and run a for-loop over , ij or whatever .
Further i have to find the min of each list inside but first i need do this
Then , Python is telling you it objects to calling min on an array ( or array slice ) .
` min ( data [: , #URL )` .
However , if you run this with your sample data you'll get a ` RuntimeWarning : invalid value encountered in divide ` warning due to the zeros in the division .
You can avoid the warning if you filter by zeros : #CODE
for example ` blit ( zeros (( 7 , 7 )) , ones (( 3 , 3 )) , ( 4 , 4 ))`
if I did ` blit ( zeros (( 7 , 7 )) , ones (( 3 , 3 )) , ( 5 , 5 ))` I would get : #CODE
So , the problem boils down to finding the indices , right .
You should definitely make use of the fact that sliced indexing on numpy arrays behaves as Python slicing on built-in types , with respect to out-of-bound indices .
Quote from Python introduction to strings : " Degenerate slice indices are handled gracefully : an index that is too large is replaced by the string size , an upper bound smaller than the lower bound returns an empty string .
I'm also aware that I could filter them by using a 2-dimensional numpy ` unique ` function , but lists are quite large so this is not really a valid option .
i.e. keeping the ` min ` or ` max ` instead of the default ` sum ` ?
This uses the standard Python dictionary hashing to find the unique keys .
I had to construct a special object array , because ` unique ` operates on 1d , and we have a 2d indexing .
An alternative way of getting the unique index , as per ` liuengo's ` link : #CODE
@USER .F I was thinking about turning each image into an 1D array and with two ` for ` cycles grab each value in that position , put them on a new 1D array to calculate the median and put the result on another 1D array , which would be the median image , and reshape it back to 2D using ` np.reshape() ` .
Although , often times it's more efficient to pre-allocate the slab of zeros and insert the data sequentially on load .
It is a vector ( or array of vectors for an N-dimensional input ) of length ` max ( len ( a ) , len ( b )) -1 ` .
The following code calls lfilter , and passes zi using lfilter_zi such that the length of the last dimension of zi is ` max ( len ( a ) , len ( b )) -1 ` .
` ValueError : The number of initial conditions must be max ([ len ( a ) , len ( b )]) - 1 `
If , instead , we use the list ` [ 0 ]` in the ` indices ` argument , we get an array with shape ( 3 , 1 ): #CODE
This code was written by a user named pv .
@USER As pv . has pointed , his numpy function will be faster to build a numpy array due to significant overhead of converting Python iterator ( produced by ` itertools.product `) into a numpy array , because numpy arrays of objects ( tuples in this case ) can't be created directly from iterators .
However , unlike " fancy " or regular numpy indexing , using slices as indices appears to be not supported : #CODE
Another trick is to collapse all ' surplus ' axes to one with a reshape .
And then reshape back after .
However remember to replace ` None ` for stop indices with ` A.shape [ axis ]` ; and correspondingly dealing with negative indices .
What I often prefer do is to use ` np.rollaxis ` to make the axis to be indexed into the first one , do my indexing , then roll it back into its original position .
Using numpy mgrid with a variable number of indices
How do you use numpy.mgrid with a variable number of indices ?
@USER yes I have 20 nodes ( n=20 ) and I want both indices go up to n .
At least on my build of Numpy 1.9.1 it shouldn't work if your indices ` i ` and ` j ` iterates up to ` n ` .
As an aside , remember to add the transpose of the matrix ` c ` to itself .
You want to broadcast whenever possible .
However , for some operations , I think including this one , you can't broadcast because the values at each step depend on their neighbors .
numpy linalg svd memory complexity and limits ?
memory error in numpy svd and this Applying SVD throws a Memory Error instantaneously ? and a bunch of other numpy.linalg.svd questions .
I need to run svd on very large matrices for work .
I need to analyze the svd results so I could learn about the clusters model .
Have you checked the obvious requirements of > = ` NxM ` + ( ` NxN ` + ` MxM ` + ` N `) float values for a full svd ?
I think my requirements are the requirements of a full ` SVD ` and the ` max ( N , M )` are the requirements for a truncated ` SVD ` .
Do you really need the full SVD , or are you only interested in a subset of the singular values / vectors , e.g. the largest ones ?
My thought is a is a sum of all the 7 first element in the param [ 7 ] , but it does not fit to the whole process .
Then the ` .sum() ` method will sum the elements in that array .
Which results in a 1000 by 1000 matrix with 10.000 nonzero entries ( a reasonable density meaning approximately 10 nonzero entries per row )
which should result in a 100.000 by 100.000 matrix with one million nonzero entries ( way in the realm of possible ) , I get an error message : #CODE
This is probably because it's picking the * random * entries to give your matrix by selecting a ` 32 bit int ` between 0 and ` N*M ` , and the max 32 bit ( signed ) int is ` 2^ 31-1 ` ( ` 100,000 * 100,000 = 10,000,000,000 > 2,147,483,647 = 2^ 31-1 `) .
I cannot say this with absolute certainty , because it really depends on which native data types are used in the numpy / scipy C source ( of course there are 64 bit data types available on Windows , and usually a platform case analysis is performed with compiler directives , and proper types are chosen via macros -- I cannot really imagine that they've used 32 bit data types by accident ) .
" list indices must be integers , not tuple "
repmat ( sum ( data , 2 ) , 1 , 20 );
` axis=1 ` refers to the dimension along which the sum is taken . as far as I remember , matlab starts index counting with 1 , in python / numpy it starts with 0 .
To see what is happening you can use ` flatten ` explicitly : #CODE
I've numbered the indices under the array above to make it clearer .
Note that indices are numbered from zero in ` numpy ` .
Typically , I use ` std = np.std ( x )` , but to be honest , I don't know if it returns the ` 1sigma ` value or maybe ` 2sigma ` , or whatever .
NumPy's ` std ` yields the standard deviation , which is usually denoted with " sigma " .
squared deviations from the mean , i.e. , std = sqrt ( mean ( abs ( x -
It'll still work , what's returned are the indices to use to perform slicing on the df row-wise , I think you need to persist with dataframes more because at this stage you have a lot of basic questions and errors which are not that useful to answer here
For sparse matrix , ` * ` is the matrix product ( ` dot ` for ndarray ) .
For ` n =3 ` is just does the repeated ` dot ` .
For larger ` n ` , it does a binary decomposition to reduce the total number of ` dot ` s .
The ones that do work are the ones that pass the action on to the array's own method .
` data_you_want = data [: , 3 ]` gives error > TypeError : list indices must be integers , not tuple
Actually , I wanted to rotate and translate those images in order to align them or technically speaking register two images .
That way I avoid the double access to elements and I don't have exception if a list is shorter than the other ( you should use in ` xrange ( min ( len ( x ) , len ( y ))`) #CODE
When I take the norm of this matrix , I get different results .
Matlab : norm ( A . ' fro ') = 0.018317077159881
I have confirmed that they are both reading the correct number of values ( 6590x7126 matrix , 122526 non-zeros ) , and that I am using the same norm for both ( frobenius ) .
the order of the matrix shouldn't matter as a norm doesn't account for that .
Sometimes the sum of many numbers varies if you change the order of summation .
I use the norm of this matrix for some of the calculations , so this difference is propagating throughout the program .
I'll try a sum , one second .
If you partitioned your original matrix into 4 sub-matrices ( Top left , top right , bottom left , bottom right ) and compared the Frobenius norm reported in Matlab and in Python for each sub-matrix do you still see a discrepency between any values ?
I'm worried that since the norm is off , but the original data matches , that it may be a difference in how matlab / numpy implement sqrt and ^2 .
( Of course it's more likely that we'd just have four different values for the norm - two different ones from your machine and two more different ones from mine . :-)
How about if you just compute the norm yourself ?
Or the squared norm ?
Python errors out halfway during an svd computation as i am using the norm to calculate how many values I want .
I will try your suggestions but I did do the norm manually in python and got the same results .
I tested them with the 2 norm , and they both match .
I downloaded the sample matrix and ran both your python code and your matlab code to load the matrix , then computed the norm in the same way you did .
Both python and matlab report that the matrix is only 4425 rows by 7126 columns . matlab reports the norm ( A , ' fro ') is 0.0223 which is already larger than your value , and python just fails with a ' dimension mismatch ' error .
I would expect the following commands to all yield the same value , because they are all computing the square root of the sum of the squares of the ( non zero ) elements of A : #CODE
In fact , even the reported sum of the elements of the sparse and dense representations of A are ( a little bit ) different : #CODE
matplotlib histogram with frequency and counts
doesn't plot the histogram appropriately .
I guess I don't understand how matplotlib's histogram plotting works .
I altered your example to dequantize those data before creating a histogram .
This histogram displayed looks like this .
Equally , you can choose which columns to pull with a similar approach for the first dimension of array data and sum it :-)
To answer your updated question , to group the data into decades you can ` reshape ` your array and take the mean along the correct axis .
When you write ` nv_cov = np.linalg ( adjusted_cov )` do you mean ` nv_cov = np.linalg.inv ( adjusted_cov )` ? and should ` det = np.linalg ( adjusted_cov )` really be ` det = np.linalg.det ( adjusted_cov )` ?
It looks like the cov matrix multiplied to the inverse is resulting in the identity matrix .
Output : a matrix with 5 on the main diagonal , which is the correct inverse of A .
Of course , using the factor of 5 here is cheating , but ` np.linalg.det ( 3*A )` also returns a nonzero value ( about 1.19e-111 ) .
If you try ` np.linalg.det ( 2**k*A )` for k running through modest positive integers , you will likely hit one that will return nonzero .
I read that every time you append data to numpy arrays it is copied to memory , so it will be very slow to process such big database as mine .
I don't understand why you would append .
I have an array of values and have created a histogram of the data using numpy.histogram , as follows : #CODE
The histogram with the cdf is below .
Note that the native python array takes up much more memory than the numpy ones - python objects each have their own overhead , and the lists are lists of objects .
well in numpy you can create an " empty " array and then set the values yourself , or as @USER says you can use ` zeros ` ( or the other similar methods ) .
Yeah , I should have clarified that I use ` zeros ` for " empty " arrays ( numpy arrays are fixed size ) .
Now , I want to get the highest ( max ) value of ` L ` ( if there are lot of occurences of the same value , I want to get only one occurence of it ) .
numpy dot behaving strange
Why would the ` dot ` function behave differently ?
Without knowing more details of your application it becomes very difficult to know how to advise you , and the best answer may very well be to just test a lot of different parameters and see which ones work best .
Depending on what's in ` xi ` and ` y ` , you might have to strip some empty strings first , or otherwise sanitize the input .
numpy sum gives error
I wanted to used numpy.sum to find sum of multiplication of two arrays .
Aside from ` sum ` not doing anything for a single number : ` numpy ` counts things starting at ` 0 ` , not ` 1 ` .
I suppose you want to make the dot product of ` pop ` and ` fc ` .
Edit : if you want the sum of the above : #CODE
No I wanted sum of the dot product of pop and fc .
As I belive , the dot product should return a vector of the same size for pop or fc and then I wanted sum over all elements of this dot product vector .
I have a hypothesis that class 0's data is following the sin wave pattern while class 1 not so much .
You could try taking the FFT-amplitude of you signal ( subtracting the mean first ) and measure the ratio of the max to the mean .
I'd like to join numpy arrays , e.g. #CODE
That is , I'd like to join a variable amount of arrays with a truth vector in between each of them .
I would calculate the absolute value of their difference and compare to float's epsilon : #CODE
Yep , it looks like it's the * geev family of LAPACK routines for ` eig ` and * syevd and * heevd for the real-valued ` eigh ` .
I wish to remove all columns who's sum ( excluding the column title ) is zero .
If the sum == 0 then iterating over the data again and deleting the ' ith ' element from each row .
I am unsure how to delete by columns according to a condition ( such as sum == 0 ) .
but don't know how to check that the sum of the column == 0 and how to apply across the array .
I need it so the 1st column is removed because its sum is ' 0 '
If so it's because none of your columns sum to zero .
Why extreme large value to 0 frequency fft ( numpy.fft.fft method )
I have a signal ts which has rougly mean 40 and applied fft on that with code #CODE
try fft ( np.ones ( 10 ))
The FFT is not normalized , so the first term should be the sum , not the mean .
and you can see , that when ` k=0 ` , the exponential term is ` 1 ` , and you'll just get the sum of ` x_n ` .
This is why the first item in ` fft ( np.ones ( 10 ))` is ` 10 ` , not ` 1 ` .
` 1 ` is the mean ( since it's an array of ones ) , and ` 10 ` is the sum .
So you just want to append new rows , can you post desired output to your question
This question looks strange : Based on you last statement you could calculate : c*A1^n = y1 / exp ( x ) , or if you select on c and n : A1 = log ( y1 / ( c*exp ( x )) ) / log ( n )
For each ` i = 1 , 2 , 3 ` , fitting ` y_i / exp ( x )` independently gives an estimate for the coefficient ` c A_i^n ` .
This works with any two arrays , as long as they're the same shape or one can be broadcast to the shape of the other .
As it only operates on 1d arrays you also need to do the reshape at the end so that the shape of the mask matches the shape of your data .
If you want the indices of those positions you can use ` np.where ` #CODE
Also , I would use the ` max ` builtin function instead of your lambda : #CODE
You want ` min ` here , not ` max ` .
No , you want ` max ` .
The OP wrote " I want to set the elements which smaller than 1 to 1 " , that means he wants max ( x , 1 ) .
The clip is really help .
And e.g. take the absolute value afterwards : #CODE
Alternatively you could use ` reshape ` and pass in a tuple specifying the required shape : #CODE
To save an array of the indices of where ` a 5 ` holds true , you can use ` np.where ` ( or equivalently ` np.nonzero ` as per EdChum's suggestion ): #CODE
I have a histogram ( roughly normally distributed ) and I want to do some stats on the tail of the distribution and then populate it back into the array from which I create the histogram .
I found out that I could get pyjs to translate scipy by copying scipy directly to my project folder .
Is it even possible to translate imported modules / packages ?
Numpy is written in C , and Pyjamas can't translate C to javascript .
So basically ` tile ` takes a " tiling array " and concatenates it , similar to the way you would tile a kitchen floor , whereas ` repeat ` repeats each element in the vector a specified number of times before it takes the next element of that vector .
I reshape the array to a single column of values , create a string from that , format it to remove extraneous brackets etc , then output the result to a text file saved in the user's Documents directory , which is then used by another piece of software .
( I am assuming indices are starting from 0 )
Also if ` a `' s length were any smaller than that of ` b ` , it should be comparing only the indices of ` a ` with those of ` b ` .
at indices 0 and 1 , value of ` a ` is smaller than ` b ` , thus this would also pass the check .
The way to get your code to work would be to convert just the first element of each list and then append that to a list dates .
It helps ` np.arrays ` behave as ` dicts ` to help coders write a more clean cut explicit code without a lot of indices that others don't know the meaning of .
What I want is that for indices excluding those in ` b ` , values in ` a ` should be equal to -1 .
Your calculation is essentially a dot ( matrix ) product .
Then it's just a ` ( 5 , N )` array ' dotted ' with a ` ( N , 5 )` - ie . sum over the ` N ` dimension , leaving you with a ` ( 5 , 5 )` array .
And which takes more time , applying the mask or doing the ` dot ` ?
With large enough ones there might be a trade off between memory use and vectorization .
I changed my ` mask ` so it only had 400,000 True values , and calculated ` Ku ` with my ` dot ` method - 600ms per loop .
For a test matrix ` NIJ ` of shape ` ( 1250711 , 50 )` , I got ` 54.9 s ` with the ` dot ` method , while the ` einsum ` does it in ` 1.67 s ` .
However if you want an actual vector quantity that is calculated using multiple ` inarr ` elements , meaning that the two new created fields aren't independent , you are just going to have to write that kind of a function , one that takes in an array , ` idx ` , ` idy ` indices and returns a tuple\list\array value which you can then unpack and assign to the result .
Simply append all the vectors to a list .
After ` generic_filter ` completes , convert the list to an array and then reshape it .
Numpy : resize array
I get error that " ValueError : operands could not be broadcast together with shapes ( 994 ) ( 1000 ) "
Hence as per fix I am trying to pad extras / trailing zeros to the array which great size by below method : #CODE
To insert zeros front : #CODE
To insert zeros back : #CODE
If you want ` str ` s , you would need to decode the bytes .
You took an array that has 8 elements , of which only 4 are unique .
It seems like you want unique values in your array #CODE
Yes I wanted unique elements out of my array .
You want an array that contains the indices of the center point of all the dots ?
Count unique elements row wise in an ndarray
In addition to having the unique elements row-wise , I want to have a similarly shaped array that gives me the count of unique values .
Is there some way this can be re-constructed into the original array dimensions with zeros where values were duplicated ?
I'm adding a unique imaginary number to each row .
Thus , you can find all the unique values in a 2D array per row with just one call to ` np.unique ` .
The index , ` ind ` , returned when ` return_index=True ` gives you the location of the first occurrence of each unique value .
` np.put ( b , ind , cnt )` places the count in the location of the first occurence of each unique value .
It can not have a complex dtype to start with , since multiplying each row by a unique imaginary number may produce duplicate pairs from different rows .
` np.put ` can place the ` cnt ` values into ` b ` using the ` ind ` flat indices directly .
The indices in ` ind ` are indices into this flattened 1D view of ` a ` .
` unravel_index ` converts these indices into the corresponding coordinate indices you would use to index into the N-dimensional array ` a ` .
That saves a lot of time that would otherwise be spent to fill the matrix with dozens or hundreds of zeros by using extra lines of code .
The indices where to replace can be found doing : #CODE
If you just want to join your two arrays though , you can do that like so : #CODE
Now ` sum ( list1 )` does not work ( used for ` sum ( list1 ) / len ( list1 )` ( while ` len ( list1 )` works fine ): #CODE
Your sample list works just fine when converting to integers first : ` sum ( map ( int , list1 ))` .
If you have a matrix you need to sum the elements of the nested lists .
This assumes you wanted the average of all numbers across all nested lists together , so the length needs to reflect the nested list lengths , not the outer list .
Now I would like the sum of list1
Anyway ` sum ( map ( int , list1 ))` works fine now .
If you want to stack this array with the first column of your array ` g2.T ` , you can use ` column_stack ` .
I have tried to use ` GNU screen ` , which again , works fine while logged in , but when I detach the screen and log out the calculation false over with the error message : #CODE
I presume that this is something to do with my files unmounting when I log out of the machine .
So it seems that when you log out of ssh it unmounts your home directory so you can no longer write to it .
Merge two lists into a dictionary and sum over the elements of the second list
I would like to get the following dictionary ( sum over the values if it is the same key ): #CODE
OK the following is a dump of my pdb debugging session which shows how this ends up in pandas land : #CODE
In order to achieve the same output you have to call ` sum ` twice : #CODE
You can find the elements with 0 as those second indices , with ` all_data [ all_data [: , 1 ]= =0 ]` : #CODE
The min value would simply be :
Having imported numpy and created your array as ` a ` , we create a view on it using the boolean array ` a [: , 1 ]= = 0.0 ` and find the minimum value of the first column using the ` numpy ` function ` min ` , with the optional argument ` axis=0 ` to limit the search for the minimum in column ` 0 ` .
ESPECIALLY using JPEG compression as you will get compression artifacts in order to compress the image to save space .
So then if we apply gradient again . hope you got my point .
For example in ` Ipython ` I can time ` sin ` for 50 values : #CODE
I have a numpy array in Python which is n-by-n ( in the example is 3-by-3 ) and contains zero values in all the diagonal positions .
Is it possible to sort the array without modifying the diagonal positions so as to look like the one below ?
Because all of the sorting functions will take into account the " zeros " that exist in the diagonal positions and will change their relative position .
The easiest approach is to remove the zeroes , sort , then add the zeroes back along the diagonal : #CODE
I'm a little late to this question , but if you're looking for a NumPy-only solution , you could substitute ` inf ` for your diagonal , sort in your chosen order , and then shuffle the inf column back to the diagonal : #CODE
how do I concatenate a single column numpy array with a four column numpy array ?
In MATLAB , ` 1:5 ` is a row vector ` ( 1 , 5 )` , and its transpose ` ( 1:5 ) .
With ` numpy.hstack ` you can stack vectors horizontally ( when the vectors are already given in column shape for some reasons ) #CODE
Otherwise ` numpy.stack_column ` is more appropriate to add a 1d-array to a 2d-array as no reshape is required as pointed out by Mark , thanks !
` numpy.column_stack ` also works , and saves the need to reshape the column vector .
` dstack ` is also worth mentioning ( that would be " depth " stack , or along third axis )
The documentation eig stats the eigenvalues is not necessarily ordered .
To test linear dependence of vectors and figure out which ones , you could use the Cauchy-Schwarz inequality .
Basically , if the inner product of the vectors is equal to the product of the norm of the vectors , the vectors are linearly dependent .
The error in the logging file turns up after the first logging.info ( ` logging.info("Decoding starts now ")`) , therefore I assume that the error is thrown from the second line , after the third line is not in the log file anymore .
Also , if you use ` logging.exception ` instead of ` logging.critical ` you'll get a stack trace along with the error .
Which line is shown in the stack trace ?
mat matrix shape could be a 10000*5 . here just an example
It tries to find mat [: , 0 ] be or mat [: , 0 ] > ba or mat [: , 1 ] bb .
Also delete this column from matrix " mat " .
Same as mat [: , 0 ] > ba or mat [: , 1 ] bb .
For mat [: , 1 ] bb , the colume will copy to " swapt " , mat [: , 0 ] > ba don't copy , just delete .
The function will return mat , swape , and swapt #CODE
In my code , I found the matrix mat length always reduce once some columns matched the condition .
It will append and delete wrong column .
Also the append is append a address or a deepcopy ?
Then how to delete itself in mat ?
It's not entirely clear to me what you want to achieve with your code : it looks like you're trying to append to some array the indices of the columns you've checked for a certain condition .
So in where_less_than_b = mat [: , 0 ] < b , could I use bool operation here .
Such as mat [: , 0 ] < b & mat [: , 0 ] > c assert b > c ?
You can totally combine comparison operations , for example ` ( mat [: , 0 ] < a ) & ( mat [: , 0 ] > c )` .
Assuming that you have the same dataset as your supervisor , your method of plotting said complex numbers might be the ones at odds with his .
I showed him the complex numbers generating by fft and he seemed content with them .
Imaginary numbers can be separated from real ones in ` numpy ` , even if they are stored in an array .
I am accessing just the first row of that and doing a dot product with W which is a 11025x20 matrix .
Raise diagonal matrix to the negative power 1 / 2
where D is a diagonal matrix of this form : #CODE
How to compute negative power -1 / 2 for diagonal matrix .
If you can update ` D ` ( like in your own answer ) then simply update the items at its diagonal indices and then call ` np.dot ` : #CODE
Or create a new zeros array and then fill its diagonal elements with ` 1 ( D.diagonal() ** 0.5 )` : #CODE
A neat trick used in various numpy functions when indexing arrays of unknown dimension , is to construct as list of the desired indices .
NumPy - Does not sort array properly without padding zeros
Does it need to be encoded this way or could you settle for a simple permutation matrix ?
We then join the lists together and find which participants occur twice in the amalgamated list .
The participants occurring twice are the ones who have over taken participant j and hence j's column in the sparse matrix will have 1 in place to signify that .
PyCUDA test_cumath.py fails on cosh
Does anyone have a suggestion where this discrepancy between the GPU and CPU result for cosh comes from ?
What is the difference between flatten and ravel in numpy ?
' flatten '
If you read the docs you linked to , note that ` flatten ` will always * " Return a copy " * , whereas ` ravel ` will make a copy * " only if needed " * .
It also seems in further testing that there are cases where flatten will not work on an array , but ravel will .
The primary functional difference is that ` flatten ` is a method of an ndarray object and hence can only be called for true numpy arrays .
For example ` ravel() ` will work on a list of ndarrays , while flatten ( obviously ) won't .
In addition , as @USER pointed out in his comment , the flatten method always returns a copy , while ravel only does so " if needed .
Thanks pv .
I'm calculating peak width as the greatest distance between the intersection of the signal and a line at half the max .
Also , if I have the indices of intersections , I could find which pair is the smallest which also contains the peak index , I suppose .
I am in the process of writing a basic financial program with Python where daily expenses are read in as a table and are turned into a PDF ( Probability Density Function ) and eventually a CDF ( Cummulative Distribution Function ) that ranges from 0 to 1 using the build in histogram capability of NumPy .
And I want to compute the sum over a list of slices #CODE
... create a mask-like array ` z ` that , for each slice , will be used to " zero-out " the values from ` a ` we don't want to sum : #CODE
I was about to edit the question and suggest ` squeeze ` as an alternative , if the problem was simply a " trivial " dimension , but you beat me to it .
It seems a naive solution would be to iterate over the input array and either extend the input array by ` floor ( width / 2 )` elements ( simply masking out the outer elements on the first and last few pixels ) , or just modify the width used .
Otherwise , where would you want the ` std ` of an array with e.g. four elements - at the one with index 1 or the one with index 2 ?
The issue can be trivially solved by calling ` rolling_std ( arr [: : -1 ])` , then replacing the last ` floor ( window / 2 )` elements of the original std array with the first ` floor ( window / 2 )` elements of the reversed std array .
I fixed the time t=10 ( has dimension 24 here ) I iterate through the 2d array and bilerp a cell ( four data points- i , j i+1 , j , i , j+1 , i+1 , j+1 ) into one value and append to self.tempY array .
Right now , all I can do is alter the ones and zeros in the program : - #CODE
explores how ` empty ` , ` zeros ` and ` zeros_like ` are implemented .
Ok , I will try your solution and let you know later ( however I will translate it to Python )
Triangular indices for multidimensional arrays in numpy
We know that ` np.triu_indices ` returns the indices of the triangular upper part of a matrix , an array with two dimensions .
What if one wants to create indices as in the following code ?
First transpose your matrix , and then concatenate values of each row using ' \n ' as a separator .
You can use python built-in ` zip ` function and ` join ` : #CODE
Also i get an error message from a histogram i am trying to plot with the interpolated values .
If you know the grandient or your function to be minimized , you can use more advanced algorithms e.g. BFGS , and pass the gradient function as well , as described in the doc .
because your error it seems as your sqrt comes from the math module , and not the numpy module .
I have two datasets that I need to correlate in Python .
While familiar with Python I have not worked with such datasets before , and thus am seeking advice on how to correlate these arrays .
I attempted numpy correlate and received :
One idea I would try is to flatten the 3D matrix first , then use coorelate -- since coorelate only takes 1D vectors .
Different behavior of sum of numpy arrays with regards to data type when using increment operator
Consider the following sum of numpy arrays : #CODE
Therefore , for mixed precision calculations , ` A { op}= B ` can be different than ` A = A { op } B ` .For example , suppose ` a = ones (( 3 , 3 ))` .
I need to find minimum over all elements from the column which has the maximum column sum .
Then calculate sum of each column and find index of the maximum element #CODE
but get the following stack trace #CODE
" The input should be ordered in the same way as is returned by fft , i.e. , a [ 0 ] should contain the zero frequency term , a [ 1 : n / 2+1 ] should contain the positive-frequency terms , and a [ n / 2+1 :] should contain the negative-frequency terms , in order of decreasingly negative frequency " .
I'd recommend using an existing library for an fft .
The print outputs and error stack I get when calling this function are : #CODE
Interpolating it over the whole region will allow you to find the average pixel colour for each of the new ones without having to do some kind of horrible geometry madness .
@USER : as Andrew Carter says , use reshape or else you can set a 2D dtype , either way amounts to the same thing .
You can broadcast that into an array using expressions , for example #CODE
Is there a function similar as ` np.outer ` for vector / matrix " outer " product ?
Imagine how you might possibly interpolate such a dataset : there's no way you could continuously move from just below 360 to 0 , unless you'd unwrap those values ( see ` np.unwrap ` ) such that values close to 0 would be reinterpreted as those same values +360 .
add zeros to missing values of a matrix using Numpy
some of the matrices have fewer elements for example 1200x55 , in those cases I have to add zeros to the matrix to reshape it to be of 2200x88 dimension .
Since ` l ` is a matrix , ` l [ 0 ]` is a strip of that matrix , an array .
I have tried and I get : ValueError : cannot resize this array : it does not own its data
I think the answer may be to do the following c = b.copy() but do I have to create a separate concatenate function to do that ?
If you copy both matrices ` ll , rr = l.copy() , r.copy() ` then your concatenate line should look like ` var = np.concatenate (( ll , rr ) , axis=0 ) .reshape ( 1,387 200 )`
Numpy reshape on view
I'm confused about the results of numpy reshape operated on a view .
I found a ` numpy.einsum ` syntax in this post Python , numpy , einsum multiply a stack of matrices which does what I want .
Using list comprehension , p2 calculates the dot product of each matrix .
You can append your 1D arrays to ` x ` and then ` vstack ` them : #CODE
For the 1st solution got the error " AttributeError : ' numpy.ndarray ' object has no attribute ' append " .
How to quickly determine if a matrix is a permutation matrix
How to quickly determine if a square logical matrix is a permutation matrix ?
is not a permutation matrix since the 3rd row have 2 entries 1 .
PS : A permutation matrix is a square binary matrix that has exactly one entry 1 in each row and each column and 0s elsewhere .
One method is to call ` np.sum ` and pass an axis param , this should generate an array with all ones if not then you don't have a permutation matrix : #CODE
` np.ndindex ` is handy tool for generating indices .
But the code is as simple as it gets , there's not a lot of optimizations that catch the eye .
But maybe its worth to use a different convolution like scipy's fft based one ?
The version here I understand , but I haven't a clue how to specify the weights for the fft version .
However , the fft one was much slower ( due to overhead , your toy problem is too small , maybe when the pdfs themselves contain more values , you actually get a speed increase ) .
I was looking at this version of convolve for the record #URL
You'll need to pad each input PDF with zeros to the appropriate length to avoid effects from wraparound .
This should be reasonably efficient : if you have ` m ` PDFs , each containing ` n ` entries , then the time to compute the convolution using this method should grow as ` ( m^2 ) n log ( mn )` .
You might also be able to gain some speed by padding the ` rows ` array with zeros so that the total number of columns is optimal for performing an FFT .
` np.where ` returns the indices of the ` True ` values .
I have a 70x70 numpy ndarray , which is mainly diagonal .
The only off-diagonal values are the below the diagonal .
General definition of reshape us ` numpy.reshape ( a , newshape , order= ' C ')` and you call it as if you've done ` img.reshape ` .
You try ` reshape ` ing it as it was only a tuple ` ( x , y )` .
You call one ` reshape ` with ` np .
I must have messed it up when copying it into stack overflow .
The indices of the 2D array are usually marked as ` row ` and ` col ` .
Your rescaling works fine , after fixing the reshape , but your colour resampling doesn't .
If you want smooth resizing behaviour ( e.g. by using spline interpolation ) , you'll be looking towards the Python Image Library and all functions that use it under the hood or e.g. OpenCV , which also provides resize behaviour as summarized in this post .
ValueError : could not broadcast input array from shape ( 20 , 20 ) into
ValueError : could not broadcast input array from shape ( 20 , 20 ) into
I would not flatten the image , from what it sounds like you can use a list , for example . list = [ numpy.arange ( 4 ) , numpy.arange ( 5 )] , print list [ 0 ] outputs array ([ 1 , 2 , 3 , 4 ]) where array ([ 1 , 2 , 3 , 4 ]) is actually a numpy object !
All my images are different sizes , and I don't want to resize them because they're paintings so they shouldn't be the same size .
If I understand correctly , in ` model() ` you are computing a weighted sum over your image pixels using ` dot ( X , w )` , where I assume that ` X ` is an ` ( nimages , npixels )` array of image data , and ` w ` is a weight matrix with fixed dimensions ` ( 784 , 10 )` .
In order for that dot product to even be computable , ` X.shape [ 1 ]` ( the number of pixels in each of your input images ) must be equal to ` w.shape [ 0 ]` .
The indices of th elements should be removed are stored in a N*1 dimensional vector ` t ` .
I'm working with quite large rotation matrices , which have the inherent property to have a large number of zeros .
` reshape ` requires that the total size of your array remains the same , resizing a ` ( 52 , )` to ` ( 52 , 8 , 8 , 6 )` effectively extends your array for ` 8*8*6 ` dimensions .
` reshape ` is there to i.e. convert a 9 element ndarray to a 3x3 2D ndarray .
Assuming all 52 arrays have the same shape and dtype , you could convert it to an array of shape ( 52 , 8 , 8 , 6 ) by using ` np.fromiter ` and a generator expression to flatten the values : #CODE
Create Numpy matrix using linspace and ones
If ` A ` is indeed singular ( i.e. non-invertible ) then there is no unique solution for ` b ` in ` Ab = x ` .
numpy sum antidiagonals of array
Given a numpy ndarray , I would like to take the first two axes , and replace them with a new axis , which is the sum of their antidiagonals .
i.e. , ` new_array [ l , k ,... ]` is the sum of all ` array [ i , j , k ,... ]` such that ` i+j=l ` .
There is a ` trace ` function that gives the sum of a diagonal .
Part of the problem with vectorizing this even further is that fact that each diagonal has a different length .
` scipy.sparse ` has a ` dia ` format , which stores the values of nonzero diagonals .
While that's a way of getting around the issue of variable diagonal lengths , I don't think it helps in this case where you just need their sums .
It returns me only one diagonal i want all possible diagonals so what can i do ?
For example in above first matrix 1 , 1 , 2 || 1 , 2 || 2 , 1 are the possible diagonals and i got only one diagonal 1 , 1 , 2
But when I transpose / reshape it this way : #CODE
You can use ` np.delete ` giving the indices corresponding to every 8th row index .
where argmaxXnn and argmaxYnn are the indexes of the whole array ( not of the box ) , and Maxnn is the max value in each box .
Note that your first FindRefs reverses indices , so that for a tuple ( i1 , i2 , v ) , a [ i1 , i2 ] won't return the right value , whereas a [ i2 , i1 ] will .
i3r contains the index of which row in the box contained that max value .
` i3r ` contains the row index of the max of each column in the box , but we want the row for the specific column that's specified in i2 .
We want the absolute indexes .
We then multiply these by the box sizes , and add the relative max indexes , to get the absolute max indexes .
What is the difference between flatten and ravel functions in numpy ?
The difference is that flatten always returns a copy and ravel returns a view of the original array whenever possible .
If you modify the entries in an array returned from flatten this will never happen . ravel will often be faster since no memory is copied , but you have to be more careful about modifying the array it returns .
Note that in your code , ` im [ #URL .. ]` goes over indices ` j-w , j-w +1 ,..., j+ w-1 ` , the last one is exclusive , which you might not have meant .
How can I select values along an axis of an nD array with an ( n-1 ) D array of indices of that axis ?
where the 1st 2 ` arange ` indices broadcast to the same dimension as ` J ` .
Another method would be to call ` np.intersect1d ` , this assumes the values are unique : #CODE
To get unique values that are present across all three DF's , you can use : #CODE
` fromstring ` always returns a 1-D array -- see the docs at #URL -- then you have to reshape the array appropriately if need be : #CODE
Scipy NDimage correlate : unbearably slow
The goal is to correlate data_3d ( a 1000x1000x1000 Boolean ndarray ) with kernel ( a 35x35x35 float ndarray ) .
I then perform another correlation to sum over the previous result .
This type of byte by byte comparison will be much simpler than trying to decode and interpret the many , many different video formats that exist .
Note that in this case , the ` df_filtered ` uses 1 , 3 , 5 as index ( the indices sof the original dataframe ) .
If I hear you correctly , I should also stop using lists ( with the append method ) and instead create new arrays and filling them , is that right ?
how to find the unique non nan values in a numpy array ?
I would like to know how many unique non nan values I have in a numpy array .
You can use ` np.unique ` to find unique values in combination with ` isnan ` to filter the ` NaN ` values : #CODE
How to append elements to a numpy array
This is only an example , I know how to load files to numpy arrays and I know that it is better , the question is how to append values to numpy arrays in cases where I have to iterate as in a for loop .
It will require getting the axes aligned with a transpose , i.e. ` A = numpy.append ( A , row.T , axis=1 )`
@USER , my goal is to get ** a maximum permutation matrix ** B from a logical matrix A , which I described [ here ] ( #URL ) .
I don't have a problem with that , but I wonder if I am doing something wrong if I am busy writing add vectors / multiply by a scalar / dot products / etc . on my own .
Generating numpy array of indices for a deduplicated set of points
Credit : Find unique rows in numpy.array
Not at all , I just ignored the unpacked values ( and only kept the indices ) .
A reasonable approach to this is to note that the gaussian kernel is separable , so calculate the 1D for x , and the 1D for y , using the usual ` exp ( - ( x-x0 ) ***2 / sigma )` and then take the outer product .
A reasonably fast approach is to note that the Gaussian is separable , so you can calculate the 1D gaussian for ` x ` and ` y ` and then take the outer product : #CODE
@USER ' s outer product answer is probably the best for this particular case .
We also want to compute the resulting list of indices of the permutation , so that we can apply the new ordering to any list associated with the original one .
3 . join lists - O ( N )
Shuffle the list with the indices - O ( N ) 2 .
Shuffle the list with the indices , then split it in half .
By default ` np.argsort ` uses the quicksort algorithm , with average time complexity O ( N log N ) .
That way , if you prefer , you can shuffle the first half and second half as much as you want ( and just keep track of the shuffling permutation , with e.g. NumPy's ` argsort `) .
But the list comprehension to find the indices makes it ` O ( N ) **2 ` , isn't it ?
No , it would be ` O ( M*N )` where ` M ` is the number of distinct elements in the list ( since the dict comprehension here is only over unique elements ) .
The value ` 1 ` is broadcast across this selected column , producing the desired array ( it's not necessary to use a list ` [ 1 , 1 ]` , although you can ) .
Why does a list comprehension not append in python
When i try to access the sixth cell element which is supposed to be a 3x3 matrices of zeros , i have this error : #CODE
So how to append more 3x3 matrices into the list ?
When I calculate descrpitive statistics such as standard deviation , mean , sum , I get nan value , but when I calculate a percintle from the same array I get the value I expected .
Find unique columns and record the index of columns removed in numpy.array
The solution to find unique rows is described here .
Modify it to calculate the unique columns :
Here's the figure , the red colours are the ones I would like to mark as unphysical .
I need to collect the indices of triangles ( which are indices into z ! ) , evaluate whether they are good or not and then accept only the triangles for that at least one corner is valid ( reducing the dimension from ( ntri , 3 ) to ntri #CODE
same : Mode same returns output of length max ( M , N ) .
valid : Mode valid returns output of length max ( M , N ) - min ( M , N ) +
indeed , my B matrix was built as block diagonal matrix numpy.bmat , using B = numpy.array ( B ) solved my problem .
What if the values are not unique , as in this case ?
I have been asking related questions on this stack for the past week to try to isolate things I didn't understand about using the @USER decorator with Numba in Python .
Since the solution to the problem requires a lot of dot products , and numpy's dot product is not supported by numba , I started by implementing my own 3D dot product .
Get rid of redundant calculations ( subtracting two vectors ) in your calls to the ` dot ` method .
where every tuple is unique , but a , b , c , d , e , f are not .
You can get the indices of the rows that has all ` True ` s by using ` numpy.all ` on 1st axis : #CODE
A much better way is to create an empty array with the correct final size , then fill in the appropriate indices as you go along .
( still kinda new to stack overflow )
The fit actually works perfectly - I get ` mu == 646.6 ` and ` std = 207.07 ` , which are exactly equal to the mean and standard deviation of your ` y ` values .
For instance , instead of having a function ` Fillable ( G , i , j )` that returns a boolean , you define a function ` Fillable ( G )` that returns a boolean numpy array for all the ` i ` , ` j ` indices at a time ( without using loops ) , #CODE
The numpy's sum operator also uses a loop over items , but it is implemented in C not Python , which makes it much more efficient .
Advanced indexes always are broadcast and iterated as one :
But how can I use an array to determine the indices being selected ?
Note also how they are reshaped ( indexing with ` [: , None ]` is equivalent to ` .reshape ( -1 , 1 )`) so that their broadcast shape has the shape of the desired output array .
If you applied Python indexing logic to ` a [: , [ 0 , 1 , 0 , 1 ]]` , it would raise a ` TypeError : indices must be integers , not tuple ` ...
What does ` a [ np.arange ( 4 ) , [ 0 , 1 , 0 , 1 ]]` translate to ?
I know I can use " np.arange " to set the range and step value , and then use resize to get the array to the right size , but what I am totally stuck on is getting the rest of the 0 values in the array once it is resized to fit the range and step values .
I know there are options like " append " and " insert " but those don't give me the option of doing it at a stepped rate .
Your ` user_raster ` array is only 5 elements long in your example , when you resize it to 5x5 ` np.ndarray.resize ` fills the extra space with zeros .
different from a.resize ( new_shape ) which fills with zeros instead of
Iterating operations over unique values of an array
use identical draws ` z ` for each unique value in ` x ` ,
take the product of the output in the above step over items of unique ` x `
So my understanding of your question for each unique value of x you want the same z normal random distribution , you then want to use that distribution to produce a different np.dot for each unique value ?
@USER ` zip ` with ` dict ` returns only one of arrays associated with a unique ` x ` , ` list ` returns all of them
The [ dot product ] ( #URL ) is not the same thing as matrix multiplication .
In this case , `` dot `` computes the dot product between two vectors , giving a scalar output 30 .
Can you just accumulate the count till the end , and then divide by count in the end ?
You can use broadcasting instead of ` np.tile ` in order to get the row indices : ` rownums = np.arange ( 3 ); B [ rownums [: , None ] , Ind ]`
You can use ` numpy.isnan ` to check which of the items are ` NaN ` and then find the indices of the rows which with all ` True `' s using ` numpy.all ` and ` numpy.where ` .
Edit : As pointed out by ajcr ( thanks again ) , the ` transpose ` command is more convenient since the two swaps can be done in one step by using #CODE
` transpose ` notation is more clear .
So adding ` squeeze ` is relatively cheap .
However , as you pointed out , for most of the cases the transpose operator makes sense , and in my case , the number and size of dimensions is constant and always known .
Using transpose I can do things like ` a , b , c , d = np.transpose ( m , ( 2 , 0 , 1 ))` .
` transpose ` or ` swapaxes ` is often used in ` numpy ` functions to move the desired action ` axis ` to the front or back of the list .
I will have running cumulative min , max and average rasters , which represent a pixel level metric .
I need to be able to compare any single NDVI against these cumulative metrics and classify each pixel as closer to min , max or avg .
In preceding code i create the following list with ` zip ` , that is the zip of each sub array of your 3d list then all you need is calculate the sum of the elemets of subtract of those pairs then sum of them again : #CODE
This tells me that ( for example ) a2d [ 0 , 0 ] is closest to the 0-index of a3d [ 0 , 0 ] , which in my case means it is closest to the min value for that 2d position .
a2d [ 1 , 1 ] is closest to the 2-index , which in my case means closer to the max value for that 2d position .
Not standard Kronecker sum nor direct sum .
Numpy's broadcasting then takes over , and makes the resulting matrix ` c ` the required sum .
How to calculate the proportion of each cell in the matrix to the sum of the column ?
I am an absolute beginner in using python .
Matlab has two functions for converting matrix subscripts to linear indices and vice versa .
One time I was trying to compute the discrete derivative , when Amro or someone just used ` diff ` .
The first input is an array of linear indices that you want converted into locations of each dimension in your array .
Remember , in ` numpy ` you access elements in row major , not column major , and so given two arrays such that one is for row and the other for column indices ( 0-indexed ) , ` sub2ind ` for 2D matrices is very simply : #CODE
I did some basic error checking to ensure that no rows or columns for ` sub2ind ` or linear indices for ` ind2sub ` are out of bounds .
The idea is I can easily construct a dataframe of N tickers , with each row being a daily return . but am unable to compute cov with said dataframe , thus this df --> list --> dict process .
After having a lot of problems similar to the ones you describe , using sudo pip install -U matplotlib worked fine for me .
We can e.g. say that elements with lower indices stay .
I see one here that is 6500 x 1500 with 20,000 nonzero elements ( 0.2 % ) , and another one with the same dimensions with 750,000 nonzero elements ( 7.7 % ) .
As it is right now , it has the possibility of allowing more than six nonzero elements per row , if there are multiple elements with the same value as ` row_array_sorted [ -6 ]` .
I want to project this to 3 2D images - side on , head on , birds eye .
If I understand the question correctly , you have a multi-dimensional numpy array and want to index it by combining a ` : ` slice with some number of other indices from a tuple ` i ` .
The index to the numpy array is a tuple , so you can basically just combine those ' partial ' indices to one tuple and use that as the index .
How do I ensure that each attribute that requires randomness has a unique random stream dedicated to it , but such that I can repeat the simulation replication by specifying a single integer ( which corresponds to the simulation replication number ) ?
I have a code to count strings from each file ( each file is a month in a year , i.e 2012 04 , 2006 11 , etc . ) in a folder and sum them : #CODE
Also not getting the error here ( Python3.4 , Numpy 1.9.2 ) , but aside from the warning , why not just ` A [ nonzero ] = 1 ` ?
Note that for multidimensional arrays ` A ` , the input is flattened so you'll need to reshape the boolean array : #CODE
You can use ` clip ` to keep the values of an array within a particular range .
Actually no , what I'm asking is what if I need not to clip , but some custom transformation , something like " if element is 5 , replace it with 2 , otherwise multiply by 4 " ?
Ah - no ` clip ` can't do that , it just limits the values to a particular range .
Then I append the temporary variable to #CODE
The behavior I do not understand is that when I append the temporary variable to the growing list , it replaces all of the previous values as well .
Using ` transpose ` should be also quite fast .
However , none of them can generate ` _dotblas.so ` after compiling NumPy , which is a critical file to speed up ` dot ` operation between matrices .
Is it an array of only ones and zeros , or are there other values ?
If you only have ones and zeros on the input and output array , you can do it with a 2D convolution , which is simple and works .
That creates a 5x5 array of ones .
You can also do it using morphological ` dilation ` operator ( which dilates the ` ones ` in this case ) .
Is it possible that it tries to find the gradient or something by couple of quick access and does some deep calculations ?
Get row of 2d array where min value is in first col
I would like to find the min element ( ` x_min `) in first column ( x values ) and get the entire row of matrix where this ` x_min ` is .
Transpose result of sum
The calculation of the sum over ` axis=1 ` .
Then I want to transpose the obtained result .
You must be elements of the matrix 1 and 2 calculates the sum , row by row .
Do you need to transpose the array of sum or is there some other way ?
How to transpose the array ?
You're losing an axis when you take the sum and getting back a one-dimensional array .
You can either insert a new axis , or you can specify ` keepdims=True ` when you take the sum to maintain two dimensions ( and avoid the need to transpose the array ): #CODE
The sparse matrix only saves the nonzero data , but it also has to save an index .
I get even better speed by only adding the nonzero terms to the sparse matrix : #CODE
Shifting data in 2d array through shifted indices
I need to shift a 2D array field , i.e. I have a " previous_data " array which I access through shifted indices to create my " new_data " array .
By the way I think the " new " indices should be the ones corresponding to the new array , rather than the old ones , but I've left it as in your existing function .
must be combined , since we must always have pairs of ` x ` and ` y ` indices .
Finally , if the indices are really all shifted by a constant factor , you can make this even simpler by using NumPy's ` roll ` function and taking a slice of the indices corresponding to the valid area .
In particular , I can find the zeros , min , max , etc of the function over any interval I specify .
EDIT : what I actually want at the end of the day is to find the zeroes , max , and min locations ( not values ) of the function f .
If you want exact solutions to the zeroes , max , and min locations , I agree with your edit : use scipy optimize .
There is no guarantee that you will find the actual min / max with these functions .
I have a situation where I need to evaluate the integral of a function for hundreds of thousands of different a , b and sum the results .
The integral of ` exp ( -x*x )` is a scaled version of the error function , so you can use ` scipy.special.erf ` to compute the integral .
And then I want to get aggregates on that ( e.g. std and 25% quantile ) I know I can do this : #CODE
How to append to .mat file using scipy.io.savemat ?
Is there a possible way to append instead of overwriting ?
that won't work either because it involves pulling a data file in your ram memory to append it every single loop , which seems stupid from the speed perspective .
So you can open the file in append mode , and write , e.g. #CODE
You can append data to the same dictionary key .
But you can't append new dictionary keys with new data and that's what I want to do .
Creating a 4d matrix full of zeros in python , numpy
The call signature is ` zeros ( shape , dtype=float , order= ' C ')` , and so it's trying to interpret the first 2 as the shape , the second 2 as the type , the third 2 as the storage order , and then it doesn't know what to do with the last 2 .
The nested loops above iterate over the two element tuples containing only the elements ` 1 ` and ` mt ` ( outer loop ) and ` 1 ` and ` nt ` ( inner loop ) .
` np.nonzero ` ( or ` np.where `) might also be useful , giving you the indices of the the selected terms : #CODE
The sum is only computed over the non-masked values of ` masked_arr ` - you can see this by looking at ` masked_sum.data ` : #CODE
creating a numpy ndarray from an old one by dividing it with the row sum
Now I need to form a new matrix say ` c ` , such that ` c ` is the ` matrix b ` , with each element divided by its row sum from matrix ` b ` .
Define unique ` dtype ` for ` n ` fields ...
How to pad multiple lists with trailing zeros ?
For each index , I want to pad the shorter list with trailing zeros .
I say this , in the sense that one can always choose to move around which tasks are done where : this is applicable to you , in that you could move some of the calculations and processes around , so that Java does more of the work , minimizing the things you actually would need to send , to a few simple structures that could be conveyed either by a scripting language in the JVM family , ( like Rhino , a JavaScript derivative , etc . ) , or by using something like Jacl to have LAN connection through TCP , and to broadcast one program's results to another , through I / O methods .
But when I try to sort on the sum using ` np.sort ` #CODE
In general , the order of the indices matches the order of the axes .
If you're using tile , it's as simple as specifying ` 0 .
This is the standard way of putting zeros in an array ( ` np.tile() ` is convenient for creating a tiling with a fixed array ) .
x is the independent variable , and counts is the number of counts of x occurring , like a histogram .
Change set of indices
Say the indices of ` idx ` where ` idx == TRUE ` are ( not actually true , just example ) #CODE
These are your nonzero indices , in 3 arrays .
Efficiently : sum non-diagonal elements , by axis
I have a matrix , and then I need to sum over some axis , ignoring diagonal elements .
1 ) strip those -9999 out before loading them with bumpy .
use numpy ( ` import numpy as np ` ) to find out min and max pixel values using ` np.amin ( img )` & ` np.amax ( img )`
Did you read the documentation of the ` concatenate ` function to know how to specify the parameters ?
I would like to add / append the ` bt ` column at the zero column of array ` a ` .
Or , if you are starting with something shaped like ` bt ` , transpose it : #CODE
Not able to sum values while using numpy
I have created a script for calculating the sum of all the values in 32th column which has comma as delimiter .
My script is printing the values but unable to sum the values .
The suggestion by @USER ( and by @USER in a comment ) are good ones .
No that's not it . triedges is a large stack n x 3 x 3 , where n represents the number of triangles and is evaluated along the 0th column .
In this matrix there are 3 pairwise differences ( N choose k unique combinations , where order doesn't matter ) .
Using NumPy , I am trying to reshape data that looks like the following once loaded : #CODE
It has 54 items and I'm trying to ultimately reshape it into a 27x2 array .
The data mentioned above does not return a shape and I'm unable to reshape it .
I don't think it is the best way but right now calling : new_test = np.asarray ( test.tolist() ) converts it in such a way that I get a valid shape returned and can then go on to reshape .
For your " sin " case , try this : #CODE
@USER , this again gives me an evenly spaced array , just multiplied with it's sin function .
The indices of the array will always be evenly spaced ( such is the nature of an array ) , but you can change the spacing of the values as I have here .
If indices of an array are always evenly spaced , then I want it to be a sine space ( just as you can have logspace , in place of linspace ) .
Note that , for a max of 30 , the spacing is pretty similar to linearly spaced .
This is because ` theta ` is linear with ` sin ( theta )` for small ` theta ` ( and when ` theta ` is in radians , ` theta ~= sin ( theta )`) .
indices to split data in train test sets .
Pandas : ValueError - operands could not be broadcast together with shapes
It looks that when you do ` data1 [ " requests "] .add ( data2 [ " requests "] , fill_value=0 )` you are trying to sum 2 pandas Series with different size of rows .
Series.add will broadcast the add operation to all elements in both series and this imply same dimension .
I honestly never use it before and by the description of documentation I can't imagine it give a broadcast error .
` data1 [ " begin "] .combine_first ( data2 [ " begin "])` produces ` ValueError : operands could not be broadcast together with shapes ( 6482 , ) ( 2981 , ) ( 6482 , )`
` data1 [ " begin "] .combine_first ( data2 [ " begin "])` produces ` ValueError : operands could not be broadcast together with shapes ( 6482 , ) ( 2981 , )`
This memory buffer is used to store the U , S , VT and all the intermediate arrays needed during the svd computation .
I have a Series in Python and I'd like to fit a density to its histogram .
I was hoping for a kde fit that is monotone decreasing based on a histogram , which is the first figure depicted .
It's a bad idea to use a histogram with very few bins to approximate the density .
I think it's a good way to understand the benefits and limitations of the histogram approach , though : the number of bins is critical and a too low / too high number of bins can give a distorted view on the data .
Increasing the bandwidth is to some extend similar to displaying the data with a histogram with a very low amount of bins .
This produces a matrix of binomial vectors , but I do not know which vector represents which word .
The word ids ( Keys ) are not matched to the appropriate vectors .
The literature talks about sinc interpolation and filtering .
But consider ` sin ` data : #CODE
The missing elements in the last columns should be zeros .
I was trying to put them individually , but n is > = 100 so manually padding the zeros would take long .
The ` new_list = last_list [ 1 :] + [ 0 ]` just means take the " 1th " through last element in ` last_list ` and concatenate it with a zero to form a new list called ` new_list ` .
For example , suppose a = ones (( 3 , 3 )) .
Append a list of arrays as column to pandas Data Frame with same column indices
However , because columns indices are both 0 , it adds a_ to the end of the dataframe column , resulting in a single column .
Based on your comments , it looks like you want to join your list of lists.I ' m assuming they are in list structure because ` array() ` is not a method in python .
Say the function was to sum the elements of the sub arrays
You can simply use your boolean array ` a ` to index into the rows of ` b ` , then take the sum of the resulting ` ( 2 , 3 )` array : #CODE
Perhaps You wanted to append element to list in this range ?
I am trying to calculate a dot product with pool #CODE
That's not going to do what you want : in this case , try ` map ( np.dot , x , y )` , and note that what you get is simply the product of each element of x and y as a list , not the dot product .
The error you get is because ` dot ` isn't a supported function .
Since you're dealing with 1D arrays , and dot is pretty simple to define , the following will work : ` ne.evaluate ( ' sum ( x*y )')` .
You could try a few quick variations : ` np.interp ( ynew , y [: : -1 ] , x [: : -1 ])` , ` np.interp ( ynew , -y [: : -1 ] , -x [: : -1 ])` , ` np.interp ( -ynew [: : -1 ] , -y [: : -1 ] , -x [: : -1 ])` , etc ., and see which ones work and which ones run into problems .
So , first you have to read also the vertex indices from the file .
You need " -1 " because the vertex indices in the file starts with 1 , but the indices of array starts with 0 .
The behavior you're seeing is caused by the fact that ` np.genfromtxt ` uses the ` NameValidator ` class here to automatically strip certain non-alphanumeric characters from the field names .
One of the tasks involves finding the root , or minimum absolute value if no root exists , of a function .
Why is the script swapping my indices ?
I then need to find the min and max of column1 and column2 .
I need to grab the max and min values for all elements on the left or x and all elements on the right or y
Concatenation and min / max are now a little simpler : #CODE
I ended up using np.vstack to get them in the right columns , and then looked at column 0 for min and max , and then column 1 .
What is the best way to reshape and / or slice this array to create the desired pairs of items using NumPy ?
How would I reshape or slice the data such that I could have a new array that would look like the following without having to iterate through ?
I can pull out the pairs individually like this after I reshape : #CODE
If you're really stuck with the ` vstack ` ( e.g. the stacking command is in a routine you don't want to change ) , you can just take the transpose ( using the ` .T ` property of numpy arrays or ` np.transpose ` : #CODE
You can just take the transpose using the ` .T ` property .
I have a text file and i want to find numeric values corresponding to all the distinct words present in that file.By numeric value , I mean that I want to assign a unique integer value ( not the times of occurrence value ) to it so that I can use that numeric data in weka for text analysis .
Because I want to log automatically all the arguments to a function .
In your case , I guess you want the second argument to be ` (( 0 , 0 ) , ( 25 , 25 ))` to pad with zeros in the 25 columns either side of your image array .
Why does Python crash when I try to sum this numpy array ?
It is conceivable that you are running out of stack space .
If a recursive algorithm were used for sections , then an array section with many discontinuous segments might recurse too deeply and exhaust the stack .
I have a NumPy array that is sized : ( 54 , 41 , 2 ) How would I most efficiently resize it to ( 57 , 41 , 2 ) such that the additional three in the 57 dimension have zero for their values ?
Comparing ` np.concatenate ` to your original approach in which you first define an array of zeros and then overwrite a part of the array , it depends on the size of the arrays involved .
concatenate probably does it in the original way , just written in C code .
@USER , I was thinking about that as well , although it would not fill with zeros , just a ` malloc ` ( ` np.empty `) or such .
The answer to your question would be to reshape your array and then transpose it : #CODE
As David pointed out in the comments , if you wanted to unroll the 64x64 matrices column-wise , use the Fortran ordering in ` reshape ` : #CODE
Otherwise , an ordinary Python list would suffice , to which you could append each new 64x64 matrix and at the end convert the list of arrays to a full numpy array ( ` np.array ( list_of_matrices )`) .
The dimensions of the outer array will only be ` ( 400 , )` , so the reshape will throw an exception .
I hope you're not giving up , as the solution is simply a combination of ali_m's answer and mine : first create a homogeneous array of dtype= np.uint8 , then perform the reshape and transpose operations .
Let's say that we wanted to reshape this into a ` ( 3 , 4 )` array .
It's not possible to do this directly because the size of the outer ` np.object ` array ' container ' is still only ` ( 3 , )` : #CODE
To turn that into something sensible we'll first stack the subarrays in the first dimension in order to make a homogeneous array of dtype ` np.uint8 ` : #CODE
The ` None ` index is used here to insert a new ( first ) dimension that we stack the arrays over .
Now that we have a ` ( 3 , 2 , 2 )` homogeneous array , we can just flatten out the last two dimensions to make a ` ( 3 , 4 )` array .
I would like to separate the part of the array with the subarrays having -1 from the ones who don't .
i don't think you mean to be using the ` .csv ` file suffix in the ` pickle ` dump . also , shouldn't you be opening it in binary mode ( i.e. , `' wb '`) ?
You can use a combination of ` reshape ` ( to expose the target axis ) and ` concatenate ` ( to join the arrays along this axis ) , with ` reshape ` ing back to the desired form : #CODE
I have just run this through python and the mask returns an array consisting of boolean indices , and the combination of code ` mask.sum() ` returns the total number of Trues present in the array .
Maybe backing up : Suppose I put in min of 0 , max of 100 , output array of 50 , what's an acceptable output array , and what isn't ?
Or does if have to be between min and max ?
Which is fine , note that all the off diagonal elements are approximately zero to machine precision so it looks like numpy is doing an ok job with this one !
In that case you have to do ` mat.dot ( inv ( mat ))` .
Your assumed inverse is obviously wrong ; the ones can only be achieved if some entries in the inverse matrix are smaller than one and the zeroes only if some entries are negative .
Hmm . yes you are correct , I was originally thinking that I needed the integer indices but yes it's unnecessary here , I'll update my answer
@USER : Are you using ` resize ` or ` reshape ` ?
` resize ` can adjust the actual size of an array in memory , allocating more if needed .
For example if in ` Ipython ` I ask to look at the code for the sum method , I see that it is compiled code #CODE
` x.resize ` is built-in , while ` np.resize ` ends up doing a ` np.concatenate ` and ` reshape ` .
a large number of tiny , random disturbances , each with its own unique
Slow meaning it only processed ~4000 columns in a day , with each append causing subsequent appends to take longer as well .
One part which struck me as odd is why my column count of the main DataFrame affects the append speed .
This results in appends at a constant rate of ~ 0.2 seconds which is acceptable ( versus Pandas taking ~150seconds for my full dataset per append ) .
I would like to create a histogram and save it to a file without showing it on the screen .
I told myself " You're wasting an outer product , you fool !
Save one of them and use the transpose !
` g ` is the function with the double outer product , and it takes 16 ms , ` f ` is the one with the transpose and it takes 20 ms .
The 2 outers take the same time , and their sum is a bit more than their combined time .
Performing the transpose is cheap , just a matter of changing shape and strides .
So in my timings pre computing the ` outer ` sames some time , but not as much as one might expect .
So the relative advantage to pre computing the ` outer ` is reduced .
Previous comparison of ` outer ` and ` a* a.T ` omitted .
It's funny , I still have the timings the other way around , ~ 13 ms for the sum of double outer product and ~ 20 ms for the sum of the outer product and its transpose .
How to column stack a list and Numpy array and savetxt ?
Note that in many geometries d ( x , y ) = 0 iff x = y , so you may want to skip that check , and deal with the zeros later on .
If I leave in the zeros , then I'm looking for the second-nearest geometry to each geometry , which seems more complicated .
To be safe you could perform an internal buffer on the extent , select those points , then dump the rest ... then do the distance calculations .
I know how to do this by first creating a nun zeros matrix in python ( np.zeros (( 9 , 9 )) and then using a loop to populate it 1 and zeros .
Answering your question : you can build matrix with ones in positions ` pos = [( 1 , 4 ) , ( 3 , 2 )]` like this : #CODE
But from my point that is pretty much explicit one-liner " put ones on those positions " ( removed part about shapes - it was actually wrong )
Where does ` cos ` come from ?
Where does ` cos ` come from ?
I have a sparse matrix , which is most of its values are zeros .
For a start , I'd try to build a dictionary by scanning the array for nonzero terms .
I want these to be treated as zeros , but I can't initialise the array with zeros , as ` nan ` has a meaning later on and can't be set to 0 .
But in your case , you need an integer for ` friend ` and ` period ` , since your using them as list indices .
Since the optimizer does not really care about the shape of the array , you can just flatten your ` x0 ` before passing it to ` fmin_cg ` and reshape it to the desired shape in ` g() ` , like so : #CODE
It is also much easier to reshape than nested lists .
The first file had ids that were all numeric , so the read_csv function must have inferred an int type for that dataframe . the second file had mixed int + string ids , so the read_csv must have inferred string or object .
The fact that this changes when you concatenate would indicate you are mixing your types at this point .
How many of the same min values are on average within one data set ( image ) and what is the data set ( image ) size ?
You can solve this by applying finite difference gradient to each row and check sign change .
ValueError : operands could not be broadcast together with shapes ( 400 , ) ( 2 , )
` plt.plot ( t , ( exp ( a*t )) * x() , label= " lalala ")
ValueError : operands could not be broadcast together with shapes ( 400 , ) ( 2 , )
The goal is to sketch a graphic with the function : exp ( a*t ) x over the time t ...
=> y= exp ( a t ) *x on the y-axis and t is the x-axis .
Please give us information what are ` exp ( a*t )` and ` x() `
You're getting the error because ` x ` is an array of two values and NumPy doesn't ( without some help ) know how to " broadcast " these two values in relation to your time array .
What if I only want to multiply the ones that are smaller than 3 in list1 ?
A modulo of 0 ( your current start number ) also generates a dot which you don't expect .
If so , you could easily use transpose .
The [ forthcoming version of numpy ( 1.10.0 )] ( #URL ) will have [ ` np.linalg.multi_dot `] ( #URL ) , which will optimally compute chained dot products of multiple matrices .
The order in which you evaluate the dot products can have a large impact on efficiency ( see the example in the ` multi_dot ` docs that I linked above ) .
I see how the symmetry of the trace lets you replace the final ` dot ` .
Extract indices of intersecting array from numpy 2D array in subarray
B is an array extracted from A where a certain number of columns and rows ( with the same indices ) have been stripped .
such that the missing indices are [ 1 , 3 ] and intersecting indices are [ 0 , 2 , 4 ] .
Is there a " smart " way to extract the indices in A corresponding to the rows / columns present in B that involves advanced indexing and such ?
I extract the diagonal from both arrays and perform the aforementioned loop on it with a simple equality check : #CODE
Is there always one unique answer ?
( What if A is all ones , for example ? )
Yes there can be repeated values in A , and yes the answer should be unique .
I think the new method may not produce the correct result when the diagonal contains repeated values .
You are right , but errors should only occur if said repeated values are consecutive along the diagonal , which virtually never happens to me in practice .
candidate solution of indices ` [ 1 , 3 , 4 ]` .
That's quite surprising , because I'd expect pandas to read three thousand lines in the blink of an eye .
I have multiple csv files to use and compare , but need to compare / correlate night time results versus daytime results , with other data .
My issues are how to pull from separate csv files , arrange over this time range ( Daylight = time within sunrise until sunset ) round up at > =15 mins to next 30 min interval , if 15 round down to previous 30 minute interval .
You could pick a red-blue colormap and norm your matrix data into the blue part , but you can also just plot over an imshow image : #CODE
image stack population is slow in numpy
I am reading stack of separate tiff's into single 3D array via numpy / python .
However , when I try to create a 3D stack out of those files , using dstack() time starts to scale non-linearly , e.g. 0.21 sec for 10 files , 5.39sec for 100 files and so on .
Alternately , initializing the array as fortran-ordered and using the OP's original indexing style should solve the same problem and has the advantage of keeping the indices in a more logical order .
So if you collect an image stack , but then want to process perpendicular planes , it can makes sense to rotate the stack ( which is why I don't do Fortran ordering , so my ordering can be mailable and remain nonconfusing ) .
You want an outer product .
I have some data log files , where sadly the logger choose to log data with a timecode with a presicion in floating of the seconds , even though logging every 2 second .
Example start of log data : #CODE
I was thinking of going through each line in my data and compare dt to the preivous line , and then join the lines , but how do I join those data rows most effecient ?
Now we have different results , basically for the even number we have that the shape command gives actually ( 10,500 00 ) whereas the shape command in case of odd indices gives ( 10 , ) ( the 10 lists supposed ) .
But I would like some clue also because I need to insert in a loop where I do not know a priori if the indices will be even or odd .
First , there are times when you need to subtract 1 from your indices when you're porting from MATLAB to numpy , but your code doesn't have any of those .
Should just be ( so you don't modify ` am ` , and properly specify args to ` zeros `) : #CODE
You specified the wrong flatten order here : #CODE
Same thing when you reshape , you need to specify FORTRAN order : #CODE
You can do this for your entire MATLAB workspace as well with simply ` save ` -- in python ` mat ` will still just be a dictionary keyed by variable name , so you'd access the individual workspace variables just as you do above .
Without normalization , v1 should be [ sqrt ( 2 ) , 1 , 0 ] .
` lines ` will be a dictionary with integer indices corresponding to the line number .
It wouldn't make much sense in this case , since ` mat ` is obviously a required parameter for ` invCheck() ` .
So ` where ` on the 2D array ` a == 1 ` will return * 2 * lists , each list containing the indices along the first & second axis respectively where the condition ` a == 1 ` was true .
They are using 64 bits for indices now , as I can construct a matrix with ` 2** 63-1 ` but not with ` 2**63 ` .
The max index you are giving is less than the maximum index of the rows you are supplying .
well , no -- the max value in the index array is 2101454109 but ` max_index ` is 2337713001 .
I have the following problem : I have index arrays with repeating indices and would like to add values to an array like this : #CODE
However , as I have repeated indices this does not work as it should because numpy will create a temporary array which results in the data for the repeated indices being assigned several times instead of being added to each other ( see #URL ) .
Python , split array by gradient of dataset
I want to separate them every time the gradient of x changes sign .
but this just gives me two arrays , one when x has a positive gradient and the other when its negative .
This is just a simplified example of what happens in many large data files I have , so the gradient shifts happen in different places .
ok , so you only care for the sign of the gradient -- thus , your ` x ` values really don't matter at all .
Then you can get the element locations of the sign changes from How do I get a list of indices of non zero elements in a list ?
If I understood it correctly , you want to split the arrays where corresponding gradient is negative / positive .
This isn't really what I wanted , the point was that I wanted a new array every time the x gradient changed , so in the dataset example that I gave it would split into three .
What is the best way to calculate the sum by year in python ?
Then you could slice and reshape to produce an array with one year per row .
I would use a dict if you need both the year and sum : #CODE
@USER , not sure it would be easier , if you want to be able to access which year is associated with which sum then a dict seems pretty much exactly what you want . another option would be pandas but I really think a dict is what you want .
EDIT ah , sum by year .
I think the critical phrase is " sum * by year* " .
It nice because you can easy get more information like mean , median , std etc ..
I thought that ` np.reshape ` might do what I want but I'm not sure how to get it to remove just the target rows as I think if I reshape to a ` ( N-1 ) x ( N-1 )` matrix , it'll remove the last two rows and columns .
Also , negative indices mean that we access the array from the end .
... but using negative indices is much more elegant .
No , that didn't seem very efficient ( I've since changed it ) -- the efficiency was from not effectively duplicating ` data ` when I created ` data_normal ` and ` data_obsese ` -- instead , in the second method , I just create a list of matching indices ` ind_n ` / ` ind_o ` , and keep only one copy of ` data ` around .
What this gives is a vector for d2Tsub , what I want is a matrix ( N-1 ) x ( N+1 ) in size , with rows corresponding to different x values for Chebyshev polynomial extrema grid ( #URL ) , with the 1st and ( N+1 ) row missing as they are at the endpoints and the different columns different poly orders ( 1st column : 0th order , 2nd column : 1st order , 3rd : 2nd order , ... ) .
I suspect I may have neglected some diagonal matrices with nd and the x arguments down their diagonals .
x = cos ( pi*n / N )' ;
dT = sin ( acos ( x ) *n ) *diag ( n );
d2Tsub = diag ( 1 . / sqrt ( 1-xsub . ^2 )) * ( Tsub*diag ( n ) - diag ( xsub ) * sin ( acos ( xsub ) *n )) *diag ( n );
No need for matrix products to use diagonal matrices .
Note that the integer division performed by ` // ` will always pick the floor value ( i.e. it rounds down , towards negative infinity ) .
Now we want to represent rows in this array with a single unique value .
A problem is also that the first number has a varying number of digits , so trailing zeros have to be added in some way .
You are right about the zeros .
Dietrich's answer is actually proposing a different way to encode your values , whereas I just propose ways to decode as requested .
This implies than it is not possible to keep the ` Fraction ` type as the return type when using ` det ` .
Other functions such as ` trace() ` do not do the same type checking as ` det ` and the object type may persist .
` trace ` simply sums the diagonal by calling the ` Fraction ` object's ` __add__ ` method , so a ` Fraction ` object can be kept as the return type .
The ` O ` type just means that the array's values can be references to arbitrary objects : it allows greater flexibility for some methods ( e.g. ` trace ` , ` sum `) but unexpected results for others ( e.g. ` det `) .
If the object's ` log_space ` flag is on , the weights are put into log space .
I'm trying to return the indices of an array which correspond to the sorted values .
You can do it by creating an array of indexes , and sorting them using the array from the outer context , like this : #CODE
However , can replace the for-loop with , ` Array ( indices ( r ))` .
I had loaded this mat file using io.loadmat ( ' train_32x32.mat ') but when I am trying to show an images from the above numpy array , I do not get the image with good resolution and coloring .
Basically for pandas / numpy speed you want to avoid ` for ` and any ` concat / merge / join / append ` , if possible .
I know I could concatenate the df but there surely is a simpler method .
IndexError : too many indices .
You're getting a keyerror because you've blindly created a dataframe without specifying the column names , in which case some default ones are supplied starting from 0 .
I found out that when I use dot product of a 1D array and a 2D array , it doesn't matter whether I transpose it the 1D array or not .
So when you use dot product , say for the $$x ' \Sigma x$$ that appears frequently in statistics , how do you write you clear code for the 1D ` x ` ?
` a.dot ( b )` is a sum product over the last axis of ` a ` and the second-to-last of ` b ` .
( At least for my own use case , I'm interested in numpy arrays , but it's useful knowing that lists already try to squeeze as much performance as they can . )
So unless the number of springs are hugh ( say > 100000 ) , I believe that dragging around a lot of zeros will not degrade the performance .
Numpy array ( matrix ) with one axis indices being strings
A unique function but that is infeasible , so how can I create this ?
Apart from the field names of structured arrays , indices just count the row and / or column .
and dictionaries or lists of tuples that map the labels to the indices , e.g. #CODE
The first problem I had was improving the resolution of the FFT to enable viewing of the smaller bandwidths - I believe I have solved this by concatenating the sum of Bessel wave functions with a large , zero-filled array with a size double that of the array holding the wave data .
Do not pad with zeros , but increase the time period of your ` bWave() ` signal ( see code below ) to increase the frequency resolution .
Since your signal has a small bandwidth , you could demodulate it ( multiply it by `` sin ( 2*pi*f_c )``) to push it into the base band to limit the sampling rate .
Identify duplicate rows in an array and sum up corresponding values in another array
Now I would like to list the unique outcomes in ` x ` and add up the corresponding probabilities in ` p ` of the duplicate outcomes .
While there are some examples of how to obtain unique rows , see , e.g. Removing duplicate columns and rows from a NumPy 2D array , it is unclear to me how to use this to add up values in the other array .
` bincount ` can sum the ` p ` array for you , you just need to create a unique id number for every unique row in a .
If you're using a sorting approach to identify the unique rows , than creating a unique id is really easy .
Once you have sorted the rows generated a diff array , you can just ` cumsum ` the diff array .
Fourth , you are computing ` min ( dist )` twice , and further may be using the ` python ` version rather than the ` numpy ` version , the latter being faster for arrays ( which is another reason to make ` dist ` and array ) .
Get the column which have maximum sum value from matrix
Is retrieving number of column which have maximum sum of column from matrix possible ?
Additionally , can i do that like " get column which sum of its is > 0 " .
For example : ` np.where ( f.sum ( axis=0 ) > 0 )` returns a tuple telling you at what indices the sums are positive .
Use the ` axis ` parameter in ` diff ` : #CODE
Also , how do you intend to resize by different amounts on each dimension and also keep the aspect ratio ?
Perhaps you mean to resize such that the largest dimension length goes to 64 ?
The following attempt does not even attempt to sum the rows , but only to return the correct arrangement of True / False arguments .
np.histogram will return a histogram of all elements in my matrix from a flattened array .
By the way , I'd suggest you avoid using ` T ` as a variable name - it can cause confusion since in numpy ` .T ` is used to get the transpose of an array .
A simple way to do this would be to truncate the values in the ` cell_counts ` array at 1 , then sum along the rows : #CODE
In other words , I want to sum your ' cell_counts ' array if all non_zero elements were taken to be one .
If num features is m , and if num instances in train is n , then you can broadcast any m long vector with the train set and get element wise operations ( in your case , to calculate euclidean distance ) .
would also work , since the only nonzero values are those 1st 3 which are suppposed to be ints .
Let's say all of the elements of the list ` l ` ( defined below ) are 2d float arrays , but they may have different number of rows and columns , so I can't stack them all into a single numpy array .
Some operations , like basic math ones , do ' pass through ' to the inner arrays .
Zero out matrix higher diagonal using numpy
As we can see , the diagonal is being cut like a forward-slash , everything under the first slash would be zeros out .
You can use ` np.tri ` to generate a matrix that contains ones below a given diagonal and zeros above .
I get the error that from my definition of gamma , the code wants to take an array to the power of 2 / 3 which I know you can't do .
My question is how can I make a for loop so that all parts of the array and put into the gamma factor and then calculated .
I asked a friend for help and he mentioned something about my arrays should be two dimensional rather than the one dimensional ones I have here , but I am very unsure on what to do for that .
It should be ` y= numpy.array ([ 0 , 1.0 , 0.0 ])` and possibly ` if ( gamma ( y )= = 0.0 ) .all() : dy [ 2 ]= 0 `
I just needed to reshape ir again and perfect :) Thank you so much .
numpy.unravel_index not returning expected row indices
This returns an array with the values I'm interested in - to get the indices of these values I use the following function ( which I developed after reading this ): #CODE
For what I'm interested in , I can just use the result of xx_x_indx - the column indices .
However , I can't explain why the y ( row ) indices report as they do .
but I think this is slower than ` get_index2 ` in all cases since ` reshape ` is very fast so using ` np.where ` with ` reshape ` is faster than using ` np.where ` and ` np.unravel_index ` .
TypeError : can only concatenate tuple ( not " float ") to tuple
Is the problem that division promotes the dtype to accommodate decimals ( ` diff ` and ` sum ` complete successfully ) ?
` NDVInp = diff / sum ` to : ` NDVInp = numpy.true_divide ( diff , sum )` , I consistently get : #CODE
Your ` (8 191 , 8101 )` ` uint16 ` arrays ` diff ` and ` sum ` will each take up ~130MB each .
There is no automatic promotion to floats when you perform ` diff / sum ` , so the result should still be ~130MB .
Short verbal description of the formula : For each t , there are 3 indices i , j , k in the range 0- ( N-1 ) .
The indices i , j build a matrix ( or a 2d array ) , each of whose elements is a product of some ( J , t ) -dependent function ( doesn't matter what ) over the index k excluding the particular value of i , j .
@USER : well the index t need not stride across the same range as the other indices .
Precalculating the ` jmat ` sum / difference arrays makes a big ( 45x ) difference in performance .
I don't really understand what you mean by * " unique label for each user " * - in your example it seems that ` User2 ` would have corresponding ` label ` values of both 0 and 1 .
The upcoming release of numpy ( #URL - although the link will age very quickly ) has a function ` multi_dot ` that chains together multiple dot products , and will presumably speed up numpy further .
The zeros represent nothing , and can be changed to NaN if need be .
In other words , I need to find the indices in my time array that satisfy that condition ( 0-240 ) , and then apply those indices to the amplitude array in a way that outputs the mean and st dev .
My attempt : Unfortunately , I am relatively new to python and have not been able to find much information concerning manipulation / application of 2D indices .
Segmenting Python array into unique regions connected by a single cell or less ?
I have a numpy array which I wish to segment into discrete regions with unique IDs which looks something like this :
Usually for something like this I would use scipy.ndimage.label to generate unique ids for discrete blobs , but in this case I have several very large continuous regions which I also wish to be segmented into smaller unique regions , ideally when they are only joined to their neighbours by a connection 1 cell wide .
Julien : that could work , but I can see issues with long , thin areas extending off the edge of larger regions which should be assigned the same unique ID , but would be separated if removed prior to erosion-dilation .
If you set it to false , it wont transpose the array .
You can take a weighted dot product of successive columns to get a one-dimensional signal that is much easier to work with .
Could you explain to me how the dot products are weighted ?
A weighted dot product gives some of that directional information , essentially not treating all points the same .
I chose a log weight as I found it to be a smoother curve , though ` linspace ` worked too .
Take weighted sum of the vertical columns , which relates the max intensity in a column to its position .
Step 7 : Find the indices ( frame locations ) of when the sign switches .
Is dot product and normal multiplication results of 2 numpy arrays same ?
where K is the kernel matrix of dimension ( 150x150 ) , ncomp is the number of principal components.The code works perfectly fine when fv has dimension ( 150x150 ) .But when I select ncomp as 3 making fv to be of ( 150x3 ) as dimension , there occurs error stating operands could not be broadcast together.I referred various links and tried using dot products like
Are ` fv ` and ` K ` ` numpy ` arrays , or the ` matrix ` subclass ?
fv and K are numpy arrays ...
I got as the types when I print the type of fv and K ..
Without knowing more about the underlying task we really can't say whether a element by element multiplication or dot ( matrix product ) is the right one .
The other looks like a ` dot ` product , producing a ` ( 3,150 )` array ( the size 4 dimension disappears under summation ) .
On Numpy arrays it does an element-wise multiplication ( not the matrix multiplication ); ` numpy.vdot() ` does the " dot " scalar product of two vectors ( which returns a simple scalar result ) #CODE
Mine does elementary multiplication and not matrix .. but I get the error as operands could not broadcast where my dimension is ( 3x150 ) * ( 150x150 ) ...
as I mentioned the code works perfectly fine with PCA when fv is of type numpy.matrixlib.defmatrix and mX ( mean centered matrix ) as numpy.ndarray.In KPCA its that both fv and K ( Kernel matrix ) are of the type numpy.ndarray ....
Bt I use 2.7.8 python .. and the dot products give a diffrnt answr I guess ...
ValueError : operands could not be broadcast together with shapes ( 0 ) ( 26 ) when using optimize.curve
I want to sort based on the values axis 2 of the first array , and sort the second array by the same indices .
I'd like to to find the unique label for each user using ` idx ` .
If True , also return the indices of ar that result in the unique array .
This gives you the set of indices into ` user_data ` that give unique values in ` unq ` .
To get the labels corresponding to each value in ` unq ` , you just use these indices to index into ` labels ` : #CODE
I've renamed the array of ' inverse ' indices to ` inv_idx ` in order to distinguish it from ` idx ` .
As with computing averages for each unique user name , there's also a simple way to get the corresponding labels using pandas : #CODE
The way these two sets of indices work is that ` user_data [ idx ]` gives you ` unq ` , whereas ` unq [ inv_idx ]` gives you back ` user_data ` .
To compute ` predictions_user ` you want a 9-long set of indices , one for each element in ` pred ` .
You can also change the ordering of the axis at the beginning with ` numpy.transpose ` and then apply ` reshape ` #CODE
How to add sum to zero constraint to GLM in Python ?
I have a model set up in Python using the statsmodel ` glm ` function but now I want to add a sum to zero constraint to the model .
That adds the sum to zero constraint to both C and D but I am not sure how to achieve the same in Python .
Does this impose the sum to zero on C and D separately ?
Sets are a collection of unordered unique elements .
Thank you , yes I am familiar with sets , however they consider items that are very close as unique whereas I need to remove them .
My problem ist that i have a list of variables ` a [ i ] [ t ] [ s ]` and need to sum the up for one of my constraints .
For the other constraints i have used the function ` np.sum ( a [ i ] [ t ])` which gave me the the sum ` a [ i ] [ t ] [ 0 ] + a [ i ] [ t ] [ 1 ] + ...
thanks alot but if i sum over the axis i get more than one expression .
But the thing i need is just one expression where i and s are fixed and the sum goes over all the different t values .
np.sum has a second argument , which is the axis along which you want to sum => the first sum you created was most likely , np.sum ( a , 2 ) and the one you want should be np.sum ( a , 1 )
You can use indexing to select the columns then use ` sum ` : #CODE
Which you can also use in ` sum ` : #CODE
If you translate into Python data , in most cases , it will be slower and use more memory .
it's possibly a capital gamma .
Everything below the first row is a number , and definitely not a capital gamma .
I'm fairly new to python and have found stack overflow one of the best resources out there , now I'm hoping someone can help me with what I believe is a fairly basic question .
This is my first question on stack overflow , I'll remember this next time .
Find the indices of values fulfilling condition in 2d numpy array
I have a 2D array of values and I'd like to find the indices at which the values are more than 60% of the maximum value .
So it may not always be possible to reconstitute a unique timezone-aware
I am trying to stack column wise a list A and array B as shown below :
That will already happen implicitly by the column stack
Its because of ` concatenate ` function that ` np.column_stack ` use it for concatenate the arrays :
In ` test ` please check ` max ( c )` before ** and ** after ` plt.imshow ( c )` .
And are you sure that you tried ` max ( c )` ?
i tried max ( c ) and not bool ( c ) . thanks again for the answer .
I think this is basically #URL Have a look at ` 2 - ( sqrt ( 2 ) ^2 )`
So I thought I'd check with just ` exp ( array_values [ idx ] / k )`
As you can check , ` 0.36787944117144233 ` is just ` e^ ( -1 )` , and you can infer that you are passing the floor of the fractional arguments ( because you are performing an integer division ) .
I need it for a gradient calculation .
Now you have two dictionaries with keys that are the unique values of ` X ` ( note the use of ` np.unique `) , and the values of the dictionaries are the arrays where ` X ` matches the key .
you can use graphviz to draw the graph here is the code to create the dot file : #CODE
Python / Numpy - calculate sum of equal array elements
I now want to calculate a third array ( of the same length and order ) which contains the sum of the counts whenever they lie in the same field .
Aside : when you find yourself needing a ` groupby ` operation , that's often a sign you should be using ` pandas ` instead of ` numpy ` ; your operation would be something like ` df.groupby ( " field ") [ " counts "] .transform ( sum )` .
And then get the corresponding elements of ` counts ` with indexing ! and calculate the ` sum ` with ` np.sum ` .
" Ok sure , I can just apply PCA onto the transpose of this matrix instead .
And append does not add it as a new row ?
Your ` append ` line is passing a single param of a tuple rather than 2 arrays : #CODE
What can I replace for append so that the FeatureFolds [ x ] [ y ] is added as a new row ?
PS : Have to wait 5 min to accept it
i have pandas data frame that looks like below , where id and date are the indices #CODE
The best way I can think to do this involves reshaping the indices and names via pandas and reshaping the values with numpy .
First , let's reshape the numerical values in numpy : #CODE
Now let's reshape the dataframe in pandas so that indices and column names are closer to what we want : #CODE
Given that , how could i generate the ids and parent ids in the manner i desire ?
How to calculate correlation between all columns and remove highly correlated ones using python or pandas
Firstly , I'd suggest using something like PCA as a dimensionality reduction method , but if you have to roll your own then your question is insufficiently constrained .
how to write symbol for sum over a variable's subscript in sympy
If I try to simply do ` data [ row_mask , col_mask ]` , I get an error saying ` shape mismatch : indexing arrays could not be broadcast together with shapes ...
A assume that it means each time I run fft , it will allocate another portion of memory to store the spectrum , which I see as a problem , considering the amount of times it will have to be run .
What I am aiming for is something along these lines : ( where gDNA Fit is the exponential ; 1-5Nuc Fit are the 5 Gaussians ; Total fit is the sum of all the fits )
In fact with this we can generate the nd array of dtype object , and fill it with some function of these indices in one step ( ok , 2 steps ): #CODE
Or do you want to add the scalar ` x ` to just the nonzero elements of ` V ` ?
Not particularly performant , but is O ( nonzero ) .
The first 2 columns are supposed to be ` int ` and the last 2 ones are ` float ` .
( the plot in my answer below has not quadrilaterals as they are bent across one diagonal . )
The last timestamps in ` a ` that are smaller than or equal to each item in ` b ` would be ` [ 0 , 4 , 6 ]` , which correspond to indices ` [ 0 , 2 , 3 ]` , which is exactly what we get if we do : #CODE
Please have a look at the log below : #CODE
" * This question is about the outer product of two vectors , which naturally creates a fully populated array if the vectors are mostly nonzero .
" , I think we need to know * why * you are computing this outer product .
This is similar to the ' ones ' operator in the example at the top of #URL
where ` arma :: vec ` is an armadillo c++ vector for which I am using armanpy.i to translate a numpy array as an arma :: vec object and vice versa .
The columns list is the index of the columns that I need to append to .
There are several gotcha when you want to translate your code from NumPy to plain python : #CODE
After that , you need to walk along the different rows and through the zeros index simultaneously .
Although it works here , I prefer not to re-use the index variable in a nested list comprehension : sometimes it's handy to be able to access the outer index in the inner list comp .
Also if my textfile is written in a sparse format , that is , if only nonzero elements are mentioned as ordered pairs , how can I achieve the same result ?
xy = zeros ( nx , ny );
Reversing is required because other wise it ( ` return_index=True `) will return the indices of first occurrence found of an item from the start .
i am looking for unique numbers with just one size ..
I am expecting ` B to be [[ 0.5 , 1 , 1.5 , 2 ] , [ 0.5 , 1 , 1.5 , 2 ]]` I am trying to do the same using normal division , or ` numpy ` division , but get an error , ` ValueError : operands could not be broadcast together with shapes ( 2 , 4 ) ( 2 , )` .
However , if you want to perform calculations only on the points outside the circle , you can simply define the points you want and re-assign ` X ` and ` Y ` to only have the ones you need .
On the other hand , if you were using something like ` imshow ` that requires 2D input , you'd need to either mask the values inside ( as @USER mentions , though it would be better to set them to ` np.nan ` or use a masked array than to set them to 0 ) or set a clip path on the image .
It gave me the same error of ` too many indices ` .
` UnicodeDecodeError : ' ascii ' codec can't decode byte 0x80 in position 191 : ordinal not in range ( 12 8) `
But I'm unable to find the index of unique array to extract data from the main matrix .
You can get the indexes of the elements that allow you to get the unique array : ` numpy.unique ( , return_index=True )` is your friend .
You can join all 0s and 1s of every row as strings and from there it's easy to get the integer representation .
For this , however , numpy will fill with zeros the columns , until they get byte shape ( ` length % 8 `) .
In order to avoid numpy from filling with ` 0 `' s , we could add zeros at the beginning ( to not change the binary value of the array ) .
The new row and column should filled with zeros , like this : #CODE
You can use ` np.vstack ` to stack arrays row-wise and ` np.hstack ` to stack them column-wise .
I am assuming you don't want an equation of a log function that approximates the data .
Use the ` argmax ` function , in combination with ` unravel_index ` to get the row and column indices : #CODE
In that case , you don't need the indices even .
But still this does not explain how to find all the indices with maximum occuring values .
The cython code is still calling a lot of python and numpy code - ` math.exp ` , ` np.pi ` , ` np.exp ` , ` floor ` , ` ceil ` , indexing .
I would suggest experimenting with small pieces of code , ones where the inner calculations ( of the loops ) are simple math , things that can be translated to fast C code ( without external calls ) .
A combination of ` np.vstack ` and ` np.hstack ` can concatenate a nested list of compatible arrays .
Mostly they tweak the dimensions of the arrays , and then concatenate on one axis .
Join them with a combination of ` hstack ` and ` vstack ` - i.e. concatenate along the matching axes .
I max trying to create a new column where :
c_ ( i ) = max ( 0 , blocked_i - ( rolling_mean_i + k ) + c_ ( i-1 )) -- where k = 2
For example , suppose I want to make FFT of ` sin ( 50x ) +cos (8 0x )` .
The fft functions calculate the * discrete * fourier transform , and do so exactly , up to floating point precision .
But , here , as the given functions are periodic , sin ( 50x ) +cos (8 0x ) , discrete fourier transform is what we need right ?
i am trying to find the eye centers and lip corners .
Will i see the eye centers and mouth corners
On the other hand , if you want the inverse of the ` cdf ` of the normal distribution , you want the ` ppf ` method of the ` norm ` distribution , which is the " Percent point function ( inverse of cdf percentiles ) .
Python doesn't use this , in large part because you can just index the results So say you have a function ` foo ( x )` that returns the min and max of ` x ` .
You can just want the max , you can do ` foo ( x ) [ 1 ]` .
Then to get an array of shape ( 1000 , 1000 ) , use reshape : #CODE
( In fact , if you try to index on one of these guys , you get ` IndexError : arrays used as indices must be of integer ( or boolean ) type `) .
I'm used to handling NoData areas fairly easily in image processing applications that require mathematic manipulation , transforms , etc ., but I'm writing a histogram matching program which requires indexing and thus the attempted use of masked arrays .
I would say that I need to manually implement this exponential sum .
EDIT : I'm now summing fft frequencies using a reference log-scale that I generate for an arbitrary number of bands with : #CODE
Yes , spectrum displays very often convert to dB ( or other log scale ) .
If I have understood well , the * real * fft already does that for me .
finding the max element in the matrix and its indices
changing the values in a row / column with the max from another row / column
it contains a two dimensional list ( ` count ` x ` count `) of float values ` weights ` ( lower triangular matrix , the values for which ` i =j ` are zeros )
it finds the two words with the most similar value ( the max element in the matrix and its indices )
Less important but nice : I've changed the inner loop to a list comprehension ; and I've changed both loops to be directly on ` s ` since there's really no reason to go over the indirection of looping over indices then using those indices to get the ` s ` items you care about .
make an array filled with zeros
` np.triu_indices ( s.shape [ 0 ] , 0 )` returns the indices for the upper-triangle of an array of shape ` ( s.shape [ 0 ] , s.shape [ 0 ])` .
Take the transpose of ` result ` and add that to ` result ` , thus making ` result ` symmetric .
Note that the diagonal is always zero since ` mahalanobis ( x , x )` equals zero for
So for a little added efficiency , you could exclude the diagonal : #CODE
The second argument to ` np.triu_indices ` controls the diagonal offset .
When the offset is 1 , the indices corresponding the the main diagonal are omitted .
The most important requirement for me is how can I get Python to get the gradient change point .
You can call ` isnull() ` on the column , this will return a series with boolean values , you then cast this to ` int ` , the ` True ` values become ` 1 ` and ` False ` becomes ` 0 ` and then call ` cumsum() ` , we then filter the df where the cumumlative sum is less than 2 which equates to the point where the ` NaN ` count becomes greater than 2 : #CODE
Insert into MongoDB retuns cannot encode object
All is well so far until I attempt the insert , where I'm getting a ' Cannot encode object ' .
I would like to sum the first 5 values followed by the second 5 values and so on and store them in a new empty list say ` b ` .
Reshape the one-dimensional array to two rows of 5 columns and sum over the columns : #CODE
The sum along each row ( summing the column entries ) is specified with ` axis=1 ` .
The reshape happens without copying data ( and without modifying the original ` a `) so it is efficient and fast .
Yup : if the arrays are not all the same length you'll need to calculate the correct shape to use here : e.g. the shape to resize to would be ` ( n // k , k )` for an array of length ` n ` summed over consecutive chunks of length ` k ` and ` n ` had better be divisible by ` k ` .
I have to find the maximum value of a numpy array ignoring the diagonal elements .
How can I achieve the same ignoring all the diagonal elements ?
where ` a ` is the matrix you want to find the max of .
Then you just take the max .
Another possibility is to use NumPy's ` as_strided ` to push the diagonal to the first column and then slice it off : #CODE
This is still kind of a code dump -- your organization of the problem into steps is fine , so work on each step at a time and ask specific questions about the ones that fail .
I think when you specify ` dtype= ' string '` , you are essentially specifying the default ` S64 ` type , which will truncate you string to 64 chars .
You can use the ` permutation ` function from ` numpy.random ` #CODE
I want to find and replace multiple values in an 1D array / list with new ones .
I am trying to concatenate a set of numpy arrays loaded from disk .
Here I have initialised result to an array with zeroes as I cannot concatenate to an empty array .
I would recommend to first load all the data in an array and then apply numpys ` hstack ` in order to horizontally stack the arrays #CODE
Generally we try to avoid repeated concatenation ( or append ) to arrays , prefering instead to append to lists .
So , to sum up : I need a way to tell whether a copy has or has not been produced of a numpy array .
How does numpy reshape works ?
Now i decided to reshape the array into 3d array .
You want a matrix with 10 rows with numbers from 0 to 50 ? but that's what reshape is for ...
` b ` looks like a ` ( 10 , 2 , 5 )` reshape
If you have created a copy of the data , perhaps with ` np.resize() ` , and discarded ` a ` , just reshape ` b ` : ` b.reshape (( 20 , 5 )) [: 10 ]` .
genfromtxt : numpy.darray , IndexError : too many indices
However , if that is true , ` sum ` would also be a one-dimensional array which does not make sense .
I have tested your code with similar sample , but only the axis=0 should be changed to axis = 1 , since i need the sum of each row .
avg = np.sum ( C , axis=0 ) / np.sum ( m.astype ( np.int ) , axis=0 ) , the axis should be 1 , which will sum the whole row instead of the column .
To make things work with my sample , lets make zeros array .
It looks like you are assigning to selected elements of ` G ` , the mean of a subset of ` A ` ( ` sum / n `) #CODE
Assuming ` n ` , the number of terms summed for each ` iW ` varies , it may be difficult to compress the outer loop into a vectorized step .
i am extracting all the indices ( x , y coord ) in this image where there is 0 or False using ` np.where ` but it is not working .
Edit : explanation on why I do not think it is a duplicate for Cython in Win64 with TDM-GCC reports utf-8 codec cant decode byte 0x83 in position 1 : invalid start byte
And it would not run , since it'll say `" UnicodeDecodeError : ' utf-8 ' codec can't decode byte 0x83 in position 1 : invalid start byte "`
possible duplicate of [ Cython in Win64 with TDM-GCC reports "' utf-8 ' codec can't decode byte 0x83 in position 1 : invalid start byte "] ( #URL )
In both ` .csv ` files the ids are the same ` id_1 ` and ` id_2 ` .
The ids are the same , but the content is different .
According to the documentation np.linalg.cond is defined as the norm of the array times the inverse of the norm of the array which is not what you are looking for .
For so-called [ normal matrices ] ( #URL ) singular values however are absolute values of eigenvalues .
The problem is , that when you reverse the ` r ` column , ` numpy ` doesn't know your indices change .
I have been using this algorithm ( my own Fortran implementation , though ) successfully for edge detection of eye diagrams in communications engineering .
The edge between these regions can then be detected using numpy roll to shift pixels in x and y and comparing to see if we are at an edge , #CODE
For simplicity I've use roll with the inverse of each region .
You could roll each successive region onto the next to detect edges
Sorry , just saw you wanted the indices , these can be obtained using something like ` np.ma.nonzero ( ~edgex1 )`
Also roll has a bit of a problem at the edges of the image .
As roll is cyclic , you get values at edges which can easily be patched ( if necessary ) .
A bit disturbing is the red region in the middle of the white region , however I think this could be tweaked with the number of bins in the histogram procedure .
Without ` ski-learn ` , you can simply reshape , and take the appropriate mean .
For this ` ( 100,100 )` case , the reshape approach is 2x faster than the ` as_strided ` approach , but both are quite fast .
Instead all I've been able to do is to append the data to an empty list and here's the output for that : #CODE
Python Value Error : Could not be broadcast together with shapes
For matrix multiplication , you should use ` numpy.dot ` or the ` dot ` method of array objects .
i import both packages . and the log function comes from math
You can use ` zip ` function within a list comprehension and ` np.concatenate ` But as you want the last element too you can append it with ` np.append ` : #CODE
Since you want the 24 at the very end , you can either ` append ` it like Kasra does or -- to bring up some variation ;) -- forget the ` endpoint ` argument and generate ` 10 + 1 ` values from ` a ` to ` b ` .
This will append the 24 automatically ( since ` endpoint ` is true by default ) .
( Update : As Bas Swinckels indicates , you need to wrap it with ` unique ` now ... ) #CODE
I can compute the gradient and Hessian exactly .
In my case , since I have the constraint x > 0 , I decided to optimize over log ( x ) instead of x .
It works if you strip comments in the data file .
where W , h and diff are 784x20 , 20x100 and 784x100 matrices .
For what it's worth , the line is from machine learning related code , W corresponds to the weights array of of neural network's layer , h is layer activation , and diff is the difference between network's target and hypothesis ( from Sida Wang's thesis on transforming autoencoder ) .
such that one array can be broadcast to the other
One array can be broadcast to another if , when pairing the trailing dimensions of each array , either the lengths in each pair are equal or one of the lengths is 1 .
` diff [: , None , :] ` has a shape of ` ( 784 , 1 , 100 )`
The axis parameter in ` all() ` runs over the colour channels ( axis ` 2 ` or ` -1 `) rather than the ` x ` and ` y ` indices .
calculate coordinates of blocks in diagonal block matrix
I have a diagonal block matrix and I want the coordinates ( row , column ) of each block #CODE
Note also that the starting coordinates will always lie along the diagonal of the matrix , so the row and column are equal .
Instead of taking sum ( or mean ) over each window , you calculate the ` cumsum ` and then use the differences between the front and end of window .
This is almost 10x faster than the ( nearly ) equivalent ` sum ` .
Are you looking for ` mod = np.abs ( complex_array )` ?
Euclidean distance , sqrt ( x*x + y*y ) .
The points are plotted In the order you provide them , so yes you have lots of zeros in there .
This is happening because you have data with zeros in it .
I have a numpy.ndarray of 3D and I need to calculate its gradient and obtain a new ndarray with the same dimensions .
The code you've shown shouldn't produce the output you've shown ; ` gradient ` should return an array , and ` print ( type ( force ))` shouldn't produce output in that format .
Can you strip your code down to [ the bare minimum that still contains the buggy part and still demonstrates the bug when you run it ] ( #URL ) , then post that ?
give us ` phi ` so we can replicate your result . this is weird , because the docstring for ` gradient ` says it should return a list . have you checked whether ` np.gradient ( phi )` -- i.e. , sans the ` * ( -1 )` -- is a list ?
The docs say ` gradient ` returns a ( list of ) ` N arrays of the same shape as ` f ` giving the derivative of ` f ` with
displays not what I expected in xplusy and xtimesy columns for added indices .
I may have got it completely wrong , but can't you just reshape ` F ` and then perform ` np.cross ` like ` np.sum ( np.cross ( F.reshape ( -1 , F.shape [ -1 ]) , vec ) , 1 )` and then use these values to set into ` F ` ?
` np.apply_along_axis ` and ` np.apply_over_axes ` are other examples of generating indices over 1 or more of the axes , while slicing others .
Is there a way to specify to the cov function to take only the maximum number of observations for division while calculating covariance or will I have to perform the operation manually ?
If that is not what you're after , then I'm getting the feeling you want a sample that * does * take into account the invalid elements , which will result in an incorrect (= unwanted ) cov . matrix .
The dimension is equal to the number of indices .
With this calculation there is about a factor of 2 in redundancy in so far as multiplications and divisions go as the diagonals are not needed and the lower diagonal is just the negative of the upper diagonal .
Instead try ` sum ` from numpy #CODE
Using the builtin ` sum ` function is different than using ` numpy.sum ` or the ` sum ` method of an array .
For > 1d arrays , python's ` sum ` will give a very different result : #CODE
This is because python's sum is basically summing a for loop over the object .
In this case , Python's ` sum ` is effectively giving you the sums of the columns ( i.e. ` row1 + row2 + row3 ... `)
I modified this code from MATLAB , and since the sum was working in python using the line-by-line execution method , I didn't add the np .
Is there a reason the ` sum ` works differently between the two execution methods ?
Your iterator is not creating indices , but the actual elements in the array #CODE
Import a sqrt function from math module :
` from math import sqrt `
I want to achieve the gradient mapping effect that's available in Photoshop .
Actually I want something like function ` take ( indices , axis )` which selects given indices along an arbitrary axis .
The only difference is that I do have logical values instead of numeric indices .
I am also aiming at the fastest solution so converting vector of logical values to indices and using ` take ` is probably not the best solution .
I would then calculate a new column containing the sum of the rows for the toal number of medals per country .
To find the peak frequency bin I use the frequency with the highest absolute value , within a small range of frequencies .
Which have the largest absolute value in yFrame between 0.09 and 0.11 Hz for each channel .
And array transpose is the equivalent of that list ` zip ( * ... )` .
The first one only gives me the largest value ( 20 ) and the second gives me each cities max pair not just the overall max .
How can I insert ` ones ` in ` zeros ` at position ` [ 60 , 60 ]` ?
This is one way you can replace values in zeros with ones .
A square matrix is invertible if and only if its column vectors are linearly independent , or equivalently its determinant is nonzero .
Now when I tell the fft to pad out the array with a few extra zeros , to improve the calculation speed , the amplitude result becomes 0.513 - which is actually closer to the expected value for a 1V peak signal - but the fft noise floor is also higher .
Extract indices of a NumPy array
I have to find out the indices of the element 1 which is surrounded by 1 in 2 by 2 pixels in every directions .
@USER I don't think max or min filter would work here , but those filter would look for maximum value within that window .
What we want is that all elements within that window to be ` ones ` .
@USER That ` ravel ` reshapes the array into a 1D array , which makes it easier to get the sum of the elements in that array .
So , ` filt_out ` would have the convoluted output and in it the perfect score is reached when there is a ` 5x5 ` window in ` data ` that has all ones .
This only happens at ` ( 3 , 4 )` in data where the convoluted output would be ` 25 ` because all 25 elements within that window in ` data ` were ones .
Then , we use ` np.where ` to find indices of all TRUE values in that logical array .
Numpy Select indices with complex conditions
Frequency axis in a Numpy fft
A radial average between r and r+dr is the sum of the values of the pixels whoes distance r to the center is between r and dr , divided by the number of those pixels .
You also most likely want to average angularly when reducing your 2d fft for a 1d representation .
Finally , we perform ` np.any ` to look for any matches across the third dimension , giving us a 2D mask , which we can use to index into ` data ` and set the matching ones to ` 0 ` .
how can I flatten an 2d numpy array , which has different length in the second axis ?
But I can not flatten it , #CODE
If I change the array to the same length in the second axis , then I can flatten it .
You can use ` hstack ` to stack the arrays in sequence horizontally : #CODE
Note that this is basically equivalent to using ` concatenate ` with an axis of 1 ( this should make sense intuitively ): #CODE
Well , I agree with the other answers when they say that ` hstack ` or ` concatenate ` do the job in this case .
So , ` flatten ` and ` ravel ` won't work because transforming 1D array to a 1D array results in exactly the same 1D array .
I am new in python , the Numpy supplies many useful functions , so I don't need to write long codes just for small calcualtions like + , - , sum ...
In order to use ` + ` , ` - ` and ` sum ` you need to make your array numeric , and to do so , it should be fixed size .
I wan to select only the red ones .
This pairing of column and row vectors works because ` numpy ` broadcasts them to produce the ` ( 3 , 2 )` array of indices .
` itertools.product ` produces the same set of indices , but as a list ( or generator ) of tuples #CODE
Indexing with arrays or lists of indices falls under the category of fancy indexing , and always generates a copy of the relevant portion of the array rather than giving you a view onto the original data .
Here numpy casts your lists of indices to arrays , then tries to broadcast them out to the same shape , which fails because they have incompatible dimensions .
To make it succeed , you can make ` rows ` into an ` ( n , 1 )` array and ` columns ` into a ` ( 1 , m )` array , where ` m ` and ` n ` are the number of individual row / column indices .
There is actually a convenience function , ` np.ix_ ` , for generating multidimensional indices from a sequence of 1D indices : #CODE
Assigning to the result of chained indexing ( e.g. ` A [ x ] [ y ] = ... `) will only work if the first set of indices is a slice or an integer ( i.e. non-fancy indexing ) .
Found a bit of a workaround which is pretty fast that combines the arrays , makes a unique version of the array , and then compares the lengths of the two arrays .
The basic idea here is that we concatenate those two lists into one numpy array .
Next up , we do ` diff ` to get all zero rows for the matching ones , which is picked up by ` np.all ( ... == 0 , 1 )` .
@USER Yes , that ` all + diff ` part is essentially ` unique ` finding in a * raw * version , and might be efficient as it also avoids intermediate variable creation .
Your approach definitely seems more elegant than my ` unique ` method .
it's mask [ row , col ] in opencv , not x , y . also you got the shape [ ] indices wrong ( should be 0 and 1 , not 1 and 2 ) again , please ** don't ** do it this way . instead use cv2.inRange() to threshold your image .
By default ` cKDTree ` uses the Euclidean norm as the distance metric , but you could also specify the Manhattan norm , max norm etc . by passing the ` p= ` keyword argument to ` tree.query() ` .
This is what i should have done : Either check the log file ` $PETSC_DIR / arch-linux2-c-opt / conf / configure.log ` .
Either check the log file ` $PETSC_DIR / arch-linux2-c-opt / conf / configure.log ` .
Also , this is my first stack overflow post !
Parse the Python list inside ` my_setup() ` instead of trying to translate it in your SWIG ` .i ` file .
Slice a matrix in python using list of indices
if I have a list of start and end indices for a given matrix in each dimension , is there any way to do slicing in an efficient way ?
Or do you want to stack ?
Here I collect 5 ( 3 , 3 ) arrays and join them into one .
It may be possible to join the indexes , and do the indexing once , but that will require some fiddling , and may not improve the speed .
My guess is that you will need to iterate in one way or other over the values of ` i ` and ` j ` , and concatenate values .
It doesn't matter much whether you concatenate and then index or index and then concatenate .
Anyway , lets consider this example , you have couple large images ( back above example , 5 images ) and you want to pick a sub-window from each window with given start and end indices .
Generate unique values based on rows in a numpy array
A new key is needed for each new unique combination of values along the 1st axis ( arr [: , i , j ])
A much better solution would be to simply call ` np.argsort ` to get the indices of the eigenvalues in ascending order of magnitude , reverse the order of the indices , then use them to index into the vector of eigenvalues and the columns of the matrix of eigenvectors : #CODE
( Aside - the motivation for this approach , rather than storing a line in a 1D array , is that my work often requires 2D Fourier transform , and so I need the zeros everywhere apart from the line / shape / etc etc ) .
The function ` np.fromfunction ` was designed for cases where an array can be constructed from the indices , such as this scenario .
Use ` np.put ` , but you need to create the list of specific indices , that you can do it with a list comprehension : #CODE
I've implemented the integral ( sum ) in the function ` calc() ` ,
You could compress these lines : #CODE
I looked into ` z = sqrt ( real^2 + imaginary^2 )` thing , but it's not the " thing " .
If you have two objects with different ids that both have the value NAN , you'll get two entries in the set .
I'm trying to work through the beginning of the OpenGL redbook for version 2.1 and translate what I learn to the PyOpenGL binding while using Qt for the windowing framework .
Interestingly , if I try and render using the ` glDrawArray() ` function , it does render ( although it doesn't look like a cube since it's rendering the indices ) .
I fixed the code and it does render now but it's choppy and definitely doesn't resemble a cube even though the vertices and indices were pulled from a working cube program .
A better approach than approximation would be to profile the function to get a sense of exactly why it takes too long , followed by using ctypes / Cython / numba to translate the function as-is into a C function that runs without as much overhead .
Calculate the min and max longitutes and latitudes 50 miles from your starting point .
This is also amenable to simple parallelization : give each process a copy of the data ( or have it load a copy ) along with a list of indices that it is responsible for .
Then that process computes pairwise distance of those indices against all other indices , and writes them somewhere .
I was wondering whether I can do just ` sqrt (( lat2 - lat1 ) ^^2 + ( lon2 - lon1 ) ^^2 )` will that even work ?
How should the result of ` sqrt (( lat2 - lat1 ) ^^2 + ( lon2 - lon1 ) ^^2 )` be converted into miles ?
You can use ` np.ix_ ` to generate the indices and perform the assignment : #CODE
First you can use ` np.argsort ` to get the indices of ` X ` elements based on those position after sorting , then you can get the elements from ` Y ` based on the indices of ` X ` with ` np.take() ` : #CODE
As the title states , I'm having an issue with Numpy's sum function rounding to the nearest integer .
and the ' sum ' it gives for that array is : #CODE
Whereas if I sum the values manually , I get #CODE
Can you include the entire stack trace ?
Set max number of threads at runtime on numpy / openblas
numpy 2D array assignment with 2D value and indices arrays
My goal is to assign the values of an existing 2D array , or create a new array , using two 2D arrays of the same shape , one with values and one with indices to assign the corresponding value to .
How to draw N elements of random indices from numpy array without repetition ?
Now I want to draw 3 elements from this array , but with random indices and without repetition , so I'll get , say : #CODE
` nonzero ` shows the same pattern : #CODE
` M.data ` is a 1d array , but it just has the nonzero values ( or what you gave it via ` coo_matrix `) .
You need to include ` M.col ` to know where the zeros are ( or aren't ) .
Find indices of unique values of a 3-dim numpy array
Now I want to obtain an array with unique values of xyz and a corresponding array of summed up masses .
Related : [ Find unique rows in numpy.array ] ( #URL )
this looks interesting .. the accepted answer creates the indices of unique values very fast , but I lose the information about the duplicate values .
So it gets hard to reconstruct which mass values to sum up afterwards .
Then , based on those labels , you can use ` np.bincount ` to accumulate the summations , just like in MATLAB one could use ` accumarray ` .
I don't understand one thing : the [ True ] that you append in df2 is then subtracted in sorted_labels below .. why is this necessary ?
@USER Well I sorted the array and then did ` diff ` to look for unique elements along the rows .
Now in that sorted array , the first element would always be the unique one and that ` diff ` would have reduced the length by 1 .
So , to compensate for that minus one * diff*-ed array and the fact the first sorted element would always be the unique one , that True was apended at the start .
Numpy naturally treats negative array indices as counting from the end of the array .
In fact , you can get an error on negative indices when you use ` mode= ' clip '` , but not in ` mode= ' raise '` .
I am trying to solve that differential equation R ( dq / dt ) + ( q / C )= Vi sin ( w t ) , so i have the this code : #CODE
Because most ode solvers like this have both relative and absolute error tolerances .
Thus the absolute error tolerance is frighteningly close to your normal ` q ` / ` p ` values .
You'll want to either use units that will have your values be larger ( probably the better solution ) , or set the absolute tolerance to a correspondingly smaller value ( the easier solution , shown here ): #CODE
The plot is not working i think , didn't get a sin function when i plot Vc / Vi vs w
I have to convert all the black pixels inside the outer white boundaries into whites .
The black pixels outside the outer white boundaries should remain black .
Find the set of ` params ` that minimize some measure of the difference between ` y ` and ` y_fit ` ( usually the sum of squared differences ) .
Someone else was just asking about 1d sparse vectors , only they wanted to take advantage of the ` scipy.sparse ` method of handling duplicate indices .
Some of the variables ( in this example `' a '`) have a lot of unique values .
I would like to replace `' a '` with ` a2 ` where ` a2 ` has 5 unique values .
Efficient way to concatenate Numpy arrrays and extract indexes
2 ) concatenate the 14 Numpy arrays
You can streamline this action by recognizing that ` concatenate ` takes a list of multiple arrays .
FWIW I've seen the solutions presented in How to create a list of random integer vector whose sum is x and Generate multiple random numbers to equal a value in python , but none of them are vectorized .
If there are much fewer slots than coins , you can use a binomial distribution to calculate a random number of coins for each slot : to distribute n coins among k slots , generate x ~ B ( n , 1 k ) .
How do I sum a numpy array of size ( m*n , ) in groups of m ?
I think there is nothing wrong with using ` reshape ` and then taking the sum of the rows , I cannot think of anything faster .
According to the manual , ` reshape ` should ( if possible ) return a view on the original array , so no large amount of data is copied .
It's fastest to sum across the final axis , right ?
How can I append ` y [ 2 ]` to ` y [ 1 ]` ,... such that the shape is ` ( N , 2 )` ?
My guess is that as long as you are using compiled functions like ` reshape ` and ` repeat ` , the time differences won't be significant .
Could you change the numbers so it is clear which ones come from ` xx ` and which from ` yy ` ?
Now when I do this on this data frame it will strip the column headers .
You could , but since you ' raveled ' the ` xx ` and ` yy ` matrices , you would need something like [ ravel_multi_index ] ( #URL ) to get the indices into the flattened arrays .
Get unique names : #CODE
this will return the unique values you have and the labels for that column ...
looping over unique values and assign an integer to each one .
numpy append to list then convert list to numpy
I don't think there's an issue with the ` append ` .
For example , you might fill in missing values with zeros or NaNs in order to make the dimensions of all your sub-arrays consistent .
Example ( X = time series , Y = desired output with max . of the last 3 values ): #CODE
I've found this thread using numpy's convolve -function to calculate a running average , but I must admit I don't really understand what the convolve does and how to apply it to calculate the running max ( or min ) .
Implement histogram equalization without built-in functions
How can I implement histogram equalization to an image without built-in functions , namely I want to change all the pixels in lonely with running two for : #CODE
The first step is to build a histogram of the intensity values .
Note that histogram equalization assumes a grayscale image or that you are working on a single channel of a colour image .
how Implement equalization on the histogram ? and after that , how Implement the new histogram on the image ?
` tris ` - a unique python list of all the indices of the triangles
Then for each list in tri , we use the Threading and Queue modules to designate 8 workers .
I just compared it to the solution given by @USER : I used a dataset with 300000 datapoints in wich each position occures 3 times ( so 100000 unique positions . ) Your solution : 10 loops , best of 3 : 20.6 ms per loop , Will's solution : 1 loops , best of 3 : 2.16 s per loop .
At least ` nan ` issue could be resolved with ` isnan ` or the bins could be checked for ` zeros ` .
And with a final list comprehension we can flatten this and get the resulting array .
Find min at zero .
-> pn -> 1 , where the line defining the envelope the the min at each value qi .
It gives me this error : could not broadcast input array from shape ( 2947 , 1 ) into shape ( 2947 ) #CODE
( For starters , something like this ` all_data [: , 1 :] ` gives an ` IndexError : too many indices for array ` . )
When i run this the histogram is different to whats expected from the interval given by the initial data .
When you run this and you compare the intervals printed and the intervals form the histogram you should find that they are different i think .
The default x- and y-ticklabels on a imshow are the indices of the array elements in it -- note that if you change the first line to , say , ` alpha=500* np.random.random ( 10 )` the data expands vertically but the labels don't .
` dot ` product of your data with a ` weight ` vectors is another possibility ( ` scipy sparse implements ` sum ` with a matrix product ) .
The last one is probably easiest to flatten and then reshape .
If all the arrays in your list have the same number of columns , then ` np.vstack ` will concatenate the rows .
Is there a way I can append the numpy arrays as arrays , which are appended by rows ?
The final timejoin1 array should have dimensions= ( sum ( rows of all classes ) , 700 )
The original question was to append numpy arrays to a list and then convert the list to numpy array .
It doesn't really matter how you created the list , with ` append ` or ` [ ... ]` or something else .
Numpy condition for getting nested arrays if their max is above a threshold
I would like to get the indices of ` arr ` that contain an array whose max value is greater or equal than .9 .
So , for this case , the result would be ` [ 1 ]` because the array with index 1 ` [ .9 , .1 ]` is the only one whose max value is > = 9 .
Use ` max ` along an axis to get the row max values , and then ` where ` to get the indexes of the biggest : #CODE
This function returns the indices of any values which meet a particular condition : #CODE
( ` np.where ( arr = 0.9 )` returns a tuple of arrays of indices , one for each axis of the array . Your expected output implies that you only want the row indices ( axis 0 ) . )
The first one checks each element from the nested arrays while the second it computes the max , then checks from the array of maxs ?
The reason you are getting the wrong answer is because ` np.max ( arr )` gives you the max of the flattened array .
My translation is just plain not working , tested with sin ( 2*pi*r ) .
Calculate your ` w1 ` as the sum of what you have at the moment , and see whether that works .
is what I've been using , where createArrOfSize just creates an array of variable size with the same increment starting with min and ending with max .
Also , use ` t.T ` to transpose a numpy array called ` t ` .
I have to select only the depth images ( which are indices 1 , 3 and 5 ) and 4 frames out of 32 ( indices 8 , 13 , 18 , 25 )
When you build the hdf data set you can organize it so your " query " of something like " indices 1 , 3 and 5 " can be much more logical .
How to roll the axis forwards in numpy ?
Suppose I have an ` ndarray ` with the shape ` ( 3,640,480 )` and I want to roll the first axis to the last , changing the shape to ` ( 640 , 480 , 3 )` .
You can use ` itertools.product ` to get the all combinations the use ` min ` : #CODE
You can reduce this array to an array of ` norm ` s using ` apply_along_axis ` and then take the ` min ` : #CODE
I profiled the function : It appears , that the min call ( in my version of your algo . ) is the most expensive part .
in general getting the max value for your datatype with #CODE
Adding an array of shape ` ( N , )` and array of shape ` ( 1 , N )` broadcast both to be ` ( N , N )` -like , thus the result is an NxN array of all possible sums .
Then , we clip it .
We get a 2dim array that satisfies : ` c [ i , j ]= min ( i+j , 255 )` for each i , j .
What you are intending to do is called flattening an array which can be achieved by calling the flatten method .
Also it only creates a copy if required where in flatten alays creates a copy .
If what you need is really just an 1D-accessor , try ` a.flat ` , which will " just " translate 1D indices to the matching 2D indices : #CODE
NumPy array sum reduce
I would need another array such that each ( x , y ) pair appears once and the corresponding third column is the sum of all the f values that appeared next to ( x , y ) .
Then translate back to numpy : #CODE
This solution assumes that the ( x , y ) indices in the first two columns are integer and smaller than N .
Edit : Note that the above solution produces a sparse matrix with the sum values at position ( x , y ) within the matrix .
For convenience in constructing certain kinds of linear algebra matrices , the ` csr ` sparse array format sums values with duplicate indices .
If I do np.random.rand ( 1 , 1 ) , it advances the offset by 2 , or rand ( 3 , 7 ) advances by 42 instead of 21 .
The problem is not unique to CURAND , but generally one of parallel PRNGs .
However , I think in the case of this example ` kmeans ` expects the observations as different rows , so you need the transpose : ` np.vstack (( lengths , breadths )) .T `
This problem only occurs when I use the log scale for the bar plot .
The issue is probably bad clipping of the bars and there are issues with viewers not clipping properly depending on the location in the svg file that the clip path is specified ( and to be clear , the svg spec says it can be declared anywhere ) .
Any suggestions on what I use as a list of indices in ` numpy.ufunc.at ` to get that matrix ?
` indices [ 0 ]` gives all the first coordinates of the cells you want to modify , and ` indices [ 1 ]` gives all the second coordinates .
NumPy can compute the array offsets quickly by computing a dot product of the indexing arrays and the strides .
Also note that the values in the ` indices ` argument are broadcast .
I'm not sure that this is the full answer , but you can just dump the sparse matrices that make up your ` SparseLU ` instance .
Obviously , you want to use ` dump ` instead of ` dumps ` to pickle to a file .
After reconstructing the permutation matrices ` Pr ` and ` Pc ` I can dump them together with ` L ` and ` R ` and have everything I need .
For each column , I want to calculate x+y , where x and y are mean and standard deviation of absolute non-zero values of each column .
You can filter the df using a boolean condition and then iterate over the cols and call ` describe ` and access the mean and std columns : #CODE
I was looking for an answer to a similar question but to produce a mean etc on nonzero items .
What is more , small integers ( -5 .. 255 ) are interned , so all those zeros in the list point to the same object .
Please be reminded that the ` numpy.ndarray ` instance ` x ` , once created , will be assigned a unique type for the underlying data stored .
this some how inconsistent wrong behaviour makes code syntax mad , i.e. distinguish between single row arrays or bigger ones
By default , ` loadtxt ` uses the ` squeeze ` function to eliminate trivial ( i.e. length 1 ) dimensions in the array that it returns .
I tried to sum over slices of the matrix and then stack it : #CODE
Return values from array based on indices of common values in two other arrays
I have a problem I think should be easy but can't find solution , I want to find the matching values in two arrays , then use the indices of one of these to find values in another array #CODE
@USER I thinks also stack could be used for short codes ;)
Note that when you can find the intersection then you don't need the indices or use them to find themselves again !!!
This only gives the intersection not the indices .
I still think you are missing the second part of the question : " I want to find the matching values in two arrays , then use the indices of one of these to find values in ** another ** array "
@USER : but the OP doesn't * just * want the intersecting values , he specifically mentions getting the indices of the intersecting values .
If you wanted to get the values in ` b2 ` which corresponded to the indices of common values in ` a ` and ` b1 ` , you could use the boolean array to index ` b2 ` : #CODE
Change ` rand ( N , 1 )` to ` rand ( N )` and it will work as you expect .
In general you can remove unwanted single dimensions with ` squeeze ` #CODE
Is there an opposite function to squeeze ( I want to ensure that a function returns the single dimension or force it to have one )
From that , it should only be evaluating and taking ` min ` .
From the graph , in this case it seems that the min is the last one .
Find unsorted indices after using numpy.searchsorted
I have a large ( millions ) array of ID numbers ` ids ` , and I want to find the indices where another array of targets ( ` targets `) exist in the ` ids ` array .
If I pre-sort the array of ` ids ` , then it's easy to find matches using ` numpy.searchsorted ` , e.g. #CODE
Your code only worked for me when I used ` ids = np.array ([ 22 , 5 , 4 , 0 , 100 ])` for line 1 .
Actually it fails , for me , at ` ids [ sort ]`
IOW , you get * participating * indices , not * corresponding * indices .
We can construct a ' cipher ' or sorts : ` key = numpy.arange ( len ( ids ))` applying the initial sorter to this key then gives the reverse mapping : ` revsort = key [ np.argsort ( ids )]`
I've a continuous stream of data which is basically bins of a histogram .
Alternatively , I could use the variant of LU decomposition where it's the U matrix that has ones on the diagonal .
from max intensity to 36000 ( red zone )
from max intensity to 27000 ( Yellow zone )
from max intensity to 12000 ( Blue zone )
labelsRed is now an array with ints as label indices .
As for computing the top / bottommost pixels , it can become tricky if you want the line to be a diagonal one , but if you just want top / bottom ( aligned with images axes ) you can make numpy check when the masks become True for each axis , which is plainly doing the difference ( derivation ) between the array and and offset version , then first nonzero element along each axis #CODE
For exact diagonal measurement , you might want to look into specialized image measurement libraries , like scikit-image , they probably have what you want
If you really want to do it yourself , I would recommend some approach based on finding object centre , then computing diagonal lines position , and measuring the max length , but then what happens if you find a 45 degree line ?
Hmm I have no idea about specifically asc files ( never heard of the things ^^ ) but what I get from your problem is that you need to either " translate " your arrays ( in which case try ` np.roll ( array , 1 , axis=1 )`) , or create a smaller size array , and you can do that by editing the array shape at creation ( in the ` np.zeros `) .
If you keep at which index you are in the array in an outer loop , you can slice ` fifths ` array for the inner loop with a starting index from the outer loop .
You also incur a performance penalty by constructing a new NumPy array each time in the inner loop and using ` in ` to check the sum ( which is ` O ( n )` in complexity ) .
Checking if the sum of these powers is also a fifth-power using set-membership ( ` O ( 1 )` complexity ) will also boost performance : #CODE
You have found the maximum dates , but you have to translate those to either indexes in ` data ` , or select those items from the groups .
I've tried using ` flatten ` and ` reshape ` , but nothing seems to work
I'm not sure how you used ` flatten ` but ` eigvec [: , i ] = a.flatten() ` should work if ` a ` is has shape ( 20 , 1 ) and ` eigvec ` has shape ( 20 , 20 ) .
where you properly transpose the " vector " ` null ` .
In the case of conversions between 2D and 1D , a 1D array of size n is broadcast into a 2D array of shape ( 1 , n ) ( and not ( n , 1 )) .
` empty_like ` gives an array filled with who-knows-what , so it doesn't have to spend time filling it with zeros .
Following is the code , this function is to get the SSD ( sum of standard differences ) matrix of the input .
You will therefore get overflow wherever the value of the sum of squared differences : #CODE
Sparse matrix dot product keeping only N-max values per result row
I want to get dot product of this matrix to itself ( ` M.dot ( M.T )`) and keep only ` N ` max values per each row in the result matrix ` R ` .
The problem is that dot product ` M.dot ( M.T )` raises ` MemoryError ` .
So I created modified implementation of dot function , that looks like : #CODE
According to the method I need only 30 max elements per item .
each process will need the copy of huge ` M ` matrix ( for dot product ) .
I had two 20GB arrays and had to do cosine distance ( so you can use it for dot ) .
Depending on what type of result you need out of a 2D array or matrix , you can do something like sum the columns and 1D FFT the resulting row vector , or sum the rows and 1D FFT the resulting column vector .
You can use ` np.dstack ` that stack arrays in sequence depth wise ( along third axis ) .
In case of 3D on N-d ones I cannot just give it an integer .
And I don't know exactly the max number of dimensions .
do you need to transpose the matrix to get the rows ?
Sometimes I have 3D arrays or N-d ones .
How to vertically concatenate two arrays in Python ?
I want to concatenate two arrays vertically in Python using the NumPy package : #CODE
How we can do that using the ` concatenate ` function ?
To use ` concatenate ` , you need to make ` a ` and ` b ` 2D arrays instead of 1D , as in #CODE
How we can horizontally concatenate using ` array ` ?
@USER You can use the ` axis ` parameter to control which axis to concatenate , and it defaults to ` 0 ` .
The problem is that both ` a ` and ` b ` are 1D arrays and so there's only one axis to join them on .
Under the hood , ` vstack ` works by making sure that each array has at least two dimensions ( using ` atleast_2D `) and then calling ` concatenate ` to join these arrays on the first axis ( ` axis=0 `) .
Maybe it's not a good solution , but it's simple way to makes your code works , just add reshape : #CODE
You can use ` reshape ` or ` vstack ` : #CODE
Do not do so as many functions from the Python generic library overlap with ` numpy ` ( for e.g. ` sum `) .
Simplicity and directness is in the eye of the beholder #CODE
Column sum with matrix from txt file ?
I've been given a matrix that I have to put into a text file , and take the sum of one of the columns ( from a command line )
Yeah I think it's okay in the text file , but now i'm stuck on how its supposed to " read " the file and sum the column .
Since log ( 0 ) = -infinity , the line becomes vertical for x 0.00007 .
By using the smart integral images one can calculate really fast the sum inside of a ` h x w ` window starting at every pixel .
and it has really nice properties , such as the calculation of the sum of all the values within a window with only 4 arithmetic operations :
Thus , by calculating the sum of the template and matching it with the sum of ` h x w ` windows over the integral image , it is easy to find a list of " possible windows " where sum of inside values is the same as the sum of the values in the template ( a quick approximation ) .
The sum over the two windows in the image is the same , but the last step of the function filters the one that doesn't exactly match the template .
edit2 : Best of three times without the ` sum ` : #CODE
Actually , I'm reasonably impressed that ` pandas.read_csv ` manages to construct a ` DataFrame ` * and * sum it along the columns in only ~30% more time than it takes to sum the values on the fly using pure Python .
What happens when you don't sum at all ?
I've added timings without the ` sum ` .
But as @USER ' s comment points out , if you want to do more than print the sum of the rows , or if you want to transform the data in any way , you will probably find pandas or numpy to be more efficient both in processing time and programming time .
I converted to fixed width format using ` column -t ` ( compiled my own version with a much larger max line length setting ) and used the settings ` header = None , engine = " c " , quoting = csv.QUOTE_NONE , index_col = False ` .
As for ` plonser's ` suggestion that you can vectorize the whole thing , the key is realizing that ` mean ` and ` std ` take some added parameters .
How do HEALPix FITS files of CMB maps translate into ndarrays ?
How does one HEALPix pixel in a FITS file translate into an ndarray entry ?
The question is rather where do your " pixel indices " for the " ipix " input parameter come from ?
All maps have pixels with indices ?
As a first step you could load the first csv and then drop the duplicate authors and write this out as the clean csv , this sounds like a messy many to many relationship , it sounds like the publication is the unique thing here so you probably want to repeat the publications for each author
You could reshape the array in Fortran order ( or use a combination of transpositions ) and then slice the array to extract only the top 7 rows : #CODE
The new shape chosen here was based on the location of the ones in your array .
This becomes clear when you flatten the array , Fortran order , and then you see your pattern appearing .
Then , stack the other data you want to keep : #CODE
While you can reshape arrays , and add dimensions with ` [: , np.newaxis ]` , you should be familiar with the most basic nested brackets , or list , notation .
A classic way of making something 2d - reshape : #CODE
` sum ` ( and related functions ) has a ` keepdims ` parameter .
The matrix ` sum ` does : #CODE
For me it seems that taking the transpose of a ( 2L , ) array should make numpy realize you want a ( 1L , 2L ) array
Basically , we insert values into ` A ` only where values in ` B ` are greater than those in ` A ` thus replicating the ` max ` criteria and then overlap that with the condition of C being ` 99 ` with ` C == 99 ` .
The plot consist of 10 lines , so I reshape to get the region limits per value of x .
to find the index in an array with the smallest absolute difference to a value .
I have an unstructured grid in a ` .vtu ` file and I would like to extract cell data , point data ( by name , if possible ) and cell coordinates ( i.e. ids of points which make up each cell , and plus the point array ) in Python , and have them as numpy arrays ( using ` vtk.util.numpy_support.vtk_to_numpy `)
The sum of the squares of the residuals increases rather than tends towards 0 at each iteration and my resulting B vector increases .
Hi @USER - as per the link , ` np.where ( M == M [ M ! = 0 ] .min() )` should return the row and column indices corresponding to the minimum element .
The revised question is still a duplicate , see [ this question ] ( #URL ) , and [ this question ] ( #URL ) for finding the indices .
However taking the unique values does not work on your data ( apparently ) .
This has done the trick and I can see that applying your deduction method for the reshape ( for which you arrive at 7 entries for this data set ) also applies to my significantly larger data sets as well .
Have you tried a log scale ?
I used your suggestion but just switched the reshape within the dot to ravel , and increased the size of the matrix up to 1001x1001 .
I think it'd probably make more sense to have the outer groupings as :
and reshape to get #CODE
and transpose to get the 6 rows of pairs : #CODE
` order I can transpose first #CODE
Stacked histogram with bin heights defined in numpy array
I'd like to create a stacked histogram out of numpy arrays with entries that are the desired bin heights .
I'd like to turn into a stacked histogram .
But ` inv ` doesn't see the problem , and returns an inverse : #CODE
Use ` np.argsort ` to obtain the sorting indices according to the second column and fancy indexing to obtain the sorted array : #CODE
For example if I makes a zeros copy of A2 , I can replace its data buffer with ` A1's ` , and get a valid structured array #CODE
None of the array operations like ` reshape ` , ` swapaxes ` , ` strides ` , broadcasting cross the ' dtype ' boundary .
How would I go on on finding others , especially non-stable ones ?
Did you want to create an array of zeros with shape ` ( 1 , 2 )` ?
The thing is I'm currently using the Pearson correlation to calculate similarity between rows , and given the nature of the data , sometimes std deviation is zero ( all values are 1 or NaN ) , so the pearson correlation returns this : #CODE
Just change the NaNs to zeros ?
At least all ones and zeros has a well-defined meaning .
I mean , your #1 issue here is what does it even mean to have a matrix of ones and NaNs ?
This is because in some cases it's not just NaNs and 1s , but other integers , which gives a std > 0 .
I can add an array of data as an m x n matrix , but how do I add it such that it will display as an image ; such as the ones in here : [ link ] ( #URL ) using h5py ?
With a little experimentation I found I could calculate the norm for all combinations of rows with #CODE
It expands ` x ` into a 3d array of all differences , and takes the norm on the last dimension .
I managed to pull out a stack trace using gdb , but cannot really interpret it : #CODE
Based on the stack trace it looks like an assignment to an array index is going out of bounds somewhere , but it's hard to say much else .
There's still a lot going on in your example code - if I were you I would start by trying to trim it down to the absolute minimum necessary in order to reproduce the segfault .
Python numpy keep a list of indices of a sorted 2D array
I have a 2D numpy array and I want to create a new 1D array where it is indices of numbers in the first array if they are sorted in an ascending order .
You can use ` argsort ` to sort the indices of flattened array , followed by ` unravel_index ` to convert the flat index back to coordinates : #CODE
You can use ` k ` argument of ` eye ` to set the index of the diagonal : #CODE
I have to sum every 2 rows inside it own .
So you can't broadcast the assignment .
See how ` red_mask ` is the indices of each individual scalar component , while ` red_mask [: , 0 ]` is the indices of each whole 3-vector pixel ?
Can you post the full buildozer log ( everything after typing ' buildozer android debug ') ?
You can attach the full buildozer log as a file .
@USER i have included now the whole log file in the post
@USER : I see that you posted the full build log , but the link now seems to be broken .
Based on the log , my guess as to what's happening now is that numpy's ` setup.py ` finds your system-wide installations of ` libf77blas.so ` , ` libcblas.so ` and ` libatlas.so ` , but the linker can't link them to the ` _dotblas.so ` you've just compiled , since the system-wide libraries are compiled for x86_64 , whereas ` _dotblas.so ` has been compiled for ARM .
I cannot find the log that I posted originally and the server where it was hosted deleted my files .
Other users with the same error are more then welcome to upload their full log and edit my question .
Of course your bigger problem is that you have 3 different copies of Python 2.7 , and a total of 6 path entires for them , and you don't even know how you got them or which ones ' site-packages you're installing into .
` rand ( 1000 , 10 , density= 0.2 , format= ' csr ' , random_state= np.random.randint ( 0 , 2 ))`
You could simply replace the non-zero values in your random matrix with ones : #CODE
... or for an arbitrary row number you could use the [ ` .indptr `] ( #URL ) attribute to find the corresponding indices into ` .data ` , e.g. ` x.data [ x.indptr [ i ]: x.indptr [ i+1 ]] = 0.5 ` to set the value of the ` i ` th row
Then this is a math trick :) ... make a np.random.randint ( 0,5,100 0 ) and then replace everything > 0 with ones ;)
I want to find the differences between all values in a numpy array and append it to a new list .
I believe what you are trying to do is to calculate absolute differences between elements of the input list , but excluding the self-differences .
I don't think there's a way of eliminating the ` reshape ` .
Couldn't those ` k ` indices be replaced with ` ...
See this related question : #URL where the exp functions is the culprit ( for you it could be the same due to those complex cosines ) .
Whenever I use ` exp ` , I try to check that the exponent is not too big ( more than 709.78 )
Using tigonometric substitutions for the complex maths ` cos ` function doesn't help either : #CODE
When you think about it - it is not very surprising that ` exp (8 000 )` or so fails - that's a huge number .
How does Healpy pix2ang read pixel indices ?
This is a continuation of this Question : How do HEALPix FITS files of CMB maps translate into ndarrays ?
Do I have to translate each temperature-valued pixel into spherical harmonics first ?
get min and max of each group .
Using NumPy's ` floor ` method ...
@USER : You seem to be looking for a grouping using ` floor ` instead of ` round ` .
I find the outer product using ` np.einsum ` .
g = gamma = 0.5
Stick with dot .
With this indices it is easy to get the right locations .
You can also roll the axes back into the configuration you were using ( 0 : time epoch , 1 : satellite , 2 : tracking status ) #CODE
A robust error score like sum of absolute differences .
I have a state matrix filled with ones and zeros ( on and off ) .
Also , you can avoid the explicit creation of indices and use boolean / mask indexing .
I don't want to convolve the whole image , I just want the kernel response at given ( say 20 ) points .
I want to speed up the traditional " convolve image and get points " by just " calculating kernel response at given points " .
` [( 7 , 7 ) , ( 100 , 100 )]` then it might be as simple as getting the appropriate image patch ( say the same size as your provided kernel ) , convolve the image patch and kernel , and insert back into the original image .
[ EDIT : I noticed a couple errors I had in my padding and patch arithmetic . Previously , you could not convolve with a point right on the boarder ( say ( 0 , 0 )) , I doubled the padding , fixed some arithmetic , and now all is well . ] #CODE
Separately , if your points end up being a contiguous patch ( es ) ( say a large rectangle ) , you can modify the code above to just convolve the rectangle ( s ) .
` sum ( 1 )` returns a ` matrix ` , not a ` sparse matrix ` .
I can use any measure as the distance from median ( absolute deviation from median , quantiles etc . ) .
I want to concatenate pieces of those matrices .
I want to take 9000x3 array and concatenate it with 9000x1 array getting 9000x4 array .
You could preserve dimensions by passing a list of indices , not an index : #CODE
One uses the concept of indexing with an array or list , the next adds a new axis ( e.g. ` np.newaxis `) , the third uses ` reshape ` .
If I create a 2d numpy grid , how do I correlate a radial distance to each one ?
Then how to I correlate to each temperature its respective set of grid points .
When I place in my arrays , both 1890 elements long verses the 4 element long arrays used in the example code , I get an output heat map of all zeros except at the center , where the value is , oddly , 15 .
but I still don't know how to define ' previous section ' to calculate max , min or any other value of each section of prices of X .
The values need only be unique .
Python , creating a large-dimensional matrix of 3-dimensional dot products
I would like to create a ( 1000,100 0 ) dimensional array / matrix of dot product values .
That means each array / matrix entry is the dot product of vectors 1 through 1000 .
and takes the dot product with the transpose , i.e. #CODE
The ( 1 , 1 ) entry will be the dot product of vectors ( v1 , v1 ) , the ( 1 , 2 ) entry will be the dot product of vectors ( v1 , v2 ) , etc .
In order to calculate the dot product with numpy for a three-dimensional vector , it's wise to use ` numpy.tensordot() ` instead of ` numpy.dot() `
Is the easiest thing to do to construct a ( 3 , 1000 ) numpy array / matrix and then take the tensor dot product for each pair ?
PS : To be clear , I would like to take a 3D dot product .
the dot product should be
That's why ` numpy ` also has the ` concatenate ` function .
( 2 ) If you are using the same set of predictors repeatedly in a series of linear models , you can take into account that the solution to a linear regression is coef = inv ( Xs ' *Xs ) Xs y .
Notice that inv ( Xs ' *Xs ) *Xs is the same for each of your linear models .
Therefore , you can compute all of your linear models simultaneously as inv ( Xs ' *Xs ) Xs Ys .
If you wind up using Ridge regression , you will need to modify this formula slightly to be inv ( Xs ' Xs + alpha I ) Xs Ys ( where I is a 15 by 15 identity matrix ) .
is there any way I can calculate some values such as accumulate summation , or mean of the clustered data , for example , I want calculate the accumulated sum and generate the following data frame : #CODE
Also , if I try to calculate mean or max of each cluster , ** df [ " max "] = df [ " value "] .groupby ( clusters ) .max() .fillna ( 0 ) ** doesn't work properly .
it is solved by using ** cummax** . both mean and max won't lineup the index correctly .
@USER : it sounds like you should read the [ groupby ] ( #URL ) section of the docs -- then you'll understand why your ` max ` and ` mean ` won't work .
do you have an estimate of how many unique members and events you have ?
The frequencies of the dft result are equal to ` k / n ` divided by the time spacing , because the periodic function's period's amplitude will become the inverse of the original value after fft .
Just to confirm I got things right - when I compute DFT from equation provided on numpy fft webpage ( #URL ) ,
Extra bar in the first bin of a pyplot histogram
When plotting a histogram , there is an extra bar that shouldn't be there .
A bar is produced in the first bin , which can be seen midway between the left edge of this figure and the main bulk of the histogram ( difficult to see on the thumbnail , enlarge the image ):
It is not present if I use ` plt.bar ` to plot the same histogram ( ` counts , edges = np.histogram ( t , bins ); plt.bar ( edges [: -1 ] , counts , np.diff ( edges ))`) .
This was done in order to fix another bug , where snapping the first bin edge prevented the histogram bins from aligning properly with corresponding line plots .
In the mean time , you could either use ` plt.bar ` ( as I mentioned in the comments ) , or manually setting snapping on for the first histogram patch : #CODE
How to find values with minimal absolute values in several columns ignoring NaNs ?
If both columns contain ` NaN ` , the resulting column gets ` NaN ` , otherwise we get the value with the minimal absolute value .
Let , ` df ` the dataframe with two columns , apply conditional absolute minimum over rows using ` axis=1 `
Here , ` x [ np.argmin ( x.abs() )]` checks the row index with absolute minimum and return the the original value with sign .
So this will check if either column is null if so return the min value , it then compares the abs value of either column and then returns the column that has smallest abs value but the original value including sign : #CODE
OP meant to * get the value with the minimal absolute value .
Using ` np.sum ` instead of just ` sum ` usually makes a big difference .
For example this here works for me ( I use ` eig ` because ` A ` is not symmetric ) #CODE
The method that i can give is translate the multi-dimensional numpy array to a list of ` { #URL ( index present by a tuple ) , and then sort the list by the value , and get the index for it .
Reshape to one dimension , then search , then get the original indexes through arithmetic calculations involving the dimensions before the reshape ?
Maybe ` flatten() ` the original array , then use your 1D solution , finally calculate the real nD indices using the original shape ?
To implement this idea takes quite a bit of work , and the implementation also requires some special care unique to how my data is represented .
I have set up a log file record in the py function to document the calls from C , so I know that in some cases the C++ calls do not reach the py function , and in other cases the py function does get called , but , perhaps , the returned values cannot be processed .
Specifically , I found that I can call the py function ( positive log record ) when the tuple contains one object , but I cannot call it ( no log file records ) when the tuple contains two or more objects .
Below is an example when I pass two zeros as input , and expect to get three zeros back .
It was not obvious from examples I have found , which used PyTuple_SetItem , because the latter used item indices > 0 .
One more thing ... the return format from python was proper , but I was getting zeros , because my VS2013 defaulted to Unicode and I was using a non-Unicode method to report values .
c_4 to be zeros !
I get zeros for all @USER .
Links to Doc rand and Doc randn
As you can see that ` rand ` gives me values within 0-1 ,
you can see that the mean of ` a ` is close to ` 0.50 ` , which was generated using ` rand ` .
You can also get a conversion from rand numbers to randn numbers in Python by the application of percent point function ( ppf ) for the Normal Distribution with random variables distributed ~ N ( 0 , 1 ) .
I have about 30 GB of data ( in a list of about 900 dataframes ) that I am attempting to concatenate together .
However , when I try to concatenate my files I quickly run out of available ram .
I have tried all sorts of workarounds to fix this ( concatenating in smaller batches with for loops , etc . ) but I still cannot get these to concatenate .
I cannot use a straight append because I need the ' column merging ' ( for lack of a better word ) functionality of the ` join= ' outer '` argument in ` pd.concat() ` .
I am concatenating on column name with an outer join which means that any columns in ` df2 ` that are not in ` df1 ` will not be discarded but shunted off to the side .
sympy lambdify with numexpr and sqrt
Unfortunately , the ` numexpr ` -based function breaks when using the ` sqrt ` function , even though it's one of the supported functions .
The most basic operation is ` concatenate ` : #CODE
Even if they are more convenient to remember , or use in more complex cases , you should be familiar with ` concatenate ` .
I have seen that the transpose in numpy doesn't do anything to a 1D array .
Writing ` a.T ` creates a view of the transpose of the array ` a ` , but this view is then lost immediately since no variable is assigned to it .
You need to write ` a = a.T ` to bind the name ` a ` to the transpose : #CODE
` dot ( A , v )`
treats ` v ` as a column vector , while ` dot ( v , A )` treats ` v ` as a row vector .
It's meant for " quick storage " , where you dump the data and read it back in the same session e.g. , you don't have enough memory to store all your arrays at once , or you want to pass them to a ` multiprocessing ` child .
Note that ` np.linalg.svd ` returns the second unitary matrix ` V ` in a transposed form , so there is no need to transpose it again before computing the dot product .
fov = d**2 / sum ( d**2 )
Essentially it's to set a 1 wherever there's a max of a column and zero elsewhere .
In your selection there only one array , so you get every row from indices to be equal to 1 .
To overcome that , you need column indices .
numpy sliding dot matrix operation
You can use ` dot ` : #CODE
You could append a ` 0 ` to the result or append a column of zeros to ` b ` to get around this .
Numpy : vectorize sum of lagged differences
Is it possible to compute the sum
I would then like to sum these Legendre polynomials together , \Sigma_n P_n ( x ) .
But then , I would like to take the sum of all of these Legendre polynomials
So we can make a list of these matrices using list comprehension , and sum them as you suggested .
@ pv .
So , I want to sum the ( matrix evaluated at n=0 ) plus ( matrix evaluated at n=1 ) plus ( matrix evaluated at n=2 ) and etc .
` ValueError : operands could not be broadcast together with shapes ( 10 , ) ( 100,100 )`
The ` max ( z ) = 3.0 ` , it is third dimension I have not shown here .
If you are talking about multidimensional arrays , do you always want to normalize by the sum over all elements , or do you want rows or columns to sum to 1 ?
You can use ` in1d() ` and ` nonzero() ` to find the indices of the items in ` Shop_Products ` : #CODE
( ` in1d ` returns a boolean array indicating whether an item is in the second list , ` nonzero ` returns the indices of the ` True ` values . )
` np.searchsorted ` can take a sorting permutation as an optional argument : #CODE
I like this solution better than my ` in1d ` one - the ability to return the indices according to the order the query seems particularly useful .
I would like to encode this as a 2d 1-hot array #CODE
numpy sum gives an error
File " / System / Library / Frameworks / Python.framework / Versions / 2.7 / Extras / lib / python / numpy / core / fromnumeric.py " , line 1711 , in sum
return sum ( axis=axis , dtype =d type , out =o ut )
It looks like you can't sum what vectorizer is giving you .
You will need a different way to do the sum , which you should be able to find in scipy's sparse library , most likely just by calling #CODE
Which I got from documentation on coo_sparse matrix sum .
Based on what I've read I don't think your code as-is should be able to pass the output of CountVectorizer ( scipy.sparse type ) to numpy sum .
Then for the subtraction , use ` sub ` and specify ` axis=0 ` since we want to consider the row indices when matching up labels ( not the column indices as is the default ): #CODE
` fillna ` seems cleaner than my ` sum ( axis=1 )` approach +1
Also , when using .sub , I get the error : " operands could not be broadcast together with shapes ( 320 , ) ( 80 , ) "
Did exactly my example above and tried your suggestion , still just makes newcol1 a duplicate of col1 , and the .sub operation gives me the error " operands could not be broadcast together with shapes ( 6 , ) ( 3 , ) "
For the ` fillna ` thing , the only possible cause I can think of is that your DataFrame has duplicate indices .
You could create ` newcol1 ` by ` sum ( axis=1 )` #CODE
I have enough data to truncate some values from each vector without compromising the accuracy of the results .
Do you want to correlate the values , or the change in the values ?
Finally , I want to copy that array , reshape it and return it .
( The reshape stuff hasn't been coded yet )
Subsetting observations in pandas using log change of response
I want to create a new dataset from this such that , for the first column ` ST.INT.ARVL ` , I basically take the ` log ( ST.INT.ARVL_II ) -log ( ST.INT.ARVL-I )` where ` ST.INT.ARVL_II ` is the value of ` ST.INT.ARVL ` for second obs and ` ST.INT.ARVL_I ` is the value for first .
Basically subtract log of second from first and keep the second obs from other variables along with this modified log value .
Keep doing this for all rows , such that we take log diff of next with previous and keep the next observation for other features in dataset .
Relocate zeros to the end of the last dimension in multidimensional numpy array
Is there any vectorized way to relocate all zeros to the end of dimension 2 while preserving the order of nonzero elements .
Also , in the real problem , the nonzero elements are not sorted and are nonzero floats , including negative numbers .
` map ( sum , zip ( *lisolis ))` is good enough .
` | ` does not concatenate ; it takes the union , which may have an unexpected order .
To concatenate , use ` pandas.concat ` .
NumPy append vs Python append
In Python I can append to an empty array like : #CODE
List ` append ` is faster than array ` append ` .
Now , let's append some rows : #CODE
Now , let's concatenate some rows : #CODE
It's faster to append list first and convert to array than appending NumPy arrays .
AttributeError : ' Add ' object has no attribute ' log ' Python
@USER You want to solve a symbol equation , so you need to use ` log ` function given by sympy .
Usually this works but for your equation , I get an error ` NotImplementedError : multiple generators [ log (( x / a ) **n + 54365636569181 / 20000000000000 ) **m , log ( 1 + x / R )]` .
Get unique countries list by dropping NaNs #CODE
It also scales the spectrum by the norm of the window to compensate for windowing loss .
The square root of the diagonal elements of this covariance matrix should give me the error on this quantity ( ` xi ` ) for each bin ( 19 bins overall ) .
If you expect a 19x19 matrix then you probably mixed your columns and rows up and you should use ` transpose ` #CODE
By eye it's ~10dB .
The idea here is to use the diff function , since your data is sequentially sampled .
This avoids unwanted diff values that occur at earlier frequencies .
Well , let me think out loud a bit : what clearly distinguishes the gap you're looking for , visually , is not the magnitude of the gap per se ( which is what ` diff ` measures ) , since we have already seen that there are larger gaps around that valley on the left .
So one possibility would be to create another array containing the " average diff value " around a point , and look for outliers there .
How can I append these inputted values to two separate vectors with NumPy ?
Second , ` append ` doesn't modify its array argument in-place , it returns a new array .
Note that ` append ` does not occur in-place : a new array is allocated and filled .
This is a case for list append , not the array approximation .
One way is to use the ` outer ` function of ` np.multiply ` ( and transpose if you want the same order as in your question ): #CODE
Most ufuncs in NumPy have this useful ` outer ` feature ( ` add ` , ` subtract ` , ` divide ` , etc . ) .
Alternatively , ` np.einsum ` can perform the multiplication and transpose in one go : #CODE
Since I have sorted my scores matrix in ascending order of MSE for each alpha , I expect the first record to show me the alpha for which the score is min .
To relate the feature indices back to the original feature columns , you can use the ` feature_indices_ ` attribute of ` OneHotEncoder ` after fitting : #CODE
So when I use OneHotEncoder , then are you saying using the feature_indices all the encoded features are mapped back to original ones .
Can I get an array of indices like : #CODE
A " diff " of 1 means transition from False to True , and of -1 means transition from True to False .
The purpose for running it against a 2D numpy array is to allow me to identify desirable values for gamma and C in an SVM with the RBF kernel .
Smart handling of Python array with many indices
For example , what if instead of 10 indices I wanted N indices ?
` sum ` or ` mean ` would be typical , e.g. #CODE
It just has 3 attributes ( ` start ` , ` step ` , ` stop `) and a method ` indices ` .
The even indexes are ok but the odd ones are a copy of the contiguous planes .
The plot appears correctly , however , I can not find a way of removing the coloured axis join lines ( shown on the graph below ) , which make the graph fairly unsightly .
I took simply the max of intermediate result and plotted it as thicker line .
actually , just the final result is the same thing as max .
histogram of gray scale values in numpy image
I loaded an image into a numpy array and want to plot its color values in a histogram .
If you just want to compute the histogram , you can use ` np.histogram ` : #CODE
If you want to plot the histogram , the easiest way would be to use ` plt.hist ` : #CODE
Note that I used ` img.ravel() ` to flatten out the image array before computing the histogram .
How can I read the gray value from the histogram ( scale only goes from 0 to 0.8 ) ?
Instead of deleting the columns and rows you don't want , it is easier to select the ones you do want .
Also note that it is standard to start counting rows and columns from zeros .
Better work your way through some basic tutorials , and some more detailed ones about import but confusing concepts like broadcasting and the one about indexing I linked in my answer .
This line depends on the indices ` m ` , ` n ` , ` i ` and ` j ` .
The ` sum ( axis=0 )` ( called twice ) sums along the ` m ` and ` n ` axes , so that ` P ` ends up being a 2-dimensional array indexed by ` i ` and ` j ` only .
The most simple ones are the implicit Euler method , the midpoint method and the trapezoidal method .
EDIT : Adding code to answer a question in the comments regarding how to change the time to 12 hour format ( ranging from 0:00 to 11:59 ) and strip the seconds .
@USER I can edit my answer to include 12hr clock and to strip excess zeros off .
What solved this for me was generating a 5th column ( in code , not the csv ) which is the number of minutes corresponding to a particular o'clock time , i.e. 11:59 maps to 719 min .
I could then place string ticklabels for every hour ( ' 0:00 ' , ' 1:00 ' , etc . ) at every 60 min .
Some take an ` out ` parameter , especially the ` ufunc ` ones .
` ufuncs ` also have ` reduce ` and ` accumulate ` methods to handle repeated actions .
In particular , it must have the right type , must be C-contiguous , and its dtype must be the dtype that would be returned for dot ( a , b ) .
Numpy sum running length of non-zero values
We can grouping the array based on non-zero elements then use a list comprehension and enumerate replace the non-zero sub-arrays with those index then flatten the list with ` np.hstack ` .
Initialize a zeros vector of the same size as input vector , x and set ones at places corresponding to non-zeros of ` x ` .
The intention is to use cumsum again later on , which would result in sequential numbers for the " islands " and zeros elsewhere .
so that I can run further functions such as average and sum .
I'd like to insert / append one more row to this 2D array , how to do this ?
ValueError : Shape of passed values is ( 6 , 17512 ) , indices imply ( 6 ,
[ ` append `] ( #URL ) is designed for this
Also , You could append the arrays iteratively and then transpose .
Yes , it looks like the .T transpose syntax is what I was looking for .
The concatenate function is an internal C function so it is likely to be efficient speed-wise .
using ` vstack ` and transpose : #CODE
using ` concatenate ` and ` reshape ` #CODE
concatenate + transpose ( David Z )
The upshot take-away of the below is this : in all regimes , concatenate + transpose is the fastest algorithm .
In the k N regime , I notice no appreciable difference between vstack and concatenate + transpose .
In the k ~ N regime ( not relevant for my problem , but possibly in others ) , concatenate + transpose is the fastest , with pre-allocate slice-wise assignment trailing by 10-50 % .
In the k ~ N regime , column_stack and vstack are roughly the same speed , which is 500-1000 % times slower than concatenate + transpose .
So , as I said above , the upshot is that concatenate + transpose is the fastest in all regimes : #CODE
In ` numpy ` you vectorize by using fast compiled code , operations like ` + ` , ` * ` , ` sum ` .
Actually I was thinking the same and I was using ` log ` and ` sqrt ` functions from math library of ** C ** and loop over the array with if conditions .
Do you think ` numpy.broadcasting ` can be used instead of what you have used to combine summing over ` ks ` elements in ` kappa ` and ` gamma ` .
Sorry to just dump the error message here but I have no idea what's wrong :/
Can I plot dots in same plot with x values from ` vector2_x ` and dot sizes from ` vector2_y ` ?
and not worrying about trying to flatten the whole thing ?
Firstly , if ` axis=None ` , ` np.linalg.norm ` will compute either the vector norm ( if the input is 1D ) or the matrix norm ( if the input is multidimensional ) .
Secondly , ` ord=1 ` means the L1 norm ( i.e. Manhattan distance ) , not Euclidean distance , as you mention in the title .
If you don't care about the indices , and just want the number of points in the ball , it will probably be faster to use ` count_neighbours ` : #CODE
Unless really necessary , check the official installer ( #URL ) or ones from C .
How plot ployfit n log n with python ?
However I'm not able to create trend lines for data that might fit log ( n ) or n log ( n ) trends .
You can treat log ( n ) and nlog ( n ) as first order polynomials where the x values is log ( n ) or nlog ( n ) .
That is , you take the log ( n ) or nlog ( n ) before fitting and use that as the input to polyfit .
Here's an example for log ( n ): #CODE
Notice how diagonal arcs in the Matlab result all have the same slope ; but those in the scipy result are varying .
Obviously both triangulations are " correct " , since both kinds of diagonal have the same length .
numpy reshape : incompatible array size , when array grows too big ?
C :\ Users\maximih\AppData\Local\ WinPython-64bit-2.7.9.4 \python -> 2.7.9.amd64 \lib\ site-packages \numpy\core\ fromnumeric.pyc in reshape ( a , > newshape , order )
220 return _wrapit ( a , ' reshape ' , newshape , order =o rder )
--> 221 return reshape ( newshape , order =o rder )
Now I join all the people , that are too stupid to check their array sizes ( Some would say of course , this was the problem , I suppose ) ...
If I have a ` numpy.ndarray ` ` A ` and a ` scipy.sparse.csc_matrix ` ` B ` , how do I take ` A ` dot ` B ` ?
I can do ` B ` dot ` A ` by saying ` B.dot ( A )` , but the other way I can only think of this : #CODE
The dense ` dot ` probably can't handle a sparse matrix .
The ` dot ` override was added in [ this PR ] ( #URL ) , and is not present in versions of scipy older than 0.14.0 .
Also the documentation does not mention that one can use negative indices ( as with python list indexing ) to count backwards .
Are the corresponding ` transpose ` inputs any easier to understand ?
We could specify a number of roll steps ( back or forth 2 steps ) , or a location in the final shape tuple , or location relative to the original shape .
So the keys are understanding that ` remove / insert ` pair of actions , and understanding ` transpose ( x )` .
I suspect ` rollaxis ` is intended to be a more intuitive version of ` transpose ` .
After the roll ` 3 ` is the the ` a.shape [ 1 ]` .
But that's a different ` roll ` specification .
I added an alternative roll that uses a ` dest ` index instead of a ' before start ' .
So the ` i ` -th subvector would be lie between the indices ` breaks [ i ]` ( inclusive ) and ` breaks [ i+1 ]` ( exclusive ) .
In MATLAB , we have a simliar thing with [ ` unique `] ( #URL ) with its third output argument .
Open mesh of array indices
This produces the same result as ` indices ` here ( ` M , N , O = 2 , 3 , 4 `) : #CODE
If your database cannot hold native numpy arrays , you can use the ` dumps ` or ` tostring ` methods .
` dumps ` pickles the data to a ` bytes ` object in Python 3.x and a ` str ` object in Python 2.x , which can then be stored in the database as a string or raw byte sequence .
How to invert a numpy histogram back to intensities
I'm wondering if there is a numpythonic way of inverting a histogram back to an intensity signal .
Obviously it's impossible to recover the exact position of a value within a bin , since you lose that information in the process of generating the histogram .
I read them into numpy arrays and flattened the rgb codes to 1's ( black ) and zeros ( white ) .
Given the RPis computing power , would it be faster to compress and decompress the arrays ?
If you want to play around with compression , one option would be to use the native ` zlib ` module to compress the string of bytes before passing it through the pipe , then decompress it on the other side .
It handles symbolic just fine if you can encode a fixed polynomial as just coefficients in the spreadsheet .
Return a new array of given shape and type , filled with zeros .
Return a new array of given shape and type , filled with ones .
scipy.io.savemat module saves wrong structure mat
have you tried looking at the absolute difference between them ?
[ 8,39,238,196 ] ' time ' , ' depth ' , ' lat ' , ' lon ' . type is floa32 same . convert mat may cause problem ..
Have you tried ` squeeze ` -ing ` u_xy ` before saving ?
Denote by n the nxn array having entires A i , j : = sin ( z i , j ) where z i , j are uniformly distributed random numbers in ( 0 , 2pi ]
Would sin ( z ) be my f ( i , j ) ?
Yes , sin ( z i , j ) would be your f ( i , j ) .
This looks like the cumulative sum .
When you want to have the cumulative sum for each row seperately this here works #CODE
I wish to multiply it by its transpose .
While your original matrix may contain only 3348026 non-zero elements , keep in mind that multiplying the matrix with its transpose may yield a more densely populated resultant matrix .
The percentage of nonzero elements is 0.0016977 % .
So then we can stack up all of the neighbors .
And accessing the neighbors if a cell looks like this ( the call to ` reshape ` for illustrative purposes only ) #CODE
For the offset indices , we do need ` M1 ` , which comes in as ` collen ` .
Apply a function across numpy matrix row and concatenate the result ?
Of course , you probably want to transpose the result .
` apply_along_axis ` can help keep indices straight when dealing with many dimensions , but it still is just an iteration method .
Could you post ` max ( img ) , min ( img ) , img.dtype ` ?
the max ( img ) , min ( img ) and img.dtype all raise errors when running ( Image type is not iterable and dtype is not an attribute of img )
The reason I can't use this is that a ) I have collected a lot of data with the last version , and b ) I want to grade the contrast of those big long bars in the centre programatically by multiplying one array against another , e.g. using a log scale or some other function , and doing the math is easier in numpy .
That's how far it can roll back .
I have a 3D array which has one time index and two space indices .
I found another stack question about this here , but I am not entirely sure how it was resolved , I'm still a little confused .
Basically I have a solution array which is ` A [ n , i , j ]` where n is the time index , and x and y are the spacial indices .
Now I want to basically get all these index names in a list such that only those index should come whose absolute value in the right column is less than 0.5 .
If you take all the columns , get rid of the ones in your < 0.5 set , and then drop the resulting ones , isn't that the same as just selecting the ones in the < 0.5 set ?
Does this mean that ` numpy.random.RandomState ( seed=None )` is called every time you call ` rand ` ?
Does that mean ` numpy.random.RandomState ( seed=None )` is called every time you call rand ?
If it were re-seeded every time you call ` rand ` , then there would be no way to explicitly ask for a repeatable pattern .
This is not the case for C's ` rand ` ( which is still there in C++ , although you should treat it as deprecated * ) , but only because C goes out of its way to require that startup must do the equivalent of calling ` srand ( 1 )` .
When you call the top-level ` rand ` , it uses that global ` RandomState ` .
On the left figure those who have 5 neighbours , on the right only the min is selected .
If you want the indices and values , use ` inds=mask_indices ( 100 , lambda x , k : minis )` and ` a [ inds ]` .
Then I can just loop in the third dimension ( just as in the ` else ` case above ) , but still it seems to me this is a bit of a hack , and I'd have to reshape ` im_out ` before the end .
It is not much better than your own solution ( although I think your solution is fine ) but you can reshape your image without the if statement : #CODE
for multiple indices : #CODE
I'm wondering if the row indices can be given in an implicit way .
How can I use melt() to reshape a pandas DataFrame to a list , creating an index from a crosstab column and creating a new variable in its place ?
I have a matrix of data 29523 rows x 503 cols of which 3 cols are indices ( below is a subset for example ) .
For some functions ( and I think dot() was one of the ones , though I can't find ref now ) .
If you call ` np.dot() ` on integer arrays , numpy will fall back on using a very simple internal C++ implementation , which is single-threaded and much slower than a BLAS dot on two floating point arrays .
` x ` is ` cos ( t )` and ` y ` is ` sin ( t )` , so give those arrays to ` plot ` : #CODE
This means that the transpose of the slice ` bn ` will be in C order and transposing does not create a copy : #CODE
When I run my code for the second time the values that returns from reading a file to a pointer is correct , however I think the way I allocated the pointer to a memoryview has a problem since all the values in ` self.Da ` instance are zeros .
[ a option ] append : Open file for output at the end of a file .
get_support ([ indices ]) Return a mask , or list , of the features / indices
In order to broadcast , the size of the trailing axes for both arrays
In the above I use double subscripting to force the shape to be ( 4 , 1 ) but we see a problem when trying to broadcast using the first row as the column alignment only aligns on the first column .
It's also possible to broadcast in 3-dimensions but I don't go near that stuff often but the numpy , scipy and pandas book have examples that show how that works .
Faster way than concatenate of combining multiple cv2 images ( numpy arrays ) in python ?
At the moment I'm using concatenate to first create vertical strips then using concatenate to connect all of those strips but it seems kinda clunky .
` concatenate ` will accept a long list of arrays , not just 2 .
Using the original approach , if we say that an image array has size 1 , then to build a ` strip ` requires
So one strip requires allocating arrays
I want to get the indices of all the True elements .
I have two numpy arrays ( 100000 x 142 and 100000 x 20 ) that I want to concatenate into 1 , 100000 x 162 array .
if just before the concatenate you do ` print actualhhdata.shape , projectedhhdata [: , 20 :] .shape ` it should clear things up .
I would add a ( temporary ) print line right before the ` concatenate ` : #CODE
A very simple example of a fft anaylsis is show here .
You can concatenate unique elements versions of those three input arrays into one single array .
@USER I did try using ` boolean indexing ` , but then since I need to find runlengths , I figured I would need to find those shifting indices and then perform ` diff ` , so ` np.where ` might be the preferred way here .
So clearly for ' US ` sum both the ` Server1 ` and ` Server2 ` exceeds 100 .
The sum exceeds 100 , ideally sum of both the servers should be 100 to get the percentage split .
But it doesn't work when I apply it to my real case , using a unique variable name .
This padding should also occur in case of negative indices .
Another example , negative indices .
Negative indices already have a meaning in NumPy the same one as in Python in general , where -N means " N elements back from the end " .
But meanwhile , you can always do this manually in a wrapper function : work out the padding , ` stack ` on a ` zeros ` , and slice that .
Or slice them stack , which is a bit more complicated but probably more efficient .
Negative indices are interpreted as counting from the end of the array so if you are counting from ` -2 ` actually in a 5x5 array there are not any row between -2 and 2 so the result would be an empty array : #CODE
I know that negative indices are interpreted as counting from the end of the array , but I imagined there could a single numpy function that could ignore it .
There are two things : 1 ) for SO , if you want to make progress and be liked , you need to ask a question and treat the question you ask as though it's your actual question ; 2 ) different fitting problem need different approaches , mostly dependent on the number of parameters and what the fits look like in the parameter space ( eg , can one use gradient decent ? ) .
Why does pymc with gamma prior not converge with zero count data ?
Then the mapping from ( i , j , k ) to a unique , dictionary-order index of the cell is made lightning fast via the following numpy indexing trick : #CODE
As an example , suppose we have the following arrays storing the x , y , and z cell indices of Npts points : #CODE
Numpy : How to resize an random array using another array as template
What is the best way to use a 5x5 array of zeros as a template to merge other arrays into it ?
I have another array that is ones (( 12 , 12 )) .
I would like to resize the ones (( 12 , 12 )) such that it's new size will be 5x5 .
But if ' b ' is a larger array , I get an error : ValueError : could not broadcast input array from shape ( 10 , 10 ) into shape ( 5 , 5 )
@USER Nope , I have to find out the lower and upper indices in the LUT where the x1 , x2 , and x3 exists .
Now if each column increases in lock step w ( 1 , 1 , 3 ) and you have some notion of order here , find the upper and lower bounds with python's bisect module of the first column and you're done finding the indices you would interpolate with ( bilinearlly ? ) .
If they are not in lock step but a similar ordering is preserved , restrict the range of allowed upper and lower indices by producing a cumulative upper / lower bound across columns , then decide what indices you'd like to interpolate with over that range .
You can use ` np.hstack ` to stack those input lists horizontally into a single 1D array - #CODE
Python : converting Trip duration of h min sec and leave only minute count
I can play around with positions and create one column each for h , min and sec .
I have also tried to strip the characters with the following : #CODE
'` you're first finding pattern ` re.findall ( r ' \d+ ' , x )` i.e ` [ 4 , 26 , 2 ]` now multiple this with minutes ` [ 60 ., 1 ., 1 . / 60 ]` and ` sum ` the values .
I cannot use Scipy and its fft / filtering library , unfortunately , because I am running the code on Android and Scipy is not available for the code platform I'm using ( Kivy ) .
@USER : the shape of freqs is 33 , and the shape of fft_spectrum is 65 , which threw a _ " ValueError : operands could not be broadcast together with shapes ( 65 , ) ( 33 , ) ( 65 , )" _ .
Applying the filter to each sample is convolution - and boils down to a weight sum of samples .
Store 3D numpy array of outer products into block-diagonal sparse matrix
I have figured out how to compute the matrix of outer products of its rows transposed
Create array of outer products in numpy
If you just want a workaround , breaking up the file can be done dynamically rather than on-disk ; just ` genfromtxt ` against file-like objects holding each batch 10K lines instead of against a filename ( and then you can just ` stack ` them together in a one-liner ) .
find the min and max
Anyway , if your problem is any of the obvious ones , reinstalling is either not necessary ( e.g. , if you forgot to run the NumPy installer , just run it ) , or not sufficient ( e.g. , if you have two Python installations and keep mixing them up , reinstalling one of them isn't going to stop you from mixing it up with the other ) , so you probably need to find out the actual problem .
Dividing matplotlib histogram by maximum bin value
I want to do this by dividing each histogram by its maximum value so all the distributions have the same scale .
However , the way matplotlib's histogram function works , I have not found an easy way to do this .
I have attempted the norm and density functions but these normalise the area of the distributions , rather than the height of the distribution .
But these are two different distributions and I want to compare the spread in data , which is most obvious when I scale to height as one has a max bin value of 150 and the other a max bin value of 30 .
This will give you a histogram with height normalised by a constant .
Format error log
Alternatively I can change the values in the matrix to zeros and ones , and directly apply logical operators .
You have a transpose issue ... when you go to matrix land , column-vectors and row-vectors are no longer interchangeable : #CODE
gradient descent in neural network training plateauing
You usually don't want it to be zero , and it will also be very hard to get it to absolute zero because the function you are minimizing is not convex .
I would reshape it so you had n x 2 i.e. each xy location then use a distance calculation and np.with distance < threshold .
The code seems to be looking at a diagonal as x and y will obviously always be the same value from a zip ( range() , range() )
Then you can use ` np.dstack ` to rotate the array then use ` np.hstack ` to add the zeros columns : #CODE
Or you can repeat the none zero part again with ` tile ` : #CODE
For d=1 I do the same , and then sum the two so : #CODE
operands could not be broadcast together with remapped shapes [ original -> remapped ]: ( 2 , 10 , 5 ) -> ( 10 , 5 , 2 ) ( 9 , 2 ) -> ( 2 , newaxis , newaxis , 9 )
i assume <= 50k is replaced by 0 ( not 1 ) , otherwise the columns is all ones .
When I use ` numpy ` ` fft ` module , I end up getting very high frequency ( 36.32 / sec ) which is clearly not correct .
I tried to filter the data with ` pandas ` ` rolling_mean ` to remove the noise before fft , but that too didn't work .
This is my first stack overflow question so please correct me if its not a good one :
When I resize the images using ` resized= misc.imresize ( image ,. 1 )` , the resulting image will sometimes show up with different gray levels when I plot it with pyplot .
Once I set the vmin and max values to the vmin and max values of the original image the colors showed up the same .
Can you open another stack overflow question , preferably with example code and / or images ?
Is there a faster way to get a Boolean array from string comparison than the following , all the strings in the array are unique : #CODE
All the strings are unique
If all strings are unique then there's not going to be a faster way , all strings need to be compared .
If there strings are not unique , then get_loc may return a mask !!
Get indices of N maximum values in a numpy array , with random tie-breaking
You could randomize the order before sorting , and reapply that same permutation : #CODE
I'm trying to make a 3D histogram .
Initially ` h = zeros (( 6 , 6 , 8)) ` .
and also a list of corresponding quantities to accumulate into ` h ` :
To quote the docs : " For addition ufunc , this method is equivalent to ` a [ indices ] += b ` , except that results are accumulated for elements that are indexed more than once .
max value from specified column in numpy array
Get a list of all indices of repeated elements in a numpy array
@USER I need to find the indices of all of the duplicates ( or better yet , directly the ` tagId `) , divided by the corresponding timestamp .
EDIT : the following was my previous answer , which required a bit more memory , using ` numpy ` broadcasting and calling ` unique ` twice : #CODE
this will give you set of arrays with indices of unique elements .
Added : Further change in list comprehension can also discard single unique values and address the speed concern in case of many unique single occurring elements : #CODE
This will be very slow if the array contains many unique values .
scipy.optimize.minimize : compute hessian and gradient together
In scipy , functions for the gradient and Hessian are separate .
However , I have a function whose Hessian and gradient share quite a few computations and I'd like to compute the Hessian and gradient together , for efficiency .
I would like to have the ability to roll backwards if possible .
I'm positive it's not copying data because the reshape command runs on the order of micro seconds :
Also , you can take better advantage of NumPy by removing the ` for-loops ` and instead using the NumPy ` mean ` , ` min ` , ` max ` methods .
The ` axis=0 ` parameter tells these methods to take the ` mean ` ( or ` min ` or ` max `) over the rows .
The problem here ( VS . this similar question ) is that each column has a different unique name that should be recorded .
You don't specify ` x ` or ` y ` , and your ` mat [: , i+1 ]` indexing will not work with a structured array .
You can build up an array using the usual ` tile ` and ` column_stack ` provided by numpy , then use ` np.core.records.fromarrays ` : #CODE
Your ` mat [: , j+1 ]` expression has not been corrected to handle a structured 1d array .
` mat2 ` looks just like the earlier ` mat ` , except the ` dtype ` is a little different ( with ` i4 ` instead of ` i1 ` fields ) #CODE
That is , I want to keep all rows that have 2 columns meet a condition : if the combination of date ( column ) and store ( column ) # are unique , keep row , other wise , drop .
If you want to take unique combinations from certain columns `' Col1 ' , ' Col2 '` #CODE
If you want to take unique combinations of all columns #CODE
Though when I import any module at the beginning of any .py file , it shows the corresponding methods / modules after pressing dot ( . ) , but in code , PyCharm does not show the list after pressing dot ( . ) .
How I can get all things after pressing dot ( . ) ?
How can I efficiently check many conditions and assign different ids where true ?
How to column stack arrays ignoring nan in Python ?
You can iterate over the lines and split them and process only the ones that have 3 elements : #CODE
` xA yA dyA ` for histogram A
` xB yB dyB ` for histogram B
In practice I want to obtain an histogram where the y values are the weighted average of the yA and yB values , with weights given by the inverse of their errors .
I would like to extract all the data from this file to combine , for instance , the m ( 5 ) histogram that I have in several files ( combine means do a weighted average , as stated above ) .
You get the following histogram
then reparse the list , so as to fill the array according the indices given by the list , with 1 ( or True ) .
Or , better just directly index into the output array with the row and column indices - #CODE
append column to a two dimensional variable
I want to append to it a column of size by 1 .
There isn't an ` append ` for sparse matrices .
` numpy ` ` append ` is just a fancy wrapper for ` np.concatenate ` .
Also , ` append ` does not change the array in place ( like the list append ) .
It best to just avoid it , thinking instead in terms ` concatenate ` .
efficient algorithmic method to concatenate rows of an array into a 1-D array
So far , for each individual array , I have to individually break it up into N arrays ( N = unknown number of rows ) and then individually concatenate them .
The advantage of ` reshape ` is that it doesn't copy the data , so it's fast , but since it's just a new view on the old data , changes to ` a ` will also change ` x ` .
` concatenate ` here will make a copy , but note that the items copied are again just views onto the original ` x ` , so there's only one copy per element .
Below is an example for understanding how the ordering works in reshape : #CODE
@USER : I've added an example that shows how the ordering works for ` reshape ` , which I think is what you want .
( Also , if you wanted the opposite behavior , you could first ` transpose ` and then ` reshape ` , all without copying that actual data . )
Nitpick : though in the case you show , ` reshape ` does indeed return a view , in general the ` reshape ` method * does * sometimes copy data , if the results can't be represented with a view .
( ` flatten ` , ` ravel ` , ` reshape ` all do essentially the same thing . ) #CODE
` concatenate ` can also be used to ` flatten ` an array , but this isn't the usual method .
` np.partition() ` ensures that values at particular indices are the same as they would be if the array were to be fully sorted ( e.g. with ` np.sort `) .
( The order of the values at the other indices is not guaranteed to be anything meaningful . )
Here , the indices you've passed are ` ( 1 , a.shape [ 1 ] -1 )` which is equivalent to ` ( 1 , 1 )` in this case .
I then try to index a portion of it using the range function as the indices by the following way :
I wonder why they gave the ability of broadcast integer arrays instead of using them as regular slicing .
I tried as below , and each append turns ` x_pos ` into a 1D array .
I want to avoid having to transpose the matrix all the time as it is an expensive operation
This will select all columns given by the indices ` y_pos_actual ` into a new matrix ` x_pos ` .
Why do you think that ` exp ( 0 )` would naturally be faster to compute than ` exp ( 100 )` ?
Maybe I can try stacking arrays in a way that limits number of stacking operations ( e.g. case of joining 4 1d arrays : first stack arrays 1 and 2 , then arrays 3 and 4 , and resulting arrays at the end ) .
You could also create ` x ` with the numpy outer product : #CODE
In this context ` append ` works just as well as ` vstack ` , and may be faster .
The key difference is that I am taking advantage of the fast list append , and the ability of ` array ` ( and ` vstack `) to take many items in its argument list .
While ` concatenate ` ( used by ` vstack `) is compiled , I suspect it does something similar to the iterative insertion .
i am trying to create a cython class which creates a NumPy with zeros .
I am struggling with the question how to create a NumPy of zeros ( 100 , 100 ) without any Python .
As a side comment , I would directly assign the zeros ndarray to ` eTest ` : #CODE
Finally , reshape ` b ` perform the elementwise multiplication : #CODE
Make numpy.sum() return a sum of matrices instead of a single number
Doing the same operation with Python's ` sum .
Then , I want to sum all the resulting matrices .
As can be seen there are no common indices between tensors .
The list above is a list of matrices so when you use sum , it will add the arrays .
It says that if you don't define an axis it will sum over all the dimensions .
well intuition says that in the expression ` [ i*matrix for i in arr ]` you are creating a new array for each ` i ` which then np.sum will sum over all arrays .
Alternatively , you can rearrange the np.newaxis positions to make axis=0 the one you want to sum over .
` sum ([ i*matrix for i in arr ])` creates a list of 2D array's then adds them together using ` np.add.reduce ` .
Do you want ` concatenate ` ?
First a negative step always means " stepping towards smaller indices " .
This gets a bit confusing when negative indices are used , but here , as elsewhere , one must think of negative step as " interpreted as ` n+step `" .
However , the ambiguity is in the fact that -1 is not larger than 5 in physical terms , but only in terms of indices .
I need the indices that sort b to a , so here : #CODE
So , all you need are the sorting indices for b .
This uses the ` argsort ` method to return the indices needed to sort ` b ` and ` a ` .
It's calling ` array ( arg )` but that yields an array like ` [ arg , ]` , it's ` shape == ( )` so ` dot ` tries to multiply the ` A ` instances together .
I would like to calculate for sum of each element multiplied by subsequent elements #CODE
Personal curiosity ( looked at the literature ) , what are the benefits of using this realibility over the more popular ones ?
Another way to see why this works : Start with the square of the sum , say ( a+b+c+d ) ^2 .
We want all the terms except the ones that just have the elements squared , so those need to be subtracted .
It looks like you want to get every combination of two elements ( pairs ) in that list , compute the product of each pair , and sum over these products : #CODE
Even creating a numpy array from a python list , @USER answer wipes the floor with the rest of us .
Stealing the logic from user2357112 and using it on a normal list with sum python is pretty darn efficient : #CODE
For changing the shape you can use ` shape ` or ` reshape `
As this is a two-dimensional matrix , the outer " box " marks the edges of the matrix , whereas the inner boxes are the rows of the matrix with the ` , ` separating the entries .
There is totally no reason to reshape the arrays you just created , just leave them as they are .
How can I sum across rows that have equal values in the first column of a numpy array ?
Group by the first column and then sum rowwise .
Seems like ` Approach #2 : cumsum + diff ` is performing quite well .
As a note , my target variable Y is log transformed .
Yes you would still need to apply some kind of threshold since even though the numbers may still be very small if you simply ask for the max your going to get something .
One approach would be to look at the bins with no signal to get an idea of the noise floor and then look for your signal to be sufficiently above that .
Ah , it's because I'm doing ' rfft ' instead of ' fft ' .
I don't need to divide by two if I use ' fft ' .
I read earlier in your post that the # of points in your fft was 128 .
You had mentioned putting a window on the time-domain signal before I fft it .
In your example case , your code is asking : Are there no indices where matrix_a is less than matrix_b ?
To see the weird edge case consider for example with ` a = zeros (( 3 , 4 )); b = zeros (( 3 , 4 )); b [ 0 , 0 ] = 1 `
` nonzero ` returns a tuple , with an array for each dimension .
So you could test for ' empty ' ` nonzero ` by looking at the length of one of those arrays .
Numpy seems to be following the same castings as builtin python ** , in this context it seems to be because of which return true for calls to ` nonzero ` .
I suspect wim knows this , and is asking why NumPy is using its own ` nonzero ` instead of Python's ` bool ` when the only point of letting ( 1 , ) -shape arrays respond to ` bool ` is to let them transparently act like scalars .
Python doesn't even _have_ a function called ` nonzero ` .
And your link is now even more misleading , because it's still mixing up Python's ` __nonzero__ ` and NumPy's ` nonzero ` , and it looks like a link that's relevant to the latter but is actually relevant to the former .
" What I don't understand is that ` nonzero() ` returns an empty array ( no nonzero element ) for ` None ` , ` [ None ]` and ` array ([ None ])` , then : since ` None ` " isn't a number " , I understand that you were saying that ` nonzero() ` is " true " for ` None ` , so it should indicate that it is non-zero , which is not the case ( empty array returned by ` nonzero() `) .
` f ` is a pointer to another struct ` PyArray_ArrFuncs ` that holds the array's ` nonzero ` function .
In other words , NumPy is going to call upon the array's own special ` nonzero ` function to check the Boolean value of that one element .
Determining whether an element is nonzero or not is obviously going to depend on the datatype of the element .
The code implementing the type-specific nonzero functions can be found in the " nonzero " section of the arraytypes.c.src file .
To understand the results of ` bool ( c )` and ` bool ( d )` , we see that the ` nonzero ` function for string type arrays is mapped to the ` STRING_nonzero ` function : #CODE
Perhaps it would make more sense for the nonzero function for strings to be implemented in terms of ` np.char.str_len ` rather than whitespace .
I have a multidimensional numpy array of shape ( 7 , 3 , 7 , 3 ) and I would like to modify the generalized diagonal in which axis 0 and axis 2 coincide .
This generalized diagonal would be defined as those elements of the array whose 0th and 2nd index coincide , and would have shape ( 3 , 3 , 7 ) .
I can access to the elements of the diagonal , but I cannot modify them ' in place ' , at least in version 1.8.2 of numpy .
Alternatively , I have tried indexing the diagonal elements directly [ with input taken from numpy.indices (( 7 , 3 , 7 , 3 ))] , but with no success .
How could I access the elements of the generalized diagonal to modify the original array in numpy 1.8.2 ?
One way to create such a generalized diagonal view is to use the ` as_strided ` function from the module ` numpy.lib.stride_tricks ` .
The stride for the axis associated with the diagonal of the two axes is the sum of the strides of those axes .
Create a view of the " diagonal " associated with axes 0 and 2 .
or simply by casting ( but it does not translate into pure C ): #CODE
I was just noodling around trying things out as the question caught my eye , but I'm entirely new to ` cython ` ( i.e. setup the environment today ! ) although some experience in both C and Python previously .
This scenario is useful as it allows a direct diff of the produced C code .
C code diff result
create a matrix from array of elements under diagonal in numpy
I would like to create a matrix using a list whose elements would be the elements of the matrix under the diagonal .
In the last line , the indices are reversed since your ` x1 ` is ordered for the upper triangle .
There are other things you can do to squeeze out a few extra % try replacing the ` ** 2 ` with self-multiplication or the ` sqrt ` with ` ** .5 ` , or ( probably best ) replacing the whole thing with ` np.hypot ` .
I have , currently , 3 arrays of data and I want to make a 2D histogram of the first two while using the third array as values that get summed up in each particular bin .
I also want to include a color bar that shows the scale of different colors you see in the histogram .
But I successfully created a histogram using your suggestion .
I thought I have " a [ 0 ]" and " sum " to be the same type , but do I ?
This problem is much more simplified than the full problem in which the index of a depends on all i , j , k , l and sum is no longer incremented by a constant , so I need to find out the root cause of this problem instead of moving a [ 0 ] out of the loops , etc .
Please compile with ` cython -a ` , then show us the C code that the ` a [ 0 ] += sum ` line turns into .
I'm going to guess that ` sum ` is living in a register and that ` a [ 0 ]` isn't .
You might have better luck if you just add ` sum ` to ` a [ 0 ]` once at the end of the whole function .
@USER .Robinson how is the index to your array ` a ` built based on the indices ` i , j , k , l ` ?
You say that a [ 0 ] += sum is the trouble line that makes the code slow .
So it's not that a [ 0 ] += sum is slow .
While multiplying large matrices ( say A and B , A.dot ( B )) , does numpy use spatial locality by computing the transpose of the B and using row wise multiplication , or does it access the elements of B in column-wise fashion which would lead to many cache misses .
Numpy dot is implemented in ` multiarraymodule.c ` as ` PyArray_MatrixProduct2 ` .
However , at least on my machine , doing this ( or just a dot product in general ) with a transpose and einsum is ten times slower than just using dot , because dot pushes to BLAS .
Often i use it to reshape my data , get an overview before starting to do data mining etc .
I am currently running Python's Numpy fft on 44100Hz audio samples which gives me a working frequency range of 0Hz - 22050Hz ( thanks Nyquist ) .
Once I use fft on those time domain values , I have 128 points in my fft spectrum giving me 172Hz for each frequency bin size .
I would like to tighten the frequency bin to 86Hz and still keep to only 128 fft points , instead of increasing my fft count to 256 through an adjustment on how I'm creating my samples .
My thought would be to run fft on any Hz values between 0Hz to 11025Hz only .
Currently the code I'm using to create my samples and then convert to fft is : #CODE
I've just added my code that I use to take the incoming audio from sample creation through to fft conversion .
The RMS meter is basically taking the square root of the sum of the squares of the output samples from the filter .
Please clarify whether you want to interpolate between your data points ( in other words , is one of the columns in your data file a " z " value ) , of if you want to do a 2D histogram .
you want to aggregate densely packed points by counting how many fall in a particular region ( 2D histogram ) .
I can plot 2D histogram very well ..
If you look into the source of ` cov ` ( #URL ) , everything passed in will be copied and converted to an ` np.array ` anyway .
So you don't save any memory by passing generators to ` cov ` .
I have very large matrix , so dont want to sum by going through each row and column .
At the end , you need to divide those summations by the number of ones in kernel , i.e. ` kernel.sum() ` as only those contributed to the summations .
The first peak , unlike all the other ones , consists of the same value twice ` -16329 , -16329 , ` .
You could also check the indices ` argrelextrema ` with ` np.greater_equal ` returns .
numpy squeeze along an axis wont work
I try to use numpy squeeze to remove an axis .
Thanks , your answer help me to find the issue , because it's raise me a attribute error with reshape !
Even numpy diff results in clearly incorrect values for an array of this dtype .
If you convert to signed integer by replacing the definition of ` p ` with the following : ` p = np.asarray ( I ) .astype ( int 8) ` then the results of diff are correct .
Actually , I moved the " transpose " call to im = ax.imshow ( I ) , i.e.im = ax.imshow ( I.transpose ( Image.FLIP_TOP_BOTTOM )) , and then i got the exactly same result as your image .
The use of unsigned integer in numpy routines like diff / gradient seems like a bit of a gotcha .
As suggested I've moved transpose into the imshow , changed to int8 and remove the minus from dy .
What is the best way to implement a function which takes an arbitrary number of 1d arrays and returns a tuple containing the indices of the matching values ( if any ) .
Does it always have unique values within each of those input arrays ?
The values are not sorted but unique yes .
` np.in1d ` takes time to sort and ` unique ` arrays , so it has overhead .
Then , I would like to sum these values .
The list comprehension should just produce a list of 10 arrays , which ` np.sum ` should sum pretty quickly .
Since ` special.eval_hermite ( nar [: , None , None ] , x )` produces a ` ( 10 , 50 , 50 )` , you just apply ` sum ` to axis 0 of that to produce the ` ( 50 , 50 )` .
Just apply ` sum ( axis=0 )` to that ` ( N , M , M )` result .
But if I have elements at the same position , it will stack multiple arrows on each other , i.e. if I have ` positions = [ 1 , 1 , 4 , 4 ]` and ` forces = [ 4 , 5 , 8 , 9 ]` , it will make two arrow at position 1 and two arrows at position 4 , on top of each other .
Instead , I want to sum the forces and only create one arrow at position 1 with force 4+5=9 and one arrow at position 4 with force 8+9=17 .
To me " sum elements in array " just means ` np.sum ( array )` but that's clearly not what this is about .
I didn't find any library in numpy that translate the MATLAB change of coordinates so I write them for my self .
You can gain a small amount of speed by factoring out some terms that are used more than once , e.g. ` xy2 = x**2 + y**2 ; elev = atan2 ( z , sqrt ( xy2 )); r = sqrt ( xy2 + z**2 )` .
Also ` cos ( elevation )` needs to be computed only once .
Unfortunately this doesn't work in my code ` gram_matrix [ i ] = vect ( x , Y ) ValueError : could not broadcast input array from shape ( 600 , 2 ) into shape ( 600 )`
What is the most efficient way to get log returns in numpy
What is the fastest and most elegant solution to building a sequence of log returns ?
for a function and simple array I can define the log returns as follows : #CODE
` np.log ` takes the log of every value in ` ar ` , and ` np.diff ` takes the difference between every consecutive pair of values .
How to take n-th order discrete sum of numpy array ( sum equivalent of numpy.diff )
I know that it is possible to take the n-th order discrete difference of a numpy array by using the numpy function ` numpy.diff() ` , but is there a way to do the same with the n-th order discrete sum ?
The expected result for the 1st order discrete sum would be : #CODE
So , basically you can sum these two versions of the input array to have a vectorized output in ` B ` , like so - #CODE
Or will it be better to extract indexes of ones from a row and store those in the memory .
If i store just index of ones , later i will again have to convert those indexes in 400 bits arrays .
To minimize overhead , you could dump the raw binary data from memory to disk with : #CODE
MATLAB find function translate to python - 3 input arguments version
that returns at most the last k indices corresponding to the nonzero entries of X .
Which is numpythonic way to translate ` find ` with 3 arguments ?
When comparing the sum values for diameter , to the sum values calculated by the program that processes the data , I have a tremendous error .
After searching the forums I think that this is due to numpy.sum taking the sum across the rows instead of the columns similar to this issue :
Can anyone shed some light on what I can do to get the column to sum correctly ?
try using ` pd.Series.sum ` instead of ` np.sum ` this will sum the columns
and would like to concatenate the ` data ` to the ` table ` according to the ` id ` .
We merge the left side on ' id ' and the right side on column ' 0 ' and perform an outer merge , we have to drop the ' 0 ' column as it's superfluous to what we want : #CODE
The ones currently defined are ( starting at line 4 8) : #CODE
The core math functions have the same API as the C99 ones , except for the npy_* prefix .
First it should be pointed out that ` argsort ` returns an array of indices that index data into a sorted array .
There are only " 2**16 " unique float16 values so you should never need to " sort " 1e9 of them .
You could use some ` broadcasting ` here to get all the indices of all those sliding windows in one go and thus with indexing achieve a ` vectorized solution ` .
I sometimes have to histogram discrete values with matplotlib .
In that case , the choice of the binning can be crucial : if you histogram [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] using 10 bins , one of the bins will have twice as many counts as the others .
This finds the unique values in your data ( ` np.unique `) , finds the differences between then ( ` np.diff `) .
The unique is needed so that you get no zero values .
I have the list of the indices for each row , i.e. [ 1 , 0 ] in this case .
Pretty straight forward , except that the deflection equation changes depending on what side of the point force you are on , so I will split the beam into two ranges , calculate the deflection values at each interval in the ranges and append the result to a list .
and you could easily load the first 6 lines to automatically get the lat and long to match your numpy array indices
As is said the you can get the max integer with ` sys.maxint ` then you can use ` np.random.randint ` to get a random number between the ` maxint ` and ` -maxint ` .
Polynomial Curve fittings with GNU Scientific Library ( GSL )
I'm going to use GNU Scientific Library ( GSL ) for solving Polynomial Curve fittings .
How to append numpy.array to other numpy.array ?
How can I append each new ` array ` in the next row of the ` matrix ` ?
Every time you append to an array it generates a new copy .
Why do you want the outer array to be a numpy array at all then ?
list does not have the downside of being copied in the memory everytime you use append .
` append ` just adds sets of 3 zeros to that , resulting in a ` ( 11 , )` array .
Now if you started with a 2 array with the right number of columns , and did concatenate on the 1st dimension you would get a multirow array .
A better way of doing an iterative construction like this is with list append #CODE
` mat ` is #CODE
` append ` would work , but it also requires ` axis=0 ` parameter , and 2 arrays .
It gets misused , often by mistaken analogy to the list append .
It is just another front end to ` concatenate ` .
But a 2d append wouldn't work , because an 2d array can't be ragged .
which yields the error " SyntaxError : ( unicode error ) ' unicodeescape ' codec can't decode bytes in position 2-3 : truncated \UXXXXXXXX escape "
The Unicode error sounds like you have some non-ascii characters in the filename , but without the full error stack it's hard to tell .
The entire error stack is File " " , line 3
SyntaxError : ( unicode error ) ' unicodeescape ' codec can't decode bytes in position 2-3 : truncated \UXXXXXXXX escape
The result of the reshape ?
@USER yes it is the result of the reshape .
If you really want to do it , you can try to get unique values of the first column and split the array for each unique value .
Rescale Python Array with user-defined min and max
I have a function that rescales an array to values between 0.0 and 1.0 , but I am stuck as to how to rescale the returned array to a user-defined new min / max range .
I know I will have to add an additional default min / max parameter , but I am fairly new to python , and programming in general , and I am at a loss of how to proceed .
Do you want take in the min / max parameters as arguments to the function or as user input ?
Hey David , I want to pass the min / max parameters to the function from the rescale call ., so something like * rescale ( numpy.arange ( 10 , dtype= np.float ) , 2 , 7 ) where 2-7 is the range of values for the array , rather than 0.0-1.0 in my code above .
Numpy linalg norm not accepting scalar when order is specified
I suppose ` norm ` * could * handle a scalar , and I don't know why it doesn't , but the docs * do * mention it .
I don't know how to broadcast this function to be estimated for this two dimensional array over ` M200 ` and ` conc ` .
What do you mean by ' broadcast this function ' ?
I assume 0 and 1 are the variable ones .
then you can determine the magnitude of the resulting vector ( sqrt ( a^2 + b^2 )) by #CODE
Find the max sum for each line and find max list and line number of maximum list count in python
I have list of diagonals now i want to find the sum for each list in each line .
Also how is the max 3 for the third line ?
The sum in your second line from the bottom should be 2 .
If you want the max of all the lines , we can use a generator expression to get the max from each line then use operator.itemgetter to get the max based on the second element of our tuple which is the max : #CODE
How to find the max when there are chunks of lines separated by blank line
Yes it works perfectly fine i want to ask that if there are more than one inputs in this file separated by balnk line then how to find the max upto a blank line ?
Yes and then again start from next nonempty line and find max upto next blank line
You would need to break on finding an empty line and get the max of all you have collected so far , then reset the container or max value and continue on again
I break this where blank line came but i am unable to find the max from the collected numbers
The ` numpy ` function ` correlate ` requires input arrays to be one-dimensional .
` correlate ( arr_one [ n , :] , arr_two [ m , :]) ` .
How to use numpy's einsum to take the dot product of an subarray ?
And I'd like to use einsum to non-iteratively take the dot product of each vector in ' a ' with a matrix : #CODE
AttributeError : ' module ' object has no attribute ' mat '
Use ` np.where ` to return the indices : #CODE
It's because only the ` x ` condition is passed then the return is the output from ` condition.nonzero() ` in this case ` ( a [: , 1 ]= =2 ) .nonzero() ` as to why ` nonzero ` returns an empty tuple that is because it returns a tuple for each dimension : #URL and in this case there is nothing to return
Numpy resize without modifying original object
I want to expand dimensions of two tables so I can use numpy's broadcast multiply on them .
However , the resize method doesn't have return value and it will modify the object itself .
However , the ` resize ` method doesn't have return value and it will modify the object itself .
Yes , but if you look at the docs for the ` resize ` method it gives you the answer : the ` resize ` function .
Depending on parameters it can end up using ` concatenate ` .
@USER : You may be right that ` resize ` isn't what he wants , in either form .
Looking further at ` resize ` , I'd say that it , in either form , is the wrong function to use when all you want to do is add are singleton dimensions .
` reshape ` is the correct function / method .
Compare the timings for the 2 functions - ` resize ` is much slower .
The in-place ` resize ` is fast : #CODE
But you can look at Christoph Gohlke's site and find wheels for all of the most popular ones .
In the first one , the sum is 40 and the len is 8 , and 40 / 8 = 5 .
As a side note , if you're interested in taking the mean of floats , there are other problems with just using ` sum ` / ` div ` .
For example , the mean of ` [ 1 , 2 , 1e200 , -1e200 ]` really ought to be 0.75 , but if you just do ` sum ` / ` div ` , you're going to get ` 0 ` .
Ok that is the problem , you have an overflow error , the max value of a signed 32 bit long is ` 2,147,483,647 ` , ` np.prod ( range ( 1 , 34 )) -> 3400198294675128320 ` .
Test the ` max ( abs ( signal ))` expression alone , and also the operator priorities in the ` assert ` .
Updated : Created unique fill value to avoid potential problems with ` None ` as list value .
` object() ` is unique : #CODE
In ether cases , the ` min ( ... )` expression is what we want .
( It might be better if you reduced it to a [ minimal , complete , verifiable example ] ( #URL ) , however . You're really just asking for help with your ` dot ` function , not your whole program . )
Now given the nature of how ' log_lower ' and ' log_upper ' is defined , there's overlap at the end of each row's ' log_upper ' and the subsequent row's ' log_lower ' , with those values being the same - does the append function in the loop or numpy.concatenate later take that into account when making the array and cancel one of the overlaps ?
You can do this by simply finding the indices where they are both positive : #CODE
I was interested in Jaime's suggestion to just using the boolean indexing without calling ` nonzero ` so I ran some timing tests .
The results are somewhat interesting since they advantage ratio is non-monotonic with the number of positive matches , but basically , at least for speed , it doesn't really matter which is used ( though ` nonzero ` is usually a bit faster , and can be about twice as fast ): #CODE
There is no shortcut that is going to accumulate ALL the possible x , y , z integers into a nice little array for you just because you know the ranges .
@USER Hey I am deleting my answer because the one in your link is better and comes to the same conclusion : the concatenate / meshgrid combo seems to get the best performance .
I guess you can reshape the array afterwards if you know the dimensions .
Frankly I would not serialize with numpy , my advice is to dump the lot into JSON and parse it at the other end ...
How to get indices from a list / ndarray ?
Now I want to find the indices of all the elements which are greater than 2 , so for the whole matrix , the indices should be : #CODE
These column and row arrays are broadcast together to produce a 2d array of coordinates #CODE
I have a lower diagonal matrix like this #CODE
in a text file , and I want to read it into a numpy array with zeros above the main diagonal .
which makes sense , because in the text file , there isn't anything in the upper diagonal part of the matrix , so numpy doesn't know what to interpret as missing values .
Extracting values from indices without using loops
@USER can I operate / broadcast the result I get from zip with other vectors / matrices ?
quick point on the list created . can I sum it with a vector / matrix ?
Using ` vector.flat ` to get iterator of ` [ 1 , 0 , 2 ]` ; ` matrix.T ` to get a transpose view of the current ` matrix ` , ` np.choose ` will use ` vector ` as an index array .
where() is capable of returning multiple indices but is significantly slower for my purpose .
Might be worth mentioning ` argwhere ` as well -- depends on whether NH needs the values to be usable as indices or as values in an array .
Revisiting my last comment -- this works for indexing or as a sequence of indices , but only because ` argmin ` returns just one value , even if the minimum occurs multiple times .
To be compatible with ` imshow() ` I need to either squeeze the array to be 28-by-28 ( removing the redundant dimension ) or replicate the 2D matrix ( 28-by-28 ) to 3 ( RGB ) or 4 ( RGBA ) channels .
` squeeze ` returns the squeezed array , it doesn't modify the one you pass in
You can pass squeeze or reshape : #CODE
You don't need the ` squeeze ` after using ` reshape ` .
How to extract a list of elements given by their indices from a numpy array efficiently ?
The elements that I need to take are given by their indices , for example : #CODE
you can simply assign the transpose of ` inds ` to ` i , j ` : #CODE
numpy multinomial function returns value error
ValueError ( ' sum ( pvals [: -1 ]) > 1.0 ' , )
However , I checked the sum of the input array val using ` val.sum() ` and it sums exactly to 1 .
I don't know if the multinomial function has some bug .
Alternatively , is there any other python module that I can use for sampling using multinomial distribution ?
Actually I had a case where the ` val ` summed to 0.9999996 , but it contained a negative element and due to this Multinomial function throws up the ` ValueError ` , which completely misleading of course because the sum never exceeded 1 .
Furthermore , when you ` concatenate() ` , there is no need for creating intermediate NumPy arrays : ` concatenate ([ 0 ] , indices , [ len ( a )])` is sufficient .
' vectorization ' is difficult with serial operations - ones where the calculation in step ` i ` depends on results from step ` i-1 ` .
find square roots , sum it
on ` myarray ` originating from my netCDF files before without any problem , but I get ` TypeError : tuple indices must be integers , not tuple ` and a crash with these weird ` -- ` -filled arrays ...
Not sure whether this is all that much more efficient , but you could ` pivot ` and then add the frame to its transpose , something like : #CODE
The third line ` df.add ( df.T , fill_value=0 ) .fillna ( 0 )` just adds the transpose to convert the triangular matrix to a symmetric matrix .
@USER 1 million out of 0.1 billion , 1% nonzero entries .
The basic idea is to use the nonzero coordinates of ` A ` to select rows and columns of ` U ` and ` V ` , and then use ` einsum ` to perform a subset of the possible dot products .
The use of ` dot ` in ` foo ` would have computed too many products , ` ij , jk- ik ` as opposed to ` ij , ji- i ` .
you could always make a container class that overloads ` __getitem__ ` to do this , at the cos of not being able to use tuples as indexes in your nested dicts
The ` scipy.sparse ` ` dok ` format is a dictionary , which uses tuples of indices as keys .
Using the diagonal structure , as detailed in this answer regarding " Construct adjacency matrix in MATLAB " , I create only the upper diagonals and add them in the appropriate positions to a sparse diagonal matrix using scipy.sparse.diags .
This sparse matrix is added to its transpose to give us the adjacency matrix .
The basic rule is that it doesn't create a copy unless you're doing indexing with either an array of indices ( e.g. ` A [[ 1 , 2 , 4 ]]`) or with a boolean array ( e.g. ` A [[ True , False , True ]]`) .
Possibly depends what columns was - if it was a list of indices I think that's expected .
oh it was a list of indices ...
You can reshape the input array to force it to be a ` M x N ` dimensional array , where ` M ` is the number of elements for the first dimension .
Then , slice it to get the first column and sum all its elements .
if ` det ( b )= 0 ` , even due to precision loss , it means your problem is not well defined .
That is was I though the conjugate gradient method would be useful ( it's an iterative method to solve systems ) , but it doesn't run either .
They are the last ones for both Scipy and Numpy .
Since we are summing the ` a-th ` index across all three input arrays , one can have three different methods to sum along the a-th index .
For this you need a norm object that scales the z-values to the range [ 0 , 1 ] .
After interpolating data to a target grid i am not able to reshape my data to to match the original shape .
I am not sure if using mgrid , meshgrid or reshape if the right way to solve this problem .
You're after ` np.roll ` , this will roll elements pass the end and wrap them back to the beginning along an axis : #CODE
The following works : ` numpy.mean ( numpy.array ( a2 ) , axis =( 0 , 1 ))` ( with ` a2 [ a2.mask ]= ` first if another value than 0 is required to replace the masked ones ) .
I tried different solutions and found using numba to calculate the sum of the product leads to better performances : #CODE
Using the numba version of the sum function ( sum_opt ) performs very well .
Whereas , it looks like autojit creates a compiled function for each unique set of argument types it gets called with .
If you don't need the extra zeros around the boundaries of the output arrays , you could simply use the outputs from ` signal.convolve2d ` as they are , which must further boost up the performance .
Is there any method like ` np.ravel() ` to flatten this array ?
Then ` np.where ` returns the indices of the non-zero elements of this row , which in this case is every element : #CODE
Indexing as ` a [ indices ]` returns you the rows corresponding to the values in ` indices ` , which in this case are the first five rows .
Then I want to obtain unique values of the panel data without the ` NaN ` entry .
I then want a unique vector of all elements , excluding ` NaN ` .
6 columns , 92370574 rows , 2496502 locations , 37 months each , unique amounts for each value .
How to export DataFrame to_json in append mode - Python Pandas ?
I want to save " df " with to_json to append it to file output.json : #CODE
* There is append mode= ' a ' for to_csv , but not for to_json really .
The existing file output.json can be huge ( say Tetabytes ) , is it possible to append the new dataframe result without loading the file ?
No , you can't append to a json file without re-writing the whole file using ` pandas ` or the ` json ` module .
Edited : Explained the reasoning behind the min values :
2 is the min
5 is the min
notice that 9 isn't min are there's a 2 window to its left and right with a smaller value ( 2 )
However , @USER ' s answer is better since it will include both of two consecutive equal minimum values if they have different indices in the original list !
Which suggest that numbers at position 4 and 8 ( ` 2 ` and ` 5 `) are the smallest ones in within a 2 size neighbourhood .
The numbers at boundaries ( ` 5 ` and ` 2 `) are not detected since ` argrelextrema ` only supports ` clip ` or ` wrap ` boundary conditions .
after transpose #CODE
I want to add values to it at the given indices , where indices can be duplicates .
I'd like this to work with multidimensional arrays and broadcastable indices and all that .
You need to use ` np.add.at ` to get around the buffering issue that you encounter with ` += ` ( values are not accumulated at repeated indices ) .
Specify the array , the indices , and the values to add in place at those indices : #CODE
The operation you are performing can be looked at as binning , and to be technically more specific , you are doing weighted bining with those values being the weights and the indices being the bins .
Why not leave the data alone in a global and pass the index on the stack ?
This would mean having lots of extra meta information where referenced indices point to that might use more memory than a copy .
You have choice of when to apply the ` index ` , now or further down the calling stack .
Are you making a histogram ?
When I try to calculate ` det ( A )` I get ` 0.0144 ` , which is correct .
As I said , I tried to solve the system with the conjugate gradient method too and the same occurs .
If that is the problem , the Conjugate gradient method shouldn't be able to solve this , since that is its purpose ?
But the conjugate gradient method should require less RAM , doesn't it , @USER ?
I was also not able to find a numpy version of the conjugate gradient method .
In my machine I get the det ( a ) ( det ( A ) , indeed ) == 0.0144 .
Numpy compare 2 array shape , if different , append 0 to match shape
Numpy provides an append function to add values to an array : see here for details .
As you have already the information which of your arrays is the smaller one , just add the desired number of zeroes with creating a zero filled array first by numpy.zeroes and then append it to your target array .
It might be necessary to flatten your array first and then to reshape it .
diff = y - x
zero_array = np.zeros ( diff , 16 )
zero_array = np.zeros ( diff , 16 )
np.zeros ( diff , 16 ) seems odd . diff is the difference , so much I've understood .
I assume that your matrices have always the same number of columns , if not you can similarly use ` hstack ` to stack them horizontally .
Converts a tuple of index arrays into an array of flat indices , applying boundary modes to the multi-index .
Those ones will not be interpolated , will they ?
How can I speed up Cython code to compute conditional log likelihood of dirichlet ?
I have a function that computes the conditional ( on kth alpha ) log likelihood of a dirichlet distribution .
That said , use ` np.sum ` instead of the built-in ` sum ` in high performance numpy code .
Note that almost all speedup comes from using ` np.sum ` over the built-in ` sum ` .
Here's an efficient approach based on zeros padding to the input array .
Like in the answer below but symmetrical and with diagonal filled with ones .
To convert this into a symmetrical square matrix with ones in the diagonal : #CODE
Do the same steps to your std dev data if it is not already in a matrix form .
Do you know how to add the upper triangular part and add ones on the diagonal ?
Now I want to use only Theano variables but I don't know how to translate the previous code because I'm using "` for s in xrange ( nS )`" to add a dimension to the resulting variables .
Note 1 Changed ` convolve ` to ` correlate ` so that the code considers ` v ` the right way round .
Isn't array of only zeros good by this definition ?
You want to consider all shifts of the second array by 0 , .., h indices to the right and take the inner products with the first array
@USER Yes that is right , except it is shifts of the second array by 0 , .., h-1 indices so that there are h shifts in total .
@USER I want to find a long array so that NO non-zero short array gives all zeros .
" but in fact the code checks if ALL such vectors have a non-zero dot product .
Additionally , you really want to know whether this space intersects the nonzero vertices of the {-1 , 0 , 1 } hypercube .
@USER if the dimensions are large , any two randomly generated vectors are nearly almost diagonal to each other .
You will find 99% of the vector pairs you generate are within 5 degrees of being diagonal of each other , or something similar .
Another thing would be from smaller n to guess information about the structure of vectors that are good and to use this to bias the sampling in order to favour good vectors ( for instance , good vectors may contain fewer zeros )
I'm using bit operations to calculate dot products , encoding the resulting vector as a long integer , and using unordered_set to detect duplicates , taking an early exit from a given long vector when a duplicate vector of dot products is found .
This has the option to use either the edge values or zeros for comparison at the edge of the array .
Looking at the code for ` roll ` , it generates an index like your ` yf ` , and then uses ` take ` .
Although I guess if ` roll ` uses ` np.ix_ ` indexing then there probably isn't a faster alternative though
Numpy and dot product on 3D matrices
You can select from ` xx ` and ` yy ` individually , and stack the two arrays together : #CODE
It's having problems matching the ` usecols ` , the converters ids , ` names ` and maybe the dtype .
I am trying to use Cython to speed up a Pandas DataFrame computation which is relatively simple : iterating over each row in the DataFrame , add that row to itself and to all remaining rows in the DataFrame , sum these across each row , and yield the list of these sums .
My feeling is that you won't gain a huge amount - most of the work is either in the ( vectorised , float+array ) addition , or in the sum .
You could get a ( non-Cython based ) speed-up by doing the ` sum ( axis=1 )` once outside the loop .
` span_y ` is a list of 1D-arrays so ` min ` doesn't work as you expect and returns a function .
Pass your key function ( lambda ) in ` min ` as a named parameter as said in documention .
If the indices are increasing ( as appears to be from your example ) , you could use ` itertools.groupby ` on an ` enumerate ` of the list .
You can tell that the first item is the group , and the second is a list of indices .
You need to define the corresponding row indices .
You can calculate the linear indices from ` y ` and then use those to extract specific elements from ` x ` , like so - #CODE
The numpy.diff solution will still return a diff going from -1 to 0 or 1 to 0 , counting those as a zero crossing , while your iterative solutions don't because they use the product being less than zero as their criterion .
This version avoids the problem with zeros : #CODE
With the option to count zeros as well as crossings ( or not ) by using either ` < 0 ` or ` <= 0 ` .
I want to find the 1st and 2nd largest eigenvalues of a big , sparse and symmetric matrix ( in python ) . scipy.sparse.linalg.eigsh with k=2 gives the second largest eigenvalue with respect to the absolute value - so it's not a good solution .
I will later use vstack() to join the arrays , if the shape doesn't match vstack ( ) does not work proeperly
AttributeError : ' numpy.float64 ' object has no attribute ' sqrt '
How do I append a list to an empty numpy array
At the end of certain number of iterations ( say 1 million ) I want to append the list to a numpy array , then empty the list and continue the process .
I have tried " concatenate " and " vstack " , but the problem is the dimension for a ( when empty ) and b doesn't match , so the code gives error .
The list is going to be as big as 1 million , so need an cost effective method to deal with the append .
Is there any way I can append a list to a numpy array without performing the check .
If you want to append to an empty array , you have to specify its dimensions .
In this one I am creating a ` numpy ` array just to sum it in one axis and discard the array after .
You can use ` dot ` for multiplication of matrices in higher dimensions but the running indices must be the last two .
In the end , I concatenate all the results and return them .
Array3 for example shows the absolute difference between every value in column 3 with every other value in column 3 ( 26 by 26 array ) .
14 seconds now instead of 28 min !!!
Unlike some of the references ( such as array and mask indices ) assignments are always made to the original data in the array ( indeed , nothing else would make sense ! ) .
My goal is then to make a groupby ` ids ` , and sum the 1 for every category column , as follows : #CODE
But then ` df3 = df2.groupby ( ' ids ') .sum() ` is very quick , although it takes indeed quite a lot of memory ( c . 6Go ) .
a solution may be to avoid it by sorting data on ids , finding where each id starts and stops , and then just .sum ( 0 ) on slices of the sparse matrix
Just to expand a bit , sorting would be ` df.sort ( ' ids ' , inplace = True )` and ` df.groupby ( ' ids ') .size() ` would tell you how many rows to slice for each successive ` id ` in your sparse matrix .
Nevertheless , the matrix you obtain does not look as a get_dummies one ( the index should be df [ ' ids '] .unique() , and the columns should be one per distinct cat values .
I edited the code so that you get ids as rows and categories as columns .
I create the final dataframe with 0s and populate it : with ` name ` , I iterate over ids and for each id / name : instruction ` df_final.loc [ cat , name ] = 1 ` writes 1 for categories relevant to this id in the column dedicated to this id .
Finally I just transpose the dataframe to get ids as rows and categories as columns .
It came out with " E --- function --- etc " just stack wouldnt let me print the whole statement
' ( 300 , 300 , 3 )' I would like to reshape it to be only 300x300 .
PS if you generate E with ( 0 , 1 , 1 ) and run it on x = y = img then it will just return sum ( img ) - sum ( img * img ) the matplotlib site says it converts RGB uint8 to float32 scaled 0.0 to 1.0
Numpy concatenate 2D arrays with 1D array
I am trying to concatenate 4 arrays , one 1D array of shape ( 78427 , ) and 3 2D array of shape ( 78427 , 375 / 81 / 103 ) .
I tried to transpose the 1D array before concatenating it , but that also didn't work .
Any help on what's the right method to concatenate these arrays would be appreciated !
The " secret " here is to reshape using the known , common dimension in one axis , and -1 for the other , and it automatically matches the size ( creating a new axis if needed ) .
Convert einsum computation to dot product to be used in Theano
You can use the concatenate function to do this #CODE
For your case , I think using ` append ` is " better " since it is less error prone as it accepts scalars as well , ( as my own mistake clearly shows ! ) and ( arguably ) slightly more readable .
Also , I think ` append ` is probably a better answer .
But knowing about ` concatenate ` might be useful if you have multiple smaller arrays .
I don't like ` append ` because so many beginners misuse it .
@USER , What kind of abuse are you thinking of that would have been better solved by e.g. concatenate ?
2 uses come to mind - assuming it is an inplace function like the list append , and using it repeatedly in a loop , again like the list method .
For the loop example , is it actually better to append the result to a ordinary list , and then append that to the array ?
In case it isn't clear - ` append ` returns a new array .
It is an alternative way of using ` concatenate ` .
hstack has the advantage that it can concatenate as many arrays / lists as you want ( e.g. ` np.hstack (( a , b , a , b , [ 0 , 2 , 4 , 6 , 8]) `))
Is there a way to " fully " compare the hstack and append results ?
Like I said , I wanted to find out whether hstack and append really produce identical results .
` hstack ` and ` append ` are both coded in Python , so you can study the code .
Both end up using the compiled ` concatenate ` .
When I convert a numpy array via ctypes to an int pointer some values are lost and additional zeros are added .
So , what seems to be happening is that you're cutting each number in half -- the first half contains the bits to construct the number ( because you don't have any numbers bigger than maxint ) and the second half is all zeros .
Is there a way to generate a list of indices using numpy
Can I use numpy to generate repeating patterns of indices for example .
Just to further clarify I am generating indices to triangles in opengl in various patterns .
I could reshape the final array needs to be flat but its always repeating lengths , like 0 , 1 , 2 , 1 , 2 , 3 for the first quad and 4 , 5 , 6 , 5 , 6 , 7 for the second quad . the other example it repeats in sets of 9 points so maybe that could work .
I'm not sure I understand exactly what you mean , but the following is what I use to generate unique indices for 3D points ; #CODE
this is what i am after but not in sequential order , in a repeating order but not one after the other because that repeats indices and points . also you could probably us np.arrange and skip generating the list if you putting them in order in this way .
This codes returns a list of * unique * points and an array of indices of all points into the array of unique points .
I am not 100% certain this is what you're after , I think you can achieve this using pair of ` range ` values and increment ` n ` times 3 ( the gap between each group ) , then use ` numpy.concatenate ` to concatenate the final array , like this : #CODE
thanks for the example its an interesting way of doing it , I will have a play around with it and look into concatenate a bit more as I dont know much about that method .
@USER , yes have a play around and I hope this helps , ` numpy.concatenate ` basically with join a sequence of arrays together .
Sum rows of numpy array where start index of each sum comes from another array
I want a new length ` M ` array where the ith element is ` sum ( data [ i ] [ start_indices [ i ]: ])` .
^ indeed .. but the zip operation is not really needed though , might as well index the arrays without making new ones
though using a mask array here may be inefficient and iterate too many indices .
Besides the zip iteration ( several forms ) and the masked sum , ` cumsum ` might be worth testing #CODE
This is a test expression , and doesn't take into account the time it takes to generate the ` indices ` list #CODE
I want to get the element that divides the array evenly , i.e the sum of array before the element equals the sum of the array after it .
First , we build the cumulative sum of ` a ` .
Cumsum means that ` c [ i ] = sum ( a [: i ])` .
Than we look where the absolute value the difference between the values and the total weight becomes minimal .
You should store the ` cumsum ` in variable , and use ` c [ -1 ]` instead of ` sum ` for speed .
If all input values are nonnegative , it seems likely that one of the most efficient ways to do this would be to build a cumulative sum array , then binary search it for the location with half the sum on either side .
chw21's second solution , the one based on explicitly minimizing the absolute difference between the left and right sums , is much easier to reason about and more generally applicable .
Is there something like a simpe function to get the indices as a tuple from a numpy array in which I don't need to have for-loops iterate through the array ?
Now after this , I want to also make an array with the length of the ' indices ' array containing " XYZ coordinates " as in each element containing the XY ' indices ' and a Z Value from ' arr ' .
For your indices : #CODE
Then you can stack the indices with the original array along the 2nd dimension : #CODE
what happens if M1 and M2 have different sizes than the ones you use here ?
You could perhaps use ` np.multiply.outer ` instead of ` np.outer ` to get the required outer product : #CODE
I have a numpy matrix A and I need a function that will count ( A [ i , j ] / sum of all elements in i-th column ) - A [ i , j ] / sum of all elements in j-th row
I've looked at ` clip ` , but it does not quite do what I need it to .
You may use ` np.searchsorted ` to obtain the indices : #CODE
@USER there is no problem with dot but i have to it without using this functions , i think if i make the matrix values int64 it will be ok but i dont know how to do it !!
` transpose ` is a method , you need to call it .
You can also reshape to a 1D array and the native ` map ` should just work .
Then you can use reshape again to restore the original dimensions .
I have an array of datetime objects and I'd like to histogram them in Python .
The Numpy histogram method doesn't accept datetimes , the error thrown is #CODE
When ` dt_array ` is your array of ` datetime ` objects , this would give you the histogram : #CODE
Here I suspect one ` index ` matches on ids , the other tries an ` == ` test .
Or ` [ id ( b )= =id ( x ) for x in l ] .index ( True )` if you want to ensure it compares ids .
I have an ndarray where each row is a separate histogram .
It employs np.where to find all the indices of a matrix meeting a certain criteria .
I am trying to use indices stored in one set of arrays ( ` indexPositions `) to perform a simple array operation using a matrix .
` indexPositions [: , 1 ]` contains the ' lower bound ' indices and ` indexPositions [: , 2 ]` contains the ' upper bound ' indices .
This reflects the fact that I want to set to zero anything in between them and therefore want to iterate between these indices .
So instead of one complex slice , you have to create 2 or more simple ones .
How can I select values along an axis of an nD array with an ( n-1 ) D array of indices of that axis ?
Also , the actual masked int values become min int , i.e. #CODE
Is not the key of the question , but your " edge detection " is simply the gradient magnitude .
One approach would be to get the remaining row indices with ` np.setdiff1d ` and then use those row indices to get the desired output - #CODE
PS : For the curious ones , this is a variant of the prize-winning solution to the famous NetFlix million prize problem .
Note that the result you want resides in the diagonal of the dot product between matrices .
Note that independent sets are made by indexes in both rows and columns that are unique .
So as both rows and columns need to be unique , we end up with 2 sets of tuples : #CODE
But , the cost of finding independent set might be higher than doing all your sequential updates if your updates are likely to be dependent in previous ones .
Now ` idx ` contains the positions of rows_idx and cols_idx that are ` unique ` , hopefully this can reduce your computational cost a lot .
Thus , for the first set of values , e = Entry [ 0 ] -U [ 1 , :] *M [: , 2 ] ( dot product ) , then U [ 1 , :] is updated .
You can try the numpy ` unique ` approach at the end of the edit to see * roughly * ( because it finds only completely ` unique ` rows and columns sets ) if the approach would work for you .
I've tried a couple obvious alternatives , including `'' .join ( ' prefix ' , str ( day ) , ' _ ' , s )` , `' prefix{ : }_ ' .format ( day )` , and using ` numpy.core.defchararray.add ` to concatenate a prefix array created via ` numpy.tile ` with ` signals ` .
EDIT from info pointed out in the interesing comments , I add another solution : inside of the tight loop we are only joining two strings together , so we can concatenate them directly instead of %-formatting #CODE
@USER you are right , to join only two strings it is more efficient to concatenate them instead of % formatting .
How can I reshape the Image so that it is compatible with ` Image.fromarray ` ?
You don't need to reshape .
How to reshape numpy image ?
I have an image as a numpy array with shape ` ( channels , height , width )` how can I reshape it so that it has shape ` ( height , width , channels )` ?
I want to append a 375th artist class with a string " other artists " .
AttributeError : ' numpy.ndarray ' object has no attribute ' append '
I saw found this problem a few times on stackoverflow , to which the answer in one case was to use concatenate instead of append .
It seems to be a problem that the datatype does not match the datatype that I , trying to append / concatenate , which would be of type string .
Any advice on how I can setup the " other artists " string so that I can append it to C_ClfGtLabels ?
I just want to append the string " other artists " to the array .
A quick workaround is to convert your ` C_ClfGtLabels ` into a list first , append , and convert it back into an ` ndarray ` #CODE
The max works fine , but the min gives me 0.0 .
I played around with this a little and as best I can tell there is not any fast way to build a large , sparse dataframe ( even one full of zeros ) column by column , rather than all at once ( which is not going to be memory efficient ) .
join function of a numpy array composed of string
I'm trying to use the ` join ` function on a numpy array composed of only strings ( representing binary floats ) to get the joined string in order to use the ` numpy.fromstring ` function , but the ` join ` function doesn't seem to work properly .
As you can see , using the join function on the list ( ` binary_list `) works properly , but on the equivalent numpy array ( ` binary_split_array `) it doesn't : we can see the string returned is only 72 characters long instead of 80 .
I found an alternative way ( I should know how to use the search bar by now ... ) to join using the ` tostring() ` function .
But any idea on why ` join ` doesn't work ?
Now , I want to just take the average of Column [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 ] from new.csv and append it to another file or this file .
Apologize for the lengthy description , but I can't seem to append the columns in python .
This will join the rows and write them to a new csv : #CODE
I have written a function which contains nested loops and a conditional statement ; the purpose of the loop is to return a list of indices for the nearest elements in array x when compared to array y .
For example x = np.array ([ 1.1 , 2.3 , 5.9 , 8.5 ]) , y = np.array ([ 0.2 , 5.5 , 12 ]) and idx = np.zeros ( np.shape ( y )) should return idx = [ 0 , 2 , 3 ] ( the indices of the closest items in x to the items in y .
It ran around 270x faster on my dataset ( finding around 1e4 indices in a set of around 1e6 items ) .
With my code the complexity is O ( ` ( m+n ) log ( m )`) and additional memory usage in the order of O ( ` m+n `) ( I think .. not sure about the sorting ) .
( in python multiplication / division is default index by index - no dot needed )
Whether that means multiplying by a slice of the array , indexing the array using your input values as indices , or something completely different , it has to be something NumPy can do for you in C , or NumPy isn't going to help you .
These numbers represent the grey values of a strip of 14 pixels , I would like to fit a curve to the distribution and save the x location of the vertex .
For each row of pixels , I would like to plot a curve to the data and append the x-location of the vertex to a growing list .
get the max of them ?
Get forward and backward derivatives ( their absolute value as we care about magnitude of differences not direction ): #CODE
The hardcoded ` 5 ` for the example could be replaced by the ` mean ` or the ` median ` + ` std ` or something .
Adjust aspect ratio of a histogram with square-shaped images with matplotlib
I'm trying to make a histogram of an array by replacing each count by a square-shaped image instead of a simple bar / step / etc .
What I would like to do is to automatically set the aspect ratio and zoom factor of the ` OffsetImage ` so that the images align in a nicely packed way ( no overlap , but not too much space between adjacent ones ) like the following image .
Convert indices to vectors in Numpy
You can pass the list of indices ` ind ` as a Python list or a NumPy array : #CODE
This will create your data matrix ( of zeros ) in disk .
If not replace ` cx.row ` , ` cx.col ` and ` cx.data ` from the ones provided by your matrix representation ( should be something like it ) .
save numpy array in append mode
I am curious why there is no append mode for ` np.save ` .
It looks , like .minimize is for problem like F =( f1 , f2 ,.. ) -> min ( that is more seems to be ( df / dx=0 ) problem ) , not for equation systems , as represented above .
In general , log and exp functions should be roughly the same speed .
Why do you think that log and exp should be " roughly the same speed " ?
E.g. , an algebraic identity can be used to transform the original exp .
Here are some SO links : [ Using strides for an efficient moving average filter ] ( #URL ) , [ Python - vectorizing a sliding window ] ( #URL ) - a couple of the answers look relevant , [ Divide an image into 5x5 blocks in python and compute histogram for each block ] ( #URL ) , [ Elements arrangement in a numpy array ] ( #URL ) - the ``` extract_patches ``` answer would work .
It will sum up all two consecutive rows and in the end multiply every element by ` 0.5 ` .
The general form of the average over ` n ` elements is ` sum ([ x1 , x2 ,..., xn ]) / n ` .
The sum of elements ` m ` to ` m+n ` in vector ` v ` is the same as subtracting the ` m-1 ` th element from the ` m+n ` th element of ` cumsum ( v )` .
Changing numpy array with array of indices
and I want to change one value in each row with ones according to another array ` N= np.array ([ 7 , 2 , 9 , 4 , 5 ])`
Since you want to set a single element per row , you need to fancy-index the first axis using ` arange ( 5 )` . this can be thought of as setting indices ` ( I0 [ 0 ] , N [ 0 ])= ( 0 , 7 )` , ` ( I0 [ 1 ] , N [ 1 ])= ( 1 , 2 )` , ...
Because Panda's cov formula calls np.cov , there shouldn't be a substantial difference
The resulting ' mat ' object is also of type int64 .
So , what you'r trying to do will never give zeros array for the simple reason that ` np.einsum ` has a more precise floating point than ` np.dot() ` ( because of the positive sign of the first ' result )
I don't uunderstand this sentence " So , what you'r trying to do will never give zeros array for the simple reason that np.einsum has a more precise floating point than np.dot() ( because of the positive sign of the first ' result )" .
Because it's attempting to implement a binomial lattice , used in option pricing .
@USER " log() , cumsum() and exp() " - Do you mean log , rolling_mean , and exp ?
Speaking of tired , [ this page ] ( #URL ) might help you figure out the numerical stability of your log , rolling-sum , exp scheme , but I'm too tired to go through it .
Here's the original exp / sum / log version : #CODE
and so on for a lot of zeros .
How to compute item histogram for a column for each particular id group ?
I would like to have a numpy array where each row is the histogram of event counts per ID , like ;
In my very slow implementation , I iterate all the unique ids , find the corresponding rows R , iterate all the events E , count the number of occurrence of that event in R , place the count into row of that id in my numpy array .
For ` csr ` the indices are a bit obscure .
Or rather , the column index for each nonzero value is present in ` M.indices ` , but it takes a bit of calculation to determine which ones belong to which row .
You can also take ` nonzero ` for the whole matrix #CODE
To make this short , I want sum all the elements in a " row " of a matrix I'm reading in .
I want to sum all elements of each row excluding index 0 ( the 4 digit numbers ) .
this should return a list which contain the sum of all rows
Well , I was surprised too or I haven't searched the stack well enough .
Assuming you want to align all the arrays to the left , and pad to the right with zeros , then you could first find the maximum length with #CODE
and then pad using ` zeros ` : #CODE
HOG splits the image in ` M x N ` windows of size ` m x n ` each and calculates a histogram oriented gradients with fixed ` W ` number of bins ( number of orientations ) in that window .
Combine all the features of an image in one , this is , perform an average ( or weighted average or norm ) over the ` K ` features to end up with a vector of size ` W ` for each image ( the number of orientations ) .
To preserve ( more or less ) the spatial relationship of the features , another more common approach is to concatenate all the features in order to end up with a flattened 1D vector of size ` Z ` , with ` Z = K x W ` /
If I understand what you're after you can just create arrays from the lists and compare directly , you can then get the count by calling ` sum ` : #CODE
when you call ` sum ` on this the ` True ` values are cast to ` 1 ` and the ` False ` are cast to ` 0 ` allowing you to sum the ` True ` values
the top-level ` sum ` is significantly slower which is to be expected .
But when I run it for a very large dataset , then coverting the whole np.array into sets and then performing a.itersection ( b ) , give approximately 10 times better performance when compared to sum ( a == b )
unique ( and other numpy functions ) on timezone naive pandas columns convert to local timezone
My work around is to use strings to do the unique , but I am constantly running into these problems ... and would value suggested working practise ( eg always set UTC ? )
So I would expect unique to return a na ve datetime rather than interpret as local ...
I need two matricies to be output : ` S ` and ` T ` , such that ` X ` is the sum of all positive values in ` X ` and ` Y ` , and ` T ` is the sum of all negative values in ` X ` and ` Y ` .
As well as ` clip ` you can do this by multiplying by boolean arrays : #CODE
Probably ` clip ` is better .
Deep in matplotib source code in lines.py , line 730 or so , it checks markevery and plots accordingly ; however , if markevery is not in the visible indices , the Exception occurrs .
` inv ` is ubiquitous enough to warrant this effort , whereas out-of-the-box arbitrary powers are not .
The gradient may not be that sharp as the colours would suppose .
At first , your ` result ` does not look like a complex FFT output , because you calculated the absolute values of the FFT .
The energy of a signal is the sum of the squared amplitudes ... alternatively you can write it as #CODE
I want say thank you koffein because it not only solved my problem but also get me on the right track to calculate spectrum energy , i used think total energy is sum of magnitude , but it's actually sum of magnitude^2
I am trying to use python generators to do the chunking , but the need to continually recreate the spent generator within the outer for loop is killing my runtime .
Note that in order to implement a generator solution to this problem , I have to continually re-create ychunk_gen within the outer loop .
Have you tried list comprehensions instead of generator expressions , and creating both chunk lists before the outer loop ?
What I am doing is computing pairwise distances , and updating a histogram .
I assume ` x ` and ` y ` are ` numpy ` arrays , so you can reshape the arrays and then loop through every line : #CODE
In #URL I read that if you wanted the data not to be copied by ` reshape ` , you should change the ` shape ` -property of the data : #CODE
It looks like it doesn't , since reshape has hard requirements on array length preservation , but maybe there's something I don't understand ?
Since Cython 0.13 , some generator expressions are supported when they can be transformed into inlined loops in combination with builtins , e.g. sum ( x*2 for x in seq ) .
In total it has to write n ones , so you are in ` O ( n )` .
Now we can use ` cumsum ` to compute a cumulative sum for each column ( False is treated as 0 , True as 1 ): #CODE
You could also use the dot product .
I am trying to call a python function that takes an absolute path as an argument , but the file I want to reference is on the web .
But it seems that zeros values are included in calculations somehow .
Because , in another test I simply assign zeros instead of masked value and results are the same .
It does accept ND arrays , you just have to flatten them beforehand , #CODE
What is the easiest and most practical way to determine if this operation is executed as a Hadamard ( elementwise ) or dot product ( pointwise ) operation ?
For a dot product only the column size of A must be the same as the row size of B , correct ?
I'm looking for the most memory-efficient way to compute the absolute squared value of a complex numpy ndarray
where we exploited the property of the abs of a complex number , i.e. ` abs ( z ) = sqrt ( z* z.conjugate )` , so that ` abs ( z ) **2 = z* z.conjugate `
I found on the matlab homepage the following example ( #URL ) that the result of the dot product is
The argument you're referring to is the seed ; it should preferrably be unique to each function call , since if it's called with the same seed twice , it will generate the exact same sequence of numbers .
I've done some searching on indexing and for looping in python , but I'm having difficulty understanding how to reference specific indices and modifying them within the for loop .
A bit of explanation : Each row of ` positions_to_overwrite ` represents the column indices of ` donors ` and ` recips ` .
Now , both ` donors ` and ` recips ` represent the row indices that are to be manipulated .
fft bandpass filter in python
How to add a numpy array A to elements of a numpy array B with indices given by an index array C ?
` add.at ` does what ` += ` does , but with repeated indices handled the way you want .
to concatenate the two arrays into a bigger array .
How can I concatenate such two big numpy 2D arrays ?
` concatenate ` and ` hstack ` do the exact same thing .
Write ` X1 ` and ` X2 ` out to disk , concatenate on disk , and read them back in .
Convert ` a ` , list , to NumPy array , then flatten it to array elements using ` ravel() ` , then convert to list using ` tolist() ` .
Now , each distribution returns a specific set of parameters in ps , each with their names and so on ( for instance , for ' alpha ' it would be alpha , whereas for ' norm ' they would be mean and std ) .
It'd be easier to just concatenate all your arrays together and write it out it one go using ` savetxt ` I'd do that personally
just concatenate them !
But append and extend are both not working .
Yes , pickle / dump gave me a memory error for the big array .
I have a ` DataFrame ` from which I'd like to select a unique value from each row based on a boolean criteria from a separate ` DataFrame ` with an identical index .
My kluge solution is to convert all the ` NaN ` s to 0 and then sum each row #CODE
One way is to stack the result : #CODE
Oh , that's interesting you can do it with stack .
Also , I think the problem is essentially turning multiple columns into one , which I think naturally falls into the realm of pivot and stack etc .
On the other hand , it would seem that stack is around 2-3 times slower
I want to make two matrices same dimension / shape padding with zeros ..
, just quick q . here we are creating an empty array of zeros for x1 , y1 of maximum shape , and than updating it --- I have matrix of 2k x 1k ( approximately ) since rows and column varies a little bit ..
~16MB ? as they are all ( same ) zeros ( floats ) , wouldn't it optimize memory space ? and only updated matrix will be actual size ?
I would like to change a single element in an array , which also changes the numbers of zeros , which is what the class below do .
` nonzero ` is only being set once .
The problem you have is that nonzero is only calculated when your object is first instantiated .
The best option is make nonzero evaluate rather than return the stored value .
or alternately if you want nonzero cached you could hide the matrix from users of the class and overload the index operator to get to it e.g. #CODE
I would like to be able to show the user both nonzero and matrix at all the time .
Sorry yeah you need to include the nonzero method from your origonal
In Python 3 , since ` PyFile_AsFile ` and the ` file ` object are gone they have to resort to accepting file-like objects and will work with anything that has ` fileno ` , ` flush ` , ` tell ` , and ` seek ` ( see the alternate source of ` npy_PyFile_Dup2 ` ) .
I then used vstack to stack rows to make a matrix : #CODE
I want to sum only one of the elements ' fields in each column .
I know that numpy has a ` np.sums ( axis=1 )` method for use on normal numpy matrices , but I can't figure out how to get it to sum just one field .
Here's a test where I append ndarrays to a list 10000 times and then generate a new ndarray when I'm done : #CODE
And here's a test where I concatenate each row : #CODE
I would like to reshape this DataFrame such that I have 3 columns ( X , Y , Z ) and 1681 rows with a new arbitrary index that ranges from 0-1680 .
How can I concatenate arrays of arrays index by index ?
If I have two or more n-dimenisonal arrays , each of which contains also arrays , how con I concatenate all of them index by index ?
This is a neat , but somewhat unique , interaction of 2 different uses of ` + ` .
` flat ` and ` flatten ` are handy tools for iterating over nd arrays as though they were 1d .
I am rather new to OpenCV and need to translate some Python code to OpenCV ( C++ ) .
Any function of matrix or matrices and scalars that returns a matrix or a scalar , such as norm , mean , sum , countNonZero , trace , determinant , repeat , and others .
The functionality that OpenCV's repeat offers is similar to NumPy's tile command .
One way to do that is via a call to ` min ` but with the proviso that you need to handle an empty array if the value is beyond the last element : #CODE
In all cases , that gives you the index of the element you need to insert before ( or one beyond the highest index if you need to append to the list ) .
The bisect module does exactly that ( in ` O ( log n )` time ): #CODE
Assuming ` A ` is sorted , you can do this in O ( log n ) time with ` np.searchsorted ` ( ` A ` can be an array or a list ): #CODE
You could use ` argwhere ` and min : #CODE
The problem is that the indices are written with one decimal place ( they are integers ) and there is no new line between consecutive rows in triplet format .
I have numpy compiled with OpenBlas and I am wondering why einsum is much slower than dot ( I understand in the 3 indices case , but I dont understand why it is also less performant in the two indices case ) ?
Why does np.einsum not just call np.dot if it notices a dot product ?
That is true , but my question rather related to the two indices case .
The answers in the given post give me the impression that there are issues with three indices .
Because einsum is written to be generic and does not special case the two index dot product to use blas .
If you know you only have two indices , just use dot .
` tensordot ` reshapes and swaps , so it can then call ` dot ` to the actual calculations .
Func ( x ) has to return a scalar , so instead of returning the deviations , as you are doing , I suggest returning the sum of all the squared deviations .
So , I suggest returning the sum of the square of the whole array you are returning .
A classic method would be to minimize the sum of the square deviations : #CODE
As @USER suggested , you can minimize the sum of squares . scipy.leastsq() is designed for such problems .
numpy : fill offset diagonal with different values
I need to make a ` n*n ` matrix ` m ` whose elements follow ` m ( i , i+1 )= sqrt ( i )` and 0 otherwise .
One way could be to create the array of zeros and then use indexing to select and fill the desired indices with the square-root values .
If we knew that a reasonably small number of consecutive zeros are possible , we could use something based on ` numpy.roll ` .
The problem is that the number of consecutive zeros is potentially large ...
If the ` 0 ` s only come in strings of 1 , this use of ` nonzero ` might work : #CODE
Probably the best way would be to look at the difference between a max and the next min and take the four largest ones of those
@USER ` maximas += [ _ ]` has the same result as ` append ` .
This method takes your row , then subtracts it from each other row in the dataframe , then calculates the norm for each row .
The column names do not appear to be it , and I was operating under the assumption that the indices can match or not - it shouldn't matter - which is why I am using this package to being with .
` x ` and ` y ` are actually poor names for array indices , because they do not represent a mathematical cartesisan coordinate system , but a location in memory .
The documentation is a bit long but has a good in-depth description of indices and shapes .
The calculation of the function , its gradient and its hessian is al relatively simple , quick and accurate .
How is theano dot product broadcasted
Could anyone example how i theano dot product broadcast .
It counts unique elements in the DataFrame irrespective of rows / columns , but I need to count for each column with output formatted as below .
You could do a transpose of the df and then using ` apply ` call ` nunique ` row-wise : #CODE
As pointed out by @USER the transpose is unnecessary : #CODE
Similarly I think ` df.apply ( pd.Series.nunique )` would also work ( and avoid the need to transpose if that's an issue ) .
@USER yes that is better actually , initially I thought that the transpose was necessary to get the columns as the index values
resize an array with a specific value
and I would like to resize with the method resize #CODE
Python [ arrays ] ( #URL ) do not have a ` resize ` method , and neither does lists .
Because Python arrays don't might resize arrays you can use numpy or write own functions , similar below : #CODE
I go through each row of the array , if the sum of the row is 0 , I don't have to do any calculations and the output is just 1 .
Done . flat gives an iterator , and we've basically created an array with the col indices for every number .
So my suggestion would be to try this using pandas instead as it conserves indices while masking .
Also I would warn against checking if the sum of the array is 0 .
Also if you have -5 and 5 the sum is zero and I'm not sure thats what you want .
I used numpy's any() function to see if anything was nonzero .
That works perfectly , it seems and thanks for the warning regarding the sum !
We take a tuple of those indices and then use ` array.itemset ` and ` array.item ` , thankfully , column index is available for free to us , so we can just take the element @ that index in the list ` l ` .
How would I approach this in Python , other than finding an upperbound on both and store a lot of zeros ?
This circumvents the automatic flatten which happens in np.append() when axis=None ( default behavior ) , as documented here .
I have a code that implement a 2D Laplacian for finite differences integration method for partial differential equations , using the roll method of Numpy : #CODE
I would like to cythonize my code - will the roll method work in my pyx code or should I implement the roll method using C ?
The short answer is : ` roll ` will work in Cython , but it won't be much ( any ? ) faster .
If you want speed you should probably avoiding using something like ` roll ` altogether ( it's slow because it creates a complete copy each time it's called ) and instead use indexing to get views of large chunks of the numpy array ` u ` .
You might have to adjust dimensions so that ` x ` , ` y ` and ` p ` broadcast nicely .
I have not been able to test it yet with the real sample , but tested it with smaller ones and Approach #2 works perfectly !
How to determine a numpy-array reshape strategy
Assume you want to reshape the following 3d-array #CODE
where as a different transpose ( that does not switch the order of 3 , 4 ) #CODE
That way , if I do mess up some transpose or broadcasting , dimensions errors will jump out at me .
You start with dataframe columns [ type , name , values ] and want a new dataframe with [ type , name , values , min , max , calculation ] ?
I'm stull confused by what is in these columns : [ min , max , calculation ] .
Do you want to group the frame by all unique instances of [ type , name ] ?
Also , you can pass common function names such as min and max in as strings .
` div = arr [: , 0 ] / arr [: , 1 ]` but don't know what's the best way to reshape and add zeros to get the result .
Here is the error I get from the terminal ( not a full log ) , it seems to be related to numpy : #CODE
If the line you need has unique feature , you can also determine the line number by searching features .
Given a 2D array to be sampled and a 2D array of indices ( axis 1 of the index array is aligned with source and destination axis 1 , and axis 0 of the index array is aligned with the destination axis 0 and contains indices of the source array ) How can I produce a destination array without iterating over one of the two axes ?
I have also been suggested to avoid Python loops and use NumPy arrays and vector operations instead - actually I do not see how these can help ( it seems to me that NumPy operations are similar to Matlab ones , and I am unaware of any way the code above can be vectorized in Matlab either ) .
Their sizes become smaller from 3 to 5 to 15 and thus each takes less time to construct than the one before , after they are constructed you only need to take the sum and do some arithmetic .
With regards to that particular problem , take a look at the function fD in the code below , which just calculates how many multiples there should be in each range and then calculates their sum , rather than generating the array .
Basically I need to compare each unique bidder in df_A to bidders in df_B .
Then I need to find all the unique timestamps where the bidder ids match , and for those unique timestamps , i need to iterate through df_B to find the number of times a bidder in df_B appears for the same timestamp
EDIT : Just to be clear , the script should output the unique bidder id , the number of times it made simultaneous bids , and the timestamp when the simultaneous bid was made .
The Pandas merge is like a SQL INNER JOIN , where by default it will join on any common columns ; in this case the column ' bidder ' : #CODE
Here we are just doing count , but you could do sum or something else if you wanted .
First get unique bidders from ` df_A ` #CODE
Then extract data of selected bidders ` df_B [ ' bidders '] .isin ( bidders )` , groupby on ` bidders ` and count unique values of ` time ` #CODE
If you want to get bidder IDs for each timestamp then , groupby ` time ` , get the unique list of bidders-ids via ` x [ ' bidders '] .unique() ` in ` apply() ` #CODE
But there was no stack
If a load images individually in the interpreter and append pixels , i can perform np.array ( dataset , dtype= theano.config.floatX ) ..
- yields either one row by 3 columns ( the way above ) or if I transpose it is 3 rows and 1 column
Obviously , ` flatten ` consumes more memory and cpu , as it creates a new array , while ` flat ` only creates the iterator object , which is super fast .
If you need an actual flat array ( for purposes other than just explicitly iterating over it ) , use ` flatten ` .
Think ` flatten ` without the copy .
sum zeros and ones by another vector in python
@USER well if you ignore the figure I posted and use the code I provide as it is , it is producing exaclty the same as your matlab script , which says ` sigma = sqrt ( 2 ) .
However your python script is integrating over ` norm ( t*n , 0 , 1 )` for t=0 to x which is the same as ` normcdf ( x , 0 , n )` .
Sorry , just realized I mixed something up ... integral over ` norm ( t*n , 0 , 1 )` for t=0 to x is equivalent to ` normcdf ( t , 0 , 1 / n )` , which looks similar to ` normcdf ( t , 0 , sqrt ( 2 ) / n )` which is why your script * looks * like it is producing the same figure but technically it is not .
I am writing a function which computes sum of squares of errors .
If I were to guess , I'd say it's how the ` .dot ` function works , which , while has the same result , would calculate the dot product on the fly , rather than square all the numbers first , and then add them together .
The " transpose " of a 1D numpy array is itself .
by using diff recursively .
There is currently an open pull request at scipy for adding the function ` exprel ( x ) = ( exp ( x ) -1 ) / x ` : #URL feel free to comment there if you like .
@USER Using exp - 1.0 is slightly better ( 14% ) in terms of speed but assuming that np.expm1 gives better results around zero ( as claimed by the documentation ) I prefer to use the second .
@USER I forgot to mention that as pointed out by @USER , with using expm1 the only thing left is to catch the " zeros " ( adjusted by the tolerance ) .
I am using your approach to do this but the non zeros ` zero = ~nonZero ` become useless if you define ` fb_ = np.ones_like ( zeta )` , then you can be sure that you will have 1 for very small numbers ( negatve or positive ` idx = np.abs ( zeta ) > EPSILONZETA `) .
You're using ` np.linspace ` and ` np.piecewise ` along with Python's ` sum ` , ` math.pow ` and ` list ` .
Note : instead of using the Python ` sum ` function , you should also call the NumPy ` sum ` method for better performance .
It should normally extract the indices of nonzero entries of a matrix ( rows1 , cols1 ) .
However , I can also extract the indices manually ( rows2 , cols2 ) .
` nonzeros ` isn't a thing , did you mean ` nonzero ` ?
Sorry , I meant ` nonzero ` .
But what you are doing is unclear , first there is no ` nonzeros ` function in numpy , only a nonzero which returns a tuple of coordinates .
Sorry , indeed , I meant ` nonzero ` .
I understand that you could create an array of zeros and iteratively change the values in each column , but I also understand this is not an efficient method .
You could also make the blocks ( with ` np.ones ` etc ) , and concatenate them .
` hstack ` and ` vstack ` are just alternative APIs for concatenate .
But concatenate ends up using , in compiled code , this initialize and assign method .
The problem with this is that B exceeds the bounds of A , but what I really need is for the exceeded bounds to clip off .
While the #URL documentation is not entirely clear on this , I think you can store 1d arrays with shape ` ( 11 , )` and ` ( 38 , )` with ` vlen ` , but not 2d ones .
It's 11 values taken from the transpose ( F order ) , but shifted for each sub array .
It seems to " vectorise " the function , which means that instead of passing the actual indices it passes a numpy array of indices : #CODE
So , if we try to use numpy's vectorised ` sin ` function : #CODE
The point here isn't about ` sin ` in particular : I want to be able to write arbitrary python functions like this ( whether a numpy vectorised version exists or not ) and be able to run them inside a tight C loop ( rather than a roundabout python one ) , and not have to worry about integer wraparound .
If ` indices ` does not produce the right ` i ` values , don't use it .
I think the issue of integer wraparound is unrelated to numpy's vectorized ` sin ` implementation and even the use of python or C .
If you want to replicate the same behaviour in Python , you'll need to transpose the matrix first .
Once you find these non-zero elements , you can simply index into the transpose of ` C ` added with ` 0.1 ` to get the values you want .
The first part of this statement ( inside the ` abs ` call ) is performing the outer product of two vectors .
In MATLAB , ` imv ` would be ` N x 1 ` , and you are multiplying this with a ` 1 x N ` vector of ones .
You can use ` numpy.outer ` to help you do this outer product step .
Take note that for 1D arrays , ` numpy ` does not distinguish between row vectors and column vectors and so multiplying a vector with the transpose of another unfortunately won't give you what you expect .
The second part of this statement also performs an outer product but on the transposed version of the first part of the statement .
The first part of the code declares a vector of ones for convenience .
Why is it not a vector full of ones ?
EDIT : I just now remembered we initialized 0 as our vector of ones .
I have written a code in OCV-python for image processing ( detection of veins in eye ) , its not working , can somebody help ?
Okay , I found out the real problem : when you create the histogram with those bin-edge settings , the histogram creates bars which have equal size , and equal outside-spacing on the non-log scale .
When converting things to log scale , bar widths and edge widths all go for a huge toss .
The basic idea is to drop the plt.hist function , compute the histogram by numpy and plot it with plt.bar .
Note : frequently , computing the actual indices is unnecessary overhead .
I was rather disappointed to find out that this actually operates quicker than the numpy ` ceil ` function !
I know the ` norm ` function will contain many ` if else ` detecting the input and select the right ` norm ` .
Both use the ` eig ` function respectively .
As of version 1.9 ( and possibly older ) this is special-cased so that ` norm ( x )` and ` ( x.dot ( x )) ** 0.5 ` perform comparably .
@USER I'm using ` line-profiler ` to locate why my program is so slow and just find ` norm ` took a large among of time .
First , I determine all the indices and values of the non-zero entries .
In the second step , I loop through the list of indices and move them according to their moving probability which is done by choosing from the allowed rows and columns , assigning the respective value to these new indices and set the " old " value to 0 .
Also , after addressing that , it's quite possible for your code to move a nonzero entry over another nonzero entry ( either before or after the other nonzero entry is moved ) .
@USER : They're identical in * this * case , but stick another nonzero entry in the array and they won't be .
` indNZ ` and ` valNZ ` have length equal to the number of nonzero entries ( so they're always the same length ) , but ` nzEntries ` has length equal to the dimension of the array .
If I sets ` 0 ` after , there are more zeros .
Conditional maths operation on 2D numpy array checking on one dimension and doing different operations on diff dimensions
Though I think the indices should be swapped and a bit more complex ? in the simple code I wrote ( i.e. the kernel ) my pantilt [ 0 ] is pan and pantilt [ 1 ] is tilt .
Then the vector that you need to return from ` deriv ` is #CODE
The list provides the order given by the indices of the values in new_values which should be in descending order where 0 corresponds to the largest number .
I am willing to change the 2nd array , order , to specify the indices in another way if this was advantageous to sorting with a better method .
The lazy , less efficient option , is to do ` sorted_array = values [ order.argsort() ]` , since calling ` argsort ` on permutation indices reverses the permutation .
I am trying to translate some Matlab code I have into Python ( using numpy ) .
Have numpy argsort return an array of 2d indices ?
What I need is the 2d indices that sort this matrix in it's entirety .
How do I get " 2d indices " for the sorting of a 2d array ?
Apply ` numpy.argsort ` on flattened array and then unravel the indices back to ( 3 , 3 ) shape : #CODE
I have a 3D numpy array consisting of 1's and zeros defining open versus filled space in a porous solid ( it's currently a numpy Int64 array ) .
Could it be that ` pos ` for you is simply the ( x , y , z ) indices of the 0's and 1's in this matrix ?
If the number of zeros is much smaller than the number of 1's , then you could start from each zero and update the distance of surrounding 1-voxels iteratively .
often it will be the other way around - mom any more zeros than 1's . the ones are connected pixels from skimage.measure.label
If you make ` samples ` a DataFrame with columns ` user ` and ` item ` , then you can obtain the desired values with an inner join .
I'm looking for a function that would do what the function ` indices ` does in the following hypothetical code : #CODE
Specifically , I want to produce a dictionary whose keys are the unique elements in the flattened array and whose values are lists of the full indices of the respective key .
Retrieve indexes of min and max values in np.ndarray
I know I can use indices but , I just want to know if there is a way to access the array column wise without indices .
sorry I do not want to use indices
I want to computer gamma fit over the axis 0 from my array ( 3d ) .
Is there any lazy solution to achieve the minitab algorithm with NumPy or will I just need to roll out my own code and implement the algorithm ?
I think you will have to roll your own .
@USER : Could've been an outer product , could've been a convolution , could've been something else .
You can then flatten that array to get the same output as you requested :
EDIT : @USER ' s answer showed us that ravel will do the same thing as flatten , except faster o.O So use that instead .
odd that ` ravel ` is faster than ` flatten ` .
Where ` rand ` is ` numpy.random.rand `
Could you explain how does that ` rand ( * x.shape )` works ?
Numpy array with symmetric indices
How do I create a numpy array with a symmtric range of indices ?
You could try writing your own subclass of ` ndarray ` , but you'd have a lot of awkward design decisions to make ; for example , if you have an array with indices from -100 to 100 , where do the indices of ` array [ 1 :] ` start and end ?
And how do you broadcast operations across arrays with compatible shapes , but different indices ?
What would the bounds be of the result of something like ` dot ` ?
You can see that it is successfully printing ` 1.8.2 ` in this build log .
Building scientific python modules from sources ( whether compiling directly or with ` pip `) in a continuous integration work-flow is slow ( 15 min for numpy , another 15 min if you need scipy , etc ) , and a waste of resources .
The fact that the main outer loop is in Python should matter only if the first dimension is actually the very big one .
The following works , you can calculate the row-wise mean and pass this as the values to replace the ` NaN ` values , you have to transpose the mean so that the alignment is correctly performed : #CODE
In order to do so , I fit a curve to each line of pixels in an image , and append the location of the vertex to approximately model the location of the object in the image .
Taking the sum of the squared differences between your current fit ( a Gaussian ) and your data divided by the variance .
I would like to concatenate numpy arrays in the same way as ` hstack ` .
If you have a list of arrays instead , you have to somehow insert the arrays of zeros in your array list before you call hstack .
All the ' cover ' functions for ` concatenate ` iterate over the input list to massage the arrays into the correct shape .
It's one thing to iterate over a 1000 rows of an array to do a sum , quite another to iterate over a list of 30 arrays prior to concatenating .
If your input arrays all had the same number of columns , you could ` vstack ` them , ` hstack ` on an array of zeros , and reshape that .
So you are stuck with some sort of iteration that interleave the zeros arrays .
The inner ` concatenate ` pads each array with the zeros .
Plus it ends up using ` concatenate ` anyways .
I could also construct the equivalent of ` [ arr [ 0 ] , pad , arr [ 1 ] , pad , ... ]` , but to do that I have to ' flatten ' this array : ` [( i , pad ) for i in arrs ]` .
is an element-wise product with respect to ` i ` , and an outer product with respect to ` j ` and ` k ` .
@USER : you can tell ` einsum ` not to sum along axes by controlling the output labels , e.g. ` np.einsum ( ' ij , ik -> ijk ' , a , b )` would work in NumPy .
The difference appears to be that the first examples trigger fancy indexing ( which simply selects indices in a list from the same dimension ) whereas ` tuple_index [: 31 ]` is instead treated as an indexing tuple ( which implies selection from multiple axes ) .
This makes ` a [ tuple_index [: 3 ]]` equivalent to ` a [( 0 , ) , ( 1 , ) , ( 2 , )]` , hence the " too many indices " error ( because ` a ` has only one dimension but we're implying there are three ) .
The above call to ` numpy.sum ` actually sums over all of the dimensions by default , so there's no need to call nested ` sum ` calls to sum over the entire matrix ( Thanks Divakar ! ) .
@USER I don't get your edit , why can't you show the actual log of commands you ran
mapping a numpy array to a function , passing along the indices
where i , j are indices , where myfunc will look up the correct model parameters to apply .
@USER I'm asking for the simplest case : is it possible to pass indices ?
Alternatively , what about applying the same function without indices along the depth axes ?
generate sequence by indices / one-hot encoding
I found the answer below that indicates how to roll the formula down , but I'm missing how to save the updated ratings on the player dataframe
For which I have been attempting to translate successfully from a MATLAB equivalent .
From a programming point of view , the problem appears to be your value of ` gamma ` and therefore the size of your collapse operators .
Print out ` gamma ` - it is of the order ` 10**25 ` - this seems to be what is preventing the solver from converging .
Just for testing ( I'm an engineer , not a quantum physicist ... ) , I put in a smaller value of ` gamma ` ( e.g. 0.1 ) , the solver seems to work and gives apparently reasonable output in ` results.states `
I don't quite understand your ` gamma ` - it seems to have units of cm -1 s -2 as you have set it up .
OP indicates in comments that the wrong order of magnitude / units for ` gamma ` does seem to be the programming issue ( i.e. preventing numerical calculus from converging ) , but isn't totally clear on how to calculate gamma .
This makes it clear where the expression for gamma comes from and thereby clarifies that the constant terms presented as simply ` 30 ` and ` 150 ` in this question actually have units ( Energy and frequency respectively ) .
This changes the dimensional analysis - the units of gamma are s -1 or , with appropriate conversion , cm -1 .
I have realized that the value for gamma should be closer to about 300 cm^-1 but I want a more accurate value .
Also , gamma should not be square rooted .
I'm not sure what gamma really represents , or the exact scenario you're trying to model / investigate and it's off topic for this site .
For example : I've no idea what the 35 and the 150 represent in your gamma .
Of course , as it's a constant , you can just set the value of ` gamma ` to get you going - check the graphs / states you get out are the right kind of shape and then put in a more exact value once you have got a handle on the correct way to calculate it .
when I set gamma to about 300 , it still givess the same error !
Increase gamma until it starts to fail .
At that point increase nsteps or the convergence tolerance and keup increasing gamma if you can .
In patallel , ask about the correct way to calculate gamma elsewhere .
NumPy doesn't seem to provide an error-handling mode that skips invalid indices , probably because skipping them doesn't make much sense when the result array is multidimensional .
` np.take ` has several ` modes ` for dealing with out of bounds indices , but ' ignore ' is not one of them .
So , you need to get the linear indices with that fused format using ` B ` .
Of course , before doing all that , you need to reshape ` A ` to a 3D array with ` A.reshape ( shp [ 0 ] , shp [ 1 ] , -1 )` .
Applying transformations to dataframes with multi-level indices in Python's pandas
I wanted to use hierarchical / multilevel indices to represent this and then use a split-apply-combine like operation to group the data , apply an operation , and summarize the result as a dataframe .
if I try ``` pandas.DataFrame ( c.groupby ( level= ' time ') .apply ( lambda x : x - m ) .mean ( axis=1 ))``` I still get the weird extra column " 0 " as outer column / index
When you index the way you're doing it , NumPy doesn't interpret it as selecting those indices of each dimension .
To get the behavior you're looking for , you need to reshape the arguments you're passing , so that broadcasting them against each other produces an array of shape ` ( 2 , 2 )` instead of shape ` ( 2 , )` .
One option is to tell ` imread ` to flatten the image into a 2D array by giving it the argument ` flatten=True ` : #CODE
And then ` argwhere ` to find the indices of the values in , for example , column 0 of ` A ` : #CODE
To minimize the amount of reads I want to compress the array to whole 8 cells per 1 integer .
Note that ` numpy.unpackbits ( numpy.packbits ( x ))` won't have the same length as ` x ` if ` len ( x )` wasn't a multiple of 8 ; the end will be padded with zeros .
` df1 ` and ` df2 ` are the two ` DataFrame ` objects from measurement with their own relative time stamps ( no absolute time ) .
, ` join ` and ` concat ` methods , but I didn't find any solutions to my question -- the lack of an absolute time in order to compare the two objects seems to be the problem .
Unfortunately , there is no way for me to get a unique time stamp out of my files ( or even save them with one after measurement ) .
How can I extract the elements of a list corresponding to the indices contained in a 1D ` numpy.ndarray ` ?
Just use ` np.concatenate() ` and ` reshape ` this way : #CODE
However , if you know the total number of minutes in advance , it would be better to define your array first ( e.g. using ` zeros `) and gradually fill it - this way , it is easier to stay within memory limits .
Scipy's correlate function is slow
correlate from the numpy package ( np.correlate in plot )
correlate from the scipy.signal package ( sps.correlate in plot )
This statement says : ` Put 1 in the indices where the value of X is between 0.8 and 1 ` .
You can make lists and append to them using Cython .
Also , do you want the " dead pixels " which many cameras include but clip off ?
I recently learned how to use the Poly1d class in Numpy so that I can use its deriv method to get the n level derivative .
I suspect this is happening due to limited stack space , and list is taking more than available stack space .
= chunk swidth : only then it enters into loop and shows an error message valueerror : operands could not together broadcast with shapes ( 4096 ) ( 2048 ) . how to solve this problem ?
How to find the set of indices where two vectors have equal elements in Python
What I would like to do is to find out the set of indices where these two vectors have equal elements .
So the set of indices where the two vectors have equal elements would be : #CODE
` y [ y == 0 ] = np.nan ` will set all zeros in ` y ` to ` nan ` .
Change ` dtype = object ` by ` dtype = float ` so division by zero will not throw runtime error but it will be considered as ` inf ` and than change ` inf ` by zeros .
Numpy roll in several dimensions
I believe ` roll ` is slow because the rolled array can't be expressed as a view of the original data as a slice or reshape operation can .
I guess I was obsessed with numpy's roll and googled around that .
So it is calculating new values , not just moving the existing ones around .
copying a 24x24 image into a 28x28 array of zeros
tile is a 24x24 ROI works as planned #CODE
blank_image in my example does not get updated with the values from tile
One simple thing to do is instead of calculating Euclidean distance between ` np.array ([ data ] *len ( centers ))` and ` centers ` , just use the absolute value : ` dist_ = np.abs ( np.array ([ data ] *len ( centers )) - centers ` .
The only way to make ` getLabel ` fast , with fixed ` getNearestCenter ` , would be to somehow make ` np.all ` or ` list.append ` faster actually , you could try replacing the for loop and ` append ` , depending on how many elements ` history ` has , with a list comprehension / ` map ` .
@USER I'd go for ` scipy.spatial.cKDTree ` to find the nearest centers and get the indices to the closest entries , [ check this example ] ( #URL )
sort 3D array by the sum of the second value
I want this to be sorted by the sum of the second value .
The criteria that I use to order is the sum of the second value .
For example , vector ( 2 , 3 ) has no sorting relation with ( 3 , 1 ) , but if you map it to the norm ( one dimension ) then you can sort in some way .
` a [: , : , 1 ]` accesses the second element of the innermost dimension , so you can sum over this ( on the horizontal axis , i.e. axis=1 ) to get the values you need for sorting : #CODE
Use ` np.argsort ` to get the sorted indices , rather than the sorted array : #CODE
Finally , access your array using the indices , as desired : #CODE
Write double ( triple ) sum as inner product ?
Since my ` np.dot ` is accelerated by OpenBlas and Openmpi I am wondering if there was a possibility to write the double sum #CODE
Another way to say this is , if you can ` reshape ` the problem into a matrix-vector product , then ` einsum ` is just as fast .
But , if you can only ` reshape ` the problem into a matrix-matrix product , then ` tensordot ` will be faster .
Here is how tensor dot performs in this setting : #CODE
@USER I tried minimizing the sum of squares using t8 as a constraint , and also root finding for 7 1d problems given w .
Now is there a way for me to tell which hard drive a file lives on ( in terms of ' sda ' , ' sdb ' , etc ) from its absolute file path ?
I want to find the indices in ` bigx ` where ` x ` and ` bigx ` match to 2 significant figures .
I need to do this extremely quickly as I need the indices for each step of an integral .
If you want the indices , try this : #CODE
I just want the indices .
Since ` bigx ` is always evenly spaced , it's quite straightforward to just directly compute the indices : #CODE
First you can find the unique ids with following command : #CODE
Then find the different between the ids if ` a ` and all of ids : #CODE
Then find the items within ` b ` with the ids in ` diff ` : #CODE
And at last concatenate the result with list ` a ` : #CODE
I have asked another question How to arrange three lists in such a way that the sum of corresponding elements if greater then appear first ?
Is there a method to use a boolean mask so that all of my values which are 0 become 1 and all my numbers which are nonzero become zero ?
If you are going for speed though consider if you really need the array to be 5x5 , then converted , or if you could ` np.arange ( 25 )` , apply the list comprehension directly and then reshape .
First of all you don't need to use reshape , you can create your random matrix directly like this : #CODE
Repeat the linear fit on the log data and plot your line in the same plot as the logA , logB data .
The only difference is , now ` y ` and ` x ` happen to be logarithms of two other variables , which I'll call ` log ( F )` and ` log ( R )` .
The linear regression algorithm doesn't care that ` y ` and ` x ` are actually ` log ( F )` and ` log ( R )` - it makes no difference to the algorithm .
` log ( F ) = m log ( R ) + b `
In your code , you're using A and B when calling ` polyfit ` , but you should be using ` log ( A )` and ` log ( B )` .
Python Pandas Group by c3 find max of column 2 and get column 1
but failed to get the max ( salary ) and emp_id by each department wise .
puts the department max salary on every line and then all you have to do is subset from there .
a is Numerator i , e emp_id based on the max ( salary ) for each department .
Your data shows multiple employees with the max salary in any given department so you need to specify what you want the output to look like , if you want duplicate records or some kind of field with the ids concatenated ?
` hstack ` is a thin wrapper around ` concatenate ` .
For 1D arrays you need to reshape to 2D first .
I have a numpy array of shape ( 444,445 ) , and I need to dump it as a ` csv ` file .
There is also ` reshape ` .
So how to concatenate them rowwise without using for loops ?
Or , using ` reshape ` : #CODE
or you can use the resize function : #CODE
or during creation with reshape #CODE
Both assignment to ` shape ` and ` resize ` should be avoided unless you know what you're doing because they both have behaviour that will likely surprise new users .
numpy already has a " reshape " method that is numpy.ndarray.shape you can use it to change the shape of your array .
The numpy module has a ` reshape ` function and the ndarray has a ` reshape ` method , either of these should work to create an array with the shape you want : #CODE
` np.loadtxt ` and ` np.genfromtxt ` collect the ` csv ` data in a list ( of lists ) ( via list append ) , and convert that to an array at the end .
I chose to construct an array for each line , and concatenate them at the end because that required fewer changes to your code .
Sure for this case it does not matter but I was just curious if it would be possible to do it without an append .
Append to an array is slow , append to a list not so much .
I have tried including the absolute path in the file I pass to imread .
Converting MATLAB to Python : too many indices return error
however , on line 11 , I am returned an error of " index error : too many indices .
I don't know if this affects your error but you have ` indexdn ( j , :) = -indexdn (: , j )'` ( transpose ) in your Matlab code but not in your python .
Neither does any std :: type I have tried so far , such as vector .
Open and edit in append mode ( ' a ' , default mode ) ..
Why does Python create this extra dimension instead of sum the values ?
You can also make it broadcast , so if ` X ` and ` Y ` are thought of as arrays of 2D matrices , the following call will perform an appropriately broadcasted ` dot ` : #CODE
You can also do this with ` dot ` .
You want a result array with ` result [ i , j , k ] == A [ j , :] .dot ( u [ i , : , k ])` ; you can get this with ` rollaxis ` or ` transpose ` with an ` axes ` argument .
Where ` einsum ` makes broadcasting easy , ` dot ` for high-dimensional input is kind of like an outer product .
where c , p and gamma are parameters that I want to pass to odeint .
You are simply trying to compare speeds of 1 operation and there plain python is going to win ' cos it doesn't have to setup numpy arrays or bitarrays .
Can I reshape the array that packbits returns somehow ?
Thus , it appears to me that ` np.delete ` cannot handle Boolean indices .
If you have Boolean indices you don't need to use ` delete ` .
` float_equal ` could probably be changed to broadcast ` x ` against ` y ` , eliminating the list comprehension .
i was getting the wrong result when i used boolean indices , but i'm not sure whether that was specific to the values i was using .
ah , i see -- you're not using Boolean indices in your calls to ` np.delete ` .
numpy.where() on grayscale frame , wrong indices ?
Compile failed and ascii codec error when installing python scientific stack
UnicodeDecodeError : ' ascii ' codec can't decode byte 0xe2 in position 66 : ordinal not in range ( 128 )
I am trying to broadcast the difference between two vectors .
NumPy isn't able to broadcast arrays with these shapes together because the lengths of the first axes are not compatible ( they need to be the same length , or one of them needs to be ` 1 `) .
numpy search array for multiple values , and returns their indices
It should return the indices of those values .
In my case query elements are unique and always exist .
You can use ` np.searchsorted ` on the sorted array , then revert the returned indices to the original array .
if ` a ` is of size ` n ` and ` query ` is of size ` k ` , this will be ` O ( n log n + k log n )` which would be faster than ` O ( n k )` for linear search if ` log n k ` .
Actually in the case where ' n ' is large and k is small , k is considered a constant , and this solution can reduced to ' O ( n log n )' , versus ' O ( n )' for searching unsorted array .
That's because all those it would have to test all those zeros .
4 ) As for determining whether your solution is correct without matplotlib , you can use an L2 norm to quantify how big your error is .
After getting this list , I want to draw histogram using matplotlib .
I am trying something as below , but I am not able to draw a proper histogram .
You need histogram or bar chart ?
In histogram , the x-axis is a continuous variable over real numbers .
@USER tell me difference between histogram and bar chart according to your knowledge .
Dude histogram is on sorted numerical quantities .
This is why your code above returns a histogram of equal bars , because you are giving ax.hist the numbers from 0 to 99 once each .
What I want is to efficiently ' extend ' ` x ` to a new dimension , filling new columns with zeros .
I already tried ` reshape ` : #CODE
In the CSR format , the underlying ` data ` , ` indices ` , and ` indptr ` arrays for your desired ` y ` are identical to those of your ` x ` matrix .
Note that the constructor defaults to ` copy=False ` , so this will share the ` data ` , ` indices ` , and ` indptr ` between ` x ` and ` y ` .
@USER : This shares the ` data ` , ` indices ` , and ` indptr ` between ` x ` and ` y ` ; it's not an expensive operation .
Well I believe it's correct , ` TLabels [ dist_ == dist_ [ i ]]` will return values from ` TLabels ` which have indices where ` dist_ == dist_ [ i ]` .
You want to divide the ` dist_ ` into groups with a common value , and sum some function of their corresponding ` TLabels ` values .
Is there a fast way to find all indices where a 2d array is inside a 3d array ?
And I would like to find all indices where ` [ 0 , 4 ]` occurs .
This reduces the number of dimensions to two , and ` argwhere() ` returns the desired array of indices .
Also , I tried the reshape and it also didn't work .
did you tried to reshape your arrays ?
Let's say , I want the sum of the grayscale values for each region , and if it's 200 , I want to set those points to 0 ( in this case , all the points in region 2 ) , How would I do that with numpy ?
roll your own from first principles .
How can I decode the full array ?
Actually numpy.fromstring() returns a single dimensional array of 1024X1024 intead of a 2 Dimensional array , All you need to do is reshape into 1024X1024 ,
actually , it prints 0 , but the calculation is correct at the end , so I believe the error is in the sum ?
It's not a simple sum , you need to implement s ' ( x ) , f ( x ) and S ( x )
After this you get a JSON dump ( string ) of your data , containing a list representation of its data type and shape as well as the arrays data / contents base64-encoded .
JSON dumps are efficient and cross-compatible for many reasons but just taking JSON leads to unexpected results if you want to store and load numpy arrays of any type and any dimension .
` pickle.dumps ` or ` numpy.save ` encode all the information needed to reconstruct an arbitrary NumPy array , even in the presence of endianness issues , non-contiguous arrays , or weird tuple dtypes .
By using the same interface as json ( providing load ( s ) , dump ( s )) , you can provide a drop-in replacement for json serialisation .
Does numpy broadcast in * all * of its functions ?
Is broadcast the right word here ?
[ { ' __fid__ ' : 0 , ' sum ' : 20} , { ' __fid__ ' : 1 , ' sum ' : 15} , etc . ] .
stats = rs.zonal_stats ( output_df.to_crs ( epsg=5070 ) .geometry .values , cost_surface , transform= cost_file.GetGeoTransform() , stats =( ' sum '))
stats = np.array ([ x [ ' sum '] for x in stats ])
stats = np.array ([ x [ ' sum '] for x in stats ] , dtype= np.float )
You make it further simpler with ` [ sum ( b == 2 ) for b in a ]` ?
I'd also play with ` flush ` and ` driver ` .
You could use a ` try ` / ` except ` block to handle invalid indices #CODE
If I use a dense matrix , then these operations are slow ( have to init matrix to zeros ) #CODE
I tried using the ` take ` function , where I use ` range() ` to create an array containing the indices of the row .
Your ` csr_matrix (( 50000 , 50 ))` has 0 nonzero elements .
Python : What is the difference between math.exp and numpy.exp and why do numpy creators choose to introduce exp again
` exp ` means exponential function
` exp ` in ` math module ` : #URL
` exp ` in ` numpy module ` : #URL
The numpy one will perform ` exp ` on the entire array , it is a vectorised method of performing the function on the entire array this is what it's designed for
The ` exp ` function isn't alone in this - several ` math ` functions have numpy counterparts , such as ` sin ` , ` pow ` , etc .
You append ` h ` to it , but then try and write ` n ` to the file , and don't actually use ` hh `
If you open the file in append mode ( ' a ') you can add to it without having to read the existing stuff and rewriting it .
Sort ` y_pred ` descendantly and use Kadane's Algorithm to calculate an index ` i ` such that the subarray of ` y_true ` from 0 to ` i ` has maximum sum .
as well as np.squeeze for MATLAB's squeeze function .
The ` squeeze ` call in the ` find ` command for MATLAB is redundant .
There's no need to squeeze out any of those singleton dimensions because there are none .
I didn't necessarily need someone to sort out the code for me , but rather someone to translate what that error message meant .
If ` rand ` is the random number , ` r : ( 1 - 0 ) :: rand : ( y - x )`
To make myself clear , in Java I would simply call [ nextInt ] ( #URL ) and take the absolute value .
Plus , rand is not truly random , so if you're looking for cryptographic random numbers , you should use a different protocol .
Why don't you use the built-in sum ?
Are you asking hot to sum arrays ?
For instance , where ` l ` is a list of 10 integers and ` a ` is an array of those same integers , then ` a.sum() ` is 10 times slower than ` sum ( l )` .
If there are ` k ` zeros in the series , and ` log k ` is negligibile comparing to the length of series , an ` O ( n log k )` solution would be : #CODE
I'm currently trying to append multiple Numpy arrays together .
I'll append a2 into a1 by using the append function and simultaneously creating a new array called a , which contains both a1 and a2 .
Those are building blocks for all the proposed variations ( ` vstack ` , ` column_stack ` , ` append `) .
you can use the transpose of ` numpy.column_stack `
He doesn't need to transpose the ` column_stack ` if he wants to create a ` csv ` with ` n ` columns .
or you first create your array as an ordinary python list using ` append ` which you can transform at the end into an ` numpy.array ` .
Changing the amount of points changes the result of the fft
I wrote a script for generating a function ( sech ( x )) , generating the fft , and afterwards measuring the FWTH of the result .
Now the strange thing is that when I change the amount of points I use for generating the function , the result of the fft and the FWTH-measurement changes also quite a lot .
Ultimately , this is more of a math question than a code question . fftshift / fft / ifft is behaving correctly .
By applying the fft ( ifft() ) I want to get the original shape back ( after I have to process this shape with FWHM ) .
Call ` dropna ` and then ` cov ` : #CODE
Enforcing that inputs sum to 1 and are contained in the unit interval in scikit-learn
How can I ensure that ` x_best =( A , B , C )` sum to 1 and are all within the unit interval ?
Numpy mean now requires reshape ?
ValueError : operands could not be broadcast together with shapes ( 3,862 ) ( 3 , )
Why does the mean vector now need a reshape , when it didn't before ?
Hence you need either to reshape data to ( 862 , 3 ) or mean to ( 3 , 1 ) .
You can also add an axis to an array so that it will broadcast .
Q : " Why does the mean vector now need a reshape ?
To broadcast , NumPy looks for axes compatibility , and ` 1 ` is compatible with any axis .
Without this , operations like ` mean ` and ` sum ` remove a dimension .
` reshape ( -1 , 1 )` and ` [: , None ]` also work to add a dimension back in .
I'm not aware of any changes in ` numpy ` that would have enabled the ` axis=1 ` case without some sort of reshape or keepaxis .
` np.diff ` has an argument that lets you specify the axis to perform the diff #CODE
Worst case yours makes one pass and mine makes one pass for the diff and a * shorter * pass for ``` np.all ``` .
I have a list of ` float ` numbers and I would like to convert it to ` numpy array ` so I can use ` numpy.where() ` to get indices of elements that are bigger than 0.0 ( not zero )
Looks like you need to strip off the parentheses .
1 ) I put a ` print ` to ` max ( parray )` before the loops and I obtained : #CODE
So , the question : If I never changed " parray " why I'm getting different values exceeding ` max ( parray )` ?
I have changed the code as you advised me , and I'm still observing " k1s " such that ` parray [ k1 ] > max ( parray )` .
I understand that pandas has a interpolation function for series but it requires all indices for missing data to be available .
I've tried append and concatenate , but with no luck .
in1 and in2 and just two indices to traverse the array for eg , in1 = 0 , in2 =1 .
I furthermore conducted a cProfile study to see if there is something showing up in the runtime of the call stack .
I'm currently attempting to use SymPy to generate and numerically evaluate a function and its gradient .
Or , if there are better ways to speed up my gradient calculations , I'd appreciate hearing those as well .
I added a few additional sin and cos terms to add possible dependencies from previous subexpressions .
numpy.repeat() to create block-diagonal indices ?
Basically , the code builds the matrix of outter products of a matrix ` C ` and stores it as block diagonal sparse matrix .
I use ` numpy.repeat() ` to build indices into the block diagonal .
Much slower , for example , than the ` tile ` piece .
I moved ` *K ` to the smaller ` arange ( L )` , and use faster ` tile ` .
As per comment , the reshape should be ` ( K*K , L )` .
` np.arange ( K ) [ None , None , :] + k1 ` is ( L , 1 , K ) , so we need to tile it #CODE
Another way to generate these arrays would be to use ` np.ix_ ` to reshape the ranges , and then just sum values .
Also , the reshape part of the last suggestion should read ( K*K , L ) instead of ( L , K*K ) .
I found a way around most of the tile / repeats using broadcasting .
Once that happens , ` ConCellMonomer_real ` is no longer the same shape as ` ConcCellTotal ` , so the line ` ConcCellMonomer_positive [ ConcCellMonomer_positive ConcCellTotal ]` throws a broadcast error .
Are you adding the ` vec ` values to just the nonzero values of ` mat ` , or to all values ?
I'm mainly adding to a few indices of a matrix with shape ( 100k , 100k )
So your original matrix is sparse , the vector is sparse but in the resulting matrix the columns corresponding to nonzero coordinates in your vector will be dense .
I also get an efficiency warning , due to the ` mat [: , i ]` assignment .
Some tests with 10x10 sparse mat and vec : #CODE
The sumation has 38 nonzero terms , up from 10 in the original ` mat ` .
For a really large ` mat ` , memory issues might chew into the dense performance , but the iterative sparse solution ( s ) isn't going to shine either .
I may be able to squeeze more performance from ` addvec2 ` by indexing ` Ml ` more efficiently .
It doesn't affect the time for the dense version , but the others iterate over the nonzero values .
For every odd ` i ` , the result of ` -10**i ` is negative , thus the operand of ` exp ` is positive , and you will get values above the threshold .
I also have tried the numpy.resize [ link ] #URL , but still i dont know what happens with the data that are left out from the " base_freq " and which ones are left out .
Actually I need all of them , so I cannot resize to the " test_freq "
When I do the append command I get the following error : #CODE
PS : This is not similar to the questions asked before because I am not trying to concatenate two numpy arrays .
` concatenate ` gives array ([ 1 ., 2 ., 3 . ]) and ` vstack ` throws an error because of unequal sizes .
The last few lines of the traceback indicate the likely problem : the data file is read as a flat ( 1D ) array , and then scipy tries to reshape the array to an ( n , 3 ) array , which fails .
That means the size of the flat array is not a multiple of three ( you'd get the same error if you tried to reshape ` np.ones ( 4 ) .reshape ( -1 , 3 )`) .
I realized that list wasn't representative - I'm starting to see where the error may come from - form some value that a natural log may not work on in the 3000+ values I am transforming .
Then your sum is also ` NaN ` .
You can also get rid of the ` n.abs ` , ` sum ` is fine since your squares are positive .
For the others , in cases where you just put the value incrementally just create empty lists and use ` append ` , and for cases where you need to access an arbitrary element pre-allocate with ` None ` or empty strings `''` .
Although it is usually possible to directly translate MATLAB code into Python , efficient , well-written MATLAB code generally does not translate into efficient , well-written Python .
One can see that going row-wise , there are exactly ` 7 ` (= m-2 ) zeros separating two triplets of ` [ 1 , -2 , 1 ]` .
Hint - you can use regex for extracting the data in the columns , for your above case the expression will look similar to this ` exp = r'column .
It looks like your cmap ( which I assume is a ` cm.ScalarMappable ` ? ) doesn't contain norm and autoscale required for colorbar .
I've written a method using norm from the numpy linear algebra module ( numpy.linalg ): #CODE
Don't use ` / = ` unless you are aware of datatype issues ( it will produce an array of zeros with any integer datatype ) and in-place modification , though !
Python numpy : reshape list into repeating 2D array
` numpy.random.rand ` was designed to mimic MATLAB's ` rand ` function which , incidentally , now also accepts shapes as arguments .
Assuming the ` x ` and ` y ` values in your file directly correspond to indices ( as they do in your example ) , you can do something similar to this : #CODE
If you have regularly sampled x y points , then you can convert them to grid indices by subtracting the " corner " of your grid ( i.e. ` x0 ` and ` y0 `) , dividing by the cell spacing , and casting as ints .
Unfortunately my ` x ` and ` y ` values don't actually correspond to indices .
If x and y are not indices how is the 2d array of z values defined ?
In this example , it's the min / max of the data with a 10x10 grid in between .
@USER : excellent answer , I liked the histogram method .
This allowed me to show areas with lower populations in a lighter shade and the higher ones ( due to overlay ) in a darker color .
10 number combinations and see how they stack up against the posted odds .
For python , there is something called the pydata stack .
That being said , I see no questions about any other package in the pydata stack that parses CSV , so it's fairly safe to assume , this is how people do it in python .
You might try to ` shuffle ( wrk )` in-place instead of returning another array from permutation , but for me difference was negligible
Integer Division ( Only for Python 3 ) You divide by integers which will floor your division .
I suspect the original formula was right but you didn't encode it right in Python .
A function that consisted of two loops ( outer and inner ) should be optimized - ideally by replacing Python loops with Numpy functions .
The inner loop could be optimized easily ( variable ` inner_loop `) , but can the outer loop be changed as well ?
The problem is that ` inner_loop ` reads the vector ` U ` , one element of which is changed in each outer loop .
If I optimize the outer loop by using the matching Numpy functions , the " recursive " element ( updating ` U [ i ]`) is lost .
` np.hstack ` ake a sequence of arrays and stack them horizontally to make a single array .
This is functionally equivalent to ` reshape ` ( in the sense of data ordering , etc ) .
Obviously , it still requires the same information about ` x `' s shape , but it is a more concise way of handling the reshape .
Your ` (: , :) ` reduces it to 2 dimensions , combining the last ones .
Your ` reshape ` example does something different from MATLAB , it just collapses the last 2 .
I had the same problem with a column that was of dtype ' o ' , and whose max value was 9999 .
Distance being ` sqrt (( x1-x2 ) ^2 + ( y1-y2 ) ^2 )` .
I have been using the norm approach but I have nothing to compare times to see whether pdist or cdist provide a really drastic improvement in process speeds considering point sets < 1000 or so
Can anybody tell me how to implement the steady state finite difference on the roof side ( arbitrary or irregular domain ) and join it with the rectangular domain code like the goalpicture above ?
such that diff would be converted to a np.int16 numpy array .
But it could be that some values in B are larger than the respective in A so doing ` diff = A - B ` would result in overflow for some values .
I wan't diff to be able to hold negative values as well and not overflow essentially .
just to note , you've clobbered df's ` max ` and ` min ` methods .
` df [ ' max '] = ...
` is fine and does not clobber a DataFrame's ` max ` function .
It is safer to you brackets when referencing ( to avoid this issue ) , but dot notation is pure convenience and works fine as long as you're not using one of the DataFrame's named properties or methods .
I should've thought to take advantage of the fact that df.B will never be both ( greater than the max for A ) AND ( less than the min for A ) .
my above program is working fine for single frequency sample . but for a song it shows an error at the line indata = np.array ( wave.struct.unpack ( " %dh " % ( len ( data ) / swidth ) , data )) *window . error msg is operands could not be broadcast together with shapes . how to fix the problem ??
So you basically want to perform a fft with python ?
The most common numerical implementation is called fft ( for Fast Fourier Transform ) .
Do you even know what the fft transform of your audio should return ?
This takes longer ( ~15 min for numpy ) but has the advantage that it could be potentially optimized with compilation flags to the current CPU architecture and therefore marginally faster ( that shouldn't matter much in practice though ) .
One matrix is the ' pattern ' whose rows have to be compared separately with the other matrix rows ( all rows ) , then to be able to extract statistical values of mean equal to pattern , std ,...
the problem with this code is that pattern rows can contain only zeros or ones which makes m [ 0 ] [ 0 ] or m [ 1 ] [ 1 ] not to exist returning a TypeError .
However , as I said , when you have a row from pattern that has no values of one label ( let's say all zeros on a row ) , m [ 1 ] [ 1 ] does not exist .
Your A matrix ` cov ` is 2x2 hence you get an error .
If the sizes matches you can column concatenate ` mat ( c_ [ x-x0 , y-y0 ])` or use any other stacking option .
You could use ` cdist ` from ` scipy.spatial.distance ` to efficiently get the euclidean distances and then use ` np.argmin ` to get the indices corresponding to minimum values and use those to index into ` B ` for the final output .
I then must sum these values , using the expression :
where the ` axis=0 ` denotes which dimension to sum .
It can translate and compile your OWN code .
It does nothing with existing Python and numpy functions , especially ones that are already written in C .
` eval_laguerre ` and ` sum ` are already compiled ; there's no Python code for numba to rewrite .
equivalent with a list comprension inside the ` sum ` : #CODE
I do not just want to stack 2D-arrays on top of each other but create a 3D volume by merging multiple binary 2D slices which are separated by empty space .
Define a 3D-matrix with zeros : #CODE
Put a rectangular area with ones in the first slice of ` A ` : #CODE
How can I now merge these two slices linearly , so that the result would be a ' pyramid ' of ones inside the 3D-volume .
You should evaluate distance and angle between every dot at first slice and every dot at the last , when for every step you should linear decrease distance with constant angle .
To get the row with the highest number of non-zero cells and the highest sum you can do #CODE
yes I'm trying to reach the longest row with non-zero values and with largest density sum . which should correspond to the diameter of the circle and then by storing all indices of these non / zero elements .. the middle index should be the center of the circle and half the length of the index list is the radius .
Expose boost :: multi_array < std :: string , 1 > to python
I tried to expose a ` boost :: multi_array std :: string , 1 ` as numpy array , but I get #CODE
I want to reshape the following data frame : #CODE
The ` apply ` method aligns the various indices for you ( and fills missing values with ` NaN `) .
Instead of returning a 3D array , we tell einsum to sum the values along axis `' j '` by omitting it from the output subscripts , thereby returning a 2D array .
However , we want two sets of products but to sum only one of them .
Interesting to see that ` einsum ` isn't necessarily quicker than using ` dot ` in a loop !
I think that if the first dimension is large , the ` for ` loop will drag ` dot ` down quite a bit and ` einsum ` might be the faster option .
I tried with ` i , j , k = ( 300000 , 20 , 40 )` and got ` dot ` as 1.11 s compared to ` einsum ` at 273 ms .
I've got a question for you ( but still I can't comment under your answer ): is it true that det ( AAT ) ( where A.shape == ( n , n-1 ) , so AAT.shape == ( n , n )) is always == 0 , or only most of the time ?
Well , we know that ` Rank ( AB ) <= min ( Rank ( A ) , Rank ( B ))` , If we apply that we get directly , that ` AA '` has a maximum rank of ` n-1 ` , therefore is singular and therefore ` det ( AA ') == 0 ` .
` PP = dot ( P.T , P )` is singular
If I use it so , although I have the column and row indices , but I can not interpret them .
Use ` np.random.choice ( ..., p =p robabilit )]` Also , probabilities have to sum to 1 .
So one way to fix this is to pass the ` len ` of the list of tuples , and then pick the elements with the respective index ( or indices ) , as described in the other answer .
If you turn ` lista_elegir ` into a ` np.array ` first , this will also work for multiple indices .
Second , the probabilities have to sum up to 1 , exactly .
Concatenation of zeros might be faster since this example uses matrix multiplication to achieve the required result and not allocation of the required size .
What I would like to do is plot ( using matplotlib ) the full hourly data for each of the ` ids ` , but I can't think of a way to do this .
Clarification : Essentially , for each id I want to concatenate all the hourly data together in order and plot that .
@USER Essentially , for each id I want to concatenate all the hourly data together in order and plot that .
It might also be of interest to stack the data frame so that you have the dates and times together in the same index .
well , there is no dgemm function in numbapro cublas ( #URL ) , some similar names , but they deal with diagonal matrices and this is kinda not what i need .
An interesting thing is that while these other functions give more power than indexing , with modes like clip and repeat , I don't see them being used much .
@USER , a frozenset is a collection of unique elements that cannot be modified -- i.e. an immutable set .
You can use the maximum and minimum indices of none-zero elements in row and column to get the scope of your 1's then fill your array indices based on this scope : #CODE
Find first nonzero column in scipy.sparse matrix
I am looking for the first column containing a nonzero element in a sparse matrix ( ` scipy.sparse.csc_matrix `) .
Actually , the first column starting with the ` i ` -th one to contain a nonzero element .
If we set ` i = 0 ` instead , ` D ` is unchanged since column 0 already contains nonzero entries .
The 1st value greater the 0 will the 1st column with a nonzero value .
With a ` csc ` matrix it is easy to find the nonzero columns .
The last line shows how many nonzero terms there are in each column .
` np.nonzero ( np.diff ( arr.indptr )) [ 0 ] [ 0 ]` would be the index of the first nonzero value in that diff .
Do the same on a ` csr ` matrix for find the 1st nonzero row .
Numpy and dot products of multiple vector pairs : how can it be done ?
I want to get ` dot product ` of N vector pairs ( a_vec [ i , :] , b_vec [ i , :]) .
you could use a python ` set ` to get all the unique tags in your file .
If you want to maximize instead , you can use that ` max ( f ( x )) == -min ( -f ( x ))` #CODE
Finding indices of elements in vector
Now , I want to find out the indices of these elements ( in ` sampled_vec `) from ` orig ` .
are elements in orig unique ?
If elements are unique in ` orig ` .
Index ` a ` with a list or array representing the desired indices .
( Not ` 1 , 3 , 5 , 7 , 9 , 10 ` , because indexing starts from 0 . ) It's a bit confusing that the indices and the values are the same here , so have a different example : #CODE
Calculating distances between unique Python array regions ?
I have a raster with a set of unique ID patches / regions which I've converted into a two-dimensional Python numpy array .
As the array was originally a raster , a solution needs to account for diagonal distances across cells ( I can always convert any distances measured in cells back to metres by multiplying by the raster resolution ) .
The other issue with your time measurement of the parallel function is that you call ` reshape ` on your array object after you run your parallel function .
It could be the case that the parallel function is faster , but then reshape is adding extra time ( although ` reshape ` should be quite fast , but I'm not sure how fast ) .
But item lookups and string methods decode the strings on the fly using the correct encoding .
On one hand , I was just looking for a straight up listing somewhere , but on the other hand this is fast and easy and will get me the answer I need for any built-in codecs ( and even the result for non-builtin ones can be determined on the fly pretty quickly ) .
Although I accepted @USER ' s answer , I am also adding my own answer here that was inspired by @USER ' s , but goes a little further in that it gives the widths of encodings for all characters supported by a given encoding , and the character ranges that encode to that width ( where a width of ` 0 ` means it is unsupported ):
How to implement the fast fourier transform to correlate two 2d arrays ?
For example , A*A would be equal to A ( 1*1 = 1 , zeros everywhere else ) .
So in effect , take the fft of A , take the fft of B , multiply the two results , and then get the inverse of that result .
Sorry , I'm guessing there's something I'm misunderstanding about the fft .
Note that ` fft ` / ` fft2 ` / ` fftn ` / etc all take a shape argument which will handle the zero-padding if you pass it the shape that you'd like things to be padded to .
To obtain the desired linear convolution you need to pad both arrays with zeros , to a length at least twice the size of your original matrix 1 : #CODE
It means I'll have to transpose the array in memory sooner or later .
Generation of values above diagonal numpy array
I am only concerned about a method in which I can only consider vertices which are above the diagonal .
In this manner , when adding vertices to our newly formed graph , the only new vertices formed will be those that reside in locations above the main diagonal .
what do you mean by " I am only concerned about a method in which I can only consider vertices which are above the diagonal " and what is ` range ( M1 , M2 , M3 ... M10 )` ?
That is , the key point here is to make the index of the inner loop dependent on the outer loop .
If you don't want to include the diagonal , use the inner loop with ` range ( i+1 , 4 )` .
I do not see how this will help me examine specifically the column and row locations only above the diagonal of my matrix without harding coding exclusion of locations 5 , 10 , 15 , 4 , 8 , 9 , 14 , 12 , 13
In your example you used ( x-y ) > =0 , which would imply including the diagonal .
I tried to calculate the absolute distance from each value of lat and long and return that index to extract that data , but i am getting wrong value .
Just add absolute values for the difference ` np.abs ( lats-latpt )` , etc . and your implementation should work fine .
I square the difference instead of taking the absolute value , because the former is slightly faster and results in finding the same index anyways .
With 2 ` np.genfromtxt ` reads I can load your data file into 2 arrays , and concatenate them into one 9x9 array : #CODE
would give a matrix ` mat2 ` such that ` mat2 [ i , j ] = mat [ i , j ] !
Will numpy.roots() ever return n different floats when a polynomial only has < n unique ( exact ) roots ?
Assuming the polynomial with said coefficients has exactly k unique roots , will the following code ever set number_of_unique_roots to be a number greater than k ?
To be more precise , the perturbation ` f ( x ) +eps g ( x )` of a polynomial ` f ( x )` with a k-fold root will in general have unique roots and the k-fold root separates into a nearly symmetric star with k vertices around the original k-fold root with a distance proportional to ` eps^ ( 1 / k )` .
Efficiently count the number of occurrences of unique subarrays in NumPy ?
I have an array of shape ` ( 128 , 36 , 8) ` and I'd like to find the number of occurrences of the unique subarrays of length 8 in the last dimension .
I've seen this question but it's about finding the first occurrence of a particular subarray , rather than the counts of all unique subarrays .
It would only need to access each element once and then at the end you automatically have the number of unique subarrays as well as their locations if you stored them .
The question states that the input array is of shape ` ( 128 , 36 , 8) ` and we are interested in finding unique subarrays of length ` 8 ` in the last dimension .
Get the number of unique subarrays #CODE
Get the count of occurrences of unique subarrays #CODE
This is great I hadn't thought to use ` np.lexsort ` and I didn't know about ` np.diff ` but we're actually interested in finding the * number of occurences * of unique subarrays , not just the number of unique subarrays .
Could this method be adapted to return the unique subarrays along with their counts , as @USER ' s answer ?
Here I've modified @USER ' s very useful answer to return the counts of the unique subarrays , as well as the subarrays themselves , so that the output is the same as that of ` collections.Counter.most_common() ` : #CODE
Which I want to convert to a 1D numpy array where each entry is the sum of each 2x2 submatrix in the above img numpy array .
` sum ` allows you to specify an axis or axes along which to sum , rather than just summing the whole array .
This allows NumPy to loop over the array in C and perform just a few machine instructions per sum , rather than having to go through the Python bytecode evaluation loop and create a ton of wrapper objects to stick in a list .
@USER : There's nothing stopping you from using ` sum ` on an array of dtype uint16 , but if you're worried about it overflowing , ` sum ` has a ` dtype ` parameter .
However , note that when the dtype of an array has size less than the default platform integer , NumPy will already autopromote the sum to the default platform integer size , so you may already be getting a ` uint32 ` or ` uint64 ` instead of a ` uint16 ` .
I recommend updating , but if you can't , you'll have to compute the sum one axis at a time : ` img = img.sum ( axis=2 ) .sum ( axis=1 )` .
If you've put an ` str ` into the array , numpy uses ascii to encode it , so unless you've gone to special effort to do something different , `' ascii '` should be right .
NumPy : Sparse outer product of n vectors ( hyperbolic cross )
I'm trying to compute a certain subset of the full outer product of ` n ` vectors .
The computation of the full outer product is described in this question .
*ik = K ` ( indices start at one ) .
My current approach is to create a hierarchical list of the indices fulfilling the condition above and then calculating the products recursively , which has the advantage of reusing some factors .
This implementation is a lot slower than the computation of the full outer product using NumPy ( for same ` n ` and ` k `) .
To get a visual idea of the indices I'm interested in , here is a picture for ` k=3 ` , ` K=n ` :
In particular there could be multiple sets of indices that verify ` i1*i2* ...
*ik < K ` , so how do you know which ones to take ? or am I missing something ?
( this condition is not mentioned in the other post about the outer product you link to ) .
I am trying to concatenate two arrays : a and b , where #CODE
I tried using hstack and concatenate as : #CODE
Now the 1st dimensions of both arrays match , and you can easily concatenate on the 2nd .
There a many ways of adding the 2nd dimension to ` b ` , such as ` reshape ` and ` expanddims ` .
I don't want to assemble one large array containing the smaller ones from the list , because the smaller ones are not really small either ...
Then don't you just want a generator to generate a random pair of unique `` ( x , y )`` coordinates ?
Generator random unique coordinates : #CODE
For small n a simple method might be to generate the set of all possible xy coordinates using ` np.mgrid ` , reshape it to a ` ( nx * ny , 2 )` array , then sample random rows from this : #CODE
Following @USER ' s suggestion , an alternative method is to sample from the set of nx*ny indices into the flattened array then convert these directly to x , y coordinates , which avoids constructing the whole nx*ny array of possible x , y permutations .
If you have scikit-learn installed , ` sklearn.utils.random.sample_without_replacement ` offers a much faster method for generating random indices without replacement using Floyd's algorithm : #CODE
I thought a random choice of the linear indices would be better , but in practice it's not faster .
@USER Good point - there's a very slight performance improvement to be had from sampling from the linear indices , but in practice the runtime is dominated by ` np.random.choice ` in both cases
I've found an interesting approach based on ` scipy.signal.resample ` function , but the problem with this function is that it return absolute value for each ` y ` value of the given vector .
My 2 cents solution : Since you are downsampling , and you know the downsampling ratio , you can just extend the peaks so to not lose them after downsampling , i.e. so that the peak duration in the downsampled signal is at least one sample , that is , is at least ` ceil ( SR1 / SR2 )` samples before downsampling , where ` SR1 ` is the original sample rate , and ` SR2 ` is the downsampled rate ..
A guess , but it looks like ` matplotlib ` was compiled with a version of ` libm ` , that has some vectorized implementations for sin / cos ( e.g. MKL ) , and then run on a different machine that doesn't have the same ` libm.so ` .
Extract the max height above all images
Zoom the image so that each image reaches the max height keeping its ratio
Does the matrix A have two rows with three zeros or two columns with three zeros ?
Instead of doing the " OR " inside the append , you'll need to do an if statement : #CODE
Weighted data problems , mean is fine , but Covar and std look wrong , how do I adjust ?
I'm trying to apply a weighted filter on data rather the use raw data before calculating stats , mu , std and covar .
Means all look in line , but when we look at covar and std they are significantly different in terms of scale and also direction #CODE
You're applying a filter of the form ~ 1 / x x / n weighted so that it's sum is equal to the original .
Hi , sloppy from me , I wanted to use Pct change rather that absolute to address stationarity , just noticed my error readiing your response and have edited .
Using pct instead of absolute return would remove the non-stationary if you use EWMA properly ON A ROLLING WINDOW .
Just find each NaN , and linearly interpolate to the adjacent four values ( which is , sum up the values at ( y +- 1 , x +- 1 ) ) -- this will seriously limit your error enough ( calculate yourself ! ) , and you don't have interpolate with whatever complex method is used in your case ( you didn't define ` method `) .
To sum that up : The basic idea is to precalculate a " mean-array " with the mean value of neighbours along the z-axis .
Then find all the indices of of * NaN * and replace them with the according value from the pre-calculated " mean-array " ?
So we need to convert the list inside the list to tuple to convert the outer list to set
If ` xmin ` , ` xmax ` , ` ymin ` and ` ymax ` are the indices of area of the array you want to set to zero , then : #CODE
Then only the first fifth diagonal elements are chosen , not the whole 5x5 block .
This provides a convenient way to construct an open mesh from sequences of indices .
Instead of " zipping the two together " , numpy uses the indices like a mask .
In other words , whether the input is a row / column vector is a shorthand for how the indices should repeat in the indexing .
Efficiently finding range of indices for positive values in 2D numpy array
The first is to replace any zeros between positive values that are less than three columns apart in each row .
Otherwise I end up replacing all of the zeros in a given row instead of just those between positive values .
For your second problem : Create an array and add the indices of the locations of positive values .
You could technically create an arraylist of objects containing the indices , the value , and whether or not it was positive .
Then you could use a forloop to grab and return all the ones you want .
NetworkX requires that each node have a unique name .
You could generate unique names and then set the elements of your array to be attributes of the nodes , e.g. #CODE
@USER : The reason I need lazy evaluation rather than accessing the array with key ( even efficiently ) , is that each affectation might be related to different kind of indices .
For the example above , I only set the diagonal to be variable .
Are any rows populated only by zeros ?
Are any columns populated only by zeros ?
The rank of the matrix will tell you how many rows aren't zero or linear combinations , but not specifically which ones .
D is the diagonal matrix of eigenvalues .
So create any matrix V , the diagonal matrix D that has at least one zero in the diagonal , and then A will be singular .
- the unique order in each category .
( in this case I don't know what it means to consider their unique order )
OK , first let's prepare your data set , by selecting the relevant columns and removing leading and trailing spaces using ` strip ` : #CODE
If you have too many levels for this to work , or you want to consider the individual words in ` catB ` as well as the bigrams , you could apply your ` CountVectorizer ` separately to each column , and then use and use ` hstack ` to concatenate the resulting output matrices : #CODE
So that when ` loc ` is nonzero , #CODE
You can use advanced indexing to slice the first item of the subarrays and then wrap that in an outer array : #CODE
Find the sum of each four second period and find the minimum of these intervals #CODE
Look at the internals of ` roll ` .
You can roll the data back and forth to achieve the same result .
Actually my real task is much more complicated but I think this wonderful roll function should do the job .
so copy paste , add the " } " and replace " log ( " with " Math.log ( " and you'll see that your code runs perfectly .
However , keep an eye on JyNI ( #URL ) , which will vastly improve this issue , but is still in an early state .
Numpy sum function returns 1.67772e +07
Then , I want to know how many elements I selected , so I want to calculate the sum of the elements of this matrix .
` diff == 0 .
` will give an array containing ` True ` s in the positions corresponding to zeros , and ` False ` s elsewhere .
@USER Note , " if ` diff ` is a float array " in my comment .
But to say that ` diff == 0 .
` won't match zeros is simply false , even if ` diff ` has a floating-point type .
The type of your ` diff ` -array is the type of ` H1 ` and ` H2 ` .
Since you are only adding many ` 1 ` s you can convert ` diff ` to ` bool ` : #CODE
I tried to find a way to us the sum function , but I am afraid I can't see how it can help me .
( Note that booleans can be directly converted to integers . ) Once you know how to do it for single numbers , see if you can broadcast the operations used across entire arrays .
Numpy reshape array of arrays to 1D
Is it possible to reshape x or y when I have declared them like I did ?
When you create the array , concatenate the lists with ` + ` instead of packing them in another list : #CODE
Have you tried ` append ` ?
It is just a form of ` concatenate ` , and is often used erroneously .
Note that this ` append ` is different from the list append .
I prefer that people use ` concatenate ` directly : #CODE
When I find a figure I append the board states that can be achieved by making moves with that figure to a list and then keep searching for figures .
To get the index of the ones , you can use ` a.where ( a == 1 )` for an array ` a=array ( ... )` .
Improve Polynomial Curve Fitting using numpy / Scipy in Python Help Needed
These data were extracted from cassandra table which stores the details of a log file .
I have a function ` f ( x , t ) = cos ( t ) *t + x ` and i want to display the change of the result over the width ` x ` and time ` t ` at discretised time steps ` t_i ` and discretised width steps ` x_j ` .
Numpy provides a very nice function for doing differences of array elements : ` diff `
Note that ` diff ` is applied twice : once in each dimension ` axis= ` .
This looks like a good approach , but why are you using ` diff ` ; that is , what in the question looks like a diff ?
" display the change of the result over the width x and time t " -- So interpreting " the result " as F , computing the changes in x and t between each step is exactly what ` diff ` will do .
But as tom10 suggested I didnt want to plot the change / diff , but the absolut positions ;) I may not have expressed myself very well .
Creating NxM indices from a Python array
Pandas is built on top of NumPy and works well with the whole Python scientific stack .
I have several bumpy arrays and I want to concatenate them .
I can't used neither append nor extend because I have to deal with ` numpy arrays ` and not ` lists ` .
` concatenate ` takes a list of arrays ( or tuple ) , It isn't restricted to a tuple of 2 .
` concatenate ` can accept a sequence of array-likes , such as ` args ` : #CODE
Using PCA from sklearn I can compress this to 500 rows and 15 columns .
So for example , similar points would be ones on the same axis .
Independent ones would be ones in different axes .
You can then stack the original axis with the PCA result if you like : #CODE
I essentially want to find all the points ' orthogonal ' to the special axis , and the ones which are most ' similar ' to that axis .
Using the matrix notation in ` numpy ` ( you would have to manually account for an intercept by adding a row of ones to X ) : #CODE
Finding indices that fall between given ranges
Extract indices of a 2D binary array
I want to extract the indices where ' 1 ' is surrounded by neighboring 5*5 elements consisting of 1s .
This could be fixed , manually padding the ` kernel ` array with zeros when using ` binary_erosion ` .
The mean of ones that are not surrounded by ones is less than ` 1 ` and is rounded to ` 0 ` as it is an integer array .
I thought , maybe the python3 range-generator is the slow part , so I handicapped my c++11 implementation by storing all 100M numbers in a std :: vector first ( using iterative push_back's ) , and than iterating over it .
Your python version is measuring the time to create two huge lists , plus the time to create two arrays from those lists , plus the dot product .
Moreover , when I create one std :: vector in c++ , it take approximately 4 seconds , like stated .
I think the reason ` dot ` might not be multithreaded is just that it's very unusual to try to take the dot product of a 100 million-component vector .
Far more common is taking the dot product of a million small vectors in parallel , e.g. A lot of numpy's low-level core routines aren't multithreaded , only the higher-level ones .
Also note that any discussion of BLAS is irrelevant because the usage case in the question is an integer dot product , and that doesn't exist in the standard BLAS .
I'm not aware of any level 1 BLAS dot implementations using multiple threads because there is normally no advantage in doing so on what is normally a memory bandwidth limited calculation .
For a large set of randomly distributed points in a 2D lattice , I want to efficiently extract a subarray , which contains only the elements that , approximated as indices , are assigned to non-zero values in a separate 2D binary matrix .
It seems you can squeeze in a noticeable performance boost if you work with linearly indexed arrays .
I think you need to look more at the error message and stack , to identify where that ` .A ` is .
I recall from other SO questions that one of the learning packages had switched to using sparse matrices , and that required adding ` .todense() ` to some of their code ( which expected dense ones ) .
It has to be buffered , because the ` I ` values are not guaranteed to be unique .
Incorrect reshape of vtk -> numpy array ?
I tried reordering dimensions in ` reshape ( ... )` , it did something , but it has never shown the data it is actually supposed to show .
EDIT : I also tried ` reshape (( nx , ny , nz ) , order= " F ")` ( fortran ordering ) and now I get a much better image ( with jet colormap for better clarity ) which is almost correct , but the data is suspiciously rotated by 90 , plus I would like some authoritative explanation which ordering to use and why ( which one is used by VTK internally ? ) .
@USER : ` vtk_to_numpy ` returns 1d array ( that seems to be correct also as indicated by other examples at SO which always reshape afterwards ) .
I want that my indicator will be represented not only by a simple index i , but as a two indices x , y so I can know where is my indicator along the list .
I tried to reshape the list like that : #CODE
Update : Is there any way to find the indices of x , y for an integers that are smaller than NUM ( let's say NUM=12 )
Is there any way to find the indices of x , y for the data if for example I need to find all the elements that are smaller than 12 ?
Using reshape is better , as @USER ' s answer .
Inconsistency between gaussian_kde and density integral sum
And calculation of integral sum of it : #CODE
I got resulting integral sum completely different to 1 : #CODE
You can also truncate to a non-negative Gaussian kernel with similar logic .
The Pandas csv reader is supposed to be quite a bit faster than the ` numpy ` ones .
You seem to be missing the limits on the ` y ` value in the histogram redraw in ` update_data ` .
May be difference in version of matplotlib but the indices threw an error for me .
NumPy : Pick 2D indices of minimum values over 4D array
I've approached this by attempting to retrieve the indices of the values of ( v , w ) that give the minimum values of A over ( x , y ) .
so , given a value of x and y , you'd like to find the indices of v , w that minimize A ?
If you also want to sort the outer array itself , then , well , sort it : #CODE
I wouldn't add this ( confusing ) comment about the outer array .
I assumed row by row means you want to sort outer list also
How to set x lim to 99.5 percentile of my data series for matplotlib histogram ?
Do you want to just set the visual limits of the plot , or do you want to actually exclude the outliers before even computing the histogram ?
Also , it may not be possible to exclude just the outliers depending on how the histogram bins divide the data .
I think in many cases it makes more sense to just exclude the outliers from the histogram computation at an earlier stage .
Instead of posting a picture , can you post sample code and data showing how you actually create the histogram ?
Your sample code doesn't include any histogram nor any sample data .
Then you could do your histogram on ` trimmed_data ` .
With that , interpolation can be done easily by getting multidimensional indices : #CODE
the vectors above contain all the indices needed to index your original matrix .
I didn't even know about the possibility to work with indices as vectors .
I had to change : ` conv = np.round ( alt / 500 . )` to ` conv = np.round ( alt / 500 . ) .astype ( int )` to use conv as indices .
Vector indices are very useful .
For numpy arrays I would suggest using ndarray.flat but h5py Datasets don't have a flat / flatten attribute .
For instance to simply chunk along the outer dimension : #CODE
Then I also have a separate time series with ` N ` time points that I would like to correlate with all the time series in the matrix .
Weird numpy.sum behavior when adding zeros
However , it surprises me that adding zeros to ` sum ` can change the result .
Here is one more example , which also demonstrates python's builtin ` sum ` behaves as expected : #CODE
Indexing a 4D array using another array of 3D indices
A have a 4D array ` M ` ( ` a x b x c x d `) and an array ` I ` of indices ( ` 3 x f `) , e.g. #CODE
However the order for input values and output gradients is corret , so when the first value corresponds to the first gradient ...
To use ` concatenate ` you need to add an axis to your arrays .
` axis=0 ` means ' join on the current 1st axis ' , so it would produce a ( 200,400 ) array .
If you look at the code for ` dstack ` , ` hstack ` , ` vstack ` you'll see that they do this sort of dimension adjustment before passing the task to ` concatenate ` .
The ` np.array ` solution is easy , but the concatenate solution is a good learning opportunity .
Deep Learning IndexError : too many indices for array
Basically , your total data is 16 points , and the ` y ` value of each is a unique value ( something between 1 and 16 ) .
If I understand correctly , instead of overwriting Sound_Fc and y at each iteration , you want to append them to growing x and y vectors .
By taking its transpose with ` .T ` , we make it vertical and now its shape is ( 16 , 1 ) .
Pandas / Python : 2D histogram fails with value error
so , I try to feed them into a numpy 2D histogram with #CODE
can obviously not be a good 2D histogram vector , i.e. , when transforming the original values I have to catch such cases .
another thing : numpy histogram had some issues for me with DataFrame series , so I had to get a proper numpy.arrary from the series to plot them properly , e.g. , #CODE
if you try to merge the first and the last array with the middle , you end up with only the middle as long as you reshape the original array derived from the empty list to match the shape of the middle array .
As long as the other dimensions match , ` concatenate ` has no problems with ` 0 ` dimensions .
I am using TextWrangler which should be able to encode these charaters .
I would stick to arrays and use the ( somewhat inconvenient ) dot function for now , converting back and forth to matrix is annoying .
` tensordot ` just reshapes the arrays and does ` dot ` .
Matrix ` * ` probably does ` dot ` as well .
With arrays you need to use ` dot ` .
If ` m ` is a one dimensional array , you don't need to transpose anything , because for 1D arrays , transpose doesn't change anything : #CODE
How to find the number of unique elements in a list of ndarrays ?
I have a ` list ` of ` ndarray ` s and I want to find the number of unique ` ndarrays ` in it .
possible duplicate of [ Efficiently count the number of occurrences of unique subarrays in NumPy ? ] ( #URL )
first of all , your arrays in ` l ` are in ** 1d** , can you show what ` ndarray ` you actually tried to get unique out of it and what is your expected outcome ?
Thanks Dan but that flattens all arrays and gives me a unique on that , I meant to count the number of unique arrays that I have .
Is that what I can expect to ultimately squeeze out of the method or are there optimizations that I can use to further increase the performance ( still using Python ) ?
Or even parallelize with OpenMP in Cython the outer ` for ` loop .
You can try to thread the outer loop replacing ` range ` by a ` prange ` ( see [ Cython documentation ] ( #URL )) , but overall I think that's pretty much as fast as you will get : You do process rather large arrays .
The result you get is , that the gradient at ` x ` is not even near to zero , so the current x cannot be a local minimum .
Not exactly an answer but more of a suggestion : you could load your data into a 2D list with ` csv ` , parse through its lines to append a ` None ` when it s too short and then convert the list to a ` numpy.array ` using the constructor .
Secondly , its cardinality is directly related to the sample size and I need to make more samples than the cardinality , e.g. make a list of 1000 elements from a Zipfian distribution of only 6 unique values .
The ` reduce ` call is essentially a cumulative sum : #CODE
Therefore , the solution is not unique .
Is it possible to append a small amount of metadata to numpy memmap files ?
but I can't manage to figure out how to translate the max / where statement to the this 2d array ...
This one is based on ` np.cumsum ` to generate a mask of elements to be set to zeros as listed next - #CODE
This one starts with the idea of finding the last negative element indices from ` @USER ' s answer ` and develops into a mask finding method using ` broadcasting ` to get us the desired output , similar to ` approach #1 ` .
Since your method for a 1D array uses ` max ` / ` where ` , you may also find this approach quite intuitive .
The ` data , i , j ` arrays of a ` coo ` are assigned without change , similarly if you give a ` csr ` the ` data , intptr , indices ` arrays .
I tend to focus on questions without an answer , and often skip ones with an accepted one .
( Also , not related to the memory usage , but you've got the operators in the wrong positions if you want to calculate a root mean square . Currently , you're taking the mean , then the square , then the root . You want to do square , then mean , then root . Otherwise , you're just getting the absolute value of the mean . )
You could use ` dot ` to sum as you square , reducing the memory usage .
Higher up in the stack trace it says #CODE
The results appear distorted ( y-scale is unusually small ) when I plot them after the transpose operation .
Efficient way to pair up indices and calculate a sum
I want to create pair all the points , creating six pairs , in a way that each point can only pair once in one combination and that the sum of distances is as big as possible .
It might have to do with how matlab and python handle boundary conditions in the ` diff ` operator .
In Matlab , ' is the conjugate transpose .
@USER - MATLAB and ` numpy ` compute ` diff ` the same way for 1D arrays .
Now , ` np.hstack ` horizontally stacks 1D arrays together and so this will append a 0 at the front , then apply the ` diff ` operator , and the perform the dot product with ` prec ` .
There is no such thing as the transpose of a 1D array .
If you explicitly want to make the 1D array a column vector , you need to include an additional dimension and make the second dimension 1 , then transpose it .
` px = 255* ( min - px ) /( min - max )` .
Note , ` min ` and ` max ` are simply the 1th percentile and 99th percentile values and are actually stored as floats .
Also min will most likely be negative .
you should not shadow builtins such as min / max ... just an aside totally unrelated to your problem
I want in use advanced indexing to pull out the nonzero values .
I know the indices of the nonzero values so #CODE
The actual ` x ` array is much larger along the first dimension , but the nonzero structure is always indicated by ` idx ` so I need it to broadcast along the first dimension .
` idx ` then has the nonzero entries row-for-row .
Notice in the first row of the both ` 4 x 9 ` nested arrays in ` x ` that the ` 4 3 1 0 ` entries are nonzero .
It's not clear how ` idx ` relates to which elements are nonzero .
With this or the lines above it's the casting of the floats as bools that provides a mask that eliminates the zeros .
Using this technique every element in ` np.arange ( idx.shape [ 0 ]) [: , None ]` ( which has shape ( idx.shape [ 0 ] , 1 ) and therefore is a column vector ) will be broadcast with every row in idx .
I want to do a dot product of p and q .
Soooo , how am I to do dot products between a sparse matrix and a 1D numpy array ( numpy matrix , I am open to either ) without losing the sparsity of my matrix ?
Unusual histogram after image decimation
After simple image decimation using : ` img_decim_arr = img_arr [: : 2 , :: 2 ]` , I obtained histogram very similar to original image histogram :
Decimation using : ` skimage.measure.block_reduce ( img_arr , block_size = ( 2 , 2 ) , func= np.mean )` ( 2x2 block averaging ) which is recommended method for downsampling ( found on stackoverflow in some discussion ) produces very characteristic histogram :
Can anyone please explain and give some theoretical hints about how downsampling affects image ( 2D signal ) histogram ?
It definitely screws with matplotlibs histogram function , because having floats makes it think differently about how to place the bin borders .
Please give a method to fit the best possible curve with these x and y values and the values of x ( or the indices of the value of x in array x ) where local maxima exists .
array minm and maxm contain indices of minimas and maximas respectively ...
The function returns an array of indices that you can use to reorder the rows of ` arr ` : #CODE
Both are sets of indices into another , large 4D matrix ; their data type is ` dtype ( ' int64 ')` .
Alternatively , if the values are indices , i.e. non-negative integers , you may use ` pandas.groupby.get_group_index ` to reduce to one dimensional arrays .
I know this can be done by default using the library , but the catch is I need the dates in the X-axis to be unique , and if there is no corresponding ` Camapign_id ` , then that value should be 0 .
How to divide matrix elements by corresponding matrix row sum in numpy ?
Then I could maybe transpose ` s ` , and copy it along the columns , then do an elementwise division of ` M / s ` , but even this seems too hacky .
Use ` tile ` to repeat it along the dimension on which ` sum ` operated .
I'm not sure this will work reliably , it'd be better to just use a variable that defined the slice ranges / indices / mask and use this to reference back to the original df
The reason you cannot join is because you have different dtypes on the indicies .
Need to iterate through each column and find max and min
I am trying to iterate through each column and find the max and min but my code so far results in an " TypeError : ' numpy.float64 ' object is not iterable " .
First column max would be 2.2 , min = 1.61 etc .
with ones if two nodes are connected and zeros otherwise .
So as a next step I want to transform the relevant_eigenvectos to contain only zeros and ones like above .
MATLAB also has this supported , and so if you want to translate between the two , Boolean ( or ` logical ` in the MATLAB universe ) is the better way to go : #CODE
However , ` np.flatnonzero ` would compute row-major indices of those points that satisfy the input condition into ` np.flatnonzero ` .
The reason why you're getting an error is because you are trying to use row-major indices to access a 2D array .
I need to translate a Python program into Javascript and I see numpy.apply_along_axis many times but do not know how to do that in Javascript .
There are , for example , ` zip ` and ` flatten ` functions .
@USER : assume that block of zeros have constant length which we know .
Should the zeros constitute * exactly * 25% of the total array , or is that allowed to vary with the randomness ?
You can create random indices list with the length of ` 0.25 ` main list then change that indices in main list to 0 .
` randint ` chooses indices with replacement ; you'll get indices chosen twice if you do this .
@USER : ` rand ` is an array of indices , but there's nothing guaranteeing that all those indices are distinct .
Try it for yourself on an array of size 1000 : you'll likely end up with * fewer * than 250 zeros because of repeated indices .
How to append multiple items to NumPy Array
How do I successfully append these values to one field based on the number it occurs in the dictionary , i.e. value1 = containercolor1 , value2 = containercolor2 , value3 = containercolor3 ?
Current code to append to NumPy Array : #CODE
Is there some overflow in the calculation , so that the mean result is actually mod 256 , and so when I subtract I am getting the correct number .
Vectorized sampling of multiple binomial random variables
( You can use ` df.to_csv ( ' my_data_file.csv ')` to dump all data into a csv file . ) Your code seems to work all fine on my PC with some artificial data .
Likewise I found by trial and error that I want ` maximum ` not ` max ` .
That each row / total sum of column
So on with many rows and columns for this txt file I need to find the column sum for each column from .
2nd column to last column and then divide each numerical row with the column sum .
So far I have done until split and strip and from there I am not able to select select from second row .
yes , that is the problem I need numbers alone so that I can find the sum of column , for [[ ' Aaa 0.4567 0.6780 '] I need just the numeric part so as for all rows and so then I need to find the sum of column
and the output i need is row of colum sum ( column )
I saw the " numpy " tag but you might consider python's " pandas " as alternative where you get the desired output within only a few lines ; this way you can easily divide each entry by the sum of its column / row .
Along with the line I have to calculate entropy on the output data , and so I appiled the formula for that on output as , entropy = - sum ([ p * math.log ( p ) / math.log ( 2.0 ) for p in df ]) but it throws error as , NameError : name ' math ' is not defined May I know how can I get it done .?
remove , append , extend , sort , index , for lists
append : #CODE
For instance , creating a list in a for loop with ` append ` is fine , while doing so with Numpy should be avoided at all costs .
@USER My point is that , ` remove ` , ` append ` , ` extend ` are rarely used in numpy , because often there is a more efficient way of achieving the same goal .
@USER : I am sorting x values and corresponding indices will be stored in sortId .
@USER : wen u will plot the graph with these x and y values , you'll get peaks at x [ 1 ] , x [ 2 ] , x [ 3 ] and troughs at x [ 2 ] , x [ 5 ] , x [ 7 ] . these are the indices of peaks and troughs
When I plot the data , I see local maxima at ( 2 , 5 ) , ( 4 , 9 ) and ( 7 , 10 ) and minima at ( 3 , 1 ) , ( 6 , 2 ) and ( 8 , 7 ) which would correspond to indices 3 , 5 , 7 and 1 , 0 , 8 , respectively in your original array x .
The plot looks like this ; I shifted the x-values so that they correspond to the returned indices in ` minm ` and ` maxm `) :
minm and maxm contain indices of minima and maxima respectively .
I want to concatenate two csr_matrix , each with ` shape =( 1 , N )` .
` mean ` and ` sum ` are performed by ` dot ` multiply with a dense array ( of ones ) - and the result is a dense matrix .
Even if there is only one nonzero value in a row , that row sum will be nonzero .
Is there any way to create a ` sympy.cosd ` function which evaluates ` cos ( pi / 180 * x )` but prints ` cosd ( x )` ?
` y.subs ( x , pi / 180*x )` works here , but if the expression in the cos is complicated ( like polynomial expression of various parameters ) it might not be convenient to search which parameters must be replace with a ` pi / 180 ` and which must not .
Thanks , but it prints ` cos ( 0.0174532925199433 *x )` , which is not really readable either .
I think it gives ` cos ( 0.0174532925199433 *x )` because the argument is a symbol : ` x= sympy.Symbol ( ' x ')` I'm using it to do symbolic calculus ( so I can check the formula ) , then I lambdify it for numerical application .
Does ` import math sum ( df.applymap ( lambda x : x * math.log ( x ) / math.log ( 2.0 )) .values )` give you what you want ?
How can I extend the sin function so that it predicts the future points of the series ?
Let's say I have an array with a finite amount of unique values .
And I also have a reference array with all the unique values found in ` data ` , without repetitions and in a particular order .
And I want to create an array with the same shape than ` data ` containing as values the indices in the ` reference ` array where each element in the ` data ` array is found .
So , to do the ` 0's ` to ` 1's ` and vice versa changes , we need to bring in the indices used for sorting ` reference ` , i.e. ` np.argsort ( reference )` .
Don't you need to reshape it after shuffling ?
@USER ` flatten ` doesn't shuffle anything , it just reshapes the array into a 1-dimensional array .
Get the indices of the least often occuring values in numpy
My goal is to find the indices of the labels in the labels_array that occur the least often .
If there are several labels in labels_array that satisfy that condition , then I want to get all the corresponding indices .
Currently I count the number of occurrences of each label , then get the indices of the ones that occur the least .
Next , I need to get the indices at which labels_array is 2 or 3 and that would be exactly the result I'm looking for .
I think ` reshape ` executes in constant time and for creating the indexes you only need a single ` for ` .
I understand that ` reshape ` is about as efficient as it gets .
However , when I attempt this method , I am returned an error ` list indices must be integers , not numpy.float64 ` .
Think harder about the pixels you need to sum for the average .
Because you're using ` numpy ` to do the computation for you ... why don't you just declare an array of ` zeros ` via ` np.zeros ` first ?
Thanks for your answer , but x , y is for determine which part of the selected image to resize , not resizing the whole image .
If you have a list of values , say v , it is relatively easy to find indices i with the property that v ( i-1 ) , v ( i+1 ) < v ( i ) - which would make v ( i ) a local max -- but in your picture you are not simply computing the distances between successive local max but are ignoring some of them ( the little spikes before the big spikes ) .
You need to find a criteria that would distinguish between the little spikes which you don't want and the big ones which you do .
You can use ` scipy.signal.argrelextrema ` to find the index of the max values : #CODE
After finding the max values , you can use ` numpy.linalg.norm ` to find the distances : #CODE
This is very helpful , but I have problem with finding index of max value.I have did excactly like you but my plot mark only 2 max value.Mayby you know why ?
Ok I know why i have this problem.Becouse some of my max value are repeating.If you look closly at my chart you will see that most of my tops are flat .
this data is just a list [ 1 , 2 , 1 , 2 , 1 , 2 ... ] where the max values is always the same : 2 .
Python list append is better for iterative work .
` reshape ` and ` transpose ` change values like the ` shape ` and ` strides ` , but otherwise don't do anything to the data buffer .
` peek ` should return like ` pop ` without removing the element . again , this question and the next ( about ` append `) are regarding standard arrays , not numpy arrays .
without ` peek ` , i need to ` pop ` and ` append ` a value off a potentially large standard array .
i need the value so i can use matlab's ` cast like ` to match its type . although it's only a single ` append ` , i worry it requires a copy of the whole array . again , we're talking about standard arrays here , i know ` numpy.append ` copies .
since it changes the order of elements wrt the flat standard array , i worry other ` numpy ` functions will depend on the order -- but you're saying either order behaves the same , except for the order of elements when ` reshape ` ing ?
Yes , the use of ` [ ]` is a syntactic issue that does not easily translate across languages .
there are many ways to flatten an ndarray . which one would be fastest and make the least copies in this use case ?
In the example , the model function is ` a * exp ( -b * x ) + c ` , where ` a ` , ` b ` and ` c ` are some constants to be determined to best represent the data with this model .
I've been playing with Python's FFT functions in order to convolve a 2D kernel across a 2D lattice .
I compared a list comprehension using ` gmpy2.sin ` , ` map ( sin , list )` , and ` gmpy2.sin ( list )` and there wasn't much difference ( around 10% ) .
For each element in a randomized array of 2D indices ( with potential duplicates ) , I want to " +=1 " to the corresponding grid in a 2D zero array .
You could speed it up a little by using ` np.add.at ` , which correctly handles the case of duplicate indices : #CODE
You can convert ` R , C ` indices from ` idx ` to linear indices , then find out the unique ones alongwith their counts and finally store them in the output ` grids ` as the final output .
The better solution I've found so far ( for doing it dynamically ) , relies in the transpose operator : #CODE
But I wonder if there is any better / easier / faster function for this , since for 3D the transpose code almost looks more awful than the 3 if / elif .
If I'm not wrong , transpose just returns an array with the ` strides ` changed , right ?
Set duplicate elements as zeros
Once we have the indices to sort ` data ` , to get a sorted copy of the array it is faster to use the indices than to re-sort the array : #CODE
To revert a permutation you can sort the indices that define it , i.e. you could do : #CODE
You can detect all zeros with ` data == 0 ` which will give you a boolean array and then perform ` np.any ` along each row on it .
You can use ` numpy unique ` .
Since you want the unique rows , we need to put them into tuples : #CODE
prints out the unique elements in the list .
I want to solve some system in the form of matrices using ` linalg ` , but the resulting solutions should sum up to 1 .
After solving the system their values should sum up to 1 , like .3 , .5 , .2 .
If the solution you found does not sum up to 1 , then adding the extra constraint would yield no solution .
sum up to 1 , then adding the extra constraint would yield no solution .
and the sum of the squares of the difference , ( also computable as ` f ( xbest )`) is : #CODE
The only place there could be another intersection is to the right , but for that I'll need to allow the ` sqrt ` in the def of ` B ` to get a negative argument without throwing an exception .
An easy way to do this is to add ` 0j ` to the argument of the ` sqrt ` , like this , ` sqrt ( 1+0j- ( 1 / x ) **2 )` .
So it's not a crazy looking function , and the jumping between Re=const and Im=const is just the nature of sqrt ( 1-x -2 ) , which is pure complex for ` abs ( x ) 1 ` and pure real for ` abs ( x ) 1 ` .
Where the min on the right is the one initially found , and the zero , now at ` x=-0 ` and ` 1 .
By analogy : it's easy to make a ball that will roll down a hill , but harder to make a ball that will automatically test all hills .
Most optimizations use a gradient descent approach , and just " roll down the hill " .
So my suggestion is understand how to find local roots ( zeros , minima , or whatever ) , and realize that it's also reasonable to just leave the question at local roots .
The second solution uses a polar representation , e.g. A ( x ) is given by r ( x ) exp ( i theta ( x )) , and offers a better understanding of the behavior of the square root as x passes through unity towards zero .
The difference between the sqrt ( 4 ) and the sqrt ( 5 ) is around 10% .
@USER +1 for your solution although don't you want to include the max number for the complete range ?
You can use ` newline ` instead of ` delimiter ` as ` np.savetxt ( ' fo.txt ' , result , newline = ' \t ' , fmt = ' %s ')` or use ` reshape ` to save array in a single row as ` np.savetxt ( ' fo.txt ' , result.reshape ( 1 , len ( result )) , delimiter = ' , ' , fmt = ' %s ')` .
I tried to change the indices in np.arange , but it didn't work .
That is , I need to input two indices and get the element that corresponds to it ( matrix [ 0 ] [ 9 ]= 34 ) but I also need to be able to get the indices upon just knowing the value is 34 .
The elements in my array will all be unique .
I'm quite new to python , so if you could make sure to let me know what the functions you find do and the steps to retrieve the indices for the element , I would very much appreciate it !
You can use the ` nonzero ` method : #CODE
I would like to append these values together into an array of ` len() = 100 ` such that the output is #CODE
I set the parameter ` max ` to be a positive integer , and the data output has the index format #CODE
The parameter ` max ` determines the output of the array ( i.e. the length of the array ) and the ordering .
However , this is determined by the parameter ` max ` .
My only idea is to make some sort of sorting tree , but I am not sure how to execute that with this ` max ` parameter .
The only pattern I see is ` [( 00 10 20 30 40 ) ( 11 21 31 41 ) ( 22 32 ) ( 33 43 ) ( 44 )]` which is ( number of elements in each set ) ` [( max-0 ) ( max-1 ) ( max-2 ) ( max-3 ) ... ( max-n )] where n <= max ` each set is ` 11 ` more than the last set and each number in each set is offset by ` 10 ` .
It isn't clear what you want to do with these numbers or indices , but here's an example of putting them in a 2d array : #CODE
There's a ` numpy ` function to generate those indices : #CODE
which generates an array of 10 values , shape ( 10 , ) with a mean of 10 and values spread within + / - 3 std deviations .
In my problem , ` poly ` is an ndarray with the shape ( 5 , 2 ) .
The two outputs I have seem to be the same , but error occurs when I input to ` ImageDraw.Draw ( img ) .polygon ( poly )` .
In any case , to do the double dot with a ( 100,100 ) element you don't need ` .T ` .
so , assuming you have a three-dimensional ` numpy.ndarray ` , you get three indices ; so you just set all these boundaries to 0 : #CODE
The fourier transform of a rectangular window is the ` sinc ` function .
@USER of course , that would only apply to your continuous mathematical considerations , and not to your simulation ( ` sinc ` having infinite carrier and all ); but it illustrates the important fact that if you need to simulate a field , and you need things to be non-periodic , you will need to apply a window , and you will need to chose both a window and a " safe " padding area large enough to sufficiently suppress the effects of the side lobes of the sinc you inherently get when rectangularly windowing your data ( which is what you do when you set 0 around it ) .
To sum it up , my primary question is : How can I use ` numpy.random.poisson ( lam , size )` to model a number ` n ` of objects being scattered over a 2-dimensional field ` dx*dy ` ?
Now I just need to join the two arrays of x- and y-values to an array of ( x , y ) tuples .
How can I reduce the memory footprint of this numpy gradient descent code ?
I think what @USER is trying to get at is that you are taking the dot product of two large matrices , gradient descent may not be the best way to go when you have this large of a data set .
See for instance conjugate gradient .
The middle ` 2 ` comes from ` [ 2+x , 2-x ]` pairs , and the 1st ` 2 ` from the outer list .
One option is to apply a transpose or axis swap to ` arr ` .
` np.concatenate ` and its variants ` hstack ` , ` vstack ` , ` dstack ` , ` column_stack ` , join arrays .
Look at the code of the ` stack ` functions to get some ideas on how to combine arrays using these tools .
On the question of efficiency , my time tests show that concatenate operations are generally faster than ` np.array ` .
Thanks , I didn't know this usage of transpose .
each row of ` idx ` defines the row / column indices for the placement of each sub-array along the ` 0 ` axis in ` x ` into a two-dimensional array ` K ` that is initialized as #CODE
However , it's slightly probelmatic to pass a list of 2D index arrays , and corresponding arrays to add at these indices , to ` np.add.at ` .
You can temporarily ravel ` K ` and ` x ` to give you a 1D array of zeros and a 1D array of values to add to those zeros .
The only fiddly part is constructing a corresponding 1D array of indices from ` idx ` at which to add the values .
You could think of those ` idx ` places as ` bins ` of a histogram data and the ` x ` values as the weights that you need to accumulate for those bins .
I think there might have been a time when negative indices didn't count from the end of the array ; the documentation line you're looking at might date from that time .
In this question six months ago , jez was nice enough to help me come up with a fast approximation for the outer product of row differences , i.e. : #CODE
But now I am trying to do Local Fisher Discriminant Analysis , where each outer product is weighted by a matrix A that has information about the pair's locality , so the new line is :
What is a fast way to calculate the weighted sum of pairwise row-difference outer products ?
Also , I want to stay low on memory , so I need to avoid create a big matrix , then applying the function to every value and sum over the columns ( the length of v is more than 5e+4 and the length of w is 1e+3 ) .
Other construction commands , ` ones ` , ` empty ` , etc , may also be useful .
Using ` set_yscale ( ' log ')` doesn't work because it wants to use powers of 10 .
If I set the yscale to ' log ' , and ylim to [ 0.1 , 1 ] , I get the following plot :
The problem is that a typical log scale plot on a data set ranging from 0 to 1 will focus on values close to zero .
What do you mean when you say that the log y-axis * " doesn't work " * ?
It isn't mathematically possible to represent 0 on a log scale , so the first value will have to either be masked or clipped to a very small positive number .
You can control this behavior by passing either `' mask '` or `' clip '` as the ` nonposy= ` parameter to ` ax.set_yscale() ` .
Also , I choose not to make the x-axis on a log scale because my particular data has a good linear line without it .
@USER ` encoder.classes_ ` will gives you all the unique labels
Getting an arrays of equal length from numpy histogram or plotting the uneven array
The values of the histogram .
To get probabilities out of an histogram you have to normalize ( i.e. divide by the sum over all histogram values ): #CODE
Broadcastable Numpy dot
#URL has timings for another multidimensional ` dot ` , involving ` ( 100 , ) * ( 10,100,100 ) * ( 100 , )` arrays .
I'm working on a Raspberry Pi project in which I need to take about 30 images per second ( no movie ) and stack each 2D image to a 3D array using numpy array , without saving each 2D capture as a file ( because is slow ) .
I found this Python code to take images as fast as possible , but i don't know how to stack all images fast to a 3D stack of images .
Does anyone of you know how to stack the 2D images taken in this code to a 3D numpy array using Python and the Raspberry Pi camera module ?
In any case , ` np.ndindex ` generates the indices that will iterate over an array of a given shape .
Another option is to reshape the initial dimensions , so you can do a flat iteration on those : #CODE
The mean vector observed has the first few entries as non zero , while the last ones tend to zero .
As I said in the question I want to parallelize the processing of the indices of the vectors .
Maybe I am missing something but why would it sum to zero ?
Your kernel itself sums to zero , ` sum ([ -1 , 1 , 0 ]) = 0 ` .
Because the kernel is normalised by its sum , you get a kernel full of ` nans ` and thus a convolved array full of ` nans ` .
Why would you need to create new ones ?
Then you have an array ` std ` with all the values you want .
I can see that we append all the ` div_array ` into one list ` std `
Why not use the power of ` numpy ` for division if you are using it for ` std ` ?
Once you have your ` t ` matrix ( ` 9x19 `) , the ` std ` function gives you the standard deviation on every column ( as well explained by Matt ) .
@USER The numpy array isn't holding other numpy arrays , it's holding Python ones .
I'm using xlrd to read data from an excel-sheet , then I filter it with a filter function and then I create a histogram with the numpy.histogram function .
If I eliminate the empty cell in excel , the histogram gets right .
I being doing a numpy dot product .
But what if I wish to take the dot product again ?
@USER , a ** matrix ** must be of ` ndim ` == ** 2** , in your case ( some array's ` ndim ` > 2 ) , the general solution is exactly * dot product of two arrays* .
Now , your \bar p_i^2 are defined by a dot product .
Note the T , for the transpose , because the dot product takes the sum of the elements indexed by the last index of the first matrix and the first index of the second matrix .
The transpose inverts the indices so the first index becomes the last .
You H_t is defined by a sum .
No , only the datasets 3 , 4 and 5 ( the ones which have the exact same number of rows and columns ) .
Thereby I can avoid the creation of the array fitparam and the concatenate command .
I get the error ` ValueError : could not broadcast input array from shape ( 1 , 2 ) into shape ( 1 , 61 )` in the line ` np.insert ( matrix_of_coupons_and_facevalues , row_no+1 , rowtobeadded , 0 )` and I understand why , but I don't know how to proceed .
and I am trying to append data with : #CODE
Now in order to concatenate ` lineitem ` to ` Ticket_data ` , it must first be implicitly cast from nested lists to an array .
An easy workaround would be to apply ` .values ` to the series first and then apply ` std ` to these values ; in this case ` numpy's ` ` std ` is used : #CODE
For numpy , the std is unbiased by 1 / n .
For pandas , the std is biased by 1 /( n - 1 ) .
It's a blend of me not explaining it right and subtle tricks : ` symMatrix ` contains the 6 values of the diagonal + upper triangle of the 3x3 matrix , for each point in a 3D array ( thus explaining the [ 6 , x , y , z ] shape ) .
A simpler way of iterating over all but the first axis is just to reshape into a ` [ 6 , 500 ** 3 ]` array , transpose it to ` [ 500 ** 3 , 6 ]` , then iterate over the rows : #CODE
In other words , stack all the arrays on top of each other .
I've tried numpy's ` vstack ` , ` concatenate ` and some others , but without any luck .
if all images are the same size ( i.e 8421 ) then you should be able to use vertcat ( vertical concatenate ) , if not , please can you specify what your matlab input ( from Python ) looks like ( i.e. is it a cell array or individual vectors ) etc
What is ` mat [ ' descrs ']` ?
Again , tell us what you get from ` mat.dtype ` , ` mat [ ' descrs '] .dtype ` , and ` mat [ ' descrs '] [ 0 ] .dtype ` .
Perhaps a workaround is to use transpose ` x.T.fillna ( x.mean ( axis=1 )) .T ` #CODE
` pd.Series ` have a clip method ( defined in pandas / core / generic.py ) .
` np.clip ` ( defined in numpy / core / fromnumeric.py ) defers to the first argument's ` clip ` method if it has one : #CODE
This is important because CSR matrices store their data as a triple of ` ( data , indices , indptr )` , where ` data ` holds the nonzero values , ` indices ` stores column indices , and ` indptr ` holds row index information .
the column indices for row i are stored in
` indices [ indptr [ i ]: indptr [ i+1 ]]` and their corresponding values are
So , to find rows without any nonzero values , we can just look at successive values of ` M.indptr ` .
so that if ` det ( A )` is big , then ` det ( A^{-1} )` is small .
For the norm of the 2 matrices , ( if you pick a sub-multiplicative norm ) , you have : #CODE
where || is a reasonable choice of a norm that is sub-multiplicative .
For the elements on the diagonal of ` 1 ` at the RHS , you need very small numbers from ` A^{-1} ` if the elements of ` A ` are very big .
Try for example ` from numpy.random import rand ; from scipy.linalg import inv ; A = rand ( 4 , 4 ); print ( dot ( A , inv ( A )))` and you'll get a unitary matrix .
you shouldn't be that sure of numerical errors for matrices of this size : using a random matrix generated with ` A=rand ( 400,400 )` gives a result ` res = dot ( A , inv ( A ))` with off-diagonal elements smaller than ` 1e-11 ` in absolute value ( It depends of course on the matrix itself , not just on the size )
They use different BLAS calls ( ` gesv ` for ` solve ` vs ` getrf ` followed by ` getri ` for inv ) , but the result is indeed the same ( and with only ~10% computational overhead for the solve call with ` 1000x1000 ` matrices . ) and both are based on LU factorisation , so you are right .
At least in the current dev version of numpy , ` inv ` also uses ` gesv ` .
See [ here ] ( #URL ) - the ` lapack_func ` used in ` inv ` is ` gesv ` - it just passes an identity matrix as the " B " parameter .
Is there some sort of ` h5py ` ` flush ` method ?
Do you get an error stack ?
It seems to be running out of dset_ids rather than dtype ids , but still the problem only occurs when I specify the dtype .
I haven an array and need to access certain indices of elements as below #CODE
` np.where ` returns a ` tuple ` of indices .
In this case the tuple contains only one array of indices .
It returns a tuple containing multiple arrays which together define the indices of the non-zero elements .
How to filter out data into unique pandas dataframes from a combined csv of multiple datatypes ?
Answering ones own question is encouraged on StackOverflow if you solve the problem for yourself .
But , I will have to sort and append ( in case the word already exists ) - Which will be better in this case ?
However note that this array is likely to be both very big ( 1M * number of words ) and very sparse , which means it will contain mostly zeros .
Because this numpy array will take a lot of memory to store a lot of zeros , you might want to look at a data structure that is more memory efficient to store sparse matrix ( see scipy.sparse ) .
np.histogram2D with fixed colour gradient
` imshow ` has the paramter ` norm ` too and it should work .
This is a comment regarding laplace versus the hessian matrix ; this is no more a question but is meant to help understanding of future readers .
The following pictures show the difference in results between using the minimum of ` second_derivative_abs = np.abs ( laplace ( data ))` and the minimum of the following : #CODE
The color scale depicts the functions values , the arrows depict the first derivative ( gradient ) , the red dot the point closest to zero and the red line the threshold .
Might it be sufficient if I calculate the gradient on the sum of the absolute first derivative list entries ?
` second_derivative = np.gradient ( sum ([ df*df for d in first_derivative ]))` ( with ` sum ` conserving the shape for the sake of argument )
I'd try not to use some second derivative at all , but calculate the absolute gradient at all points ( sum over the squares of the first dimension of the result of ` np.gradient ` , like you said in your comment ) , and then find the threshold region from that , and find the minimum inside the threshold region ( if you function is sufficiently complicated , finding global minima can be really hard ) .
@USER The sum of all gradients will not yield the flattest area ; in this test case picture it would yield the center of the 2d gaussian . this is by no means the flattest area .
Note that if you are only interested in the magnitude of the second derivatives , you could use the Laplace operator implemented by ` scipy.ndimage.filters.laplace ` , which is the trace ( sum of diagonal elements ) of the Hessian matrix .
I do not have the space to post the 2D testcase without polluting the question ; difference in results between laplace and hessian seems to be that they yield different points .
I evaluate either the minimum of the laplace or the sum of squares along ` x.dim , x.ndim ` for the hessian .
I added pictures of a 2d testcase for laplace and hessian .
I divided my data set into smaller ones every 0.5 K , the data is Voltage vs Temperature .
The debugger will get a first chance exception , so you can print a stack trace and inspect local variables .
Selecting elements with boolean produces a copy ( diff pointer ) #CODE
Go get the previous row , concatenate [ 1 :] and [: 1 ] , assign the result as the next row .
But you can use ` roll ` instead of concatenation : #CODE
I suggest a way using python ` zip ` built-in function.You can concatenate your array with itself then all you need is picking every 3 followed elements !
You can get the desired output by creating an array of indices which you can use to index into you're original array : #CODE
No , you would need to expand your 2D arrays to the same size , filling the extra columns and rows with some stopgap ( zeros , nans , masked data ) .
Something fun happens when you use boolean NumPy arrays as indices for other NumPy arrays : #CODE
But where the boolean index array is shorter than the dimension of the indexed array , it will assume ` False ` for the missing boolean indices : #CODE
The ` resize ` function ?
The documentation says that when the requested size is larger than the array , it will ( other than the ` resize ` NumPy array method I tried to use here ) not fill in zeros ( ` False ` in case of a boolean array ) but instead repeat the array .
* All * real ( non-zero ) numbers have a pair of complex cube roots in addition to a real cube root , not just the negative ones .
Thought it would be as simple as doing ` mat [ -idx , range ( len ( idx ))]` but that doesn't work .
` mat [ -idx , range ( len ( idx ))]` isn't going to work since negative indexes already have a meaning - ` count from the end ` .
I have a dataframe which has a value of either 0 or 1 in a " column 2 " , and either a 0 or 1 in " column 1 " , I would somehow like to find and append as a column the index value for the last row where Column1 = 1 but only for rows where column 2 = 1 .
Taking the partial transpose with respect to one of the qubits , you get the partially transposed quantum state of the restricted 2 qubit system .
From the graph , we see that the non-parametric density is nothing but a smoothed version of histogram .
In histogram , for a particular observation ` x=x0 ` , we use a bar to represent it ( put all probability mass on that single point ` x=x0 ` and zero elsewhere ) whereas in non-parametric density estimation , we use a bell-shaped curve ( the gaussian kernel ) to represent that point ( spreads over its neighbourhood ) .
Never append to ` numpy ` arrays in a loop : it is the one operation that NumPy is very bad at compared with basic Python .
This is because you are making a full copy of the data each ` append ` , which will cost you quadratic time .
Instead , just append your arrays to a Python list and convert it at the end ; the result is simpler and faster : #CODE
You would have to create the initial array with two dimensions , then append with an explicit axis argument .
IIRC , if it's feasible to calculate a ` svd ` , you can easily use it to calculate a pseudo inverse .
Since SVD factorizes your matrix A as U*S*V ' , where S is diagonal and U , V are orthogonal , its inverse is V*inv ( S ) *U ' , and the inverse of a diagonal matrix is just the inverse of numbers on the main diagonal .
It means that for some matrix ` M ` , then we can express it as ` M=UDV* ` ( here let's let * represent transpose , because I don't see a good way to do that in stack overflow ) .
Taking the inverse of a diagonal matrix is as easy as taking the multiplicative inverse of each of these elements .
It is unfortunate that ` itemfreq ` returns the unique items and their counts in the same array .
Get indices that puts ` counts ` in decreasing order : #CODE
I'm working on a Raspberry Pi project in which I need to take about 30 images per second ( no movie ) and stack each 2D image to a 3D array using numpy array , without saving each 2D capture as a file ( because is slow ) .
I found this Python code to take images as fast as possible , but i don't know how to stack all images fast to a 3D stack of images .
Does anyone of you know how to stack the 2D images taken in this code to a 3D numpy array using Python and the Raspberry Pi camera module ?
numpy.sum can receive any array-like argument to sum ( be it list or a numpy array ) .
* The relevant bit of source for ` inv ` is here .
However , when I try to call the function by typing ` clip = sigclip ( 0 )` It says ' NameError : name ' sigclip ' is not defined ' .
I have some 3D image data and want to build a stack of RGB images out of single channel stacks , i.e. I try to concatenate three arrays of shape ` ( 358 , 1379 , 1042 )` into one array of shape ` ( 358 , 1379 , 1042 , 3 )` .
If you don't specify a dtype , ` zeros ` will assume you want float64 , and a ` ( 358 , 1379 , 1042 , 3 )` float64 array will take up ~ 11.5 GiB .
When you create your array of zeros , the kernel does not immediately set aside a correspondingly sized chunk of RAM - this only occurs when you actually try to write to those memory addresses , hence why you only see the ` MemoryError ` when you try to assign to ` rgb_stack ` .
Thus creating ` zeros ` as another ` dtype ` solves the problem , e.g. #CODE
Or , since there is no actual sum being computed ( i.e. this is a form of an outer product ) , you can use numpy broadcasting with regular array multiplication after reshaping the input matrix ` M ` appropriately : #CODE
Comparing two lists of strings and getting indices of duplicates
I want to get the indices of the values ( strings ) which are duplicated .
You can use ` max ` and ` min ` function within a list comprehension : #CODE
Partial sum over an array given a list of indices
I have a 2D matrix and I need to sum a subset of the matrix elements , given two lists of indices ` imp_list ` and ` bath_list ` .
What would be a better solution to perform the sum ?
In the general case you can use ` np.ix_ ` to select a subarray of the matrix to sum : #CODE
I thought to generate a list of zeros N long but that doesn't seem to work , and I can't find out how to create a blank array to add to , can anyone help me ?
In the function ` make_spot_3d_spherical ` you got the ` sin ` and ` cos ` mixed up in your definition of ` x0 ` : #CODE
Maybe not so fast , it takes 1 min to process a 10,000,000 row dataset .
In function ` new_RGBtoYCrCb2 ` we slice out ` r ` , ` g ` , ` b ` ( these are only views into the original image data , no copying ) and then stack them together at the end .
You can also reshape and use a dot product without slicing and glueing , which may be faster ( function ` new_RGBtoYCrCb2 `) : #CODE
Axes with repeated subscript labels are summed over - in this case we want to sum over the second axis of ` matr_to_ycrcb_mult ` and the third axis of ` rgb_image ` ( the ` j ` subscript ) .
python - replace last n columns with sum of all files
I have 8 csv files with 26 columns and 600 rows in each . now I want to take the last 4 column of each csv files ( Column 22 to column 25 ) , read the files and sum them up to replace all the 4 columns in each file . for example ( I am showing some random data here ):
Now , I want to sum each element of " h , i , j , k " of from these 2 files , then replace the files last 4 columns with this new sum .
Also join the first column accordingly .
The default format `" % .18e "` results in a 24-character-long scientific notation format padded with trailing zeros .
Here is the full stack trace , but I think you can reproduce it .
You can try using the ` numpy.sign ` function to capture the sign , and just take the square root of the absolute value .
I would like count to be aggregated on a row by row basis where the total count sum is grouped by the fruit type .
As you can see , this makes it impossible for me to properly adjust the ` ret.time ` variable , since it attempts to modify it multiple times , often with nonsensical indices .
I have tried working around this in many ways , including copying the object and editing that copy instead , taking various views of the object , and many other hacks , but none seem to overcome this issue that ` __getitem__ ` is repeatedly called with negative indices that do not line up with the requested slice .
However , for your example , you could use a similar approach to this question to fit an exp .
Taking the ` log ` of both sides will not work in this case because of the constant term ` c ` in the ` a* np.exp ( b*x ) + c ` .
On lines that call its functions ( example : ` self.possibleIncomes = np.concatenate (( x1 , x2 ))`) , Eclipse is giving me an error : ` Undefined variable from import : concatenate ` .
I would like to have a numpy matrix that is filled with unique objects .
I'm interested in all nonzero elements in " test "
` index = numpy.nonzero ( test )` returns a tuple of arrays giving me the indices for the nonzero elements : #CODE
For each row I would like to print out all the nonzero elements , but skipping all rows containing only zero elements .
@USER Thanks for your time but the problem I have is with creating a constraint which would take array of ` x0 ` ( ` x0 ` is the internally passed set of variables ) values and checks if sum all ` x0 * a0 ` values satisfies some conditions .
To save the new ones You need to catch the returned values of cv2.calcOpticalFlowPyrLK
How to perform regression on complex numbers dataset in scikit-learn as most regressors I guess will truncate the imaginary part .
In my task , I represent a concave polygon as a matrix of ones and zeros , where one means that the given point belongs to the polygon .
how do you decide that these are not positions ( 3 , 0 ) , ( 4 , 0 ) are not the ones that are supposed to be filled ?
is it related to path of zeros touching the " border " of the whole matrix ?
You mean the middle two zeros in the very first row ?
I'm not sure if I understand it correctly , but those two zeros are outside the border of the u-shaped polygon defined by the outermost layer of ones .
For each 0 either exists a path of adjacent zeros , so that at least one of those zeros is at position ( y , x ) so that ( x = 0 or y = 0 or x = maxx or y = maxy ) or this 0 should be changed to 1 .
But @USER for brevity I didn't do the full explanation my " sets " are only talked about here as sets because I wanted to make sure it was clear that they were unique , and you wouldn't have to look up the INT for dog six times .
jonrsharpe the question is specific to the speed at which you find the indexes of items in a list that is unique , and the best way to build that list .
I'm being told it can't concatenate str and float objects , but I used convert_objects ( convert_numeric=True ) beforehand , so I'm not sure what the issue is , and when I just print the dataframe I don't see anything wrong , per se .
I have a sparse matrix , G , whose values ( the non-nan ones ) need to be split into test / train sets .
The test_train_split function from sklearn splits on rows but I want it to split on actual indices .
I want to use boolean masks to select the indices but I'm not sure how to do that .
You can split the indices using a random vector with the size of your data ( the number elements ) .
Are the elements unique ?
Just transpose the matrix using standard python functions , like ` zip ` which you can wrap in an array : #CODE
I need to read line by line , count unique words per line and find the rolling median .
Where I've assumed you have a function ` num_unique_words ` that calculates the number of unique words in a string .
This solution works if the entire contents of the file ( number of unique words ) can be loaded into memory .
Given a numpy array like the one below , can you convert it to a multi-channel cv mat , collapsing the data into a single column ?
Done this way , the resulting mat has 3 rows and 2 single-channel columns ( the mat is of type 32FC1 ) .
I need the resulting mat to have 3 rows and a single 2-channel column ( specifically , to be of type 32FC2 ) .
Note that , I set the number of dimensions to be 10 shouldn't the norm be 0 .
Checking it in a debugger , a numpy array has the fields like max , min , type etc apart from the data , which I am not sure a python list has .
I'm trying to fit the home and away defence ratings in a vector ` B ` such that ` Y = exp ( X*B )` where X is a matrix representing the results of the games .
Now since the left-hand-side of the model equation above is in terms of e^x I take logarithms of the vector ` Y ` ( hence entering 0 as 0.001 , log ( 0 ) is not defined ) .
How can I reshape the z-axis to create an array shaped ( 20 , 20 ) ?
If other people want , the can replace your x and np.logspace with x = np.logspace ( 3 , 4 , num=20 ) ( repeat for y ) to see amore pronounced affect with a more appropriate range for log data .
I want to add append each value in y to x individually , and then run this new set through my SATSolver function .
I just tried that and no 20150707 isn't that last element , which would explain why it isn't returning it as the last element , but I'm not sure why it isn't being append it to the list even tough it is in the text file .
Don't open and append to the file in the loop .
Why does numpy.concatenate work along axis=1 for small one dimensional arrays , but not larger ones ?
I couldn't get my 4 arrays of year , day of year , hour , and minute to concatenate the way I wanted , so I decided to test several variations on shorter arrays than my data .
EDIT : I added method t to the end of a section of the code that was already fixed with vstack , so you can compare how vstack will work with this data but not concatenate .
Again , to clarify , I found a workaround already , but I don't know why the concatenate method doesn't seem to be consistent .
The following is the code for creating the mass / stiffness matrix , finding ` U0 ` and then the dot product #CODE
My function returns the matrix multiplication product of transpose ( datavector ) * matrix * datavector , which is a single value .
I get the following value error : ` ValueError : operands could not be broadcast together with shapes ( 50 , 1 , 1 , 50 ) ( 1,400 0,400 0 )` .
` reshape ` in ` numpy ` as different meaning .
When you start with a ` ( 100 , )` and change it to ` ( 5 , 20 )` or ` ( 10 , 10 )` 2d arrays , that is ' reshape ` .
This is like taking an outer product of the 2 , passing all combinations of their values through your function .
With the ` None ` , ` x ` is reshaped to ` ( 10 , 1 )` and ` y ` to ` ( 1 , 5 )` , and the ` + ` takes an ` outer ` sum .
Since ` datarr ` is shape ` ( 4000 , )` , transpose does nothing .
Also you aren't creating a 2d array from an ` outer ` combination of ` x ` and ` y ` .
Inside your function you are adding dimensions to arrays to try to get them to broadcast .
This expression looks like you want to broadcast a 1d array with a 2d array .
When you flatten the array , ` * ` produces all 6 items .
Keep in mind that machine precision for a 32-bit double is ~ 10^-16 , which will be an absolute limiting factor .
The most recent versions of ` dill ` ( e.g. on github ) has " settings " that allow variants of how the pickle is constructed on ` dump ` .
@USER yeah , But since ` np.put ` need to fine the relevant indices for each element and doing this job may not be easy I think using ` np.place ` is more appropriate here !
Python numpy : sum every 3 rows ( converting monthly to quarterly )
I need to aggregate them by quarter , creating a new array where the first item is the sum of the first 3 items of the old array , etc .
You could use reshape ( assuming your array has a size multiple of x ): #CODE
which represent a unique coloring of every node in my graph .
With this I directly get a permutation from graph 1 to graph 2 that is a bijection and so both graphs are isomorphic .
The permutation in this case is this #CODE
Now I want to check if the permutation really is correct ( because I could get a few different permutations that are contenders and so I have to try them all . In this given case I only get one permutation ) .
now I apply my permutation to that matrix #CODE
I had the permutation the wrong way around .
Example 1just didn't throw an error because the permutation was symmetric ( only ` 1 - 3 ` and ` 3 - 1 `) .
When I switched the permutation , I got what I wanted .
I also want to append all the results into one pandas dataframe .
are you trying to do a bootstrap on ` sum ` statistics but requires different sample size for each fruit ?
It's also worth noting that instead of ` for ` -looping , you could always generate a huge sample with ` np.random.choice ` and then reshape : #CODE
Can you show me how to properly append this ?
Because you sum in your OP .
Even with exploiting symmetry , many implementations will max out at about 65000 instances .
I need to reorganize the data so that the rows are the unique ( ordered ) values from SEC as the 1st column , and then the other columns would be VEH1_POS , VEH1_SPEED , VEH1_ACCELL , VEH2_POS , VEH2_SPEED , VEH2_ACCELL , etc .
You can concatenate your inner arrays : #CODE
Actually , the first one does not work ( at least with my version of ` numpy ` and ` python `) because ` numpy ` is able to flatten a 2d ` numpy.array ` but not a ` numpy.array ` of ` numpy.array ` .
@USER Yeah ` flatten ` doesn't works on this case sine it is a list contain numpy arrays !
I've updated the code the find binning indices to fit my request :)
I am guessing there might be an issue with the initial stepsize ( ` old_old_fval = None `) for ` fmin_bfgs ` , and this stepsize may not be flexible for all gradient sizes .
Implementing gradient operator in Python
I'm having trouble re-implementing an old gradient operator more efficiently , I'm working with numpy and openCV2 .
I've tried using numpy's gradient operator but the result is not the same
Using my ` gradient ` returns #CODE
@USER OP is clearly trying to use numpy , but is saying that the default numpy gradient doesn't give the same result as his pure python function .
Therefore , he wants to know how to rewrite his gradient function using numpy .
Your gradient function returns the same thing as ` numpy.gradient ( img ) [: : -1 ]` with certain rows / columns set to zero .
Same as what you can achieve with the gradient operator in Matlab .
As @USER noted , the result was identical except for a row and a column of zeros .
Answering my own question , a good numpy's implementation for my gradient algorithm would be #CODE
I'm also posting the working code , just because it shows how numpy's gradient operator works #CODE
I'm creating a csr matrix using the ` csr_matrix (( data , indices , indptr ) , shape =[ row , col ])` method .
It took me more than 4x more time to execute the construction method ` csr_matrix() ` than building up ` data , indices , indptr ` themselves .
Since I already have the ` ( data , indices , indptr )` tuple , shouldn't it be trival ( and fast ) to construct a csr matrix ?
` indices ` and ` indptr ` need to be of an appropriate index dtype .
I tried to build ` data ` ` indices ` and ` indptr ` as ` numpy.array ` .
One way to do this would be some sort of stochastic gradient descent or simulated annealing .
An obvious generalization is to use some sort of cluster updates , eg Wolff-type ones where you flip a connected cluster of ones or zeros .
The true max is 56,640,625 according to #URL .
A couple of things first off : 1 ) 56 million is the max value when the size of the matrix is 21x21 , not 30x30 : #URL .
I get a max value of around 3.4 billion .
The reason I have ` -det ( mat )` in the energy function is because the simulated annealing algorithm does minimization .
In Octave , if I do ` [ V , D]= eig ( K , M )` , I get : #CODE
The eigenvalues should be the ones in the D array .
eigenvalues / eigenvectors are not unique .
FYI , eigenvectors aren't unique , but eigenvalues should be .
Unless there's degeneracy , the eigenvectors are unique , to a scalar multiple .
So it seems that when ` B ` is a diagonal matrix , Octave simply neglects it by assuming the unit matrix ...
I implemented a LOWESS smoother ( which is the curve you see ) with a tight fit to eliminate noise , since the real waveforms have a non-trivial noise component , and then tried doing a rolling max with a window over the data , but I can't get anything solid .
For the more complicated solution in the link , I think this will involve building up the ystep by looking for local min / max in a given range , something like : #CODE
EDIT : complete example giving local min / max for a range #CODE
I see using this with a rolling window across the function with a Nsteps = 1 to keep adjusting that single max , and returning each value that makes a significant jump ,
If there is more than one min / max you may also need to change this a little as ` indx ` will be an array .
I think I can do this here if I find a max within a window , then increment that window with minimal overlap and look for another max .
Ip ( also includes orthogonal superscript ) is the isophote at point p , which is the gradient turned 90 degrees .
It looks like its a vector orthonormal of the gradient with its modulus .
Computing the directional gradient on an image is very very commonly used technique in image processing .
You can get the X and Y derivatives of the image easily ( hint : numpy.gradient , or SO python gradient ) .
Then , the total gradient of the image is described as :
` p2-p1 / norm ( p2-p1 )` will give you a unit vector in that direction .
V =p 2-p1 / norm ( p2-p1 ) .
With a recarray , you can access columns with dot notation or with specific reference to the column name .
However , if i use ` np.zeros (( 3 , 3 ))` instead of arange and reshape , I get an array that has two triples in a row : ` [( 0.0 , 0.0 , 0.0 ) , ( 0.0 , 0.0 , 0.0 )]` .
` ones ` , ` zeros ` , ` empty ` also construct basic structured arrays #CODE
Output value at [ i , j ] should be equal to twice the sum of d [ i-1 , j ] and d [ i , j ] and add the ouput at instance [ i-1 , j ] to it .
You have to reset the ` temp ` accumulator each iteration of the outer loop .
The unexpected output you got with the original code is because of the way Python lists work : When you append an object to a Python list , only a reference to that object is stored , not a copy of the object .
@USER ok . this is the result : [ ' 5019449 ' , ' 5019450 ' , ' 5019451 ' , ' 5019452 ' , ' 5019453 '] but numbers are different with 10 min ago.because i took new numbers from server .
However some of my data may just contain zeros , in which case I get an V #URL array to reduction operation minimum which has no identity .
@USER correct I noticed the NaN's when I was trying to make the example ( my original code produced arrays of zeros from a csv file ) .
This will generate an ` ( nblobs , ny , nx , nz )` array , which you could then sum over the first axis .
Another option would be to initialize a single ` ( ny , nx , nz )` output array and compute the sum in-place : #CODE
wouldn't you even be able to use a factor of ` spread**-2 ` in the ` meshgrid ` ? and while we're at it , also include the ` **-1 ` from the negative sign in ` exp ` in that ?
Mirroring doesn't come for free , but is still far less expensive than ` exp ` .
I've been running into a TypeError : list indices must be integers , not tuple .
TypeError : list indices must be integers , not tuple
I have a program that takes some large NumPy arrays and , based on some outside data , grows them by adding one to randomly selected cells until the array's sum is equal to the outside data .
So , supposing I want to grow the array until its sum is 1 , 000 , 000 , and that I want to do so by repeatedly selecting a random cell and adding 1 to it until we hit that sum , I'm doing something like : #CODE
It repeats until the number of times by which it has added 1 to a random cell == the difference between the original array's sum and the target sum ( 1,000,000 ) .
However , when I check the result after running this - the sum is always off .
From this you would expect the total sum to be ` 100,679,697 = 200* ( 1,000,000 - 499,097 ) + 499,097 `
However when the random integers are identical ( say , 45 and 45 ) , only 1 is added to every entry of column 45 , not 2 , so in that case the sum only jumps by 100 .
The problem with your original approach is that you are indexing your array with a list , which is interpreted as a sequence of indices into the row dimension , rather than as separate indices into the row / column dimensions ( see here ) .
A much faster approach would be to find the difference between the sum over the input array and the target value , then generate an array containing the same number of random indices into the array and increment them all in one go , thus avoiding looping in Python : #CODE
Pass the ` return_inverse ` argument to get the indices into the sorted array that give the values of the original array .
Then , you can get all of the indices of the tied items by finding the indices of the inverse array whose values are equal to the index into the unique array for that item .
I'm trying to complete the following function , but I have been running into problems with the indexing , resulting in " ValueError : operands could not be broadcast together with shapes ( 0 , 9 ) ( 5 )" .
Additional information : binomialFilter5() returns a 5x1 numpy array of dtype float representing a binomial filter .
For each value at i , j , multiply the binomial filter of length
frames before until two frames after , and take the sum of those
Multiply that by the binomial filter weights at each i , j to get
Of course , we weigh all these sums by the binomial filter , so
I have some which I am fitting to the gamma distribution using scipy.stats .
NOTE : I don't use curve fit directly because it is not working properly and most of the time is not able to compute the parameters of the gamma distribution .
For example , c0 changes betwenn 8 up to 20 ( changing , of course , the std err ) .
I call the function 22 times in a for-loop and fill a pre-allocated array of zeros ` full_arr = numpy.zeros ([8 68940742 , 3 ])` with the values : #CODE
Here are some snippets of the different interpolation files - the smaller one is about 30% smaller in each case and far more uniform in terms of values in the second column ; the slower one has a higher resolution and many more unique values , so the results of interpolation are likely more unique , but I'm not sure if this should have any kind of effect ...?
Say I sort ` sub_arr1 ` along column 1 and then ` sub_arr2 ` comes up and it's column 1 values bisect the sorted ` sub_arr1 ` all over the place , are you saying that with merged sort I can just insert these columns into the growing array at the correct indices and then move to the next ` sub_arrN ` ?
At the moment each call to ` np.argsort ` is generating a ` (8 68940742 , 1 )` array of int64 indices , which will take up ~7 GB just by itself .
Additionally , when you use these indices to sort the columns of ` full_arr ` you are generating another ` (8 68940742 , 1 )` array of floats , since fancy indexing always returns a copy rather than a view .
If you use ` np.argsort ` to obtain a set of indices to sort by , you might see a modest performance gain by using ` np.take ` rather than direct indexing to get the sorted array : #CODE
Regarding your question about why sorting the second array is faster , yes you should expect sorting to be faster when there are fewer unique values in the array .
Values that are equal to the pivot are already sorted , so intuitively , the fewer unique values there are , the smaller the number of swaps needed to fully sort the array .
The only difference is that the " slower " array has a larger set of unique values .
unfortunately , the structured array approach is significantly slower than the other more memory heavy ones , and even with just the first 10% of ` full_arr ` I timeout .
` sum ([ N1 .. N2 ]) = 868940742 ` i.e. there are close to 1BN positions to sort .
I stack the 1D arrays into a new ` ( N , 3 )` array and insert them into an ` np.zeros ` array initialized for the full dataset : #CODE
call stack : get_sorted_arr --> sort_arr --> build_arr --> process_sub_arrs
I posted this question initially because I was puzzled by the fact that a dataset of identical size suddenly began choking up my system memory , although there was a big difference in the proportion of unique values in the new " slow " set .
@USER pointed out that , indeed , more uniform data with fewer unique values is easier to sort :
the fewer unique values there are in the array , the smaller the number
I need some sort of join or merge which will give me dataframe like this : #CODE
Off hand ` x.shape ` is the only ` ndarray ` property that I set , and even with that I usually use ` reshape ( ... )` instead .
See also ` dynamic axis indexing ` : #URL Some ` numpy ` functions use ` transpose ` ( ` rollaxis `) , others construct an indexing tuple .
There are some solvers , such as the ` ode ` ones , that let you define a function that takes a vector variable , and returns a matching vector derivative .
One simple one is to ` flatten ` the input , do the 1d iteration , and then reshape the result .
If I do y.dot ( r ) , I get 14 , assuming that numpy applies broadcasting on y , making it ( 1*3 ) and then it does the dot product with r ( 3*1 ) so resulting matrix will be 1*1 .
For N dimensions it is a sum product over the last axis of ` a ` and
In ` np.dot ( y , r )` , the last axis of ` y ` is ` 3 ` , 2nd last of ` r ` is also ` 3 ` , multiply and sum , the shape is ` ( 1 , )` .
As an alternative answer you if you want the average you can use ` np.cumsum ` to get a cumulative sum of the your elements and divide with the main array using ` np.true_divide ` : #CODE
Sorry for the typo max
( e.g. , What's the max number of columns we should anticipate ? )
I don't want to log in to anything to get the data .
I would like to add or subtract each of the entries with a number that is less than a particular number , in this case 0.5 , so that certain conditions are met , e.g. sum of ( Bi-Ai ) ^2 is minimized , much like an optimization problem .
As an example , let us take A23 , that has a value 2.804 , I need to vary it between 2.304 A23 2.804 so that for a particular value in this range , the sum of of ( Bi-Ai ) ^2 .
It does not append to the same row .
You want to append if you want to add into a list .
You want to concatenate if you want to ' merge ' two lists together to make a single list - which is why your implementation was not making a nested list .
I got the difference between append and concatenate .
this is wrong . numpy creates a new array each time you concatenate / append values . depending on what you want to do / space constraints you can just use python lists and then convert to numpy at the end .
` append ` is natural for lists .
I discourage using ` append ` with arrays .
You need to put the indices in a list not tuple , numpy use tuples for indexing multi-dimensional arrays : #CODE
You need to wrap the indices in a list , not a tuple : ` x [[ 1 , 2 ]]` .
This triggers advanced indexing and NumPy returns a new array with the values at the indices you've written .
I used ` flatten ` to remove a dimension , and to force a copy ( to get a new contiguous buffer ) .
It's easy to choose indices at random when the array is one-dimensional , so I'd recommend reshaping the array to 1D , changing random elements , then reshaping back to the original shape .
I have 2 arrays containing zeros ones .
since they are 2d , ` concatenate ` works nicely ; ` hstack ` and ` column_stack ` .
Have you considered viewing the array as a recarray and using its dot notation property ?
Is Y meaningless in your max ?
I have to concatenate columns of numpy matrix to create new matrix .
I am wondering why concatenate is not working for me ?
Check the dimensions of each array that you concatenate .
For 3 , 2 I took transpose of this any direct way of getting 3 , 2 ?
Suppose you have a 2D numpy array with some random values and surrounding zeros .
Now I want to find the smallest bounding rectangle for all the nonzero data .
You can roughly halve the exectution time by using ` np.any ` to reduce the rows and columns that contain non-zero values to 1D vectors , rather than finding the indices of all non-zero values using ` np.where ` : #CODE
I have defined 2 numpy array 2 , 3 and horizontally concatenate them #CODE
I tried append as Suggested : #CODE
It gave error testing doesnot have attribute append as its of None Type .
According to the ` vstack ` code , it first passes the arrays through ` atleast_2d ` , turning your ` ( 3 , )` arrays into ` ( 1 , 3 )` ones .
` numpy.concatenate ` join a sequence of arrays together.which means it create an integrated array from the current array's element which in a 2D array the elements are nested lists and in a 1D array the elements are variables .
So this is not the ` concatenate `' s job , as you said you can use ` np.vstack ` : #CODE
Also in your code ` list.append ` appends and element in-place to a list you can not assign it to a variable.instead you can just ` append ` to ` testing ` in each iteration .
Thanks how can i then get 2 , 3 matrix using concatenate only ?
@USER That's not the ` concatenate ` job !
I tried list.append but it gave error ' NoneType ' object has no attribute ' append .
Check the Generalized Linear Models page at section Polynomial regression : extending linear models with basis functions .
I know the value of one of the variables and I know that in theory I can find a unique solution for the system if I can somehow plug in that known value .
How do I tell ` numpy.solve ` ( or another function in Python , but please don't recommend least squares , I need the unique , exact solution , which I know exists ) to use the known value of one of the variables ?
well in your case , it doesn't matter if you already have the value of ` t1 ` , any simultaneous eqns of form ` A.x = B ` , can be solved for all the value of ` x ` as ` x = inv ( A ) *B ` .
If the solution you are finding is unique , you can check the correctness of your solution by comparing the calculated value of ` t1 ` with the already known value of it .
" please don't recommend least squares , I need the unique , exact solution , which I know exists " ...
There is no unique , exact solution .
I suggest that you don't concern yourself with whether you're " doing least squares " , or whether it's unique or not .
Let ` linalg.solve ` find the optimal solution ( in the L2 sense ); if the solution is unique , then it is unique in the L2 sense as well .
By definition , if it's an overdetermined problem , there is no " unique , exact solution " .
It's not a problem , but it means we'll need to use least squares , and there isn't a completely unique solution .
Knowing one of the variables makes these problems analytically solvable with a unique solution , the remove-a-column trick lets me successfully find that solution using numpy.linalg.solve , so my question was answered .
Separating the indices returns the shape that I would expect #CODE
How does numpy order array slice indices ?
The double ` dot ` is doing a sum of products twice .
` einsum ` is calculating all the products , and then doing the sum over the 2 dimensions .
There's no intermediate sum to cause rounding errors .
interrupt the slow program , and look at the stack trace ; repeat 5 times .
Assuming the boundary is given as a list of the polygon vertices , you could have matplotlib generate a mask for you over the data coordinates and then use that mask to sum up only the values within the contour .
If you want the mean , just replace ` sum ` by ` mean ` .
There's one thing that is odd : the error message says ` from version import git_revision as __git_revision__ ` , but the actual numpy repository shows this code as ` from .version import git_revision as __git_revision__ ` ( note the leading dot at ` .version `) .
How do I use scipy.ndimage.measurements on an image stack efficiently ?
I have a stack of images , and I have already done image segmentation on the initial image of the stack .
I need to get the mean brighntess of each object in each image of the stack .
So , I have a set a labeled_array , num_features , and a image stack .
I currently process all the images in the stack with a loop , something like this : #CODE
It seems inefficient as is , but when I try and do the calculation on the entire stack , scipy gives me the mean values for the objects over the entire stack , not per layer .
pv wrote a [ numpy cartesian product implementation ] ( #URL ) .
Find indices based on datetime
How do I find all indices where , say , ` date_list == datetime.datetime ( 2015 , 3 , 31 )` ?
Yes , well , all indices or a ` True / False ` list .
Considering my example , I would ( like ) get three indices , or a list of ` True / False ` where there are three ` True `' s and the rest ` False ` .
In the case of a matrix mat n x n , i can do the following #CODE
To resume , the goal is to find a symmetric form for a 3D array with respect to the third index and to preserve the values in the diagonal for the original ndarr [ i , i , i ] .
The problem here is that you're not using the correct transpose : #CODE
I see , ` .T ` reverse the axes in order to extend the transpose to arrays with different axis dimensions ( which is the expected behavior ) .But when i do the slices to work with nxn matrices ( first attempt ) i do not expect to have the axes reversed ...
It's easy to convert from a ` numpy ` matrix to NetworkX ; however , I can't seem to figure out how to broadcast the Levenshtein distance function across an n-by-n empty matrix where the each row and column corresponds to a string .
On that point of string size , how come if we know the max string length , the complexity is ` O ( 1 )` ?
I'm trying to get the dot products of each element in an nx2x3 array and an nx3 array ( the value of n is always shared between the two ) .
Also , I think numerically , it is better to just sum the gx vector .
` np.linalg.norm ` is rather slow - you could probably do substantially better by taking the norm yourself , e.g. ` np.sqrt ( diff.dot ( diff ))` for 1D
numpy sum result and transpose on it give same answer
The default action of ` sum ` is to reduce the dimensions by 1 , in this case , from 2d to 1d .
The transpose of a 1d is itself .
I presume you want to transpose first : #CODE
You can reshape the resulting array , like this : #CODE
it will be reshape (( 2 , 1 )) for every input array ?
If you want to keep the extra singleton dimension after computing the sum , you can pass ` keepdims=True ` to ` sum ` : #CODE
Apart from using ` keepdims ` , you could also reshape the output to insert a new axis to replace the one that was lost in the reduction , e.g. : #CODE
Incomprehension in the sum of numpy arrays
Calculating the sum of a large amount of numbers is a tricky problem , since computers may not represent numbers with enough precision to do that .
So when doing the sum , once your sum reaches that value , adding +1 will not change the sum anymore .
Otherwise you'll have to use an algorithm like Kahan summation , or , if there are no negative numbers , do the sum in stages .
are the same as the ones posted in the examples of this web page .
My first approach was to solve the system Ax = b with the inverse multiplication method , where A consists of the three corners of the triangle , b represents the current point , and x represents the barycentric coordinates of this point ( alpha , beta , and gamma ) .
I found the inverse of matrix A once per triangle , and then for every point within that triangle calculated the barycentric coordinates by finding the dot product of A^-1 and the point b .
The ' barycentric coordinates ' for [ 0 , 0 ] as computed with lstsq are [ 0 , 0 , 0 ]; which do not sum to one .
What is unique about ` %run ` is that the results of the run are placed in the main ipython workspace .
I am writing a program that will append a list with a single element pulled from a 2 dimensional numpy array .
When using ` view_as_windows ` , it seems like it would be faster to compute the absolute coordinates of each patch using ` numpy.linspace ` ( or possibly ` numpy.arange `) and ` numpy.mgrid ` rather than computing them in pure-python using ` while ` loops .
In fact , I think all you need is to use ` numpy.mgrid [: #URL to get the absolute coordinates ...
So is there a way to set a defined amount of my noise array to zero ( zeros has to be positioned randomly ) .
Are there ` numpy ` , ` scipy ` or some other module / package that does this , or do I have to roll my own .
In what sense is the concatenate a bottleneck ?
That concatenate action should be pretty fast .
However , when artificially creating a 50 Hz signal and applying sufficient zeropadding to enhance fft resolution , I get a top frequency of 49,997 Hz .
The last 999 seconds are zeros in order to increase the fft " resolution " to 1 mHz .
You are not taking the FFT of 1000 s of a 50 Hz wave : the array you pass to ` np.fft.fft ` is 1 second of signal followed by 999 seconds of silence zeros ) .
Print out the length of your fft as a number .
Is there a way to retrieve indices of particular values using Pycuda ?
I am looking for the indices of the values 1 in the entire array .
Internally , it uses ` thrust :: counting_iterator ` and ` thrust :: copy_if ` to find the indices of the elements which are equal to ` 1 ` .
However , the first half should never be all zeros .
What can't be done easily with this approach is getting rid of the middle line of zeros during construction , as it is central to the workings of the algorithm .
Option 2 : return indices where conditions are true
Correct way to broadcast a 100x9 to a 100x9x1x1 numpy array for computation in Caffe
Pandas DataFrame : How to get a min value in a vectorized way ?
Basically , you need to first ` shift ` last three columns and then combine with the first 4 columns , and finally calculate the ` min ` .
For a C program ( which is what your code really is now ) the GNU Scientific Library has a nice histogram module .
hmmm repeated tests ( just populating a biggish list ) show the other solution ( rand ( 1.5 ) to result in 0.5 or greater only about 50% of the time .... which seems strange ...
yes rand only returns an int ...
err apparently ( that rand only returns an int ) is an oversimplification ...
In this case , your pdf has a value of 2 / 3 for x 0.5 , and 4 / 3 for x > 0.5 , with a support of [ 0 , 1 ) ( support is the interval over which it's nonzero ): #CODE
I'm trying to append a 4x1 row of data onto a matrix in python .
A common ` numpy ` approach is to append these values to a list , and turn that into an array when you are all done .
` np.r_ ` is a fancy front end to 1d concatenate .
The interpolation function you're calling assumes that the x-values are in sorted order and that each ` x ` value will have a unique y-value .
ProjecteGradientNMF runs slower but converges to a closer solution while Lsnmf runs about twice as fast but converges to a further solution ( frobenius norm distance measure ) .
` bmat ` also does some messing around with stack frames to let you do this : #CODE
You should try casting one of the values to a ` float ` : ` ybar = float ( sum ( valeur_min )) / len ( valeur_min )` and ` R2 = float ( SSreg ) / SST ` .
Now i want to save it back into jpeg format , but since the image was not modified , I don't want to compress it again .
@USER it'll be a mat of type CV_8UC3
you could also look at sum of the ` mask ` as a rough estimate of the amount of blue pixels .
Hmh , isn't that just sorting and cutting at equal spaced indices ?
Have you considered looking at [ histogram equalization ] ( #URL ) ?
It's a technique in image processing that takes a look at a histogram and increases contrast by ensuring that all intensities are equiprobable in occurrence ... this is analogous to flattening the histogram so that each bin has the same count , which is what you're after .
Instead of looking at an image histogram , you can just look at a histogram of whatever you want - the technique still applies .
Numpy logspace return numbers spaced evenly on a log scale .
NumPy gives you numbers evenly distributed in log space .
Return numbers spaced evenly on a log scale .
now we just need to broadcast those ranges to be between 0.5 and 1.0 .
Then concatenate the two arrays together .
Make sure you concatenate using the correct axis .
Fortunately , you can concatenate another tuple to them .
and you want to append some integer to each tuple , let's say the index of the tuple in ` arr ` just for simplicity .
( You can only concatenate tuples to other tuples . ) So if you just want to add a number to a specific tuple , try : #CODE
which will append a ` 3 ` to the end of the first element in ` arr ` .
On the other hand the ` Python ` ` sum ` just takes numbers .
To concatenate that onto the original , I need to turn it into a 2d ' vertical ' array ( as shown in Out [ 366 ]) #CODE
` append ` , ` vstack ` , ` hstack ` are all variations on ` concatenate ` .
I have another array ` RGB_picture ` that contains values similar to the ones listed in ` RGB ` .
Take a look a the concatenate function .
No I don't want to concatenate two arrays .
We then plug in the indices we've found where the 3rd coordinate is 1 to np.split which splits at the desired locations .
To multiply ` dmat ` and ` bg ` , I use the dot product .
The function involves multiple small linear algebra operations , like dot products and matrix inversions .
Three years ago some one asked this question : calling dot products and linear algebra operations in Cython ?
The problem is there are many outer products involved so there is a lot of indexing to be done .
Does that mean you can do the common ` numpy ` outer product via broadcasting ?
These eigenvectors are indeed the same as one another , however the ones from the online calculator are not normalized ( though they probably should be for the sake of convenience ) .
A quick check with MATLAB ( an independent source ) shows that the eigenvalues of match exactly the ones returned by numpy .
You will notice that the numpy vectors satisfy the property that ` norm ( eigenvector )= 1 ` .
Actually the value of the norm is coming out 1.732
Divide each element of the vector by its norm and then look at the result ; it will match the numpy result .
You can append to it using its ` append ` method and get the last two elements as ` l [ -1 ]` and ` l [ -2 ]` .
There are many possible ways to transform the histogram of the input image such that the correct number of pixel values fall within each bin .
There is a numpy function called std .
The length of this array that contains these indices dictates the final length of your signal .
Additionally , what I want to do is concatenate a column vector of ones onto ` a ` , but by accessing it through ` c ` in the following way : #CODE
I need some way to have an array ( or list ) of pairs , each pair component being a numpy array , and I need to access the first array in the pair in order to concatenate a column vector of ones to it .
Your example code does not work for me , I get a ` ValueError : could not broadcast input array from shape ( 2 , 4 ) into shape ( 2 )` when assigning ` c ` .
Note that in Python , you typically do not pre-allocate a list of the final size , but simply append to an empty list .
If you are translating Matlab code with cell-arrays of multiple dimensions , or ones that are very large , you could use numpy arrays with ` dtype =o bject ` : #CODE
only ` zeros ` has a special implementation .
` ones ` is implemented with ` empty ` and ` fill ( 1 )` .
We can see that we have the number 1 four times , the number 4 twice and 3 only ones .
possible duplicate of [ numpy : frequency counts for unique values in an array ] ( #URL )
If you want your ` result ` to have the same dimensions as ` a ` , use ` reshape ` : #CODE
You can get the same result from ` unique ` doing ` _ , inv , cnt = np.unique ( a , return_inverse=True , return_counts=True )` , and then ` cnt [ inv ]` will give you what the OP is after .
What if we compare not the whole array but just most unique part of it ?
We can now calculate the absolute difference between the two vectors and find the first element for which the absolute difference is below a sensitivity threshold : #CODE
If you know that ` y2 ` is essentially ` y1 ` , except for the fact that its initial fraction is " quiet " , then why can't you simply search for the point at which ` y2 ` absolute value is larger than some threshold ?
the " Please Tell me the dot product of this Matrix " will alert at the same time with the first matrix ,
How to append 3D numpy array to file ?
It doesn't need to be human-readable ( except for the time-stamp header ) so I was planning to use numpy.save() , take one slice at a time and save it to the file , but this does not append to the end of the file , it overwrites each time so the file only ends up containing the last slice .
Figured out how to do it if I read in the header first with readline() , then use np.load() , but now load() seems to only pick up one slice ... can it deal with 3D data or do I have to read each slice in and then stack them ?
You could also use ` reshape ` or ` flatten ` on your array in order to obtain low-dimensional versions .
Use the `' a '` flag to append to a file .
numpy.savetxt takes an array structure as input , so we need to reshape it .
The code on that site is in Pascal , but it doesn't look _too_ hard to translate to Python .
The duplicate post's objective is to stack 1D numpy arrays into a single 2D array .
The answer is to simply transpose the output of the function defined by the OP .
Simply take the transpose .
Just transpose what is returned inside your ` rotate ` function : #CODE
You want to transpose it - #CODE
Im using numpy append , concatenate , hstack , vstack , and others functions , but them doesnt work or gives me this #CODE
Also after this process I will need to append Xd , Xe , Xf and so on , so I need a way to add these vectors to the array as they come .
You aren't using arrays here , at least not numpy ones .
Take fft and ifft for a few specific frequencies
I have a question regarding to the ` fft ` and ` ifft ` functions .
Note that you must convert the y to type integer , since the default will be float ( and thus cannot be used as indices ) .
And yes , the program is working properly for small datasets but not for big ones .
you trying to take the dot product between two arrays with incompatible sizes .
So what I expect to get from the dot product of the exponent and the vector v , is a vector W .
I don't really understand why I have to use my min and max to perform KDE and then use ` ravel() ` ?
why I have to transpose the data in ` f = np.reshape ( kernel ( positions ) .T , X.shape )`
I don't really understand why I have to use my ` min ` and ` max ` to perform KDE and then use ` ravel() ` ?
why I have to transpose the data in ` f = np.reshape ( kernel ( positions ) .T , X.shape )`
The code above seems to estimate zero-crossings of the gradient of the kernel density function , but doesn't include any code to plot them .
When you call ` np.where ( x )` where ` x ` is a multidimensional array , you get back a tuple containing the indices where ` x ` is non-zero .
= xdiff [ 1 :] ` is a 3D array , you will get back a tuple containing three 1D arrays of indices , one per dimension .
You probably don't want the extra set of square brackets in ` np.where ([ ydiff [: -1 ] ! = ydiff [ 1 :]]) ` , since in that case ` [ ydiff [: -1 ] ! = ydiff [ 1 :]] ` will be treated as a ` ( 1 , 100 , 100 , 100 )` array rather than ` ( 100 , 100 , 100 )` , and you'll therefore get a tuple containing 4 arrays of indices rather than 3 ( the first one will be all zeros , since the size in the first dimension is 1 ) .
There are two steps : reshape the array to a 4-d array , and then average .
If you want a 3D array as output , you can add one more reshape like so - #CODE
Try playing with those 3 until you get more or less the ` shape ` you want , and then with ` lambda ` and ` gamma ` for the * contrast * ( which is the strenght ) .
By default Matplotlib scales images to their min / max value .
It seems like ` toimage ` scales the input data so that the min / max values are mapped to black / white .
I want to reshape one array using other array .
I want to reshape ` array_1 ` so that it gets the shape of ` array_2 ` .
@USER sorry but had to roll back because the example got removed .
But , the third column is in ` string ` format but the length of character is not fixed , i.e. I don't know max length of character stored in this column
#URL - SO question on why JSON ` dumps ` works on ` a.tolist() ` but not on ` list ( a )` .
Memory growth with broadcast operations in NumPy
( 3 ) ` sin ` and ` q ` are not defined .
I have imported sin from math and q is a 1024*360 matrix .
I have changed r to square to sqrt of the what it was previously .
In that case , you could use my last suggestion and set the used colors to transparent or something , then you can even avoid ` resize ` .
Swap the element with the last element , and then resize the array .
EDIT : @USER suggested replacing the resize with creation of a view #CODE
` numpy.unique ` with ` return_index=True ` will give you a list of indices to take from .
First , use ` ravel ` to flatten ` X ` and ` Y ` into NumPy arrays of shape ` ( 10000 , )` .
Finally , reshape the output array into the desired shape of ` ( 100 , 100 )` .
It's easiest to create a zero array , and then find the indices of the single point and set that point to ` 1 ` .
To find the ( x , y , z ) indices , you can either use ` searchsorted ` or do the math yourself .
You can also calculate the indices directly .
` indices ` contains exactly what you asked for .
A trivial change to my answer at Finding the consecutive zeros in a numpy array gives the function ` find_runs ` : #CODE
You can use ` np.split ` , once you know the interval of non-zeros ' lengths and the corresponding indices in ` A ` .
The colors are the ones shown with ` imshow `
Assigning histogram bin to each data point
I have an array ` days =[ 1 , 4 , 5 , 2 , 7 , 2 , 7 , 8 , 3 , 10 ]` .I created a histogram from this array by using ` a , b= numpy.histogram ( days , bins=5 )` . value of a and b comes out to be #CODE
You want to append to a different list each time : #CODE
Then I reshape this to form a 2D numpy array .
The second if statement throws the error , " if b <= 3 : " , but the posters below are correct that the error is actually in how I reshape a .
From this I am finding the min and the max .
I want to join this to an pandas dataframe .
Each simulation will have its own min and max respectively .
I also would like to keep the min and max in the output ( why 1 and 5 are in the example output ) .
basically how to create a dataframe . with the constant min and the max in the first two columns but then the other values in the third column
Sorry for the late response but if I have the np.random.choice within a loop to produce a bunch of outputs how can I append them all to one dataframe ?
if you get a chance please look at how I can append this from a loop
But if produce multiple np.random.choice , you can use np.concatenate to concatenate the result first .
Simply iterate through simulation and append values into dataframe : #CODE
DataFrames , like arrays , occupy contiguous memory and it is very expensive to append to them .
It's always better to append to a list ( which is designed for that ) and convert to a dataframe at the end .
Only until recently did pandas allow the ` df.loc [ i ]` as a row append .
And this [ SO post ] ( #URL ) shows the popularity of the row append .
The idea is to work only on the nonzero entries for factorization which reduces computational time especially when the matrix / matrices involved is / are very sparse .
Also , one of the authors from the same article created NMF implementations on github including the ones mentioned in their article .
And I'd like to find the mean and sum of column 2 and maximum and minimum values of columns 3 and 4 , and the total number of lines .
Once you have the column in list form you can apply max ( list ) min ( list ) avg ( list ) functions to the data to get whatever calculations you are interested in .
note : You may need to revise where you added the data to the list and convert the numbers from str to int form so that the max , min , avg functions can operate on them .
Might want to check that max [ i ] contains data as if it is 0 then it will count any negative numbers as larger .
True point , although I am already doing ` max [ i ]= ( max [ i ] && max [ i ] > $i ) ?
max [ i ]: $i ` .
Nope , imagine one line had -1 , this is now ` max [ i ]` , the next line has zero , ` max [ i ]` is now 0 , now if the next line has anything but ` null , "" , 0 ` then that will be set to max as ` max [ i ]` will fail and so the ` && max [ i ] > $i ` won't be executed defaulting to just setting whatever $i is to be the max value .
So by saying ` max [ i ]= ( length ( max [ i ]) > 0 && max [ i ] > $i ) ?
max [ i ]: $i ` it should be fixed , right ?
Don't really need to check if it is more than 0 as it will be ` false / 0 ` anyway if it was empty , does make it easier to read though i suppose :) Also it will cause the same problem with min as well but the opposite way round .
The numpy ` dot ` operator does perform matrix multiplication , so it is likely that something is going wrong with your initialisation of ` A ` which you don't show .
Note that the ` reshape ` operation is not necessary ( the same results are seen regardless ) .
@USER True , I've changed it to a reshape .
The vector you are multiplying is a column , so use it as a column to have a final 6x1 vector and not a 1x6 vector , as you are doing a dot product of 6x6 by 6x1 in MATLAB .
I have taken the ` cumsum ` and ` tile ` combo from the answer by @USER .
Assuming you want to end at the point that has the second spacing added , you could initialize a zeros array , put the starting value as the first element and then put the first and second spacing alternately and finally do cummulative summation to get the desired output .
python : creating numpy nonzero index , value pair
Now I use ` numpy's argsort ` function to get the indices for the sorted list : #CODE
I just like slicing , using indices etc so I was wondering whether there is a way to do it .
2 ) Assemble the ` A ` arrays for different coordinates , into one block diagonal matrix ( see [ ` scipy.linalg.block_diag `] ( #URL ) , and similarly concatenate B vectors .
Surprisingly , the approach to put all this into one block diagonal matrix and call ` scipy.linalg.solve ` once is indeed slower both with dense and sparse matrices .
The solution of the linear system , with sparse arrays is faster , but the time to create this block diagonal array is actually very slow .
Sparse arrays can beat the Python loop by construction the ` data ` , ` indices ` and ` indptr ` arrays directly .
There are multiple of ensuring that your array has 2 dimensions - ` reshape ` , extra ` [ ]` , ` [ None ,... ]` , ` np.atleast_2d ` .
I am struggling to find a method in numpy or scipy that does this for me , the ones I have tried give complex valued eigenvectors .
One brute force way is to find all the eigenavlues , then from those store the ones where the imaginary part is zero into a separate array .
I don't think there is a method that finds only the real ones and not the imaginary ones , but could be wrong
The eig function returns a set of eigenvectors which are not unique to the matrix under consideration .
Your proposal does not find a complete set of real eigenvectors for the matrix under consideration , it simply takes the arbitrary set of complex eigenvectors which eig has returned , and selects any that happen to be real .
I don't know if this function does the weighted sum from input and filter , because I see no sum here .
The reason for mismatch is that the output of the convolution doesn't have the mathematical indices specified , but instead they are implied .
The center , which is mathematically indicated by the indices ` [ 0 , 0 ]` corresponds to ` x [ 3 , 3 ]` in the matrix above .
numpy flatten - how to print output
` flatten ` is a method , call it .
` s1 ` would contain the actual ` flatten ` method , and you can call it by doing ` s1() ` .
theano : row-wise outer product between two matrices
The complicated ones don't .
Use ` flatten ` and then create the dictionary with the help of ` enumerate ` starting from 1 : #CODE
Do you just want a dump of the bytes ?
P-values and D-Values from a K-S test need to be displayed on a pylot histogram .
Maybe also a clip from the file .
If you need a copy , take a copy * explicitly* , but it's better to know you've done that than to wonder why suddenly your performance went through the floor after you added a new variable .
` ravel ` returns a view , ` flatten ` a copy .
How can you sort all occurring values in this array ( not along an axis like with e.g. np.sort ) and show all indices of those values ?
I tried looping over all elements , putting their values and indices into a list and sorting that list by values .
But a quick fix is to just append the python version you want to pip like so : #CODE
I want to create a density map of unique names per area .
Instead of the total number of entries per hexagon I would like to have the total number of entries , with unique " Names " , per hexagon .
I don't want to transpose the whole array , add and then transpose back .
Addition with ` b ` would then broadcast along the column ( the second axis ) instead of the rows ( which is the default ) .
This last is a masked array , but the mask is the default ` False ` , and the masked values were included in the sum .
I am attempting to create a dataframe histogram and save it as a file .
Because this question shows the get_figure() being applied to series.hist() , I have also tried using ` ax =d f [ ' ColumnName '] .hist() ` , which successfully produced a histogram but led to the same error message when I attempted to implement get_figure() .
The min / max for my example is : ' ( 0.0 , 254.999999745 )'
What you need to do is provide where you want the bins to be located in the histogram , and this must be in increasing order .
Also , you want to plot a 1D histogram of values , yet your input is 2D .
If you'd like to plot the distribution of your data over all unique values in 1D , you'll need to unravel your 2D array when using ` np.histogram ` .
I add that the min / max values are in float type .
Do you want to plot a ** 2D ** histogram or 1D ?
So do you want a 2D histogram or 1D ?
A simple 1D histogram with the values from the 2D array .
write numpy array to CSV with row indices and header
I want to write to a CSV that includes the column along with row indices and a header .
I'm not sure how to add the indices ( 1 , 2 , 3 , 4 , 5 ) .
As you can see numpy is approximately 5 time faster . but most surprising thing was that its faster without using transpose , and for following code : #CODE
I want to get the indices of each row in arr2 as they are in arr1 .
If you are ensure that ` arr2 ` is a permutation of ` arr1 ` , you can use sort to get the index : #CODE
The second array is not a permutation of the first .
No the rows are unique , there are no repetitions .
The sum from an elements of all possible combinations can not exceed the sum of the scalar .
For the array case just get the numbers n and m from the sum and length of the array .
If I then attempt to take the log of this variable using numpy ( imported as np ): #CODE
But with large ones it probably will lag .
From the 3.0-beta c++ , " Returns vector ` std :: vector Mat covs ` of covariation matrices .
The indexing arrays can have indices occurring in a variety of different frequencies .
ufunc.at ( a , indices , b=None )
Performs unbuffered in place operation on operand ` a ` for elements specified by ` indices ` .
For addition ufunc , this method is equivalent to ` a [ indices ] += b ` , except that results are accumulated for elements that are indexed more than once .
I can run your code with ` N , Np , Ne=9 , 3 , 4 ` and ` np.ones (( Np , Np ))` in place of the ` dot ` expression .
Another solution is to use the sparse matrix product ( ` dot ` or ` * `) .
From that list I wan to extract only unique polygons removing the duplicates .
Shouldn't you use ` p.equals ( poly )` to find duplicate geometries ?
You can create a list that stores the unique polygons , and then for each polygon in your list , loop over the polygons stored in the outer list , and if none of them intersect the new one , add this to the list , you can use ` any() ` function for that .
We could write a custom ` ufunc ` , and use its ` accumulate ` .
#URL suggests that ` frompyfunc ` is a way of writing a generalized ` accumulate ` .
Have you tried the reshape command ?
I need to sum over the second axis .
` vstack ` ( ` concatenate `) is probably faster than ` np.array ` , but both might work .
It's simplest to use ` reshape ` for this task : #CODE
Whenever possible , using ` reshape ` will create a new view of the array ( and not copy any underlying data ) .
If a reshape is not possible without copying data , NumPy will raise an error instead of copying .
Use the reshape command : #CODE
It is just to illustrate that I am trying to fill each bucket to the max .
If you want all indices for element 5 , you can use ` numpy.where ` , Example - #CODE
numpy.max vs builtin max
What are the differences between ` numpy.max ` and ` max ` ?
This suggests that ` numpy.max ` and ` max ` treat the basic idea of " maximum " differently , at least in some edge cases such as this .
Is there a reason why the two functions take different conventions , and in particular , why the behavior of ` max ` depends on the ordering ?
I'm just curious about the two functions take different conventions , and why ` max ` has this odd order-dependent behavior .
My guess is ` max ` simply loops and update if ` value > current_max ` , since both ` 1 > nan ` and ` nan > 1 ` are false , he simply keeps the first element .
` np.max ` is " conceptually more correct " imo , since you don't really know what is the max between a number and something that's not a number .
The behavior of ` max ` can be explained if you assume that it works as follows : assume the first element in the list is the largest , compare the current largest to the next element in the list , and take the next element if ` next current ` .
Since a comparison to ` float ( ' nan ')` always returns ` False ` , ` max ` will think it is the largest element if it is the left-hand operand of the comparison , but the smallest if it is the right-hand operand .
That is , ` nan ` would not only be the maximum of any list , but also the minimum , or the median , or the sum , or the product , etc .
How would be elegant solution for summing all 2's from an array based on their indices ?
So how I can use with numpy to calculate sum of elements in ` x ` based on indices in ` y ` .
And sum them directly : #CODE
If x contains only non-negative ints , you could sum the occurrences of each value with #CODE
I want to resize an RGB image using Python 2.7 .
Is there a more pythonic way to resize the RGB image other than looping through the three channels and resizing each one separately ?
You need to ` ceil ` before dividing : #CODE
which you can then append to the list of lists you created before : #CODE
Each matrix contains decimal numbers ( I tried to create 180 matrices of zeros , and the error occurs in the same way ) .
What the max size is , and why it seems to have changed , is harder .
or create a new array of type float using ` a ` and assign ` np.nan ` to its indices using ` p ` : #CODE
This requires the binomial expansion of ( 1 + x ) ^ 4.8 .
I don't think you need the binomial expansion .
You can get additional simplification by taking the log of your equation and plotting it .
With a little algebra , you should be able to obtain an explicit function for ` ln_y ` , the natural log of ` y ` .
Numpy's eigh and eig yield inconsistent eigenvalues
Currently I'm trying to solve the generalized eigenvalue problem in NumPy for two symmetric matrices and I've been running into massive trouble as I'm expecting all eigenvalues to be positive , but ` eigh ` returns several very large numbers that are not all positive , while ` eig ` returns the correct , expected values ( but is , of course , very , very slow ) .
Though running the same test with a 50x50 matrix does work --- even after rebuilding the SciPy / NumPy stack and passing all unit tests .
Also fix ` zeros ` -> ` np.zeros `
Well , a few more , anyway : ` cos ` , ` pi ` , ` diag `
Can't you fill some list-based data-structure ( somewhing which is more performant for append ) and convert this to a numpy array at the end ( when dimensions are known and only one array-creation step is enough ) ?
Try to define one outer list object , which will be filled within the loop withjout replacing this object .
In last two steps I attempt to find the indices of the matrix I_row , I_col ..
Points I_row and I_col have the max distance ..
For each variable I want to create a column array and then concatenate them all in a single matrix to be written to the text file .
` concatenate ` is better if you give it a list of items to join all at once .
Note the use of ` .T ` , which takes the transpose of the array so that your ` results_tmp ` is in the columns .
Assuming I wish to use " my " way I still don't understand if I can append columns to a np.empty array .
I know how to append rows :
To start , you would need to initialize your ` result ` array to have a matching dimension in the ' other ' direction from your append direction .
If you need to add data to an array , you use ` concatenate ` to create a new array , one that contains both the old data and the new .
There is a ` np.append ` , but it really is just a front end to ` concatenate ` .
A common ` numpy ` idiom for constructing an array is to append the pieces to a list , and passing that list to ` np.array ` at the end .
Python ` list ` has an efficient append method .
I have a large ( n=50000 ) block diagonal ` csr_matrix ` M representing the adjacency matrices of a set of graphs .
For almost non-sparse matrix ( ` D ` will have some ` 0 ` due to repetition in the indices , but not many probabilistically speaking ) , it is still faster , not much , but faster .
And I wasn't sure if ` scipy norm ` could have helped here .
If it can't I would say that you should calculate the differences , then if most of them are positive , correct the negative ones accordingly or vica versa .
If you know the rotation is always in one direction , you can calculate the diff and then go through and fix the errors .
However , it only works with radians , so you need to convert it to radians , unwrap , then convert it to degrees : #CODE
But , yes , unwrap worked very well .
I would prefer something with a smooth gradient covering the full space .
Pad with zeros if needed - #CODE
@USER : they are all the same size except for the last one which may be shorter if ` len ( data )` not divisible by the bin width . the last bin can be shorter that the other ones
If the bins are always the same size , the simplest approach would be to ` reshape ` and then use ` any ` to find if any value in a row is ` True ` : #CODE
If the bins aren't always the same size , the simplest approach would be to get the indexes of ` True ` values , then do a histogram , then find all the nonzero histogram bins : #CODE
First you can get the unique keys then create a dict view of arrays and use a list comprehension to create the desire out put : #CODE
Edit 1 : Added a basic solution to the question , and asked about the transpose case .
Also note that the transpose case you are wondering about doesn't actual change the underlying storage in any way , it just signals to the API to use the alternative storage order to that which the array use during reads , so you can probably forget about it in this context
to delete the lines that had zeros in them !
Python ValueError : operands could not be broadcast together with shapes ( 5 , 4 ) ( 5 , 5 )
My entire code works up to the last step , where it gives me a ValueError , saying ` ValueError : operands could not be broadcast together with shapes ( 5 , 4 ) ( 5 , 5 )`
I want to normalize each pair of frames and take the dot product .
This does what I'd expect for a while ( specifically giving a norm of 1 for tf and pf ) but then I start to see lines like this :
For what it's worth , I've tried ` numpy.linalg.norm ` , ` scipy.linalg.norm ` , and defining a function to return the square root of the dot product .
But the norm computes fine if I do it in a new python session .
I'm actually computing the norm on two frames , a t_frame and a p_frame .
If the sum of the ' dot ' becomes too large for the ` dtype ` , it can be negative , producing a ` nan ` when passed through ` sqrt ` .
I checked ` Sb ` and ` Sw ` , both have unique rows and columns and are symmetrical .
Is there a way to partition and reshape a 2D array to a 3D one .
I want each and every process ( there will be thousands of them ) to append to the same array .
How do the the arguments in reshape ( -1 , 1 , 2 ) shape coar to be an " array within an array within an array " ?
A -1 in the case of the reshape function tells numpy to infer that dimension by the length of that dimension .
So in this case if you check the shape of coart after the reshape you will find : #CODE
So as you can see the horizontal stacking tells it to stack row 0 on top of row 1 and row 1 on top of row 2 and row 2 on top of row 3 .
Below is the data frame I wish to represent as a histogram , with each row as a point .
Graph each row as a point on a histogram , but also be able to pick out a particular set of data ( eg all points from all cells would be in purple below , those belonging to just ` DU145_PROSTATE ` would be in red , and ` 22RV1_PROSTATE ` in blue ) and graph this as an overlaid histogram .
Also note that ` i ` must be converted to a string or you will receive ` TypeError : cannot concatenate ' str ' and ' int ' objects ` .
So if you want to make sure that the result is 2D , make the indices arrays 2D .
If you concatenate columns 4 , 1 , 2 and 3 you create the unique identifier for each block .
For every different ` current_id ` , open a temporary file and append value of column 8 to that file .
I then use ` groupby ` to get all the values in your 8th column , store them in a list for the respective identifier in your column 4 ( note that my indices start at 0 ) and convert this data structure into a dictionary which can then be printed easily .
Or transpose the vector .
What you can do is transpose the vector ( using ` myvector.T `) so you get a 1x4 vector and multiply that with your 4x4 matrix .
Without concatenate , it puts the two lists into data as a list of two separate arrays .
to join the list of arrays into a single array : #CODE
Additionally when I sum the intensity in the image using : #CODE
Also , notice that your intensity sum in ImageJ is a decimal .
In other words , B [ i , j ] is the sum of A [ k , l ] taken over all indices k > i , l > j ; this is sort of the opposite of the usual cumsum applied to both axis .
You could use ` np.cumsum ` on the reverse-ordered arrays to compute the sum .
Here we remember that ` np.cumsum ` starts with the value in the first column ( in this case last column ) , so to ensure zeros there , you could shift the output of this operation .
My question is about building a simple program that detect digits inside images , i did some researches and found this topic Simple OCR digits on stack and i found it very educational , so i wanted to us it for my own need .
Common practice is to reshape ` x ` ( as needed ) so it has at least 1 row .
After I make a 2D slice of each star , I must stack these arrays on top of each other to make a model of the point-spread function .
You could express the centre coordinates of the pixels in each ' slice ' relative to the centroid of the star , then compute a weighted 2D histogram .
Weighted 2D histogram : #CODE
Split a large numpy array into separate arrays with a list of grouped indices
Given 2 arrays : One for a master dataset , and the second as list of grouped indices that reference the master dataset .
You just have to reshape the index array : #CODE
I get the error log #CODE
As it is said on your link it is unofficial libraries . should should consider using official ones to be sure they correctly linked together .
I want to get the sum of the equivalent indexes of 2 arrays and then threshold them .
However I get the error '' IndexError : arrays used as indices must be of integer ( or boolean ) type '' .
As the error message says , indices have to be of integer or boolean type .
Shouldn't ' np.fromfunction ' just apply ' f ' to all tuples ' ( i , j )' that are combinations of indices ( between 0 and d1 ) ?
You see what the indices ` i ` and ` j ` look like .
I just benchmarked np.inner ( A1 , A2 ) versus a double loop over the indices with A [ i , j ]= np.inner ( A1 [ i ] , A2 [ j ]) and your solution is MUCH , MUCH faster !
For the first example , you can do an outer product of the input and the template and reshape the result : #CODE
For this example looks like using the calls to outer are direct equivalents to the broadcasting approach below from @USER .
Edit : As others have stated , you can actually do all that with outer products in a much more concise way , which will probably match your algorithm more closely , e.g. , shamelessely making YXD answer a one-liner : #CODE
This approach and the ` ufunc ` approach with ` outer ` seem equivalent but I'll go with this one since it was first .
Some things worth testing are insert at the beginning vs at the end vs append vs contatenate .
` np.stack ` ( or ` np.concatenate `) takes a list ( or lists or arrays ) , so there's no need to iterate through the list , doing a concatenate item by item .
` np.append ` is just a pairwise concatenate : #CODE
or something a bit faster ( ` tile ` uses repeat in a similar way ) #CODE
I have seen the numpy algorithm which uses the std deviation but since i have to consider the series of the list , this algorithm does not work for this case .
What you can do is compute backward and forward gradient of your data assuming a constant step of 1 .
absolute value of backward gradient greater than 1% of the absolute value of the left neighbor
You can use this in combination with the numpy algorithm for std deviation .
Keeping track of dropped indices when dropping elements from numpy array
However , I also want to keep track of the indices of the deleted elements and I was wondering how I could look to do this .
If you'd like to keep track of the indices of elements that were dropped , just hold on to the boolean mask you use to index into the array and use ` np.where ` : #CODE
You can convert your PDF to a CDF by taking the cumsum , use this to define a set of bins between 0 and 1 , then use these bins to compute the histogram of an N -long random uniform vector : #CODE
You can use ` np.bincount ` for a binning operation alongwith ` np.searchsorted ` to perform the equivalent of ` roll dcs ` operation .
The outer loop is implicit as I am performing non-linear constrained minisation on the wider functions which tends to iterate 100+ times
When I do this , and try to verify the results by printing to the command line , the array is reported as all zeros .
Any subsequent actions on it will maintain this int array status , so after adding your small number element wise , it casts back to int ( resulting in all zeros ) .
translate matlab to python .
Note that there are even fancier extensions of this , such as Joe Kington's rolling_window which can roll over multi-dimensional windows , and Sebastian Berg's implementation which , in addition , can jump by steps .
I am looking to append them such that they would look like this : #CODE
Try something like ` min ( data , key=lambda point : ( point.x-your_point.x ) **2 + ( point.y-your_point.y ) **2 )` ( it's a general idea , I know that you won't be able to get coordinates as properties ) .
If you are 100% sure that l2 would only be one column then you can reshape that array to make it one dimensional before doing the subtraction .
One question : would it be a good practice to reshape the tag_train into a 2D array first ?
I have matrices where the diagonal is the negative of the sum of all other elements in that row .
I'd like to write a Theano function that takes in these matrices and return a matrix with the same number of rows , one less column , and the diagonal removed .
then concatenate them
Using NumPy arrays as indices to NumPy arrays
Now I create an ordinary list of indices : #CODE
Why can't I use NumPy arrays as indices to NumPy array ?
I want to calculate the max cross-correlation of the timepoints for every pair of electrodes , for every trial .
Specifically : for every trial , I want to take each of the pair combination of electrodes and calculate the max cross-correlation value for every pair .
That will result in 4096 ( 64*64 ) max cross-correlation values in a single row / vector .
That will be done for every trial , stacking each of the rows / vectors on top of each other resulting in a final 2D array of shape 913*4096 containing max cross-correlation values
But from what I've just ( briefly ) read , the einstein summation method can convey sum and multiplication operations , which is fine for a 0 lag cross-correlation , but there is also a sliding window element as well ... how does the function realign the vectors ?
I then need just the max of these correlation values .
My code still contains a lot of double calculations though ( e.g. we cross correlate 1 with 2 , and then 2 with 1 later , which are the same thing ) , so I'll need to figure out a way to remove those later ...
difference between exponential and log functions for numpy and math
This sounds like a naive question , but I can't figure out why there are two instances of functions like e , log etc ., one for each numpy and math .
You can't do too much * without * e and log .
@USER : Agreed about ` log ` , but I'd beg to differ about ` e ` , which IMO is a bit of an attractive nuisance : you see people using ` e ` to write ` e ** x ` , which is significantly less accurate than ` exp ( x )` .
If it were up to me , I'd have ` e ` removed from the standard library : on the rare occasions that someone actually needs the * constant * ` e ` rather than the exponential * function * ` exp ` , it can be computed as ` exp ( 1 )` .
` numpy ` functions are much more powerful than the math ones ( when working on vector / matrix / etc . ) , but ` numpy ` is not a standard library .
If you check the ` type ` of the ` exp ` function , you get the following : #CODE
Where you can see that ` numpy ` has defined its own ` exp ` function , whereas the ` math.exp ` function is ` builtin ` .
python : how to calculate the l1 norm of a vector ?
" In effect , the slice is converted to an index array np.array ([[ 1 , 2 ]]) ( shape ( 1 , 2 )) that is broadcast with the index array to produce a resultant array of shape ( 3 , 2 ) .
So it's right to say ` [ 1 , 2 ]` is broadcast with the index array , but it omits the bit about adjusting dimensions .
` y [[ 0 , 2 , 4 ] , [ 1 , 2 , 3 ]]` is a case of pure advance index array indexing , the result is 3 items , ones at ` ( 0 , 1 )` , ` ( 2 , 2 )` , and ` ( 4 , 3 )` .
Then , I tried using the + and then the join operations to concatenate a new title for each array , but turns out that is illegal when defining arrays .
A Python dictionary is a much better way of collecting information by some sort of unique id .
Some columns will have string data ( ids ) others integers or even dates , others float data .
Round labels and sum values in label-value pair 2d-numpy array
I have combined several of these matricies , but I'm hoping to round the label to 4 decimal places and sum the values , such that : #CODE
In each iteration of the ` for ` loop , I attempt to insert the ` file_data ` into ` all_data ` at the indices ` [ start_index : start_index + n_measurements ]` .
Each ` file_data ` contains a timeseries that I'm trying to join , in order , within ` all_data ` .
Maybe do a loop over the files and append the key .
Unfortunately I don't think the question suits the stack overflow format or rules on opinion based questions .
When it comes to how to deal with the situation as it looks now , here it is a matter of taste and its not a very pertinent question , however , when it comes to if and how this issue will be addressed by the ` numpy ` or ` matplotlib ` guys , this has very little to to with ones personal opinion .
You could also use vertical lines and squeeze the labels in above ( optionally with double-ended arrows to represent the ranges ): #CODE
I used the function to set the tops of the rectangles , but if there are several ` y `' s then you could precalculate them all and take the max or just use ` axvspan ` with the default y's to use the full y range .
IIUC , it's not that the maximum value is shared between columns , it's that you probably want to divide by the maximum absolute value instead , because you have elements of both signs .
And these boolean indices : #CODE
But if I had an array of boolean indices : #CODE
Is there a vectorized / numpy way of iterating through the array of boolean indices to return [ 1 , 2 ] ?
alternatively , you can manually broadcast using ` np.tile ` : #CODE
That manual broadcast will be handy !
What are the realistic dimensions of ` cond ` ?
#URL use of ` tile ` looks good .
As expected , the list comprehension times scale with the rows of ` cond ` , while the tiling approach is just a bit slower than a single row case .
Above is the error log generated while i build numpy using MinGW-w64 in my 64bit Win7 .
Imagine that I have 1 billion keys in the ` Counter ` object , iterating through the ` most_common() ` would have a complexity of going through a corpus ( non-unique instances ) once and the vocabulary ( unique key ) .
You could create a new Counter with the same keys and only the increment then sum that with the original : #CODE
Singular value decomposition ( svd ) and mean does not exclude masked values during computation
This time I have a set of 12,000 image data which I am performing singular value decomposition ( svd ) on and calculating their mean .
` numpy.ma.masked_array ` to exclude them from both svd and mean computation .
And some images are smaller than others and they were padded with zeros values to make all images to have the same ( pixel ) dimension .
But I also don't want the ' zero paddings ' to be used during computation , so I used ` numpy.ma.masked_array ` to exclude them from both svd and mean calculation .
The problem is that when I perform both svd and mean calculation , the masked values ( array elements ) are not excluded during computation .
` svd ` and ` dot ` just work with the regular array .
How should masking affect the ` svd ` ?
In the figure above , I want to mask all zero pixels in each images which was used to pad them to make all images have the same dimension of 128 by 128 , so during ` svd ` computation these padded pixels should not be used .
I found out from documentation ( 1 ) that ` numpy.linalg.svd ` returns a transpose of ` v ` , so I just needed to perform the dot product without transposing ` v ` .
All I need is a way to parallelize a for loop that will append its results to a list in the order of the iteration .
python numpy left join recarray with duplicated key value
I'd like to left outer join two recarrays .
The first is a list of entities with a unique key .
It seems like the type of left outer join I describe is a really common operation , does anybody know how to achieve it using numpy ?
Just treat them like two numeric arrays that need matching ( possibly with unique and sort ) .
I get this error : IndexError : too many indices
Moving_average of Mar_1761 = ( value_of_Mar_1761 ) /( sum of values from Sep_1760 to Aug_1761 )
The easiest approach is to reshape to data to a long format using ` .stack ` , which can be be passed straight into rolling mean .
Load the whole thing into memory with numpy bytes array , create byte masks with ` numpy.zeros ` and add ` ones ` at the edges to pad , then just and it within numpy .
As a possible solution to that , you can compress your data by a factor of eight by actually storing each set of 8 ` 1 ` s and ` 0 ` s in a byte .
Surely not an integer as here ( what of leading zeros ? ) .
I want to check if all entries of a matrix A within 10 indices of a given entry ( x , y ) are zero .
That's not giving you " all entries of a matrix A within 10 indices of a given
When the sum of these is equal to 1 , I want to print `" YATTA "` .
I have checked that both diff and multiply took to much time .
Find indices of large array if it contains values in smaller array
Is there a fast ` numpy ` function for returning a list of indices in a larger array where it matches values from a smaller array ?
The problem with searchsorted is that it will return results even when their is not an exact match , it just gives the closest index , but I only want indices where there are exact matches
EDIT : the set of values in both the smaller and larger arrays are always unique and sorted .
Guess you need to index into ` np.searchsorted ( b , a , side= ' left ')` with the mask though to get the actual indices .
You would still need to use ` np.where ` to get the indices with respect to ` a ` .
We can also search for it from the ` right ` direction and the ones that are same in both these set of indices would be the ones to be avoided from the indices outputted from the ` left ` option to get the desired output .
You have to add extra handling , because ` searchsorted ` may return indices out of bounds , but a potentially quicker way of discarding non-matches , rather than re-searching with " right " ...
Unfortunately , this way the column indices are converted from integers to doubles and back .
Collecting ` indices , data ` into a structured array avoids the integer-double conversion issue .
For a case where there are differing numbers of entries per row , this approach works ( using ` intertools.chain ` to flatten lists ):
Can't edit comment , so copied : It finds the one that is closest to unity , based on the criterion of " difference in absolute value should be less than 1e-8 " .
The ` where ` call returns a tuple containing arrays , where each array corresponds to a dimension , and such arrays contain the indices where the predicate evaluates to ` True ` .
In the 1st version the result is normalised by the sum ( and I wonder why this is necessary ) , and in the 2nd version it's not .
I'm trying to use the numpy.fft.fft() function to transform a square pulse ( 1-D diffraction slit function ) to a sinc function ( 1-D diffraction pattern ) , and make the output plot identical to the analytical transform of the square pulse , given by the equation :
F ( u ) = sin ( ? au ) /( ? u )
this case is the periodic sinc function ( also known as the aliased sinc function
I need to find a way to remove any elements of this expansion where the indices match ( e.g. anything with a1 and b1 , or b2 and c2 ) .
So setting the diagonal to ` 0 ` and summing over gives the result needed for the first expansion .
Python adaptive histogram widths
I think you should first remap your data , then create the histogram , and then interpret the histogram knowing the values have been transformed .
One possibility would be to tweak the histogram tick labels so that they display mapped values .
Find the steepest part of this distribution , and choose a horizontal interval corresponding to a " good " bin size for the peak of your histogram - that is , a size that gives you good resolution ;
Create the bins using the vertical span of that bin - that is , " draw " horizontal , equidistant lines to create your bins , instead of the most common way of drawing vertical ones ;
If the distribution resembles some well known algebraic function , you could define it mathematically and use it to perform a two-way transform between actual value data and " adaptive " histogram data ;
You can use ` X ` and ` Y ` to create the X-Y coordinates on a ` 0.1 ` spaced grid extending from the ` min to max of X ` and ` min to max of Y ` and then inserting ` Z's ` into those specific positions .
Until now I've used a super-duper simple approach , just by diffing two of the second part of the tuple and if the diff was bigger than a certain pre-defined threshold I'd put them into different groups .
Unsatisfactory results : if you only diff , you might diff measured results that were measured timewise too far appart and then the grouping won't work anymore .
edit : I realize the mod is unnecessary .
Would be interesting to see performance numbers with the new one against a diff based one !
I had to tried to convolve but only with your ' K1 ' , and couldn't figure out how to correlate ' 0 ' -s together .
Of course , given that the outer loop is happening in python instead of in native code in the ` numpy ` library means it's not as fast as it could be .
which is simple to write a vectorized version of , but this is unfortunately outweighed by the inefficiency of calculating a 10-billion-plus element matrix and only taking the diagonal ...
As it turns out , ` scipy.linalg.solve_triangular ` will happily do a bunch of these at once if you reshape it properly : #CODE
where ` diff [ j , i , k ] = X_ [ i , k ] - U [ j , k ]` .
I don't know what you mean by a stack .
I have a hundred 512x512 matrices and I would to create a stack .
In any case , it's going to be difficult to see the interior ones on a 3-D projection .
Are you sure it isn't the [ dot product ] ( #URL ) that you're after , then ?
Dot product only gives the absolute distance , not information about the positioning as such relative to the axes .
Based on your comment about wanting to know the angle between the arrays , I think you do in fact want the dot product .
Are you certain that the other arrays have values for these indices ?
I want to use this mask array containing only ones and zeros as input for kcluster in Pycluster .
So the ` dot ` sum ` j ` dimension is 1st , and the 3d of the arrays follow : #CODE
I expect to return 2 2 6 2 as a list and then concatenate it with the second array .
Either case , just before deleting you have to find such indices .
How to remove rings from convolve healpix map ?
I'm applying convolution techniques to convolve 2 datasets , a healpix map with nside = 256 and a primary beam of shape ( 256 , 256 ) in order to measure the total intensity from the convolved healpix map .
In my code below , i used the query function in scipy to search for the nearest pixels in my healpix map within a given radius and take the sum of the product of the corresponding pixels in the primary beam using map coordinate .
Making this corrections gives ( after a log is taken , because I can see the log in the plot in the question ):
How exactly did you convert to the log of the function ?
i did amost the same as tom10 except that i took the log of your expression directly , which turns the factors into summands and may make things easier to debug .
I am trying to find the most efficient way of downsampling an aribtrarily shaped 2d numpy array into a smaller ( or potentially larger ) square array - I want to take the max of each sub-array and put it into the new array .
Here's a vectorized approach that does the blockwise ` max ` finding calculations in as NumPythonic way as it could get - #CODE
@USER Yes , for such corner cases , ` padding with zeros ` would be the preferred approach I think .
Finding the row indices of zero elements in csc_matrix
Now , what I want is the row indices of all zero elements in each column .
Plotting sin wave with amplitude , frequency and phase angle
I was trying to make sin wave of the type #CODE
I know to plot simple sin curve like this : #CODE
I have looked at Pearson Correlation Coefficient but not sure how translate it to handle multiple dimensions like the data structure provided here .
Efficient way to create a dense matrix from diagonal vectors in Python ?
If I transpose the array , one of two cases can probably happen :
The transpose seems to be faster than the non-transpose .
Since the numpy reference is simply a view into the array , it is possible matplotlib uses this to its advantage to cleverly decide whether or not to transpose last minute ?
You want to flip the x and y , but you don't want to transpose .... that is the definition of transposing
When it does so , it might have to transpose , or might not have to .
What if it has to do a transpose ?
Then if I have to transpose my image to matplotlib only for it to also have to transpose its image before drawing , that is inefficient .
If all this is true ( I am making a lot of assumptions , so please correct me if I'm wrong ) , I'd rather not transpose myimg .
( It might have to transpose depending on how the array is stored in memory for the graphics card or whatever etc . sorry for the long comment )
I doubt that the transpose operator will drop your performance .
I would bet quite a large sum of money that this is not going to be the performance bottleneck in your code .
I found the transpose to be faster ( see figures linked to in current edit ) .
( Oh forgot to mention this strange result would make sense under the assumption the graphics card requires to transpose the array before drawing from the first case )
During the debugging I have printed out in the log mentioned ` feature_log_prob_ ` from Naive Bayes model and it looks as : #CODE
The problem is that you've used i as the index in both the inner and outer for loops .
You can change the inner index to k , say or simply eliminate the inner loop and use ` zeros = number_of_zeros * " 0 "`
You are using the same index i for your inner and outer loop , which is causing you your indexing into the numpy array to get corrupted .
In other words , I want to apply ` f ` to all combinations of indices over the second axis ` a2 ` .
You just need to populate where the breaking points are , i.e. when ` floor ( t / .5 )` changes :
and the sum over each interval is : #CODE
and the mean would be sum over length of interval : #CODE
Solve Polynomial equation of 6th order with Python efficiently
I want to Solve Polynomial equation of 6th order with Python .
Given 2 large arrays of 3D points ( I'll call the first " source " , and the second " destination ") , I needed a function that would return indices from " destination " which matched elements of " source " as its closest , with this limitation : I can only use numpy ...
I also have an array containing " seed " regions with unique values ( seed_array below ) which I'd like to use to classify my habitat regions .
Further discussion : Watershed method tries to grow regions from seeded peaks by flowing through the image gradient .
If you really need fast lightweight dynamic C-arrays with in-place ` append ` , use ` array.array ` .
For each case ( i , j ) in the MxN grid , ` lb [ i , j ]` is the min ( over the rectangles overlapping ( i , j )) of ` c*weight [ i , j ] / sum ( weight [ m : m+h , n : n+w ])` ( with [ m : m+h , n : n+w ] the rectangle with associated cost ` c `)
My ultimate goal is to delete the time from this column and join it back to the spreadsheet .
Numpy combine all nonzero elements of one array in to another
You can find the indices where the values differ by using ` numpy.where ` and ` numpy.diff ` on the first column : #CODE
Any thing non-zero means that the item next to it was different , we can use ` numpy.where ` to find the indices of non-zero items and then add 1 to it because the actual index of such item is one more than the returned index : #CODE
I get an IndexError : Too many indices for array .
I need to be able to compare two images and extract any unique pixels to create a third image .
To finish things up , you can copy over the second column of the sliced values of ` b ` and stack both of these together : #CODE
Choose slices from numpy array B if cond else from A
I can find the indices I want : #CODE
Oh great wise ones of the tubes , I beseech thee to please assist in this programming quandry .
Would first doing a `` rolling_min `` to get the minimum for each column for the last 3 rows , and then a ` min ` to get the minimum in that new rows , result in what you want ?
Let say we want to find the min from ( 2 , b ) to ( 6 , d ) for each row .
you mean you want to compute , for each day , the min from 15:00 3 day ago to 11:30 of the day in question ?
You can first stack the DataFrame to create a series and then index slice it as required and take the min .
Here's another method which avoids stacking and is a lot faster on DataFrames of the size you're actually working with ( as a one-off ; slicing the stacked ` DataFrame ` is a lot faster once it's stacked so if you're doing many of these operations you should stack and convert the index ) .
It's less general as it works with ` min ` and ` max ` but not with , say , ` mean ` .
It gets the ` min ` of the subset of the first and last rows and the ` min ` of the rows in between ( if any ) and takes the ` min ` of these three candidates .
Note that if ` first_row + 1 == last_row ` then ` middle_min ` is ` nan ` but the result is still correct as long as ` middle_min ` doesn't come first in the call to ` min ` .
Getting elements within a numpy array by using a list of tuples as indices in python
You can extract the indices for each dimension and then slice the numpy array ( assuming ` myArray ` is an array with three dimensions ): #CODE
What does ` transpose ` do exactly ?
Split dataframe on sum column ?
How can I split this DataFrame so that each split portion has ( roughly ) the same sum of ` Foo ` ?
How can I do the same , but with equal sum for a particular column ?
To match right row with certain accuracy , we can have variable diff = 0.3s .
To match right row with certain accuracy , we can have variable diff equal to 0.3s .
If what you want is to select the ` > 0 ` pixels , then max the ` <= 0 ` pixels as ` Image2_mask = ma.masked_array ( Image2 <= 0 , Image2 )` .
Numpy : Average of values corresponding to unique coordinate positions
Now I want to get the average of all values for each unique grid point .
How could I get this for all unique grid points ?
the dtype in my case would be " float " and the coordinates can take arbitrary values , also negative ones
You can sort ` coo ` with ` np.lexsort ` to bring the duplicate ones in succession .
Then run ` np.diff ` along the rows to get a mask of starts of unique XY's in the sorted version .
You can get the counts from the positions of the indices of the result of ` diff ` , that's basically what ` np.unique ` does when you ask for ` return_counts=True ` , and it is typically faster than calling ` bincount ` .
I guess that would work if I loop over all unique values , but I would like to avoid that
It is very likely going to be faster to flatten your indices , i.e. : #CODE
Also very neat , however I discovered that , when applied to my data , the flat index in not unique and the results are somewhat difference for certain combinations compared to the lexsort approach
That's probably because I messed up : you need to multiply row indices by the largest column index , not by the largest row index .
without actually having to sum the arrays ?
Looking at the question / answer today , I'm not sure why I opted for ` broadcast_arrays ` over ` broadcast ` .
` ValueError : could not broadcast input array from shape ( 5 , 5 ) into shape ( 2 )`
In most languages ( ` R ` , ` SAS ` etc ) , the default is to return std of ddof=1 .
Although I generally agree with avoiding Python loops for Numpy arrays , this seems like a case where it's easier and faster ( see below ) to use a Python loop since the loop is only along a single axis and it's easy to accumulate the comparisons in place .
If arrays are already in a single 3D numpy array ( eg , from using ` x = np.dstack ( x )` in the above ) then modifying the above function defs appropriately and with the addition of the ` min == max ` approach gives : #CODE
Concatenate along the third axis with ` np.dstack ` and perfom differentiation with ` np.diff ` , so that the identical ones would show up as zeros .
Then , check for cases where all are zeros with ` ~ np.any ` .
will apply the ` sum ` function to each column and give a series containing the
First I would convert into a stack of triples : #CODE
The key is to expand this to a 3d , shape ` ( 3 , 1 , 1 )` that can be broadcast to ` ( 3 , 5 , 5 )` and compared with ` a ` : #CODE
Actually it is automatically reshaped to ` ( 1 , 3 )` which can be broadcast agains ` ( 25 , 3 )` array .
or use ` np.where ` to convert the boolean array to 2 list of indices : #CODE
Can not the indices be pre-obtained using ` I = a == np.array ([ 150,160,170 ]) [: , None , None ]` and defining some axis like ( 1 , 3 ) ?
Looping over the smallest dimension ( s ) and vectorizing the calculation over the remaining ones is a common compromise .
I am getting overflow error at cosh function in this simplified code : #CODE
The last element in x is ` 10 ` , and for that you are trying to take cosh of ` 2000 ` , which is infinity .
It's unlikely that you really need to compute cosh values so large they won't even fit in a float .
Or in your case I'd use the ' rows ' for which ` sum ` is 1 .
The combination of ` any ` , ` and ` , and ` sum ` work this for test case , but might need adjustment with a larger test case ( s ) .
As mentioned in the question , both arrays contain only unsigned ` ints ` , which could be exploited to merge ` XYZ `' s into a linear indices equivalent version that would be unique for each unique ` XYZ ` triplet .
Because ` ind ` is symmetric in your example , it is it's own transpose .
` transpose ` and ` flipud ` are what you are looking for ; the ` swapaxes ` can also function as transpose
Note that transpose has a version that operates on multiple dimensions .
I did try both slicing and transpose , but just not the two together .
@USER ` swapaxes ` seemed to be indistinguishable from ` transpose ( 0 , 2 , 1 )` .
` distance.argsort() ` returns an array of indices .
Before I was using the csv module to append columns to a csv and re write it , but to be more efficient I want to load it into a pandas data frame .
Modified cumulative sum of numbers in a list
Input is ideal - can be splitting to subset , sum of each subset is equal .
` current ` stores the " id " of the current subset , ` total ` the sum of the current subset .
For each element ` x ` in your initial list ` l ` , you add its value to the current ` total ` , if this ` total ` is greater than the expected sum of each subset ( ` subset ` in my code ) , then you know that you are in the next subset ( ` current = current + 1 `) and you " reset " the total of the current subset to the actuel element ( ` total = x `) .
You can use ` np.transpose ` to swap ` rows ` with ` columns ` and then reshape - #CODE
Or use ` np.swapaxes ` to do the swapping of rows and columns and then reshape - #CODE
Reshuffle numpy array based on indices in a second array
You have to subtract 1 from ` B ` , because indices in numpy arrays begin at 0 : #CODE
` vstack ` is just ` concatenate ` with an added step that ensures the inputs are 2d .
` reshape ` corrects that .
` vstack ` makes them both ` ( 1 , 2 )` and does a concatenate on the 1st dimension .
Otherwise , I should use concatenate .
You need to reshape the result of ` concatenate ` .
Now , this is great , but only works for the case of calculating the dot / inner product ( equivalent in this case ) of two vectors .
This is equivalent to calculating the inner / dot products ( again , equivalent for 1D arrays ) of the 1D array and each row of the matrix : #CODE
For the first case ( what I already have implemented ) , dot and inner product are equivalent mathematically for two 1D vectors , but inner is slightly faster .
For the second case I describe , the model I'm building requires computations that require me to do exactly what ` np.inner ` does for a 1D array and a 2D matrix ( i.e. the dot / inner product of the array and each row of the matrix ) , which is much faster than iterating over the matrix and calculating each inner / dot product separately .
And in any case , this is all part of big complicated monte carlo model where I need to do all these computations many , many times , so I'm trying to squeeze every bit of speed out of this that I can .
Clearly I'm a bit rusty on my linear algebra , but I guess this does mean my implementation could just be a standard matrix-vector dot product .
Here are more extensive speed tests backing up everything I've said here : #URL ( long story short , inner seems to outperform dot reliably , and the cython approach outperforms both , and more so for smaller vectors )
Integrate ` exp ( -lambda*x )` from zero to infinity .
What I do now is create empty arrays ( initialised with zeros or
Another option is to use ` PyArray_Flatten ` ( or better ` PyArray_Ravel `) to flatten the array , and then uses the ` SHAPE ` iteration .
I think you should avoid using ` resize ` , especially when computing ' dct ' .
exp ( x ) = e^x where e= 2.718281 ( approx ) #CODE
1 ) simply by performing the fft and back , I have reduced the sine wave component , shown below .
I think yes , but in the example they use a gaussian , so it's ambiguous ( fft ( gaussian )= gaussian ) .
The two representations are [ a , b ] which are the coefficiets of the numerator and denominator polynomial , or [ z , p , k ] which is the factored representation of the polynomial i.e. , : ` H ( f ) = k ( f-z0 ) * ( f-z1 ) /( f-p0 ) * ( f-p1 )` You can just take the polynomial from one of the filter design algorithms , evaluate it as a function of sqrt ( x^2+y^2 ) and apply it to your frequency domain data .
When I run this I get an index error : " too many indices for array "
If you write it in index notation ( Einstein summing convention ) this corresponds to ` c [ i , j ] = a [ i , k , l ] *b [ j , k , l ]` , thus you're contracting the indices you want to keep .
Since contractions involve pairs of indices , there's no way you'll get a 3d object by a ` tensordot ` operation .
The trick is to split your calculation in two : first you do the ` tensordot ` on the index to do the matrix operation and then you take a tensor diagonal in order to reduce your 4d object to 3d .
It iterates simultaneously over the corresponding 3rd dimension arrays in raster and indices and uses advanced indexing to slice the desired indices from raster .
What we need in order to properly resolve your indices during broadcasting are two arrays ` a ` and ` b ` so that ` raster [ a [ i , j , k ] , b [ i , j , k ] , indx [ i , j , k ]]` will be ` raster [ i , j , indx [ i , j , k ]]` for ` i ` , ` j ` , ` k ` in corresponding ranges for ` indx `' s axes .
Now when the other two are broadcast with ` indx ` they behave exactly the way we need .
I know that there have been multiple posts on numpy and pyserial installation in python on stack overflow , but somehow none of them seem to work for me .
Transform an array of count data into a matrix of ones and zeroes
I have an array ` n ` of count data , and I want to transform it into a matrix ` x ` in which each row contains a number of ones equal to the corresponding count number , padded by zeroes , e.g : #CODE
where if you liked , ` width ` could be ` max ( n )` or anything else you chose .
` coo_matrix() ` does not actually sum duplicate values ; it just stores those 3 input arrays in its attributes ( without copy or change ) .
but with a larger array ( more nonzero elements ) I get a display error .
For example if the array is binary , how do I iterate over all ones ?
The nice way to operate on just the ' ones ' of an array is with a boolean mask : ` I = a == 1 ` .
However , what's returned from ` np.nonzero ` is a tuple of elements where each element in the tuple is the indices that are non-zero for a particular dimension .
At each iteration of the loop , we grab unique ` ( x , y , z )` triplets that correspond to non-zero entries in ` a ` .
A consistent way to get several translation units to use the same internal API buffer is in every module , define ` PY_ARRAY_UNIQUE_SYMBOL ` to some library unique name , then every translation unit other than the one where the import_array wrapper is defined defines ` NO_IMPORT ` or ` NO_IMPORT_ARRAY ` .
You can add another axis to ` arr2 ` so that it will broadcast .
Fairly old laptop on windows 7 , I am more interested in relative gains than absolute speed if possible
Accelerating max pool
With this new code , the max pool stage now takes 50% of the time .
I took the transpose of the histogram matrix and also took the mean values of the elements in xedges and yedges instead of just removing one from the end .
A multivariate lognormal distributed random variable ` Rv ` should have this property : ` exp ( Rv )` should follow a normal distribution .
Therefore , the problem is really just to generation a random variable of multivariate normal distribution and take log of it .
I have n equal length arrays whose transpose corresponds to the coordinates in an n dimensional parameter space : #CODE
The last one , which uses np.unravel_index , has the correct shape , but the selected indices are NOT the maximum values along the z axis .
Find the max value along the last axis #CODE
Add an axis to ` b ` so that it will broadcast across ` a ` and create a boolean array .
So I wasn't satisfied - I've seen cases where ` reshape ` is faster than adding a new axis so I played around a bit .
Note that conceivably this could result in multiple 1s in a row if the maximum isn't unique .
The ` max ` values along the last axis - clear enough , a ( 2 , 3 ) array : #CODE
I tried to play around with some representative quantities of my points , like norm and / or argument , like in the following piece of code .
To get them , I actually stack all the vertices of all paths contained in a pyplot.contour ( x , y , mtx , 0.0 ) collection .
Looking at the code it appears that outer ` for ` loop runs N times and the inner ` while ` loop runs over all N elements only once .
For example , suppose ` a = ones (( 3 , 3 ))` .
I've hacked up following code , it is basically histogram which does not use interval bins .
Basically I need the histogram which will not use intervals but concrete values e.g. following input :
Note that this is not my join column
Some NumPy ufuncs ( such as ` np.add ` , ` np.multiply ` , ` np.minimum ` , ` np.maximum `) have an ` accumulate ` method , however , which may be useful depending on the calculation .
" maximize the dot product between these two vectors "
I'm stuck how to add the ids of the points to a numpy array .
I need to merge records ( sum the values of the second column ) where #CODE
You just need to populate where ` x [: , 0 ]` changes which is equivalent to non zero indices of ` np.diff ( x [: , 0 ])` shifted by one plus the initial index 0 : #CODE
` Traceback ( most recent call last ): File " / home / krlk89 / abc.py " , line 8 , in unq1 = np.append ( True , np.diff ( x [: , 0 ]) ! =0 ) IndexError : too many indices `
Find unique values and count duplicates : #CODE
Numpy dot use all the memory
using numpy reshape for column major in multi-dimensional array in python
But when I do the reshape , it gives me the following output : #CODE
Just wanted your expert opinions if there is a way to achieve the reshape and swapaxes using a single method / function .
Don't be afraid of using functions like ` reshape ` and ` swapaxes ` .
There may be other ways to do it , but if this gives you the right result , it is the " right " function :) As @USER says , ` reshape ` and ` swapaxes ` are not particularly expensive , so I wouldn't try too hard to find an alternative solution
The goal is to loop files , fetch content from those files and append them to the dataframe ` df ` .
But what I don't get is , how does ` append ` know how to interpret the entry , i.e. how does it know it should merge the entry with the already existing column names ?
The values in the tuple refer to the axes as they were before you make the ` transpose ` call , the indices in the tuple refer to the axes as they are going to be after the call .
I tried getting the min and max of each parameter and making a new axis ` numpy.arange ( min ( param1 ) , max ( param1 ) , 100 )` , then comparing it to the old values via ` numpy.setdiff1d() ` but this makes the grid rectangular when it isn't necessarily .
I was assuming the immediate up and down ones .
Expand the holes to list of missing grid points using their indices to extract the starting point and the length of the hole .
Finally , concatenate
IndexError : too many indices for array
` scipy.sparse ` stores just the nonzero values of an array , though the space savings depends on the storage format and the sparsity .
Masking lets you keep the data in a rectangular arrangement , and still calculate things like the mean and sum without useing the masked vales .
I have two files first.csv and second.csv , in first.csv I have two headers LAC and reference count , second.csv no of header can be in any number but it will follow one format that is one column with LAC all ids , followed by time date series , I need to take LAC ( id ) from second.csv and search in LAC of first.csv take the reference count and subtract value of all time series of second.csv .
to find the optimal size of data ( here 1620 for a 1535 element vector , pad with zeros ) .
Python : numpy.memmap zeros , reusage , documentation ?
Is a numpy.memmap array initialized with zeros ?
Can the contents of an already existing old numpy.memmap file ( from a previous execution of the script ) be loaded into a new numpy.memmap rather than replaced by zeros ?
zeros behaviour [ documented in the source ] ( #URL ) in the Notes section .
The data in the freshly created file is all zeros
I'll state the answer here explicitly : ` mode= ' w+ '` initializes with zeros [ on systems with POSIX filesystem semantics ] ( #URL ); all other mode values initialize with the old memmap file contents .
With older versions of numpy , you can use ` reshape ` to restore the collapsed dimension : #CODE
If you substitute sample values of ` i ` and ` j ` yourself , you'll see that we get unique 8 x 8 patches for each grid in your image .
But if not , then the data is scaled according to a max and min transformation ( where 0 and 255 are the default low and high pixel values to compare to ) .
So this cascade of stuff is why you were getting all zeros in the outputted image file and why the other suggestion of using ` dtype= np.uint8 ` actually helps you .
The ` axes ` parameter to ` transpose ` provides the new order you want them arranged in , in your example : first the second , then the first , then the third .
Your transpose has switched the order of the blocks and rows , leaving columns unchanged .
There is the following comment on the C source code of ` PyArray_NonZero ` , the C function that handles all the calls to ` nonzero ` : #CODE
Probably this is better articulated in the comment numpy nonzero function , i.e. its api doc .
Numpy : Fix array with rows of different lengths by filling the empty elements with zeros
numpy count sum of costs for points in same group
I want to compute in an efficient way without loops the sum of costs for each group , to any point .
[ 10 , 10 , 40 , 1 , 4 , 1 , 2 ] so i will have to use reshape
@USER oh I thought bincount would flatten the input like so many Numpy functions do .
@USER I have tried different threshold values but at the end the indices the code returns do not corresond to the right maximas .
The equivalent way to do this in NumPy is to use the mgrid and ogrid functions . mgrid will return the coordinates in a fully populated array ( the same shape as the source image ) while ogrid returns vectors of the right shape that often can be used and will automatically be broadcast correctly .
The keywords you need to pass ` read_table ` are a little different from the ones for ` loadtxt ` , for instance , here I used ` sep=r ' '` to fit the format of Warren's file , and I set the ` dtype=float ` so that ` Nan ` s would be supported .
You don't want to append rows to an array one at a time , because each iteration requires copying the entire array .
Here are the pages of scipy.integrate #URL and trapz in Numpy #URL I try and see a lot of code about the Numerical Integration and prefer to use the existing ones ...
If you expect the data to fit a certain kind of function like a sine , log or exponential it is good to use that as a basis for curve fitting .
The end goal is to loop through a series of files using the python lasio library , select certain 1d arrays , flatten them to a 2d array , then export to a csv file ready for data loading to a database .
If I correctly understand your question , you could add an id column to each file and then concatenate all the arrays .
Next , find the ` max ` along ` axis=1 ` for this 2D array and thus have the final ` max ` output for each such unique triplet .
If you need to input a vector , you should reshape it it to be viewed as a 2D array , because the subroutine needs a 2D array #CODE
The variable " m " above is input as the number of rows in both the input and output arrays ( so i dont have to initialize large arrays of zeros ) .
See the edit , just reshape the list or a 1D array .
Or make a Python function , which does the reshape for you and which calls the old version for 2D .
If you want to do 1d clustering , then reshape your data to a 1d array , cluster the points and then reshape back your labels : #CODE
If you want to do 1D clustering then reshape your data to a vector of shape ( 25 , 1 ) , but I can't understand why you would do 1D clustering .
So reshape your image as 1d vector .
Then cluster every pixels and then reshape the labels to the initial shape .
I have an exercise which is to translate the Izhikevich model code from MATLAB to Python using Numpy .
From the disassembled code , each time the ` a [ pos1 ] [ pos2 ]= 2 ` assignment is performed , it is indeed stored in the top of the stack but then , global ( case 1 ) or the local ( case 2 ) variables are returned instead .
When you split the operations ( case 3 ) , the interpreter seems to all at sudden remember that it had just stored the value on the stack and does not need to reload it .
That way I don't have to flatten each image into a row that forms my training matrix and I lose all the shape information ( which is what was happening when I trained my SVM , LDA , PCA , etc ) .
No need to flatten a data .
Isn't there something like a C++ ` std :: map ` in Python ?
A ` std :: map ` is a binary search tree where you can overload ` operator < ` for your array and get this " almost exact " functionality .
You won't get O ( 1 ) lookup but O ( log ( n )) might be possible ...
Why does scikit neural network reshape my input array automatically ?
Looking at the source , scikit will reshape input arrays if the X ( input samples ) you pass in when initializing is a different size from creating / splitting the dataset in the MLP backend .
Equations containing outer products of vectors
We know from the diagonal elements in A , the value of x entries .
numpy get std between datasets
There should be a std value for each X .
In the end my result ` std ` should have a length of ` n ` .
Do you just want the std of the x-values from each array .
If you have a list of the arrays you can use ` np.dstack ` to make a 3D array and then take the std . along the appropriate axis
np.std ( a , axis=None , dtype=None , out=None , ddof=0 , keepdims=False ) with ddof=0 is the population st . dev ... with ddof=1 , it is the sample std dev .
This stacks the 2-D arrays into a 3-D array an then takes the ` std ` along the first axis , giving a 2 X 8 ( the number of arrays ) .
The first row of the result is the std . devs . of the 8 sets of x-values .
then you stack the values using ` column_stack ` #CODE
Or use ` ravel ` or ` flatten ` ( see also @USER ' s answer , although that does not answer the decimal precision issue ) #CODE
If it is a one-dimensional array i.e. only one row ` savetxt ` should take care of that for you , otherwise use either ` ravel ` or ` flatten ` to flatten the the array and then write to file .
The savetxt command puts out each value of the array in a new line . ravel or flatten don't seem to do anything , I guess because I already have a 1D array .
Why is there such a large speed difference between the following L2 norm calculations : #CODE
You might try a different ` ddof ` for ` std ` , eg .
@USER I can confirm that using ` std = np.std ( returns , ddof = 1 )` gives the same results as Excel using the data from [ Microsoft's kurtosis help page ] ( #URL ) .
Usually the change in ` std ` is small , but with the ` s**4 ` use , small changes in ` s ` will be amplified .
labels are just indices of centroids ...
Get the center associated to the label of the points and then reshape it to the data shape .
If performance is of essence , you can calculate the linear indices and then use ` np.take ` or use a flattened version with ` .ravel() ` and extract those specific elements from ` val_arr ` .
Given ` 0 = x1**2 + x**2 - 0.6 ` it follows that ` x2 = sqrt ( 0.6 - x1**2 )` ( as Dux stated ) .
But what you really want to do is to transform your cartesian coordinates to polar ones .
If I were to write a port of it , I'd probably do so as a macro to decode the subscript string at parse time and directly expand to a bunch of for loops ( similar to how ` @USER ` works ) .
Coming from a C++ world , I would create a ` std :: map ` and provide a compare function that can do the comparison with some user defined tolerance to check if the values have been added to the data structure .
Note that a KD-tree is efficient for looking up a point in a static collection of points ( cost of a lookup is ` O ( log ( N ))` , but they are not optimized for repeatedly adding new points .
The search time is O ( log ( N )) , but the same complexity has a ` std :: map ` because it's implemented as a black / red tree .
I've been googling but every solution that let's me reshape or find the shape just gives the error :
IndexError : too many indices for array
Try changing ` chntrain ` to a 2-D array instead of 1-D , i.e. reshape to ` ( len ( chntrain ) , 1 )` .
What i don't really like about this , though , is that I have to sort and slice the array to remove the 0.0 values which are the result of computing the distance between identical points ( so basically that's my way of removing the diagonal entries of the matrix I get back from cdist ) .
I started writing code to generate random data , but it might be easier just to show the plot here ( plotted in log scale to highlight the non-matching data lengths ) .
How can I pad and / or truncate a vector to a specified length using numpy ?
I want to return a vector of length five , such that if the input list length is 5 then it will be padded with zeros on the right , and if it is > 5 , then it will be truncated at the 5th element .
` np.pad ` is overkill , better for adding a border all around a 2d image than adding some zeros to a list .
But by itself it does not truncate .
A simple algorithm is : iterate over every data point ( r , c ); take a slice array [ #URL #URL append the indices of any data points in that slice to your results .
I've attached an image below : blue highlights are short links on the left boundary of the array which are completely missed , and yellow and green highlights are longer links which are only being identified in certain directions ( in this case , SSE and ESE links are missed , while SSW and WSW links are picked up ): #URL ( output for example above ; max distance = 2 )
Multiply the coefficients and sum #CODE
Or simply us the Numpy Polynomial Package #CODE
Using ` numpy ` s ` Polynomial ` class is probably the easiest way .
You can transpose ` b ` and ` c ` and then create a product of the ` a ` with the transposed array using ` itertools.product ` : #CODE
Or just calculate the det outside the Numba function and pass it as an argument
The intersect is the coefficient that corresponds to the column of ` ones ` , which in this case is : #CODE
Note that ` m ` corresponds to the column of ` ones ` .
Creating inverse and diagonal of numpy matrices
any suggestion for this to get inverse and diagonal ?
` diagonal ` has returned the same numbers , but for ` M ` , it returns an object of the same class , and which by class definition is 2d .
So that means that ` M.I ` use ` pinv ` rather than ` inv ` .
Also , by changing the index order after -> , you can do a reshape without an extra call to the actual ` reshape ` .
which results in the following error : ` AttributeError : ' int ' object has no attribute ' clip '` .
While an array has a ` clip ` method , ordinary numbers don't .
You're computing ` max ( yvals )` again for each point , which means you have to loop over ` len ( sortedData )` numbers each time , and you're using the Python function to do it ; you're not taking advantage of vectorization , but using slow Python-level loops ; even your progressbar seems to slow things down .
and it would return to me a list of dataframes , each one with 6 hours of data ( except maybe the first and last ones ) .
Because now it takes too much time ( more than 30 min ) .
Instead , make ` points ` a python list , and append to it .
One thing that would improve speed is to imitate ` genfromtxt ` and accumulate each line in a list of lists ( or tuples ) .
Inplace transpose of 3D array in PyCuda
I have a 3D array and would like to transpose its first two dimensions ( x y ) , but not the 3rd ( z ) .
The strides are the jumps in memory consecutive indices need to take for each dimension in bytes .
Is there a straightforward way to reshape this kind of array by specifying what columns the axes correspond to ?
I won't necessarily know ahead of time what the shape of the ndarray should be , but it seems to me that based on the values it should be possible to reshape the array properly .
From there I would stack sets of 3D ` mu1 ` data into a 4D array , and repeat the process with ` mu2 ` and ` mu3 ` .
An N-dimensional array container would only make sense if your points are positioned in a regular lattice , such that there is a single value for every combination of indices in ` data [ i , j , k , l , m ]` .
Now you can use ` np.lexsort ` to get the set of indices that will sort the rows of your 2D array of simulation parameters such that the values in each column are in ascending order .
Having done that , you can apply these indices to the rows of simulation output values .
If we're taking the Euclidean norm of an n-dimensional vector , the only case in which the norm can be 0 is when the vector is the Zero Vector .
So what could possibly be causing numpy to return a norm of 0 on a vector that is not even close to the Zero Vector ?
The Python ` float ( astr )` function works with strings like `" 0.1013 "` , not long ones with many numbers .
You need to split them into lines ( e.g. ` readline() `) , strip off ` \n ` , split them up into blocks like ` vector() ` , and further split that into strings with one number each .
If you look at ` sum ( mask_list )` , you'll find it's just a list of integers .
One possible caveat , what I don't know about the latter method , is whether it runs into problems when you try to sum more than 255 masks , where at least one ( the same ) element is always ` True ` ( i.e. , ` 1 `) .
So even the sum of a short list of mask is converted to a 64 bit integer ( which can then easily be converted to a boolean mask array ) , and you won't run easily into the integer limit .
However , this does not seem to work , as I get the error : " only integers , slices ( ` : `) , ellipsis ( ` ... `) , numpy.newaxis ( ` None `) and integer or boolean arrays are valid indices " .
Similar to what can be done in ` numpy ` , a solution would be to reshape your ( 1 , 50 ) tensor to a ( 1 , 10 , 5 ) tensor ( or even a ( 10 , 5 ) tensor ) , and then to compute the mean along the second axis .
My goal is to convolve the kernel with the population density such that the output captures the transmission risks across the landscape .
Is there a way to convolve within the context of the original , fixed boundaries ?
In order to do what you want , you need to pad ` og ` with zeros , and expand ` ker ` accordingly ( since it is already cyclically shifted , you just need to expand it more to fit the size of ` og `) .
Whenever you use ` fft ` based methods , you are always introducing periodic-ness in the problem , essentially , by definition .
You are using a bottom's up approach , which spares you the recursion stack , but typically ends up producing unbalanced merges , which reduce efficiency .
We could rewrite it to store indices in a list used as a stack , not sure if the list will trip Numba because of it making Python API calls .
While this approach is OK for smaller input arrays it's definitely no way to go for larger ones .
NumPy implementation ( mean and std can possibly be computed in one function ): #CODE
Numpy std #CODE
Or does it contain all the pre-compiled binaries , and just chooses the proper ones to install based on the operating system ?
As I understand , the installable binary package , such as Anaconda-2.3.0-Windows-x86.exe , contains needed pre-compiled C extensions for all platforms and choose proper ones to install .
Sure , you could compress the ` .exe ` installer inside a ` .zip ` archive , but that does not mean that pip will know what to do with it .
Is " norm " equivalent to " Euclidean distance " ?
I am not sure whether " norm " and " Euclidean distance " mean the same thing .
So I used the ` np.linalg.norm ` , which outputs the norm of two given points .
The concept of a " norm " is a generalized idea in mathematics which , when applied to vectors ( or vector differences ) , broadly represents some measure of length .
There are various different approaches to computing a norm , but the one called Euclidean distance is called the " 2-norm " and is based on applying an exponent of 2 ( the " square ") , and after summing applying an exponent of 1 / 2 ( the " square root ") .
` sum ( abs ( x ) **ord ) ** ( 1 . / ord )`
becomes ` sqrt ( sum ( x**2 ))` .
In mathematics , a norm is a function that assigns a " size " or " length " to each vector within a vector space .
The Euclidean norm ( also known as the L 2 norm ) is just one of many different norms - there is also the max norm , the Manhattan norm etc .
The L 2 norm of a single vector is equivalent to the Euclidean distance from that point to the origin , and the L 2 norm of the difference between two vectors is equivalent to the Euclidean distance between the two points .
You can think of entries of ` y ` being indices into rows of ` A ` .
possible duplicate of [ numpy.array . \_\_iadd\_\_ and repeated indices ] ( #URL ) and #URL
Is ` A ` initialized to zeros , i.e. ` A = np.zeros (( N2 , N3 ))` before going into the loop ?
I tried ` scipy.spatial.distance.cdist ( vec1 , vec2 )` , and it returns a 3000x3000 matrix whereas I only need the main diagonal .
To compute only the distances between corresponding row indices , you could use ` np.linalg.norm ` : #CODE
` zeros ` , ` ones ` , ` empty ` are more user friendly .
Things like portions of the stack trace , shape and type of variables , code with the problem area highlighted .
You can use ` numpy.unique ` to get the unique elements in ` data ` , and ( more importantly ) an array that maps those unique values back to the input array : #CODE
At this point , ` keys [ inv ]` recreates ` data ` .
Now we can index ` vals ` with ` inv ` to get the desired result : #CODE
I want to keep the nice structure it has and not clutter the code with too many conditions , so I was wondering if there's a way to always use ` n+1 ` indices and have the last index ignored by an indexing operation .
Look for example at the sequence 0:12 and reshape it to ( 3 , 4 ) and observe the layout .
The first column of ` f ` is the FFT of a delta function which is all ones .
Reductions ( e.g. sum , max , mean etc . ) are always carried out over all axes by default .
It's a bit of a mess , but I have to say I much prefer the default behaviour for reductions in numpy compared to MATLAB , where you have to do ` sum ( sum ( x ))` or ` sum ( x (: ))` .
I did the reshape , just so that both arrays are same shape , but I do not think you really need the reshaping , with the list comprehension the shape of array you get is ` ( length of string , )`
Even considerably faster than Ashwini's join : #CODE
As @USER commented out , ` np.fromstring ( s , ' int8 ') - 48 ` is not limited to ones and zeros but will work for all strings composed of ASCII digits .
I imagine I need to make a 2D kernel with weights changing along one direction only , but I'm not sure how to do this , or which convolve function to use ( ` numpy.convolve ` , ` scipy.signal.convolve ` , ` scipy.ndimage.filters.convolve1d ` etc . )
Then convolve it with your signal , #CODE
The actual ellipses are the blue ones .
The green ones are just how they look like with their angle 0 degrees .
In particular , I need to find the row with maximal dot product with a given vector .
What do you want to happen if there is more than one row which reaches the max ?
Interestingly , in this case ` b.base ` is a transpose of the view #CODE
Slightly different FFT results from Matlab fft and Scipy fft
For one thing , ` scipy.fftpack.fft ` is guaranteed to always return a complex result , whereas the result of MATLAB's ` fft ` function is sometimes real and sometimes complex , depending on whether there is a non-zero imaginary component .
However , that doesn't explain why the result of ` scipy.fftpack.fft ` actually contains non-zero imaginary components , whereas the result of MATLAB's ` fft ` function does not .
I suspect that the underlying reason for the difference has to do with the fact that MATLAB's ` fft ` function is apparently based on FFTW , whereas scipy and numpy use FFTPACK due to licensing restrictions .
` scipy.fftpack.rfft ([ 0 , 1 , 2 , 3 , 4 , 3 , 2 , 1 ])` still gives a result containing nonzero ( but tiny ) imaginary components .
I'd advise against calling ` flush ` on every iteration since this defeats the purpose of letting your OS decide when to write to disk in order to maximise efficiency .
To turn these coefficients into a ` Polynomial ` object , you just need to pass the array to the ` Polynomial ` constructor : #CODE
I want to convert each sample to concatenate array of earlier 1000 samples plus itself .
` flatten ` does return a copy ( see it's doc ) .
Now I know you can just concatenate , but this is quite laborious if N=200 .
I could copy the contents of ` a ` 3 times in the third dimension , then copy the contents of ` c ` twice in both the first and second dimensions , so that both of my arrays were ` ( 2 , 2 , 3 )` , then compute their sum .
When I compute the sum , the result gets ' broadcast ' out along the dimensions of size 1 , giving me a result of shape ` ( 2 , 2 , 3 )` : #CODE
If you transpose ` B ` with ` B.T ` then it matches the expected output .
This will broadcast along the first dimension , and so doing ` B [ 0 ] , B [ 1 ] ,...
The Mersenne Twister RNG used by numpy has 2 19937 -1 possible internal states , whereas a single 64 bit integer has only 2 64 possible values , so it's clearly impossible to obtain a unique integer seed for every arbitrary RNG state .
Try it again , but using the transpose of ` zi ` : #CODE
How can I append to a numpy array without reassigning the result to a new variable ?
I have a matrix ` M ` with dimensions ` ( m , n )` and I need to append new columns to it from a matrix ` L ` with dimensions ` ( m , l )` .
The problem arises with the fact that I need to append many many matrices to the original matrix , and the size of these matrices ` L ` are not known beforehand .
The problem here is clearly in this ` append ` function ( I tried it with ` vstack ` and nothing changes ) .
Is it important in your program to append iteratively ?
If not you could just collect all the matrices and then join all of them in one go .
append , vstack , etc all use concatenate
There's no way to append to an existing numpy array without creating a copy .
If I create a ` ( 1000 , 10 )` array , then decide that I want to append another row , I'd need to be able to extend the chunk of RAM corresponding to the array so that it's big enough to accommodate ` ( 1001 , 10 )` elements .
The only way to ' concatenate ' arrays is to get the OS to allocate another chunk of memory big enough for the new array , then copy the contents of the original array and the new row into this space .
Use a standard Python list to accumulate your rows inside your ` while ` loop , then convert the list to an array in a single step , outside the loop .
If you run out of space , concatenate on another chunk of rows .
You can use ` numpy.argsort() ` to get the indexes of the sorted array of datetime object , and then use the returned indices to sort the array - ` emission_each_month ` .
Now all I want it the entries in the y array for these indices .
What would be the most efficient way to find indices of elements in vector v in the corresponding row in a .
You can convert ` v ` to a column vector with ` [: , None ]` and then compare with ` a ` to bring in ` broadcasting ` and finally use ` np.where ` to get the final output as indices - #CODE
In case , there are multiple elements in a row in ` a ` that match the corresponding element from ` v ` , you can use ` np.argmax ` to get indices of the first match in each row , like so - #CODE
I am having problems in creating a ` mask ` with ` c ` true elements , or maybe this can be done with indices directly ?
@USER ` np.random.randint ` might give you repeated indices , so I don't think that would work in your case .
In retrospect it's probably overkill here compared with the transpose route , but it's handy when the multiplications are more complicated .
I did a direct copy and paste and got the error " ValueError : operand 0 did not have enough dimensions to match the broadcasting , and couldn't be extended because einstein sum subscripts were specified at both the start and end " Any ideas ?
You can add enough extra dimensions to ` a ` so that it will broadcast across ` b ` .
One possibility is that you're taking the log of 0 or a negative number .
The function was taking the log of 0 ( the first term in the linspace ) , which is why your ` np.log ( x+b )` func() worked - b made it nonzero .
The problem is very simple - since the first value in your ` x ` array is 0 , you are taking the log of 0 , which is equal to ` -inf ` : #CODE
Getting the indices of several elements in a NumPy array at once
Is there any way to get the indices of several elements in a NumPy array at once ?
You could use ` in1d ` and ` nonzero ` ( or ` where ` for that matter ): #CODE
This works fine for your example arrays , but in general the array of returned indices does not honour the order of the values in ` a ` .
This returns the indices for values as they appear in ` a ` .
Objective function and its gradient , that needs some optimizing as well : #CODE
I agree , but the sum of ` np.exp ` s and ` erf ` s are not compiled ... right ?
Maybe it is better to check which optimization method of the available ones suits better for your special case and try to initialise it in a clever way .
The question is similar to How to sum dict elements or Fastest way to merge n-dictionaries and add values on 2.6 , but in my case I've string in my dict .
The outer container in your sample isn't a dictionary .
There aren't any keys in outer layer .
See this answer , on Merge and sum of two dictionaries , for more info .
So should I be looking for something that returns views instead of arrays ( as it seems roll returns arrays ) ?
In this case is roll initializing a new array each time it's called ?
See the line ` c = b * ones (( shape ( b ) [ 0 ] , 1 ))` .
I may not be able to word it in your terminology , but I would say it as follows : ` ones (( b.shape [ 0 ] , 1 ))` results in a ` ( 3 , 1 )` array .
Thus , multiplying ` ones ( ... )` with ` b ` ( left or right side doesn't matter here ) , will broadcast to the right dimension : thus , ` ones ( ... )` gets broadcasted to a ` ( 3 , 3 )` array , since the last ( right-most ) dimension of ` b ` is ` ( 3 , )` , and the 1 in the shape of ` ones ` can get broadcasted .
If you'd try ` ones ( 3 , 2 )` , you'll find this won't work , since the right dimension can't be broadcasted to that of ` b ` .
In numpy arrays , you are able to broadcast lower-dimensional arrays to higher-dimensional ones , and the lower-dimensional part is applied elementwise to the higher-dimensional one .
In your case , it starts with the last dimension of ` ones (( shape ( b ) [ 0 ] , 1 ))` , which is ` 1 ` .
So it multiplies the array ` b ` elementwise for each element of ` ones (( shape ( b ) [ 0 ] , 1 ))` , resulting in a 3D array .
Could you please elaborate how to get something like ` concatenate (( a [: , 0 ] , a [: , 1 :]) , axis = 1 )` working ?
My understanding is that ` gradient ` returns the derivative of the argument passed to it , so I guess there's an implicit question here , which is , is that right ?
As noted in the documentation ` gradient ` assumes unit spacing of 1 unless you provide the sample distance by the ` vararg ` argument .
The gradient will have units of mV / ms or V / s .
In the first case the gradient is 1 mV / ms , in the second case it is 50 mV / ms
" In the first case the gradient is 1 mV / ms , in the second case it is 50 mV / ms .
" i see your point , but my question is what units ` gradient ` will return the result in : e.g. , if the first element of the returned array is 1 , does that mean 1 mV / ms ?
I'm getting different results when calculating a negative log likelihood of a simple two layer neural net in theano and numpy .
I get the following result using numpy's correlate function : #CODE
If True , uses the old behavior from Numeric , ( correlate ( a , v ) == correlate ( v , a ) , and the conjugate is not taken for complex arrays ) .
` scipy.linalg ` contains all the functions in ` numpy.linalg ` . plus some other more advanced ones not contained in ` numpy.linalg `
It's not a big difference over the ` decode ` , but ` astype ` works ( and can be applied to the whole array rather than each string ) .
With this ` decode ` can be applied to the whole array : #CODE
You can decode on the fly : #CODE
Also I want to find the indices of the columns with object values .
In most cases , there will be a lot of zeros in element_id_1 .
override the previous value or keep the max / min value ?
If you are going to have a lot of zeros in id_2 dict , simply don't add anything , if the key does not exists , the value would be zero .
There's already an " implicit " array of elements which is in fact element_id_1's indices .
I want to convert each sample to concatenate array of earlier 1000 samples plus itself .
Indeed , appending to a ` list ` takas ` O ( 1 )` amortised , but you don't have to append in the first place .
You didn't reshape though , please check .
You can reshape an array in place by assigning to its ` .shape ` attribute .
The distance between the centre of the red clock and the incoming random green block is fixed so I will be using that parameter to prevent the green blocks from drifting away from the red ones .
The green blocks must also overlap with the red ones ( as shown ) .
I used ` merge ` exactly like you wrote , but that apparently does an inner join , so only data points that have timestamps in all tables are written to the merged table .
I've tried an outer join as well , which does include all the data but also doesn't get the ordering right .
Use a ` join ` to merge them with the `' outer '` method
I am just trying to figure out how I can reshape the array so that the id column remains but ' shape ' is the result of combining the x and y columns .
I have no experience with this , but you could look into package like the ones presented [ here ] ( #URL )
The sum of all the values in your collection is probably larger than that , so it gets rounded up to infinity when calculating the mean .
So , I can reshape it as : #CODE
With fprime ( the gradient function there ) I get an error .
I have used it on the previous exercise , but it was failing to find the theta with that gradient function , which is ideal to the one in Octave .
Nicer way to do nested dot products in numpy ?
Is there a way to do transpose without typing ` np.transpose ` ?
You can use the ` .T ` notation instead of ` transpose ` #URL
Basically your code , but using the ` method ` expression ` dot ` and ` .T ` notation .
basically the same as the ` dot ` version .
Is there a good reason why something like ` np.dot ( inv ( np.dot ( X.T , X )) , X , X.T )` is not allowed ?
In this case , the ` np.tile ` documentation indicates what happens when you are trying to tile an array to higher dimensions than it is defined ,
If reps has length d , the result will have dimension of max ( d , A.ndim ) .
So , to get your desired behavior just be sure to append a new singleton-dimension to the array where you want it , #CODE
Another consideration - what happens when you flatten the tile ?
The first issue is thus that the ` Vec ` is having its destructor run ; that could be amended by calling ` std :: mem :: forget ( to_sort )` when you re done with it .
Scikit learn : utf-32-le ' codec can't decode bytes
But I have tested this with all my own strings and it can decode just fine .
When I printed out what it was trying to decode : #CODE
First I would want to identify the unique records as per my comments above ( i.e. eliminate redundancies -- see code above ) .
You need to plot the log of your data as well as the predictor values , then you can compare & just change your tick labels to the anti-log of the numbers that they would be .
Is there a way to completely flush memmap from memory in Python and somehow just store a pointer ?
Numpy broadcast through dictionary
1st ) ` numpy.unique ` will return a * sorted * array of unique values , including the ` 0 ` .
2nd ) To get the indices of ` np.unique ` , you have to use the keyword ` return_index=True ` .
` np.unique ` returns all * unique * values of an array , therefore every value is present only once .
How do I truncate an EARRAY in an HDF5 file using pytables ?
I have an HDF5 file containing a very large EARRAY that I would like to truncate in order to save disk space and process it more quickly .
I am using the ` truncate ` method on the node containing the EARRAY . pytables reports that the array has been truncated but it still takes up the same amount of space on disk .
The script I am using to truncate ( main.py ): #CODE
If there were a way to directly modify the contents of the earray , instead of using the truncate method , this would be even more useful for me .
Is there a way to completely flush memmap from memory in Python and somehow just store a pointer ?
Using del will flush and delete the object , flush will obviously flush
wouldn't be possible to iterate over the transpose of ` t ` , just doing : ` for col in t.T : ...
` transpose ` does nothing .
NumPy arrays can " broadcast " their values to act like arrays of higher dimension .
Since broadcasting can add new axes to the left side of an array's shape , an array of shape ` ( 5 , )` can broadcast to shape ` ( 1 , 5 )` .
Plot max of graph using python
I've figured out how to graph functions , but how do I plot a point which indicates the max and minimum values ?
So I can create the shifted column , but I don't know how to roll the bottom value around to the top .
I had to do a resize each time I added elements , e.g. #CODE
The maxshape ( None ) feature doesn't resize the dataset automatically - would be nice if it did though .
Plotting histogram of floating point numbers and integers in python
I have a set of numbers as integers and floating point , I want to plot a histogram from them .
I am not getting as to where am I going wrong , can someone please suggest as to how can I plot a histogram of floating point numbers and integers
A couple of small improvements are : 1 ) put the ` * 0.5 ` outside of the sum ; and , 2 ) sum the ` cos ` before subtracting the from ` 1 ` ( which will be more accurate as well , since the sum will hang around 1 ) .
@USER Effectively with 1 ) I gain more than 1ms but 2 ) give me an error ` RuntimeWarning : invalid value encountered in sqrt ` .
For 2 , if you sum over N items then ` sqrt ( sum ( 1 - cos ( x )))` becomes ` sqrt ( N - sum ( cos ( x )))` .
He has two options : making all of his lists the same length by padding them with zeros ( which seems like a bad idea since you are changing your input lists ) , or finding the shape and then creating an ndarray with them .
The length fetching loop could stop at the 1st nonzero value .
By the way , if you are trying to create a 5x5 matrix of zeros , you are missing an outer level of brackets : ` board = numpy.matrix ([[ 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 ]])`
Besides giveing error callback stack , you need to give some of the printout ( with added diagnositic prints if needed ) .
Thanks to the new stack information , I've deduced that the MATLAB script did not run , or at least did not overwrite the temp file that this script created at the start .
Normally the MATLAB script would over write it ( or append ? ) .
Unless you're asking someone else to plot the data for you , I don't see a need to dump them here .
But then , minimum type is not unique since signedinteger and unsignedinteger are not in the same branch of the dtype hierarchy ( #URL ) .
I simply need to iterate over each pixel in an ndarray , sum all their values , and divide by the total number of pixels ( image.size ) .
It's unclear to me what you're trying here as in numpy it won't work as the shapes cannot be broadcast : #CODE
So you need to transpose the df ( if necessary ) in order for broadcasting to work , then the series needs to flattened to a 1-D array which in this case can be done by calling ` .values ` attribute
but there is no pandas method that allows you to return the minimum like ` np.miniumum ` , also ` np.minimum ` does not care about aligning columns and indices here .
I would like to know if there is a built-in numpy ( i.e. , efficient ) way to apply a vector of functions to a vector of values , such that each function takes in a corresponding value ( almost like a dot product for input values to functions ) .
Since adding single element per go to a ` numpy ` array is inefficient , I increase its size by chunks of length 10000 initialized with zeros .
This leads to the situation that in the end I have an array with tail of zeros .
And what I would like to have is the array , whose length is precisely number of meaningful elements ( because later I cannot distinguish junky zeros from actual data points with zero value ) .
In other words , ` np.indices ` returns arrays which can be used as indices .
That should leave each frame as a 0 , 1 , 2 or 3 , as long as no frame has two ones .
Using ` FeatureUnion ` , you simply need to append your new transformer to the transformer list : #CODE
In that case , an alternative approach is to pre-compute store the intermediate results of the transformers ( matrices or sparse matrices ) , and concatenate them manually using ` numpy.hstack ` or ` scipy.sparse.hstack ` when needed .
Using the appropriate stacking function in numpy / scipy , the merged dimension ( X.shape [ 1 ]) is the sum of the dimension of the individual feature matrices .
Since you are selecting and adding up a huge number of columns from ` X ` per unique ` y ` , it might be better in terms of performance to run a loop with ` complexity ` equal to the number of such unique ` y's ` , which seems to be at ` max ` equal to the number of rows in ` W ` and that in your case is just ` 10 ` .
Note that ` W [ y.ravel() ]` is the same size as ` X.T ` ( why did you pick these sizes that require transpose ? ) .
You could use tile ( but you would also need swapaxes to get the correct shape ) .
I prefer using the reshape method to using slice notation for the indices the way Divakar shows , because I've done a fair bit of work manipulating shapes as variables , and it's a bit easier to pass around tuples in variables than slices .
ValueError : operands could not be broadcast together with different shapes in numpy ?
If you add numpy arrays , it adds their values , it does not concatenate them ( like with lists ) .
Otherwise , you could look into ordered dictionaries in the collections module ( std lib )
So , the time interval is not going to be constant , and there's no guarantee that I'm going to be entering positions in the order that I want them to be maintained ( I might want to interpolate a position between two existing ones ) .
Both containers have a lot of nice functionality ( see the docs ) , for example , if you wanted to know max value of ` x ` for each transitor , at the first voltage level , it'd be something like this : #CODE
This doesn't have anything to do with ` floor ` .
Counting number of unique values in subset of sorted array
For each user in ` users ` I need to find the data related to the user in ` dat ` and count the number of unique values .
If it is a smaller fraction , you can still use the two calls to ` searchsorted ` to figure out which slices to call ` unique ` on : #CODE
4 ) Implement gradient checking .
Here , you compare the analytical solution to a numerically approximated gradient
Or an even better approach that yields a more accurate approximation of the gradient is to compute the symmetric ( or centered ) difference quotient given by the two-point formula
I worked out numeric partials and found that my o and deltas were correct , but I was multiplying the wrong ones .
@USER sort of . the ` nan ` values are replaced with zeros . where there was a drop from ` 0.02 ` to ` nan ` , now it's ` 0.02 ` to ` 0 ` . what it should be is a very gradual descent from ` 0.02 ` closer and closer to ` 0 ` .
Alternatively , you may be able to figure out analytically what ` log ( ndtr ( x ))` is , and break it up like for the ` exp ` term .. you get the idea .
You could eliminate the ` log ` call in @USER ' s solution by using [ ` scipy.special.log_ndtr `] ( #URL )
For ` x = -20 ` , ` log_ndtr ( x )` uses a Taylor series expansion of the error function to iteratively compute the log CDF directly , which is much more numerically stable than simply taking ` log ( ndtr ( x ))` .
As you mentioned in the comments , the ` exp ` can also overflow if ` x ` is sufficiently large .
` my_func2 ( 50 )` raises a ` RunTimeWarning ` : " overflow encountered in exp .
It looks like you are trying to guess what would be the number of ` integral ( f ( x ))` by calculating ` integral ( log ( f ( x )))` and hoping to get a right answer .
i didn't calculate ` integral ( log ( my_func2 ( 50 )))` or intend to calculate that .
Mine and askewchan's solution also computes ` f ( x )` rather than ` log ( f ( x ))` - it just uses the fact that ` a exp ( b ) = exp ( a + log ( b ))` to avoid overflow inside the exponent .
Here I used indefinite integrals without constants , knowing the ` from ` ` to ` values it is clear how to use definite ones .
@USER thanks . one last question : ` erfcx ` is defined as ` exp ( x ** 2 ) * erfc ( x )` .
if i do ` erfcx ( -x )` , doesn't that give me ` exp ( -x ** 2 ) * erfc ( -x )` , when what i want is ` exp ( x ** 2 ) * erfc ( -x )` ? or , wait , i guess it gives me ` exp (( -x ) ** 2 ) * erf (( -x ))` , which * is * what i want .
Find max r-value **2 in python
And the max ` i ` with : #CODE
Numpy arrays : Efficient use of arrays containing indices
Three of these arrays contain integers ( which will be used as indices ) , the other array contains doubles .
Are the indexed points unique ( each a different point ) ?
Keep in mind that timetests on small arrays might not scale the same with big ones .
I don't think the ` reshape ` is needed .
Python DataFrame displaying zeros instead of calculated values from numpy zeros command
The calculations are all correct , but when I try to display the DataFrame once the loop is complete , some of the numbers calculated are displayed yet I also see mostly zeros .
How do I get the DataFrame to print the actual numbers , not the zeros ?
To compute some operation slice by slice in numpy , it is very often useful to reshape your array and use extra axes .
To explain the process we'll use here : you can reshape your array , take the mean , reshape it again and take the mean again .
I'd like to quickly transpose them into a series of 30000 arrays with 100 elements .
That is , an element-wise transpose of a Series of arrays into a new Series of arrays .
Label regions with unique combinations of values in two numpy arrays ?
I would like to re-label the array ` b ` by something similar to a GIS geometric union of the two arrays , such that cells with unique combination of values in array ` a ` and ` b ` are assigned new unique IDs :
I'm not concerned with the specific numbering of the regions in the output , so long as the values are all unique .
So far I've experimented with concatenating the array IDs to form unique combinations of values , but ideally I would like to output a simple set of new IDs in the form of 1 , 2 , 3 ..., etc .
If I understood the circumstances correctly , you are looking to have unique pairings from ` a ` and ` b ` .
So , ` 1 ` from ` a ` and ` 1 ` from ` b ` would have one unique tag in the output ; ` 1 ` from ` a ` and ` 3 ` from ` b ` would have another unique tag in the output .
Also looking at the desired output in the question , it seems that there is an additional conditional situation here that if ` b ` is zero , the output is to be zero as well irrespective of the unique pairings .
Make a list of all possible unique 2-tuples of values with one from a and the other from b in that order .
I need to take the ratio of two image numpyarrays , and unfortunately the data has a bunch of negative values and zeros scattered about .
` pixels = np.clip ( pixels , min , np.inf )`
There's more than one way to handle this , but the basic idea is that you convolve a simple filter such as ` [ -1 , 1 ]` or ` [ -1 , 0 , 1 ]` with your data .
` gradient ` will use a different algorithm at the edges to ensure that the array size is maintained : #CODE
However , the satellite data has the path of the sun facing absolute north , while the path of the sun I collected using my camera is not .
In the image above , the red dot in the middle is my query " point " , the blue dots are the vertices of each triangles as define in the " triangles " np.array() .
How to translate it in Python ?
I tried filling the ' a ' array with a gradient from 0 to 65535 , but what I got was not what I expected .
On a side note , ` gradient ` returns ` d_row , d_column ` .
" The gradient is computed using central differences in the interior
The returned gradient hence has
" The gradient is computed using second order accurate central differences in the interior and either first differences or second order accurate one-sides ( forward or backwards ) differences at the boundaries .
The returned gradient hence has the same shape as the input array .
The gradient function for 1.82 is here .
One way to do this efficiently is with something like the bisection method , which finds zeros .
I tried plugging in the max step size appropriate in the list , but got the following : #CODE
We get the byte array , compress it using lz4 , and then it is pretty easy reconstructing the array in other languages .
( 1 ) ` cdef float F ( np.ndarray xx , np.ndarray y_non_labor , float wage , float gamma , float one_by_gamma , float l , float one_minus_l , int lenx ): `
( 2 ) ` def F ( np.ndarray xx , np.ndarray y_non_labor , float wage , float gamma , float one_by_gamma , float l , float one_minus_l , int lenx ): `
( 3 ) ` cpdef float F ( np.ndarray xx , np.ndarray y_non_labor , float wage , float gamma , float one_by_gamma , float l , float one_minus_l , int lenx ): ` #CODE
which multiplies the points by the transpose of the rotation matrix .
` .T ` is to transpose an array
However , I want to convolve in one direction only
Ok , I googled it and found the following question on stack overflow : Trying to use open ( filename , w ) gives IOError : [ Errno 2 ] No such file or directory :
ValueError : operands could not be broadcast together with shapes ( 3 , 2 ) ( 2 , 2 )
I'm using a new Python package ( metpy ) , which contains several subpackages - one of which defines gradient and wind convergence with the following code : #CODE
vectorize numpy unique for subarrays
I want to get the number of unique values in each of the 20x20 sub-arrays .
If I try np.unique ( data ) I get the unique values for the whole data array not the individual 20x20 blocks , so that's not what I need .
An easy way to get the number of unique values for each row is to dump each row into a set and let it do the sorting : #CODE
A problem with ' vectorizing ' is that the set or list of unique values in each row will differ in length .
But how do you identify the unique values in each row without iterating ?
Counting the number of nonzero differences might just do the trick : #CODE
This is an iterative solution that is clearly faster than my ` len ( set ( i ))` version , and is competitive with the ` diff ... sort ` .
( It doesn't help in the ` diff ... sort ` case because it does not take an axis parameter ) .
:) ` np.sort ` takes about 3 / 4 of the time ; the ` diff ` part only 1 / 4 .
I tried the bincount version , and when I adapt it to my script it's actually slower ( ~11s ) that the sort / diff ( ~7s ) , I need to place the results in specific parts of a numpy array , I not sure if that take much time .
In any case I think the sort / diff version is probably as fast as it can get with pure python .
using the count_nonzero with the bitcount solution it went from ( ~11s ) to ( ~7s ) , similar time to the diff / sort solution .
Importing multiple AND specific Excel worksheets and workbooks to concatenate all
I think I can combine single-sheet Excel files ( with glob and append . ) , but when I try it with multi-sheet files the data gets messed up .
I'm guessing I have to use index numbers for the sheets because I need to choose which ones I'm importing , and the tab names are custom , so I don't want to manually type ' Sheet 1 ' etc .
The idea is to consider every unique categorical value as a feature ( i.e. a column ) and put 1 or 0 depending on whether a particular object ( i.e. row ) was assigned to this category .
min / max-lon / lat are the extreme occurrences of the latitude and longitude arrays and lon / lat_bins are equally spaced lists according to the gridsize .
numpy , return of array of indices in shape of
I want to get the result of a list ( or array ) of indices from a numpy array , in the shape : ( len ( indices ) , ( shape of one indexing operation ) ) .
Is there any way to use a list of indices directly , without using a for loop , like I used in the mininal example , shown below ?
It should be noted , that the indices list is not representative of my problem , it serves as an example , in general it is generated during runtime .
If the edited sample data is a representative one , then how about ` row_id = [ idx [ 0 ] for idx in indices ]` and then ` res = np.vsplit ( c [ row_id , : 4 ] , 4 )` ?
The calculation of ` row_id ` is using a for-loop , which I don't think you can avoid , as ` indices ` is a list .
This might be a solution , thanks . indices does not have to be a list , it could also be transformed or generated as an array of that form
I think if the indices is a numpy array or a list that is not a nested one , it * could * be vectorized .
The indices list is generated generically , it's size is not determined beforehand .
As well as it can contain duplicated indices , so i guess a split won't do the trick in my use case .
Any ideas on how the indices are generated ?
The indices that I generate this way are : #CODE
It works fine if ` indices ` is somewhat irregular : ` indices1 = [[ 0 , slice ( 0 , 3 )] , [ 1 , slice ( 2 , 5 )] , [ 1 , slice ( 1 , 4 )] , [ 2 , slice ( 0 , 3 )]]`
But often indexing on a flatten array is fastest , even if you take into account the calculation required to generate the index array .
I need to return the sin and cos values of every element in a large array .
Is there any faster way to return both sin and cos at once ?
* sin * 90-*x * = * cos x *
Basically you are asking : If I already calculated ` np.sin ( x )` , can I use this information to get ` cos ( x )` faster than evaluating ` np.cos ( x )` ?
The OP is obliquely referring to the fact that some math libraries ( and math hardware ) have a [ sincos ] ( #URL ) function that simultaneously returns both the sin & cos of a given argument .
you could use the tan ( x ) and retrieve cos ( x ) ans sin ( x ) using the common transformation function .
But if it does complex exponentiation you could use exp ( it ) = cos ( t ) + i.sin ( t )
@USER complex exponentiation is unfortunately slower than just using sin and cos
If your data are ( for ex . ) just an array of N elements between 0 and 360 , you could avoid to calculate sin and cos by mapping known values ( sorry for my english )
So with sincos ( x ) I obtain two dimensional array as result - sin ( x ) in the first dimension and cos ( x ) in the second .
You could take advantage by the fact that tan ( x ) contains both sin ( x ) and cos ( x ) function .
So you could use the tan ( x ) and retrieve cos ( x ) ans sin ( x ) using the common transformation function .
I've just timed this and it is about 25% faster than using sin and cos .
When I precompute ` sinx ` and compare the timing of ` cos ( x )` and ` cosfromsin ( x , sinx )` , ` cosfromsinx ` is slower .
The object I passed to cos and cosfrom sin was a 2-dimensional numpy array with dimensions of roughly 2000 * 1000
You can use complex numbers and the fact that e i = cos ( ) + i sin ( ) .
When I shrink a numpy array using the ` resize ` method ( i.e. the array gets smaller due to the ` resize `) , is it guaranteed that no copy is made ?
Unfortunately the documentation of resize says nothing about it .
Having that said , you could create a slice of the array effectively only using a subset of the array without having to resize / copy .
And another question : Why does ` resize ` have to make a copy when shrinking the array ?
Indeed , although it seems that numpy doesn't currently use realloc for the resize .
@USER The NumPy documentation for ` resize ` says that a * view * will be returned when possible , but in general it cannot be guaranteed that it won't produce a copy .
This means that it's certainly * possible * to resize or truncate without copying .
Without animation , this is what append for 20 consecutive filtered signals .
Fill columns of a matrix with sin / cos without for loop
I'm failing at coming up with something for this for loop , filling up columns of a matrix with sin / cos .
You can use tile and build out ` A ` with the values you use in your loop .
Then you can just use that matrix and try to do the cos and sin calculations all at once : #CODE
You could take even strides as a mask and use fill A with the cos .
Then take odd strides as mask and fill with sin .
A Hermitian matrix is one that that's equal to its complex conjugate transpose .
However , the ` fft ` of real input is " Hermite-symmetric " .
It's equal to its complex conjugate , but not its complex conjugate transpose .
On a side note , I may be getting the terms a bit confused , as the only time I've ever heard of a " hermite-symmetric " matrix is in the context of the fft of real values .
However , let's look at what happens when we take the fft of real values : #CODE
Notice that the resulting fft is ( almost ) identical to its complex conjugate ( There's a difference in the sign of one term that I don't understand . I'd appreciate an explanation , if anyone knows ! ): #CODE
But it's not Hermitian , as it's not equal to its complex conjugate transpose : #CODE
To get numbers from x , it is necessary to use ` key ` , while ` val ` defines absolute frequency ( count ) of a number .
The differences are happening because you're modifying something while iterating over it , but referring to indices of the original item when inserting .
Do I have to transpose the array ( it seems there are many functions for rows in pandas and numpy but relatively few for columns ( I could be wrong , of course ) to get average calculations for a column done ?
As an extension of my question : what is the best way to print Average value of Flavor_Score for all unique beers in the list instead of one chosen beer ?
I have created a list of unique beer names using pd.unique ( df.beer_name.ravel() ) as an array and then transferred array in the list with Beer_Name_L= Beer_Name_Arr.tolist() ( not sure if I needed to do that :)) .
The ` min ` of any array containing ` nan ` is also ` nan ` .
You could use ` np.nanmin ` , which ignores NaN values when computing the min , e.g. : #CODE
` np.where ( cond , B , np.nan )` returns an array of the same shape as the boolean array ` cond ` .
The returned array takes a value from ` B ` if the corresponding value in ` cond ` is True , and is ` np.nan ` otherwise .
Thanks , accepted !, Unfortunately ValueError : operands could not be broadcast together with shapes ( 18 , ) ( 19 , ) ( ) if A and C are NOT in same shape .
C has unique elements ,
I've tried using theano's nonzero : #CODE
Version 2 provides a workaround : compute the unique values outside Theano and pass them in .
To address your specific issue : there is no need to use ` nonzero ` ; in this case the indexing works in Theano just like it works in numpy .
I know I first tried simply using ` (( Y_hat == Y ) & ( Y == 1 )) .sum ( axis=0 )` and that the error had something to do with a boolean not having a ` sum ` method .
First flatten the list of lists with chain.from_iterable , then for each element run random.uniform ( 0 , 1 ) and if the result is less than .5 put it in the first list else put it in the second list .
where a permutation operation is used so that the partitioning is randomized , and the splitting is explicitly done through a numpy routine so that the code fits in one line ( although this may not be that important ... ) .
I will change the values from your example so that they have unique values : #CODE
You want to make some difference , ` diff ` in which you take each of the ` n*m*l ` vectors buried in ` p ` and subtract from it each of the ` N ` vectors in ` sv ` .
To broadcast , each size must match exactly , or must be empty ( on the left ) or ` 1 ` .
Now we can square it and sum over the second ( ` d `) dimension to get the norm / magnitude of each vector : #CODE
You can run this MWE and see that it handles functions of x fine ( e.g. , sin ( x ) , ln ( x ) , 3*x , etc . ) and throws exceptions for non-math functions like ' foo ' as desired .
The following code does exactly what I want , which is to compute the pairwise sum of squares of differences between elements of a vector ( length three in the example ) , of which I have a long series ( limited to five here ) .
1 ) the need to add a phantom dimension , changing the shape from ( 5 , 3 ) to ( 5 , 1 , 3 ) to avoid broadcast problems , and
Finding the positions of the max element in a numpy array if there are more than one
max = 5
You may use ` np.nonzero ` to find indices which are equal to maximum value : #CODE
Then use the returned indices to sort the original array : #CODE
Can you provide the full stack trace ?
You seem to stack three two dimension arrays of ( 2 , 3 ) to one two dimension array of ( 6 , 3 ) .
Or you could flatten the arrays , keeping all values #CODE
Or you could join all 3 into a 3d array #CODE
You can just flatten and rearrange the results .
You could flatten and assign the to a zeros , ones , or empty array of shape ( N , 3 ) or just use the vstack with a T #CODE
x1 and y1 correspond as ( x1 , y1 ) each other following indices such as ( 104.07794 , 40.9542 ) , ( 104.03169 , 40.96922 ) and so on .
In your case you probably need to ` flatten ` your 2d arrays .
The fact that ` x ` is 2d doesn't matter ; I could flatten the inputs .
But I can extract the diagonal , and get essentially the same values as ` Rbf ` ( except at the 2 ends ): #CODE
My question is , if that works for you , why would you like to translate it to another python library ?
Currently , you plot ` X1 [ 0 ]` against ` X1 [: 1 ]` , but ` X1 [: 1 ]` is the same as ` X1 [ 0 ]` , as you are saying " all indices in the first dimension up to 1 " ( i.e. 0 ) .
If i want to sum the entire array I can simply have : #CODE
If this isnt clear i would like to sum the bold x's in the below array :
To sum the array in your diagram , use : #CODE
Thanks , but I don't understand how the indices in your example ` [ 1:4 , 1:3 ]` are related to the ' coordinates ' I want to use .
` ex_list = [ ]` and then cycle through something and append to it ` ex_list.append ( some_lst )` .
Numpy arrays can be appended to ( see #URL ) , although in general calling the ` append ` function many times in a loop has a heavy performance cost - it is generally better to pre-allocate a large array and then fill it as necessary .
Well , querying an array ` b ` by the indices stored in another array ` a ` is straightforward : ` b [ a ]` .
Pandas dataframe Cartesian join
Create a surrogate key to do a cartesian join between them ...
I'm trying to join two series with ` pd.concat ([ a , b ] , axis=1 )` , but the result is a dataframe filled with ` NaN ` s , here's what I mean :
In order for this to work , I had to create unique values from the concatenation of the old indices on each Series .
What you're looking for is called a histogram .
If you have a pandas ` Series ` ( and you can just create one with ` s = pandas.series ( data2 )` , you can create a histogram by calling ` s.hist() ` .
It will create a histogram with equally-spaced bins over the range of your data ( the default number of bins is 10 , but you can adjust that by using the ` bins ` parameter ) .
If you're using ` pandas ` , as @USER mentioned , look at the ` hist ` function to plot the histogram ( similar to ` plt.hist `) .
However , the equivalent of ` numpy.histogram ` is ` pandas.cut ` , which is extremely handy when you want the histogram counts ( or want to group by a continuous range ) .
Two ways to join lists in dataframe : as rows and columns
Now I join them so that each list becomes a column of a data frame : #CODE
If , however , I join lists as rows of a data frame , then Python saves ` 66 ` : #CODE
You can create a Series for each of your lists and then concatenate them to create your data frame .
AttributeError : ' numpy.ndarray ' object has no attribute ' cos '
( The code won't even get through one loop for me because I'm using Python 3 and so the ` max ` fails -- in 2 it'll just give an answer that I doubt the OP intends . )
I ran it and it failed with ' TypeError : unorderable types : builtin_function_or_method() > builtin_function_or_method() ' on line 31 which is " epsilon = max ( epsilon_alpha.all , epsilon_L3.all )"
Python3 raises an error earlier , at the first encounter of the ` epsilon = max ( epsilon_alpha.all , epsilon_L3.all )` expression .
I have VTU files and a log file , that contains information for the VTUs I want to display .
Performance of 2 vector-matrix dot product
Each element of the matrix has to make dot product with the element on the same position of the other matriz .
However , the numpy functions can speed up the calculations as the following ones , making code much faster : #CODE
However , I would like to know if there are other sintaxis that improve this one above with maybe other functions , indices ,...
The more straightforward way to do what you posted in the question is to use ` np.sum ` , where you sum along the last axis ( ` -1 `) : #CODE
They all give the same answer but ` einsum ` is the fastest and ` sum ` is next : #CODE
You can save time in what you pasted in your comment by only calculating ` cos ` and ` sin ` once per array : ` cosazimA = np.cos ( azimA )` etc , since you're calculating the same thing twice .
How to write 2D std vector of floats to HDF5 file and then read it in python
Write a HDF5 file with the contents of a 2D std :: vector of doubles ,
You could also write ` dataset = file [ " data "] [: ]` as that will dump the HDF5 dataset into the variable ` dataset ` as a numpy array ( don't need to * cast * to a numpy array ) .
Do you need some sort of ` flush ` and ` close ` in the c++ ?
Apparently , i am not allowed to pass a std :: vector of vectors to the write function .
Below is a 3D example where I've evaluated f ( r ) = cos ( r ) ^2 on a grid , then interpolated the grid data .
AttributeError : ' numpy.int32 ' object has no attribute ' append '
' numpy.int32 ' object has no attribute ' append '
` ss=set ([ i for i in x if sum ([ 1 for ` a ` in x if a == i ]) 1 ])`
The problem I have much later on in the code is that if one of these parameters isn't in the ASCII file it throws errors up so I have to keep adding in ones I don't need .
It produced a very long log of errors and I copied the first part .
This is probably not the main problem , but in the function ` TWO_VOIGHT ` , you have the line ` sigma = sigma / gamma ` , but ` gamma ` is not defined .
Instead , if you're worried about the absolute degree of discretization , use ` numpy.spacing ( your_float )` .
Very small and very large numbers have the same " precision " , but that's not an absolute " error " .
I would like to take a vector from it and compute its outer product with itself .
And then mat=integration ( z ); print mat
Then I have created outer products from the 0 , 2 , 4-th vectors of the matrix and added them and made the diagonal 0 .
@USER then you either update your library or you can implement your own version of ` isclose() ` , which just checks whether one floating point is approximately equal to another floating point especially seeing that documentation explains how ` isclose ` is implemented : ` absolute ( a - b ) <= ( atol + rtol * absolute ( b ))`
@USER I highly doubt that there were any changes to ` eig ` function from 1.6.2 to 1.7 version .
You could reshape the result from ( 3 , 1 ) to ( 3 , ) .
I transpose , and then make a copy ( rearranging the underlying data buffer ) .
If you feed in that sliced 2D array ` A [: , 3 :] ` to ` np.in1d ` , it would flatten it to a 1D array and compare with ` B ` for occurrences and thus create a 1D mask , which could be reshaped and used for boolean indexing into that sliced array to set the ` TRUE ` elements to ` zeros ` .
For a case when you would like to set matching ones across entire ` A ` , you can use ` .ravel() ` - #CODE
why are you storing serialized objects in a dataframe ? in other words , why are you storing the pickle dump in a dataframe ?
@USER : these arrays have to be " linked " to a series of ids and some other features ( that are the other columns of a data frame ) and if possible in an indexed structure .
To change it , you can transpose the array ( i.e. ` imshow ( calendar.T , ... )`) and swap x & y elsewhere .
Otherwise you can find the index of the first nonzero like so : #CODE
Is there any way to correct that without having to reshape arrays all the time ?
` ones ` and ` zeros ` ?
@USER ` ones ` and ` zeros ` is from ` numpy ` library
` C ` and ` X ` have different shapes , and you sum row with number , geting a matrix with shape ( 5 , 5 ) #CODE
So all the time I get a column from an array , I have to reshape it ?
Maybe apply a ` max ` and ` min ` .
Maybe apply a ` max ` and ` min ` .
Numpy clip function takes twice as long as expected
I'm finding that the performance of the numpy clip function is significantly slower than just doing it myself with a mask ( 164us vs about 74us ) .
Is the clip function doing something additional that makes it take twice as long ?
This way , the time taken by ` clip ` is more similar to the alternative that you mentioned .
The list of indices returned for all rows need to be unique ( i.e. if two or more rows end up with the same column index then the row with the smaller distance to the given column index is given priority and all other rows must return a different column index ) .
assign unique column indices to rows
remove assigned column indices from each row
repeat step 2-4 until either all column indices are assigned
You actually have zeros in the valid portion of your array .
If you are completely new to python and the scipy stack , you may want to use the instructions on how to install with ` anaconda ` on that page .
Using numpy , say you have a 3D array called img filled with pixel values and you want to build an array filled with zeros everywhere but on a given color channel .
Depending on your use case , it might be acceptable to create a bigger ndarray containing all the additional zeros , and then slice it to get the " color " views you want .
@USER it contains all the NxMx3 values of the original array , concatenated with NxMx6 zeros
The main problem is that your fft call in the background thread is : #CODE
Yes , I realized this a shoft time I have a posted . numpy.fft defaults along the last axis , so I'm not doing the fft the same way .
Can I get the log scale values also .
How is it different for log scale .
X seems to have zero values , which results ` log ( 0 )= -inf ` and it can't plot it !
So , I shouldn't take ` log ( x )` right ?
If you have values at or below 0 , avoid using the log , yes .
If you have some data with either x=0 or y=0 you won't be able to print those points on a log-log-plot as log ( 0 ) is undefined .
for sorting , you could use a custom comparison key , e.g. ` max ( my_list , key=lambda x : x [ 0 ])` to access the first column . just adjust this to your data
In python the sum of 3 True's would be 3 .
The existing algorithms I have seen for this approach return indices or array masks , both of which will be a good solution .
Since I'm only looking for indices of points in voids , should I scale my axes to provide unit sd ... along individual axes or of the 3d point set ?
Consider k-means clustering on that house example : in our original data entry , floor space would dominate the clustering : a 1BR house with 2,000 sq ft would be very close to a 5BR house with 2,000 sq ft ; a 1BR house with 2,020 sq ft would be notably farther away from both .
If we scale the floor space -- divide by 1,000 -- then the 1BR houses would land very close together ... as they should , for most purposes .
Also note that this is easily modified to return the overlapping values , rather than the indices .
Is there a function in Python ( i.e. , in ` numpy ` , ` scipy ` , etc . ) that would be able to solve for the particular value ( s ) for ` x ` that produces a desired return value -- using , e.g. , gradient descent ?
I want to create a barchart ( not a histogram ) of a discrete variable that takes values { 1 , 2 , 3 , 4} .
However I get a plot similar to a histogram ( " bins " instead of values on X axis - 1 , 2 , 3 and 4 ) .
Could you just use the difference of the max / min last / first within each group ?
Using the difference of the max / min last / first of each group is not meaningful in this context as it only takes into account the first and the last packet seen .
@USER . thought about this slightly more and realised to use cummax() rather than rolling max .
The problem is that the time intervals may overlap , so I cannot simply sum the duration for each row .
` evtId ` is field created to store events unique id's .
Next , dilate the mask to join together islands of ` True ` s ( rainy days ) separated by 5 or fewer ` False ` s ( non-rainy days ): #CODE
Now that the ` True ` s represent " rainfall events " , we can assign a unique number to each rainfall event by using ` ndimage.label ` : #CODE
Problems on how to transpose single column data in python
Then I wrote the code below to transpose my data to a single-row text file .
I tried two different ways - using matrix multiplication ( the commented line in my code ) and using transpose command .
Is there anyway to transpose a single column file to a single row one ?
It is because the transpose of a 1D array is the same as itself , as there is no other dimension to transpose to .
You can use numpy.reshape to transpose data and change the shape of your array like the following : #CODE
Is there a faster way of extracting the submatrix if ` rowIndsINeed ` contains most of the original indices ?
It does not have any convenient structure , i.e. it's not block or diagonal etc .
Are you indexing with a big vector of zeros ?
It has returned a matrix with 80 copies of row 0 ( 80*6=480 nonzero elements ) .
I'm saying " almost " as you can't pass ` None ` , which would be ideal as a type-neutral value , but another unique scalar , such as a zero or a string ( e.g. `" INCORRECT !!! 1 ! 1 ! "` ;-) ) , is good enough for now .
Use ` transpose ` alongwith ` reshape ` - #CODE
Or use ` swapaxes ` that does the same job as ` transpose ` alongwith ` reshape ` - #CODE
Otherwise you will have to flatten the array first to get it into the correct shape .
Using the following example , how can I check if any match_array-shaped segment of ` field_array ` contains values that exactly correspond to the ones in ` match_array ` ?
" questions ask only for methods ; to that end , I have provided one solution , and I hope others will provide more and better ones .
To find the indices of where the top-left corner of the sub-array matches , you can write : #CODE
You can use ` transpose ` or ` swapaxes ` with some ` reshaping ` - #CODE
You can replace the ` ravel ` with ` reshape ` - #CODE
For me , cov is a numpy array of shape ( 2 , 2 ) , mu is the mean vector with shape ( 2 , ) and xtp is of shape ( ~50000 , 2 ) .
There are several ways of ' vectorizing ' a ` dot ` .
With ` ( 2 , 2 )` , the calculations one ` cov ` might be faster it done explicitely rather than with the ` det ` and ` inv ` functions .
Grouping column indices of an array based on max value of another list in python
Can anyone please give me any clue regarding grouping column indices of ` array k ` based on max value of ` list m ` as shown above .
By saying i would like to keep the index number common i meant that all I want to keep the index number of greater element in the list M common and use that index , to group with non max index as shown above .
Log log plot linear regression
It should use the log values of course .
You need to sort the arrays first before you plot it , and use the ' log ' instead of the symlog to get rid of the whitespace in the plot .
Read this answer to look at the differences between log and symlog .
Since you have data with x=0 in it you can't just fit a line to ` log ( y ) = k*log ( x ) + a ` because ` log ( 0 )` is undefined .
Note : Using ` ax.set_xscale ( ' log ')` hides the points with x=0 on the plot , but those points do contribute to the fit .
Before taking the log of your data , you should note that there are zeros in your first array .
How can I increase the memory buffer or stack for python ?!?
It may look primitive but it involves obtaining a subarray copy of the given array , then prepare two more copies of the same to append in left and right direction in addition to the scalar add .
I always see these kinds of comments but I am not sure if people are talking absolute time or relative time .
Of course , absolute times are hard to compare , but on my computer we are talking milliseconds .
I guess it is a sum of piecewise defined cubic polynomials .
In that case your primitive would be a sum of piecewise definied quadratic polynomials .
I have 2D image data that I'd like to represent as a plane in 3D , and perform various manipulations ( translate , rotate , magnify ) .
I'm sure there are libraries that will do most of the heavy lifting ( np.linalg ? ) but I just don't know which ones are where I should start .
Include this when using ` indices ` or ` meshgrid ` , and if you use ` einsum ` or ` tensordot ` .
I use a conditional list comprehension to join by lists .
Only when using numexpr it is possible for me to avoid creating array from dot .
How to append an 1-D numpy array to a multi-dimensional numpy array
I want to append the second array to the first one .
However , if I append a 5*10 dimensional array to left_padding , it does not seem to be a problem .
To make this problem simple , let's consider df2 contains only unique keys .
Then weight and sum your sets : #CODE
You can use the indices back into the sparse matrix to recover which sets these were : #CODE
The `' letter.png '` image is a 10x10 image so it's perfect safe to resize and numpy successfully resizes the image to a 1-dimensional array of shape ( 1 , 100 ) .
I am getting an error TypeError : list indices must be integers , not tuple
#URL > x = rand ( 5000 , 5000 );
#URL > y = rand ( 5000 , 5000 );
With python , it uses a unique core ( default behaviour ) and it took more than five minutes ...
YUV-to-RGB decode ---> Signal ( Image buffer ) ---> updateFrame ---> QPainter.drawImage ( ... )
Read n items ( as machine values ) from the file object f and append them to the end of the array .
But I need a fast append operation .
Later , I also need to slice the data between two indices for processing without loading the entire dataset into memory .
I was hoping that atleast the transpose would have worked .
If you want the second argument to be treated as a ` ( 1 , 2 )` matrix , you need to reshape it : #CODE
But I do not see any advantages in using ` matrix ` here ( or anywhere else for that matter , now that Python 3.5 has the dot operator ) .
Or as @USER mentioned in comment as a more elegant way use a costume shape within ` zeros ` function : #CODE
If all you need is a list of tuples , you should stick with plain python , numpy won't speed things up because you need to translate back and forth between numpy arrays and lists / tuples .
I wish ` np.full ( x , ( 0 , ) *L , dtype=tuple )` would work but numpy WANTS to broadcast the second parameter even though it actually corresponds to one entry .
@USER I know , I saw that in the stack trace .
Also I strongly suggest you access your columns using subscript operator ` [ ]` rather than as an attribute using dot operator ` .
Maybe you want ` flatten ` : #CODE
AttributeError : ' module ' object has no attribute ' join '`
If there is at least one column of ones then the
And you don't need to roll it because the satellite axis is already the first axis : try ` obs_data_chunks [ 0 ]` and you'll have your first satellite .
or , if you roll the axis for ` sats ` , you can just do : #CODE
I appreciate the clues you are giving , I've been reading up on floating point all day , built an excel spreadsheet to calculate floating point single precision , and IMO 1.0000001 is not too precise for this format , which also is supported by the link I have in my first comment that converts.32 bits can encode that number .
If Python's implementation is not giving you all you need , you can roll your own .
Alternatively , you can set the elements in one-go after you have initialized the output array with ` zeros ` and set the first element in each slice as ` 1 ` , like so - #CODE
Between the above mentioned two approaches , two more middleground approaches could evolve , again put right after initializing with zeros and setting the first element in each 3D slice as ` 1 ` , like so - #CODE
It should discard the options with too much weight , while keeping track of the ones with the highest value and acceptable weight .
The problem in itself is an attempt to bootstrap : get random samples ( equal size ) from the initial distribution and measure the mean and std .
And if you want to use the standard ` random.sample ` why not do ` rand = random.sample ( simulateData , 30 )`
` random.sample ( population , k )` - Return a k length list of unique elements chosen from the population sequence .
But only a subset of array operations work on nested arrays , and ones that do work might not be any faster than list comprehensions .
In my edit I came up with a similar idea with " append " function , but yours seems to be simpler .
Even if you figure out which ones are pure python+numpy , and manage to ensure that the operations performed in the computation preserve the data type , you'll find that many of the functions use hardcoded 64 bit constants such as ` numpy.pi ` .
I have looked through stack at some similar problems but cant seem to get a fix to translate to my problem .
for example ` max ( ' high0 ' , ' haOpen0 ' , ' haClose0 ')` return `' high0 '` a ` str ` and ` haClose0 ` is a ` numpy.ndarray `
you can try with ` max ( high0 , haOpen0 , haClose0 )` instead of ` max ( ' high0 ' , ' haOpen0 ' , ' haClose0 ')`
thanks for the feedback , I changed to max ( max ( high0 ) , max ( haOpen0 ) , max ( haClose0 )) which worked ...
haHigh1 = max ( max ( high1 ) , max ( haOpen1 ) , max ( haClose1 ))
thanks for the feedback , I changed to max ( max ( high0 ) , max ( haOpen0 ) , max ( haClose0 )) which worked ...
haHigh1 = max ( max ( high1 ) , max ( haOpen1 ) , max ( haClose1 ))
haHigh1 = max ( max ( high1 ) , max ( haOpen1 ) , max ( haClose1 ))
Also , just because someone saw the error in your first problem , doesn't mean they're the best ones to keep helping subsequent problems .
This hashtable-based approach is reasonably quick - still much slower than the super-fast ` bincount ` method Mark Dickinson suggests , but significantly quicker than ` unique ` and quicker again than ` Counter ` ( on my machine ) .
Bear in mind that the result of ` np.bincount ` has size ` max ( array_ ) + 1 ` , so if your array has large values this approach is inefficient : you end up creating a very large intermediate result .
I would like to create a new array , ` B ` of size ` nx10 ` such that in ` B [ i ]` we store a numpy array that contains zeros and a 1 in position ` A [ i ]` .
This is a list-based approach ; you can simply use np.asarray on B to get the numpy matrix , or create a 10x10 matrix of zeros in numpy and fill 1s in the positions dictated by the A array .
What is an efficient way to find the indices of that nearest grid points and interpolate it's value ?
Those interpolators create a search tree with your ' range ' values , which can then can called repeatedly with the ' rand ' values .
why not just create your desired array with slicing from the original ones ?
I want to take the absolute value ( eliminating negative values ) and have the output width be 1 column shorter than the input ( since no calculation can be performed on the last column ) .
That is a good answer but the resulting list has the same size as the previous , ideally I'd like a list with the unique dates .
Load your csv to a dataframe , then run a group by the month and find the max date using the aggregate function : #CODE
Given your expected output , it sounds like you're asking for an " outer product " -like function rather than a " cross product " -like function ( do correct me if I've misunderstood ) .
Most ufuncs have an ` outer ` method which computes the result of the operation on all pairs of values from two arrays ( note this is different to the cross product ) .
I am tying to calculate mean absolute error , but when I execute the following print statement #CODE
When I was doing the max ( z ) before , the header was coming up as the max ...
Which I already know is wrong since the max is 1576 .
But moving on .... after the append I get #CODE
When I run the code block starting with " id_max = max ( z )" it says ...
So for now , I just changed the max ( z ) to a number that I know is the max ( 1567 ) .
Then I ran the last block and got a similar error : " TypeError : list indices must be integers , not str " .
The only indices in my code are " index " , which is an integer from a " range " generator ; and " id " , which is pulled from a list of integers .
Otherwise , you could just use /// id_max = int ( max ( z ))
I updated your code to give the day delta from the first instance of the unique ID , then to reset after a success .
Python : Cumulative insertion of values in a sparse matrix ( lil_matrix ) due to repeated indices
` S = np.array ([ 2 , 3 , 10 , -1 , 12 , 1 , 2 , 4 , 4 ])` , which I would like to insert in the last row of a ` scipy.sparse.lil ` _matrix ` M ` according to an array of column indices with possibly repeated elements ( with no specific pattern ) , e.g. :
When column indices are repeated , the sum of their corresponding values in ` S ` should be inserted in the matrix ` M ` .
You could use a defaultdict that maps the M column indices to their value and use the ` map ` function to update this defaultdict , like so : #CODE
But when converted to ` csr ` format , it sums repeated indices : #CODE
With a small test case like this , the overhead of creating an intermediate matrix can swamp any time spent adding these duplicate indices .
I couldn't see a good explanation on the internet , which explains how to append a new row of data to an existing .mat file in python .
I have checked this link ; How to append in .mat file row or columnwise cellmatrix , but it didn't help .
So it might be possible to open the file in append mode ( flag ' a ') , and pass it to ` savemat ` .
Before trying this issue , I was creating a numpy array , and then saving that as a mat file .
That's why , I need to append value to the end of the mat file , and delete the appended value from the cache .
An earlier SO on appending to a .mat : #URL With some constraints I was able ( in March ) to append a new variable to an existing .mat .
I did transpose the X_scaled .
You should probably just transpose your data , using ` numpy.transpose() ` or ` yourArray.T ` . scikit expects an array of shape ` ( n_samples , n_features )` , where ` n_samples ` is your number of observations and ` n_features ` is the dimension of the space they live in .
using the transpose will throw an error :
Load and concatenate numpy arrays
I have several numpy arrays , and I want to read them and concatenate them all together :
If we have a function that indexes the structure and another function that indexes any nested structures we can compose them together by ` fmap ` ing the second function over the structure then applying the outer function .
Masking is a convenient way of handling Nan like values that you don't want to propagate through operations like ` sum ` and ` mean ` .
Everything works fine as long as the number of process = max number of worker processes : ` Pool ( max_workers )` .
How do I make it append the normal format number to say another list ?
` array1 ` is the numpy array that I want to join with two python lists
How do I join these lists and array so as to form one list which does not have above issue ?
I don't want the data copied because I want to use PIL ImageDraw functions to manipulate the mapped buffer , then flush the changes .
In order to convolve a kernel with an image , I am using : #CODE
have you even tried to use ` max ` and ` min ` ?
The min and max of a column i can be simply found with : #CODE
the result b is a ( 5 , 3 ) array full of zeros .
` numpy.sum ( x0*w0 )` is actually just a number - the dot product between the two signals , corresponding to the cross-correlation evaluated at 0 .
Find indices of common values in two arrays
To find the indices of the elements in A that are present in B , I can do #CODE
I also want to get the indices of the elements in B that are present in A , i.e. the indices in B of the same overlapping elements I found using the above code .
Do those arrays have unique elements ?
If there is a better solution for unique elements , I may be able to tweak things so they are unique .
If the two input arrays are already ` sorted ` and ` unique ` , the performance boost would be substantial .
The format is important because the integer fields are used as indices for other numpy arrays , that is my main concern .
Your code example does [ histogram equalisation ] ( #URL ) rather than histogram matching - essentially it flattens the histogram of pixel values within a single image , which is sometimes useful to enhance contrast .
I previously wrote an answer here explaining how to do piecewise linear interpolation on an image histogram in order to enforce particular ratios of highlights / midtones / shadows .
The same basic principles underlie histogram matching between two images .
Essentially you compute the cumulative histograms for your source and template images , then interpolate linearly to find the unique pixel values in the template image that most closely match the quantiles of the unique pixel values in the source image : #CODE
What's probably happening within the " solid " areas in your source image is that a small amount of random variation is being amplified in order to " stretch out " the histogram to match that of the template .
I'm also not sure if it will create multiple DFs or if I can append the groupbyobjs .
concatenate numpy string array along an axis ?
Is there a way to concatenate the strings in each row and then join the resulting strings with a separator string , e.g. a newline ?
The usual use of ` join ` puts the separator between strings , but not at the end .
If you really need the ` \n ` at the end , you can concatenate it after joining th strings .
Finding sets of vectors that sum to zero
I need to find how many combinations of vectors sum to zero .
You can use ` itertools.product ` and a generator expression within ` sum ` function : #CODE
the if sum ( i ) == 0 part is incorrect
[ 1 , -1 ] has sum 0 but isn't == [ 0 , 0 ]
Perhaps you mean from itertools import product ; len ([ i for i in product ( set1 , set2 , set3 ) if sum ( i )= =0 ]) to get the number of combinations that sum to 0 .
@USER There is no need to use a list to store the result and wasted the memory when we can use a generator expression within sum !
Right , sum ( 1 for i in product ( set1 , set2 , set3 ) if sum ( i )= =0 ) works too .
An algorithm achieving much better time complexity ( O [ n^2 log N ]) is sketched out here : #URL .
You'll need to reshape your initial sets into 1-D , but that's no loss for this purpose .
` Trying to strip b ' ' from my Numpy array `
Looks like my solutions are to either ` decode ` the byte field , or to write to a byte file directly .
Since your array has a mix of string and numeric fields , the ` decode ` solution is a bit more tedious .
A helper function can be used to ` decode ` byte strings on the fly : #CODE
Another numpy byte string formatting question : #URL Other than a custom ' format ' method it stil recommends ` decode ` .
For example , I may want to find the max value , excluding some special value of negibors at indexes at ( x-1 , y ) ( x+1 , y+1 ) at each index , and put the result into another different 2d " solution " matrix .
Now , to perform an numpy operation ( eg the max of the sub-matrix ) on each sub-matrix and store the result in a 2D matrix : #CODE
Cool that's interesting and covers the first couple steps , how do you perform the max excluding specific values and load it into a result matrix ?
For example here Id like to select all " unique " entries 0 , 1 , 4 , 5 , 8 , 9 etc .
You could reshape the array to add an extra dimension of length 2 : #CODE
and finally reshape it back to a 2D array : #CODE
Hadnt expect the reshape to be such a useful tool :-) Thank you .
Why not ` sums = [ sum ( arr ) for arr in [ a , b ]]` ?
@USER ' s comment to your post is an elegant way as well to define your magic function ( although a case could be made to use the numpy method ` sum ` rather than the python builtin ) .
It was somewhat tricky to implement @USER ' s , but I think I got it ( and it's the fastest if you use ` concatenate ` instead of ` hstack `)
I have a test matrix ( z ) of shape 40x40 , filled with zeros .
Given an array X of shape ( 100,819 2 ) , I want to copy the subarrays of length 8192 for each of the 100 outer dimensions 10 times , so that the resulting array has shape ( 100,819 2 , 10 ) .
I'm kind of confused about how the tile function works , I can sort of only copy a 1d array ( although probably not really elegantly ) , e.g. if I'm given a 1d array of shape ( 8192 , ) , I can create a 2d array by copying the 1d array like this : ` np.tile ( x , ( 10 , 1 )) .transpose() ` , but once I try to do this on a 2d array , I have no idea what the tile function is actually doing when you provide a tuple of values , the documentation is kind of unclear about that .
If x = np.tile ( x , 10 ) .reshape (( 100,10,819 2 )) , then I would want for each x [ i ] to be the transpose of the corresponding subarray .
First we find the array of indices that sorts the input func_indices , which we use to define the length-k func_ranges array of integers .
Box coordinates numpy sum
I'm just wondering how to do a sum within a specific area of a 3D map and save it to a fits file .
I want to sum everything within that box .
If you want to sum over all values of ` coords ` , just do ` np.sum ( coords )` , without specifying the ` axis ` .
Where is the " map " you want to sum over in your example ?
` np.delete ` is not doing anything unique or special .
So ` delete ` with ` s_ ` removes a range of values , namely ` 6 8 10 ` , the 3rd through 5th ones .
` numpy.meshgrid ` is obviously the clearest way to me ( as @USER has mentioned ) , you need one more step to ` ravel ` or ` flatten ` the 2D grid array : #CODE
Or as @USER mentioned , shorten your code with ` np.c_ ` to skip the transpose : #CODE
You can also use np.c_ [ xx.ravel() , yy.ravel() ] and skip the transpose .
v ` is the dot product of ` u ` and ` v ` .
To verify , look at the mininum distances and not just the min arguments from these two approaches .
Cost is calculating ( b-a *theta ) ^T * ( b - a*theta ) , essentially the norm squared of a 30x1 matrix .
Do I have to read first a vector and afterwards reshape it ?
With ` numpy.fromfile ` the reshape must happen
explicitly afterwards , but ` numpy.memmap ` provides a way to reshape
` order ` keyword argument for ` reshape ` and ` memmap ` ( among others ) .
I'm trying to stretch an image's histogram using a logarithmic transformation .
Basically , I am applying a ` log ` operation to each pixel's intensity .
When I'm trying to change image's value in each pixel , the new values are not saved but the histogram looks OK .
This is also the histogram I get :
The histogram that you have shown us illustrates that all of the image pixels are in the dark range ... roughly between ` [ 0-7 ]` .
Now , using the above image , if you actually show what the histogram count looks like as well as what the intensities are per bin in the histogram , this is what we get for ` img2 ` : #CODE
I also get this histogram :
As such , I would recommend that you choose a more meaningful image transformation if you want to stretch the histogram .
In fact , the ` log ` operation compresses the dynamic range of the histogram .
If you want to stretch the histogram , go the opposite way and try a power-law operation .
In your case , you want to expand the histogram , and so you want the first option .
I also get this histogram :
One approach would be to stack those in columns with ` np.column_stack ` and reshape with ` np.reshape ` - #CODE
Concatenating with ` np.concatenate ` is known to be much faster , so using it with ` 2D transpose ` and reshaping - #CODE
Another with ` np.concatenate ` , ` 3D transpose ` and reshaping - #CODE
You may get somewhere using ` np.diff ` and ` np.where ` : diff can find you the points where a sequence changes .
The volume is the sum of the actual values between these indices .
Using pandas Exponentially-weighted moving std as a sliding window with * fixed * window size
Using ` std :: vector ` you always end up copying data , it is not possible to create a ` std :: vector ` using a pointer without copying .
I usually use ` ArrayRef ` from ` llvm ` or make a wrapper for std :: vector and use the conventional methods from ` numpy.i ` for the swigging .
2 ) The simplest method is to use the first method on just the 4 adjacent squares , but you'll have less resolution in diagonal features .
The documentation for ` numpy.gradient ` is a bit dodgy on this aspect , but the right way of calling it is : ` gradient ( vwind , dx , dy )` .
I.e. the function signature is ` gradient ( f , * varargs , ** kwargs )` , where ` varargs ` is a list that is expanded by the " splat " or " unpack " operator .
What about non-overlapping ones ?
I want to create a mask for those bad values , but since I will be using rolling window functions , I'd also like a fixed number of subsequent indices after each bad value to be marked as bad .
I'm curious to see the timings , since " do something to a window around some indices " seems like a reasonably common problem .
They need to both increment the indices every iteration and then use them to index the mask and set corresponding values to ` True ` .
Also you're right , I meant " fancy " indexing ( by array of indices ) , which I believe is still much slower than slicing , even if it's not as slow as a boolean array .
I have included my own ( " op ") which I had started out with , which loops over the bad indices and adds 1 ...
n to them then takes the uniques to find the mask indices .
The chart does not do justice to the fastest of these contributions because of the ( necessary ) log scale .
And some of the operations would not support sparse indices .
I saved the image ( md5 sum is ` 0d5e84238299eb996a0f58716354a7e6 `) , loaded it with ` scipy.misc.imread ` , and displayed it with ` imshow ( img )` , and I got the image that you say you should be getting .
why not test lengths of v & w , slice the longer to the shorter then append the remainder of the longer to the resulting multiplication ?
I'm applying this to a DF where dt index isn't necessarily unique anymore , ie . as well as { ' count ' : 0 } there will be { ' Location ' : ' Japan ' } , if I wanted to count the accumulated value between 2 dates ranges ( as per above ) by location , ( Location -> Date -> Accumulated Count ) Do you know how to achieve that ?
Pylab stack comes with relatively complex dependencies so simple copy of an environment may be not sufficient .
I want to change the last value ( column ) of all lines of DATA if those ones are similar to a same shaped external line ( called ExtLine ) .
Linux : ` max locked memory ( kbytes , -l ) 64 `
Mac : ` max locked memory ( kbytes , -l ) unlimited `
scipy and preserving mat file ( .mat matlab data file ) structure
This is essential to preserve the existing mat file structure .
So the trick to creating these nested object arrays is to create the outer object array first : #CODE
If you are looking for matching nonzero XY indices pairs , you can use boolean ANDing between the nonzero masks of the input arrays and then use ` np.nonzero ` , like so - #CODE
You can verify these results with ` np.intersect1d ` after getting the linear indices of matches from ` img1 ` and ` img2 ` giving us a second approach to solve the problem at hand , like so - #CODE
If you can find a concise example of repeated values but different indices ( and probabilities ) that would be great .
random element indices : ` randices `
I also have the nominal ( central ) wavelengths for each wavelength bin ( technically , they're the log of the wavelength , but that shouldn't matter ) , called ` logL_ssp ` , and the desired new spacing of the logL grid , ` dlogL_new ` .
I have come up with faster options , inspired by some answers here on stack overflow .
Based on a tip from @USER , conversion from a 1D array of ones and zeros to a gmpy2 mpz object can be done reasonably efficiently with gmpy2.pack ( list ( reversed ( list ( array ))) , 1 ) .
Let's fill them up with zeros using the numpy library : #CODE
If by add you mean append , try : ` np.append ( a , b , axis = 1 )` .
It's easier to just use ` sum ` than to figure out a way to convert the ` -1 ` s to ` None ` s .
IndexError : too many indices for array while doing permutation
I am trying to do permutation and binarization of a given data , currently my input data ' x ' is 3*3 array .
This is my fist post on stack overflow , so please bear with me .
* advanced ( or " fancy ") indexing is triggered with a boolean array or an array of indices .
Suppose my image is a all ones matrix .
The argument to ` exp ` should be imaginary .
I / python ( 2143 8) : ImportError : dlopen failed : cannot locate symbol " sqrt " referenced by " lapack_lite.so " ...
You can also have a look at the unique id of the object .
How to partially flatten a cube ( or higger dimensional ndarray ) in numpy ?
I'm trying to flatten a 3d array in numpy over an axis ( that is , reducing over an axis and flattening over another )
@USER Chaudhary Nice ... but can you elaborate on the rule for those of us who are learning the association between transpose and reshape .
in an attempt to match the behavior of ` zeros ` and ` ones ` , but I get the following error : #CODE
If , for any given row , you will append at most one extra row , you could do the following steps :
2 ) append an uninitialized dataframe to the end of your original dataframe , with the same length
4 ) Zero all odd indices
Write the matrix as binary , compress the file .
Depending on what you're trying to do , you could even use a non-binary format like json and just compress it on your own .
I have designed and written binary file formats ( some in Cython ) that can be written / read from Python , including ones compressed by Python zip , I but haven't been able to come close to the speed of ` .npz ` or ` .npy ` files ( even my own version of a faster min-Pandas written partially in Cython ) .
Apply same permutation for every row in a 2D numpy array
I have a 2D array and want to apply exactly the same permutation for every row of the array .
each row should have the same set of elements as before applying the permutation , however the way you arrange the elements in every row should be the same .
your method seems to be changing the values of the rows , for example in the first row before the permutation we do not have a 2 , but after that we have a 2 .
I would like to have some kind of masking that for instance , if an element of the first row that was previously in position 0 after the permutation goes to position 4 , then the same should happen for the element that is in position 0 of the second row and all the other rows .
If you want to permute all values ( regardless of row and column ) , reshape your array to 1d , permute , reshape back to 2d .
If you want to permutate each row but not shuffle the elements among the different columns you need to loop trough the one axis and call permutation .
For labeled images in which the number of labels is on the order of ` sqrt ( size ( image ))` is there an algorithm to gather label coordinates that is faster than iterating through every image element ( i.e. with ` nditer `) ?
However , do you actually need the indices ?
Per Warren's suggestion , I do not need to use unique and can just do #CODE
Because ` img ` was computed using ` scipy.ndimage.measurements.label() ` , it is not necessary to call ` np.unique ( img )` to get its unique values .
I suspected the performance would depend strongly on the density of the labels and on the number of unique labels .
How can I plot the Fourier Series equation below , using a for loop where I can change n , instead of writing out the ` cos ( x ) + cos ( 2x ) + cos ( 3x )` , etc .?
Basically you just have a list comprehension which does the sum for you :
` sum ( np.cos ( i * x ) for i in xrange ( 1 , n ))`
The transform is simple , it is just ` data * pca.components_ ` , i.e. simple dot product .
Firstly , ` * ` is not dot product for numpy array .
To perform dot product , you need to use ` np.dot ` .
Secondly , the shape of ` PCA.components_ ` is ( n_components , n_features ) while the shape of data to transform is ( n_samples , n_features ) , so you need to transpose ` PCA.components_ ` to perform dot product .
Efficient way to create an array of nonzero values in a numpy tensor ?
I want to zero them out , then create a temporary array of all nonzero elements in ` A ` .
I noticed that the error term eij = R [ i ] [ j ] - numpy.dot ( P [ i , :] , Q [: , j ]) is not an absolute value ,
Other methods that returns an absolute value ( e.g. , math.abs() , etc . ) don't work either .
Next up , ` reshape ` ` B ` to a 4D array as well by creating singleton dimension ( dimension with no elements ) at the second dimension with ` B [: , None , : , :] ` .
histogram giving wrong bins for zero array
This should produce histogram with all values for bin with 0 but the output is :
Since you have only ` zeros ` in the input , it takes ` -0 , 5 as minimum ` and ` +0 , 5 as maximum ` , resulting in the histogram you showed above : no ' counts ' between ` -0 , 5 to 0 ` and all ` 784 zeros ` between ` 0 and +0 , 5 ` .
All the 784 zeros are distributed to the second bin , and the fist bin is empty .
You can just pass the heights of the bins to it and it will will do nothing to them and just draw a histogram .
Transfer matrix elements to another matrix's diagonal
How to convert a column or row matrix to a diagonal matrix in Python ?
1 ) set all elements of matrix A onto the diagonal of matrix B ( all other elements of B should be 0 ) and 2 ) after performing some operation on B , I want to recreate matrix A , so take the elements off B's diagonal , in the same order as was performed in the first step , and put them back in A .
Can you not do just unravel your matrix onto the diagonal of another ?
But there's nothing wrong with ` reshape ` other than the fact that it requires typing .
With ` reshape ` I had to access very often different instance attributes ( e.g. the layer sizes ) .
Trying to get to reduceByKey ( lambda x , y : x [ 0 ] +y [ 0 ]) , to sum values by key , but that statement throws the same exception as x [ 0 ] .
is there any way in numpy to get the indices that each item in my matrix / array sorts into ?
The question is about how to get the indices .
Return the indices of the bins to which each value in input array belongs .
In this case , the ` view ` method gives a flattened result , so we have to reshape it into a 2-d array using the ` reshape ` method .
Similarly , a Laplace mask sensitive to diagonal features has 8 in the center of the kernel ( right side in the figure bellow ) .
A simple check would be to declare a 2D array of zeroes except for one coefficient in the centre which is set to 1 , then apply the ` laplace ` function to it .
If you do this , then you'll see what it looks like after you run through the ` laplace ` method : #CODE
so the link I gave you above that links to the Github repo where ` laplace ` is defined is a start .
Is there a way to integrate the entire function at once by expanding the sum such that : #CODE
Cannot you interchange sum and integration ?
Define one big integrand as a sum of all your integrands .
` logsf ( x )` is the natural log of ` sf ( x )` .
I would like to sum ONLY the negative values that are continuous , i.e. only sum ( -1 , -6 , -6 ) , sum ( -5 , -2 , -1 , -4 ) and so on .
In this case sum ( -1 , -6 , -6 , -5 , -5 , -2 , -1 , -4 )
You can use ` itertools ` module , here with using ` groupby ` you can grouping your items based on those sign then check if it meet the condition in ` key ` function so it is contains negative numbers then yield the sum else yield it and at last you can use ` chain.from_iterable ` function to chain the result : #CODE
Also , do you know a way to know the sum of those negative values but knowing how many numbers ( i.e. the shape of those chunks ) were added ?
sum of negatives -25 number of summed 7
whoops ... for some reason I had 2 being stuck in my head as his target max :P
Can anyone one show me the way to compute dot product or matrix manipulation parallelly on spark cluster ?
I am now having multiple vectors ( TypeA ) in hands and trying to compute their dot products with anthoer single vector ( Type B ) .
To speed up the progress , I'd like to implement this function with python3.4 on a spark cluster in order to deploy dot product computation of each TypeA and TypeB on different nodes .
Numpy is doing the dot product in the driver .
NumPy Indexing - all pairwise elements of lists of indices
However when using lists / tuples of indices this behavior does not seem to be followed : #CODE
It is instead gets just the diagonal ( in this case ) .
This is problematic if you cannot specify indices as slices , for example ` ( 1 , 3 , 4 )` and ` ( 1 , 3 , 6 )` .
You are right , I originally have ` order= ' F '` in the reshape command producing the transpose .
Ah , so if the indices are column or row vectors this works .
I used the second one because the indices were already column vectors , so it looked like ` im [ ix , iy.T ]` .
Advanced indexes always are broadcast and iterated as one : #CODE
Note that the result shape is identical to the ( broadcast ) indexing array shapes ind_1 , ..., ind_N .
TypeError : tuple indices must be integers , not tuple
I was wondering how to do this if you want points on log scale from 0 to 1 ( including 0 ) ?
Limit sum of entries in numpy array by adjusting negative entries
I have a numpy array containing positive and negative values , and I want to adjust the negative entries so that the sum is not negative , starting with the most negative entry .
The starting array is [ 50 , -200 , - 180,110 ] , the answer in this case is [ 50 , 0 , -160 , 110 ] , so the most negative entry is set to zero , and then the next most negative entry is adjusted to make the sum zero .
For a case when you would have multiple elements with the same ` min ` value , you might want to have a list of tuples .
python , dimension subset of ndimage using indices stored in another image
I'd like to reduce the z-dim of img_a from 100 to 1 , grabbing just the value coincide with the indices stored in img_b , pixel by pixel , as indices vary throughout the image .
What do you mean by " coincide with the indices stored in img_b " ?
Does img_b contain indices ?
thanks for your comment , Evert . yes , img_b contains indices .
The code below has the method I added that explicitly creates the indices for look up purposes with ` numpy.indices() ` and then does the loop logic but in a vectorized way .
great job , KobeJohn . not familiar with mashgrids either , looks interesting , however unequal row and column sizes are common among my datasets , so I leave it with the indices method . deeply grateful , peter .
I am a absolute newbie for python .
Python - plotting histogram bestfit lines using numpy and matplotlib
` sqr ` , ` exp ` etc . are functions from ` numpy ` , so prefix them with ` np .
NumPy sum along disjoint indices
I have an application where I need to sum across arbitrary groups of indices in a 3D NumPy array .
The built-in NumPy array sum routine sums up all indices along one of the dimensions of an ndarray .
Instead , I need to sum up ranges of indices along one of the dimensions in my array and return a new array .
I wish to sum up the first dimension along certain index ranges and return a new 3D array .
Consider the sum from ` 0:25 , 25:50 ` and ` 50:75 ` which would return an array of shape ` ( 3 , 25 , 3 )` .
You can use ` np.split ` to split your array then use ` np.sum ` to sum your items along the second axis : #CODE
If we wanted the ranges different lengths along an axis , for example those given in ` np.split ( a , [ 10 , 15 , 30 ])` , we can't sum these in the way this answer shows .
@USER The indices should be ` 10 , 20 , ` .
Just sum each portion and use the results to create a new array .
To sum the slices ` 0:25 ` , ` 25:50 ` and ` 50:75 ` along axis 0 , pass in indices ` [ 0 , 25 , 50 ]` : #CODE
This method can also be used to sum non-contiguous ranges .
For instance , to sum the slices ` 0:25 ` , ` 37:47 ` and ` 51:75 ` , write : #CODE
An alternative approach to summing ranges of the same length is to reshape the array and then sum along an axis .
You don't need the repeated indices if the ranges are contiguous , i.e. ` [ 0 , 25 , 50 ]` will do the same job in this case .
In your code there is an extra source of slowness : each call ` max ( original_data )` will result in an iteration over all elements from ` original_data ` , making your cost proportional to ` O ( n^2 )` .
Filter our elements of matrices where both / neither / either are nonzero ?
I want to create arrays ` Ap ` and ` Bp ` such that ` Ap ` and ` Bp ` are all elements of ` A ` and ` B ` where at least one of ` A ` or ` B ` is nonzero and , alternatively , where neither are zero .
You use masked indices to achieve that .
Any chance this would give me ` IndexError : too many indices for array ` if my data is too big ?
How are you indexing the ` flatten ` array ?
@USER you have to build ` ind1 ` and ` ind2 ` ** after ** you flatten ` A ` and ` B ` , this could be a source of such error ( not the fact your data is too big )
If you can separate the input data into a sequence of row indices , a sequence of column indices and a corresponding sequence of value indices , you can use the fourth option shown in the ` csr_matrix ` docstring for creating the matrix .
where the first two columns are the indices and the third column holds the values .
When I try to do the fft of this array I get an array of NaNs .
How do I get the fft to work ?
If there are any ` NaN ` s or ` Inf ` s in an array , the ` fft ` will be all ` NaN ` s or ` Inf ` s .
Note the cast to bool and then int , because the dtype of ` X ` must be large enough to accumulate twice the maximum row sum and that entries of ` X ` must be either zero or one .
I've scoured the matplotlib documentation and stack , but haven't found a way to create this specific style colormap , which relates corresponding integers to a color .
Depending on how you are going to use the output , you can just reshape the 2D input array into a 3D array that is of length ` 300 ` along the first axis , which must be much more efficient in terms of performance and memory .
Readers please use the ` reshape ` part not ` ( v ) split ` !
Then it returns the sum .
This doesn't work because the append statements append the result of the descriptor call .
What I need is to append either a bound method or similar proxy .
Hello Thank you for script , but I just edited a little about my sample file in above question , because this script is throwing an error as , Use of uninitialized value in join or string at script.pl line 34
Then ` df1 ` is append to ` df ` and last is set index from list to output ` df ` .
Since those are numpy arrays , you can use ` np.in1d ` to find indices of elements of array_1 [: , 0 ] in array_2 , then index array_1 with this result .
I want to insert new column having all ` ones ` in the beginning i.e. it will be my new first column .
Now to this I want to add a new column at beginning with all ones
Just concatenate this array with a ` np.ones (( 1000 , 1 ))` .
Also , each ` np.random.normal ( r_mean /( p*t ) , r_vol / t / np.sqrt ( p ) , n )` is meant to be a column of the resulting matrix , not a row ( ie . I'd have to transpose ` np.asarray ( dataset )`) .
Create ` mu ` and ` std ` so we can see the requested means and standard deviations : #CODE
So we've had ` hstack ` and ` concatenate ` as answers .
@USER I just ran some timing and concatenate was consistently ~3x faster than hstack regardless of array length .
` insert ` , ` append ` , even ` hstack ` all use ` np.concatenate ` .
You can pass the list of indices to ` np.insert ` : #CODE
Index Error : too many indices for array While defining variable
The error ` IndexError : too many indices for array ` happens when you try to access a 1d array as though it were 2d , or a 2d array as though it were 3d , etc .
Finally , reshape to have ` 3 ` rows .
Are you wanting to [ ` clip `] ( #URL ) ?
% is the mod operator
The most time-consuming step is finding the indices on the irregular lon / lat grid of the contour points .
It will ensure a non-0 positive value for the denominator in the match ([ 1-9 ] \d* ) , and allows for leading zeros with the ' 0 * ' in front of it .
I have a 2D array of integers that is MxN , and I would like to expand the array to ( BM ) x ( BN ) where B is the length of a square tile side thus each element of the input array is repeated as a BxB block in the final array .
( Note : data needs to be copied for the ` reshape ` here , so this function does not return a view . )
Attribute error float object has no attribute ' append '
You set ` p ` to a ` float ` at the start of your outer ` while ` loop : #CODE
Then make a loop and append each time ?
Memory Efficient L2 norm using Python broadcasting
Based on the posts online , we can implement L2- norm using matrix multiplication sqrt ( X * X-2 *X * Y+Y * Y ) .
Since the matrices have different shapes , when I tried to broadcast , there is a dimension mismatch and I am not sure what is the right way to broadcast ( dont have much experience with Python broadcasting ) .
If I were you , I would either dump Jython completely , or , if it's not possible , run Jython and CPython in separate processes with [ IPC ] ( #URL ) via named pipes or sockets .
ndarray concatenate error
I wish to concatenate the following arrays : #CODE
The error message is then obvious : you're trying to concatenate two arrays with different dimensions : that won't work .
Full log : #CODE
I have a big n-square diagonal matrix , in the scipy's sparse DIA format
I'd like to retrieve the diagonal as a vector ( in the numpy array )
I there a method or a function to get directly the diagonal of ` D_sparse ` as an numpy array ?
The ` diagonal ` method of the ` dia_matrix ` object returns the main diagonal .
Conversion : np.array of indices to np.array of corresponding dict entries
I have a numpy array of indices in Python 2.7 that correspond to a value in a dictionary .
The variable ` indices ` can not be changed but I can change the type of ` dict ` .
A faster method for larger arrays would be to use ` np.vectorize() ` to vectorize the ` dict.get() ` method and then apply that on the ` indices ` array .
Timing test for the new method suggested by @USER in the comments - ` [ d.get ( x.item() ) for x in indices ]` - #CODE
` [ d.get ( x.item() ) for x in indices ]` is nearly as fast as your ` vecdget ` .
I generalized @USER ' s code ` vecdget = np.vectorize ( lambda A , B : delays_phase.get ( tuple (( A , B ))); vecdget ( indices [ ' A '] , indices [ ' B '])` .
Next , keep in mind that ` eig ` uses an iterative approximate solution , as analytic solutions for eigenvectors aren't possible for > 3x3 inputs .
Note that the individual eigenvectors of ` eig ` are in the columns , not the rows .
How Does ` eig ` Choose Which Eigenvectors / values to Return ?
You can probably read all the properties , but may be limited in what ones you can change .
return indices from filtered , sorted array with numpy
return indices of ( original ) ` x ` corresponding to these values
Hi Divakar - yes ... well it seems I was on the right track after all - annoyingly confusing to keep track of indices and elements at the same time :P .
Once you have those two , you can sort the mask array using the sort indices and use this mask on the sort indices to get back the indices corresponding to sorted indices that satisfy the masked condition .
fair , then my advice is to work both with the logic i suggested as the ` getsizeof ( number )` you can merge or work with two arrays , an array to save low factorialized numbers and another to save the big ones , e.g. when ` getsizeof ( number )` exceed any size .
yep -- looks like you posted yours while i was still writing mine , which is consistent with the time diff ( ~ 3 minutes apart , and in those 3 min i split my answer into two lines and added some comments to make it easier to understand )
TypeError : can only join an iterable
You can use ` Series.str.join() ` and give the delimiter to join by as argument .
If a row of your lil_matrix has only zeros in int , `` np.size `` will return zero .
Are you checking shape or number of nonzero values ?
With ` dtype =o bject ` , the entries are pointers to objects , and apparently the default is to point to the unique ` None ` object .
Using reshape and transpose ( or T ) will do : #CODE
i used ` sum ` for concatenting lists as it is the most self contained and readable example .
Please don't use ` sum ` to concatenate lots of lists .
Note : some further care has to be taken if you have duplicate rows indices .
numpy tile without memory allocation
The reshape and ravel are both views , so ( I think ) the only new array is produced by the summation .
The sum #CODE
I was looking to this post on stack exchange , but I cannot figure out if and how to properly define the ` #define ` directives in my case .
You can do that by switching the indices and arguments of ` einsum ` : #CODE
Obsviouly the overhead increases with the exponent but for low ones is better than 10x the time .
How to get sum of products of all combinations in an array in Python ?
and I want to find sum of all possible combination multiplications like :
And finally sum of all these sums is my answer .
Is there some better algorithm to find the sum quickly ?
Efficiently find indices of nearest points on non-rectangular 2D grid
Now I need the indices of the corresponding lon / lat points .
Any suggestions how to efficiently find the indices of the corresponding ( nearest ) points ?
numpy array 1.9.2 getting ValueError : could not broadcast input array from shape ( 4 , 2 ) into shape ( 4 )
Your example sent us on the wrong path trying to stack the lists .
I can also concatenate 2 arrays to make a ` ( 4 , 3 )` array ( ` x ` is the first column ) #CODE
FIRST : I'm guessing the reason sparse data is giving a different answer than the same data converted to dense , is that my representation of sparse was starting feature indices from one rather than zero ( because oll library that I used previously required so ) .
Not even if you reserve a large matrix at the beginning ( with padded zeros ) to replace later .
This can be because of the structure sparse matrix is stored in ( It uses three arrays , in case of crs one for row number , one for non-zero column indices in rows and one for values themselves ; check the documentation ) .
To make it random I generate a sorted list of random positions that tell me when I should add data from new instances otherwise copy from the older ones .
Which is odd since it appears to round some numbers but not others , and keeps the trailing ` zeros ` as well .
and the additions used ` append ` ( and repeated ) #CODE
Or if you'd like to collect all the lines in one string , you could append them to a list : #CODE
Python object display ( ` str ( ... )`) routinely does this sort of line append and join .
the ` sum ( -1 )` method on this result sums across the last dimension , resulting in an array of shape ` [ 1000 , 100 ]`
we sum along axis 1 to get the weights
With your scipy approach , how would you condense the D down to 1000 weight values , e.g. for each A [ i ] , the sum of the distances to all the B [ j ] points , as requested by OP ?
Ah , I missed the sum there .
For simple cases you might just want to use the ` np.all ` function without ` flatten ` or ` ravel ` : #CODE
@USER As long as you don't alter tha data ` ravel ` is better , but ` flatten ` provides the same functionality .
The problem is I can only calculate the ' evidence ' in log , otherwise its 0 .
Can't you simply log both sides and calculate ` log [ p ( model|data )]` ?
cel - log [ p ( model|data )] does not equal log ( Pr ( data|model1 )) / ( log ( Pr ( data|model1 )) + log ( Pr ( data|model2 ))) , which I can calculate .
@USER , you're supposed to calculate ` log [ p ( model|data )]` , what you suggested is to just put a log in front of every term on the right hand side ` log ( Pr ( data|model1 )) / ( log ( Pr ( data|model1 )) + log ( Pr ( data|model2 )))` and told me that this is not correct .
You are supposed to calculate ` log [ Pr ( data|model1 ) / ( Pr ( data|model1 ) + Pr ( data|model2 ))]` .
I missed that a sum of two remains and ` numpy.logaddexp ` is needed .
Let ` logpr1 ` and ` logpr2 ` be ` log ( data|model1 )` and ` log ( data|model2 )` , respectively , and suppose #CODE
Plotting histogram using seaborn for a dataframe
I wanted to plot histogram for this dataFrame using seaborn function from python and so i was trying the following lines , #CODE
Well , obviously histogram function mapping for 2D array in general case isn't defined .
Do you want a single histogram for all values in the dataframe ?
A separate histogram for each column , or for each row ?
Then , the best graphical representation I can think of is grouped barplot : one group for every sample , within every group there are gene bars ( some people call this histogram occasionally )
You can keep the indices with the elements ( ` zip `) and sort and return the element on the middle or two elements on the middle , however sorting will be ` O ( n.logn )` .
If we need to average to elements ( array has even length ) then it returns the indices of these two elements in an list .
where gamma =-1 / sigma^2 .
just another thing , I tried the same in R and got the same error with its svd method .
I know the fft function works well based on my other results .
The following should be printing zeros for example : #CODE
However I wish to append 1000s of more array which are like the ' b ' shown but varying in size .
You can easily append more items ( or use extend to add multiple objects ) .
You can also concatenate elements to an existing array , but the result is new one .
There is a ` np.append ` function , but it is just a cover for ` concatenate ` ; it should not be confused with the list ` append ` .
There's the [ ` resize `] ( #URL ) method with ` refcheck=False ` , but it is a dangerous thing to use unless you are very sure there are no other views to the old data .
Let's say I take a stream of incoming data ( very fast ) and I want to view various stats for a window ( std deviation , ( say , the last N samples , N being quite large ) .
Is there a way to append to an array without this happening ?
Generally np and pandas perform well when the array is not growing , by repeatedly appending to it you will periodically force it to allocate a new memory block and copy the values which may explain why it borks when you append just a single element . does this post help you : #URL
the 2 inputs to the ` dot ` look like : #CODE
a scalar , and a 2 element array - so the ` dot ` is also a 2 element array .
You need to explain what size you expect these 2 arrays to be , and what size you expect the ` dot ` .
You can probably replace the double iteration with a single ` dot ` product ( or if need be with an ` einsum ` call ) .
I just wanna the dot product of the one row array with all the elements of the 12 rows array .
Or shorter ` myarray [ 0 , [ 0 , 1 ]) .dot ( myarray [: , [ 0 , 1 ] .T )` , That looks like a good ` dot ` usage .
Step 2 : manage axis-labels formatting scale ( min / max ) as a next issue
I know I can concatenate an empty array of zeros at the end of lines , but how do I make it so I can call it in a for loop or the like ?
Alternatively you could extend ` lines ` with ` np.hstack ([ lines , np.zeros (( 151 , 1 ))])` ( or concatenate on axis 1 ) .
A * A completes the dot product as expected : #CODE
So , with performance in mind , you can create a concatenated indices array instead and then index into ` a ` with it in ` one-go ` .
So , in this case , I would that all pixels on the diagonal of the matrix are displayed as gray pixels .
I would like to be able to move the 3D axis around with my mouse while a dot is moving around .
However , the dot now leaves a trail that wasn't there before , and this is the problem .
Interestingly , the dot trail disappears every time I resize the window or move the axis .
I have been unsuccessful in trying to achieve both an animated singular dot , and an interactive 3D axis .
Numpy : Difference between dot ( a , b ) and ( a*b ) .sum()
With NumPy 1.8.2 , the results are identical with dot and sum , suggesting identical rounding events , on NumPy 1.9.2 , multiplication + sum() is more precise .
Also #URL which is where the change in ` sum ` was implemented .
In the typical histogram created with Numpy.histogram or matplotlib.pyplot.hist , the bins are of uniform width or the user specifies his / her own bin edges .
There are lots of choices about optimal bin width -- say sqrt ( sample size ) .
Sometimes , there are bins with zero objects in them -- e.g. , at the extremes of the histogram .
( Imagine a histogram in which nearly every other bin has effectively 0 objects , or a histogram in which the first and last bins have effectively 0 objects -- both cases lead to poor visualization of the data and make it harder to see any underlying correlation . )
Also , is such a binning algorithm considered to be optimal ( in that the resulting histogram gives you a smoother visualization of where your data is ) , or sub-optimal ( in that you are manipulating the binning to your advantage , rather than merely showing the data as-is ) ?
For what it's worth the " optimal " ( ish ) way to handle this is to use a kernel density estimate ( e.g. ` scipy.stats.gaussian_kde `) instead of a histogram .
That having been said , no , there's nothing built in to ` numpy ` , ` scipy ` , or ` matplotlib ` to generate optimal histogram bins given a dataset and a number of bins .
@USER I'm basically creating a relative frequency histogram ( normal histogram but with the # of objects in each bin divided by the total # of objects in my sample ) .
Also , I really like Kernel Density Estimation ( KDE ) compared to histograms but I can't find any easy way to create a weighted KDE ( similar to a relative frequency histogram ) .
The y-axis ( " density ") of a KDE is not as easy to interpret as the fractions on the y-axis of a relative frequency histogram .
Generating a 4-D array from 1-D data reshape
While you are at show us the same values for ` data ` after reshape .
So for the cells that are diagonal to the center , the distance ( A ) is sqrt ( 2 ) multiplyed by resolution and for the others it's just equal to the resolution .
So what I don't know is how to write a code that will differ from cells that are diagonal and which not ?
If it what you are after , you should compare the 8 slopes and take the max of them .
The calling stack is , I think , a mix of C and Python code that is hard to follow .
or using other indices in a different order and not creating a separate array : #CODE
Although you can't explicitly append rows to an hdf5 dataset , you can use the maxshape keyword to your advantage when creating your dataset in a way that will allow you to ' resize ' the dataset to accomodate new data .
I am processing a large array and I would like to run the filter once and get everything I need as opposed to doing it several times for different returns ( e.g. , one for min , one for max , etc . ) .
Adding the transpose of a square matrix to that matrix should produce a matrix that is symmetric around the diagonal .
So , when you add the transpose to the data ( in place ) , the data is constantly changing as you're doing the addition .
Basically this is a concatenation process , with zeros being appended at regular intervals ( ` 2+6 `) .
I try to emulate ` pop ` and ` append ` which are done at ` O ( 1 )` time on lists , but does the same hold for numpy arrays ?
2 ) Value check on a toy data to make sure we are picking random images and not consecutive ones for splitting : #CODE
perform 2-d operation on each layer of a 3-d stack
I want to perform the same two dimensional operation on each layer of the array ; for example standard deviation , or sum .
Each layer of 3D stack would mean axes ` 0 ` and ` 1 ` being combined for each index in ` axis=2 ` .
If you want to append each y in y_test to results , you'll need to expand your list comprehension out further to something like this : #CODE
In the example below , I test whether all unique non null values are either ' 1 ' or ' 0 ' .
I am trying to find 5 local max and min points plus the extrema .
Is there an efficient way to use FFTW / CUFFT ( they have similar APIs ) to perform an fft over a given axis of a multi-dimensional array ?
Now let's say I want to compute a 1D fft along the middle axis .
The dumentation states that for a 1D fft , the element in batch b at position x will be taken from :
istride and ostride to 4 ( so that incrementing x moves along the second axis , which is the axis along which we want to fft ) ,
then it computes the correct fft for each of 2 batches where the last axis index is 0 , but leaves the sub-arrays for which the last index is 1 , 2 , or 3 untouched .
What I want to do is a 1D fft along one axis of a 3D array , not a 3D fft .
What would be the settings for n , nembed , stride and dist to do a 1D fft over the middle axis of a 3D array ?
Then it's a 1D fft with a stride .
So you have figured out how to compute a 1D fft in a 3D data set along the dimension you want , and you're now asking how to do a batch of these ?
If the array is indexed by [ y ] [ x ] [ z ] , then I want to fft over the x axis for every combination of y=0 , 1 and z=0 , 1 , 2 , 3 .
I figured how to do an fft over x for every y=0 , 1 but z=0 only ( using the settings listed in the question ) .
Thanx but i need to modify some data before add it to array thats why i need to read it form the file first then do some modification and then I need to append it to array
I am getting a NA and the ` floor ` function in the second line of ` compute_msd ` throws an exception when trying to convert to ` int ` .
if you plot the MSD in log scale anyway , you don't need to compute it for every time #CODE
You should use ` np.where ` and then count the length of the obtained vector of indices : #CODE
I converted it into ` DataFrame ` to plot and used ` T ` to transpose .
Any suggestion which does not involve a histogram ?
I didn't use bins at all , I created the cumulative sum to represent the CDF , however I see a mistake where I didn't normalize it .
The histogram I showed earlier was just proof of concept , didn't actually use its values
The x-axis should show my values ( which are float and range 0 to 1 ) , whereas the y-axis should show the normalized sum of all values that are <= to a given value .
So you can perform point-by-point multiplication on ` AA ` an ` detrend ` , sum everything , and store in BB .
This will result in an array with the same size as BB.ix with its elements being the sum of the products of each row in AA by detrend .
I don't have negative values , and the max is a few thousand , in the range for uint16 .
how to use vstack in a for loop in order to append csr_matrix matrices together
I am using the following piece of code in order to concatenate matrices of type ` csr_matrix ` together .
It is based on How to flatten a csr_matrix and append it to another csr_matrix ?
` a ` is all zeros , so it is a ` csr ` with 0 stored elements .
To make things more obvious make ` a ` with nonzero values : #CODE
Did you copy the compressed versions of the arrays to a new .hdf5 file , or did you try to overwrite the ones in the existing file ?
One thing you can do is convert your generator function to give out single values from ` c ` and then create a 1D array from it and then reshape it to ` ( -1 , 5 )` .
Find indices of specific rows in a 3d numpy array
I am looking for a way in numpy to find the indices of outer specific rows in a 3d array .
` np.in1d ` would flatten its inputs .
Then , you need to check for all TRUE rows in it for the matching indices .
I have written a program which receives large-ish data sets as input ( ~150mb text files ) , does some math on them , and then reports the results in a histogram .
I was hoping to mitigate some of the computation time by using Python's ` multiprocessing ` module to distribute the calculation of partial histogram data to individual processes while keeping an array of the final histogram in shared memory so that each process can add to it .
is executing , you placed a lock on the entire shared dataset while you're processing the histogram .
In words , we take pairwise cuts of the data , generate the relevant data and then put them in a histogram .
The only real synch you need is when you're combining data in the histogram
Unfortunately I need the entire array for computing histogram data since it involves comparing each element in the array .
UnicodeDecodeError : ' ascii ' codec can't decode byte 0xc3 in position
So I need to apply ` decode ` at some point to get an ` U ` array .
and apply ` decode ` to individual elements : #CODE
I tried to implement a gradient descent algorithm for logistic regression .
select the max min ones from the subsets , accordingly .
permutation matrices of order n ( source ) ?
If so , then your task reduces to just computing a random permutation matrix .
) a square block of size n <= M where that block is a permutation matrix of order n , and 2 .
) zeros everywhere else .
#URL Numerical-stable sum ( similar to math.fsum )
` np.sum ` is clearly fastest ; but ` fsum ` is better than ` sum ` , probably because of its sepecialized C implementation .
( 1 ) single multidimensional dot product #CODE
( 2 ) looping through i and performing 2D dot products #CODE
How can looping through and doing 2D dot products be ~ 124 times faster ?
For N dimensions it is a sum product over the last axis of a and the second-to-last of b
dot ( a , b ) [ i , j , k , m ] = sum ( a [ i , j , :] * b [ k , : , m ])
This is probably true as long as that iterating dimension is small compared to the other ones .
I tried a ` dot ` variation where I reshaped ` A ` into 2d , thinking that ` dot ` does that kind of reshaping internally .
` einsum ` sets up a ' sum of products ' iteration involving 4 variables , the ` i , j , k , m ` - that is ` dim1*dim2*dim2*dim3 ` steps with the C level ` nditer ` .
So the more indices you have the larger the iteration space .
` dot ` does many things under the hood , it is apparent that ` np.dot ( A , x )` is not calling BLAS and is somehow defaulting over to numpy's internal GEMM routine .
Your reshape example is bypassing the looping mechanics and going straight to a conventional 2D GEMM call without copying any data , it will always be the fastest solution for reasonable sized problems given a good BLAS .
I guess I'm incorrectly using the outer function .
I would like to do this the right way than using ugly reshape hacks to pull the ( correct ) data out but repackage it in a correct shape .
But I am doing my thing because of the Ram consumption lets say.Afterall , what I am looking for is the command to stack an array to another like I described in my message .
I think it might goes witht he " d / h / vstack command " or " concatenate "
I tried doing it by multiplying the data matrix by its transpose : #CODE
Do you have the transpose on the wrong argument ?
I don't think you can do much better than a dot product , but multiplying DOK matrices is very inefficient .
You should convert to CSR or CSC before computing the dot product ([ see here ] ( #URL )) .
Depending on the max ` nnz ` per row you could use e.g. ` uint16 ` .
If I split the table into unique tables on that column , then I get the a few hundred thousand tables ( relatively small in rows ) that are grouped by a value useful for summary view computation , and we get a motivating chunk of improved footprint .
Cannot use pymc prior value in numpy exp function
Is your histogram sparse ?
I define a numpy array called sp_step , what I want to do is use the variable sp to find which segment of the data is in , then I will stack the corresponding data .
In any case , the ` ValueError ` is the result of mixing multiple valued ` numpy ` logical operations with scalar Python ones .
How to aggregate NumPy record array ( sum , min , max , etc . ) ?
I'd like to aggregate certain statistics ( sum , min , max , etc . ) from ` v ` by grouping unique combinations of ` I ` and ` J ` .
This is clip illustrates the changes I think you need to make to get this assignment to work : #CODE
To replicate the MATLAB behaviour , you need to create a mesh of such indices from the vectors .
np.triu() makes the redundant half of the matrix zeros , but I'd prefer if I can make them Nans , then those cells don't show up at all on the surface plot .
What would be a pythonic way to fill with NaN instead of zeros ?
I cannot do a search-and-replace 0 with NaN , as zeros will appear in the legitimate data I want to display .
` tril_indices() ` might be the obvious approach here that generates the lower triangular indices and then you can use those to set those in input array to ` NaNs ` .
The method that I am showing here reads each row and calculates the distance and append it in an empty list which is the new column " Distance " and eventually creates output.csv .
To try to get it , I am using ` numpy ` and ` reshape ` but it is not working .
Why would you tell Python to reshape your array to 3-by-2 if you want a shape of ` ( 6 , )` ?
You could also get the results you wanted with ` reshape ` , but it's wordier : #CODE
You can use ` ravel() ` to flatten the array .
Operands could not be broadcast together error when trying to construct an ndarray as a function of two other ndarrays with numpy
Numpy array - append named column
There are 3 values in each row ( not necessarily unique ) , for example : #CODE
Apply that to 2d array , and we see it sets the reverse diagonal .
How could we set the reverse diagonal on the 2nd plane of a 3d array ?
Pandas : mapping data series ( float to string ) WITHOUT removal of trailing zeros
However , the map ( str ) removes trailing zeros from the values in the series .
How do I stop the trailing zeros from being removed ?
If the data starts off as floats then it doesn't have " trailing zeros " .
But from your example it sounds like what you really want is to have all the numbers be at least 4 digits long , adding trailing zeros if need be .
The two tricks I'm using here are 1 ) using %i in the format string to signify that it's an integer ( %f means a float , but it renders w / o trailing zeros ) and 2 ) the apply function on df .
Is there any support for log probabilities ?
In numpy or scipy for Python , specifically Python 3 , is there any support for expressing probabilities as log probabilities for various distributions ?
For the case I'm seeking , it would be nice to specify ( all of ) these as log probabilities instead .
log normal : #URL
Numpy lets you use an array of indices as an index to an array : #CODE
Thanks for your reply @USER ... but what is the advantage of sorting through indices not directly by using value .
If i am right will it be helpful in the scenarios where we don't know the actual data but still with indices we can sort the array .
By the way , what is the pros and cons of returning sum as ( n , ) versus ( n , 1 ) ?
This property could be used to reshape the input array that could be a 1D or a 2D array into a 2D array output .
To sum up , one solution would be to put a reshaping code at the top of the function definition , like so - #CODE
A strange dot product in Python
I would like to take the dot product of the first M rows of the first matrix with the first row of the second , the next M rows with the next row of the second , etc .
But yes , HxI dot IxI .
@USER I've tried generating a block diagonal matrix from the original and flattening the second .
Instead use ` assert_allclose ` and then set the desired relative and absolute tolerances .
I've fixed that and this simple case works , but more complex ones such as still fail .
It does : ( for 2 unique arrays ) #CODE
So it makes a sorted concatenation of the arrays , and then removes the elements that aren't unique .
Then I stack the dataframe , give the index levels the desired names , and select only the rows where we have ' True ' values : #CODE
I found out that max / min / sum was faster if you re-implemented them , rather than using the python version
I want to calculate and plot a gradient of any scalar function of two variables .
And lets say I remove sin , I get another error : #CODE
As you pointed out , that caused the sin function to always return 0 ( i just noticed ) .
It looks good except that the arrows point in the opposite direction ( they are supposed to go out of the reddish dot and into the blue one ) .
But i dont know how to calculate and plot vector function that is the gradient of that scalar function ( so , grad ( V )= dV / dx * ex + dV / dy * ey , where ex and ey are ort vectors )
If you figure out how to compute the gradient , [ this ] ( #URL ) can help you use that function for plotting .
now you can use ` numgradfun ( 1 , 3 )` to compute the gradient at ` ( x , y )= =( 1 , 3 )` .
V ( x , y ) = Sum over n and m of [ Cn * Dm * sin ( 2pinx ) * cos ( 2pimy )]; sum goes from -10 to 10 ; Cn and Dm are coefficients , and i calculated
that CkDl = sin ( 2pik ) /( k^2 +l^2 ) ( i used here k and l as one of the
indices from the sum over n and m ) .
I have several problems with this : both ` sin ( 2*pi*k )` and ` sin ( 2*pi*k / 2 )` ( the two competing versions in the prefactor are always zero for integer ` k ` , giving you a constant zero ` V ` at every ` ( x , y )` .
V ( x , y )= sum_{k , l = 1 to 10 } C_{k , l } * sin ( 2*pi*k*x ) *cos ( 2*pi*l*y ) , with
This is a combination of your various versions of the function , with the modification of ` sin ( 2*pi*k / 4 )` in the prefactor in order to have a non-zero function .
The gradient is taken the same way as before , but when converting to a numpy function using ` lambdify ` you have to set an additional string parameter , `' numpy '` .
This will alow the resulting numpy lambda to accept array input ( essentially it will use ` np.sin ` instead of ` math.sin ` and the same for ` cos `) .
For clarity I included a few more plots : a ` contourf ` to show the value of the function ( contour lines should always be orthogonal to the gradient vectors ) , and a colorbar to indicate the value of the function .
i want gradient in every point of x and y .
Then if it * was * symbolic , you'd need ` sympy.sin ` , but then ` append ` will throw an error .
ah okay , ill try to figure out how to define it symbolically . about return in my function , i simply want a sum of all terms , but if i try to use one return , i get some nonsense / errors .
i will think about that some other time ( im an absolute beginner and it will take me hours to figure it out ) , right now i have to solve this symbol thingy . thank you for your help
so its electric potential as a function of x and y , and I did plane wave expansion : V ( x , y ) = Sum over n and m of [ Cn*Dm * sin ( 2pinx ) * cos ( 2pimy )]; sum goes from -10 to 10 , both for n and m ; Cn and Dm are coefficients , and i calculated that Ck*Dl = sin ( 2pik ) /( k^2 +l^2 ) ( i used here k and l as one of the indices from the sum over n and m ) .
@USER , the gradient of a function will always point from the minimum towards the maximum .
But the electric field is ` E=-grad ( V )` , so if your function is voltage then the * negative * gradient is the electric field .
Just plot the negative gradient , and your arrow directions will fix .
It's an error in the outer for loop .
Yeah , I was assigning an incorrect length to f , so there were some zeros left over .
n=5 ( min length of sequence )
But then our target result would just be along the diagonal .
@USER my first thought was that this is an evaluation of the [ binomial distribution function ] ( #URL ) .
You really want to work with log values with probabilities , and there's a lot of good info in that post on how to do it , like using gammaln .
Working in the log domain will transform multiplications into additions , and divisions into subtractions , making things more manageable .
You now only need to observe that ` exp ( log_x ) == x ` to find your answer .
` log (8 .686e1395 )` is approx 3214 , in good agreement with my answer .
Arbitrary-precision integers aren't really the way to go for this problem , since you're destroying any precision you had by calling log , so I'll just let ` scipy.special.gammaln ` speak for itself ( but see my edit below ): #CODE
EDIT : What ` gammaln ` does is to compute the natural log of the gamma function .
The gamma function can be thought of as a generalization of factorial , in that ` factorial ( n ) == gamma ( n+1 )` .
So ` comb ( n , r ) == gamma ( n+1 ) /( gamma ( n-r +1 ) *gamma ( r+1 ))` .
I think your answer could do with some explanation of how the gamma function relates to the OP's problem , though .
You can even take a histogram of a single value , and just accumulate those histograms .
It's probably faster to gather them into chunks for each histogram call though .
How would accumulate the histograms of the single values ?
One way is to leverage ` histogram ` .
You can speed it up by reducing the number of calls to ` histogram ` #CODE
When you created your X array , supposing you are giving him a size since you don't append anything , did you try just to precise the type ?
When I try to substitute the cos and sin with tan or exp I can't find a solution as well .
in other words , ` c3 ` is linearly dependent on ` cos ( b1 )` .
Or at least changing the variable names to avoid confusion with the symbolic ones .
Evaluate absolute value of array element in function
To enter the if block , do you want all elements in u to be smaller in absolute value than lambda , or just one of them ?
Note that n_exp are floating point values , and factional powers are not exactly easy to compute ; I am not sure about the precise underlying instructions used , but I can easy see the exp one being more optimized
One reason is that , comparison of ` ** ` vs ` exp ` ultimately boils down to what happens at C and hence at assembly level .
Most of today's micro-processors do not have intrinsics for power computation , while they do support ` log ` and ` exp ` using predefined tables .
Reordering numpy array indices
fixed the x , y transpose
Which is a list comprehension iterating through the tuple pairs of first sizes from 2nd item to last and second from 1st item to next to last , create a random number based on those two parameters , then append to a list that is stored as ` self.weights `
Inner1d can't take complex numbers , and fft convolutions aren't efficient with this kernel size , so the two methods I've been testing are np.correlate and einsum .
If your primary use case is the scipy stack , for example as a Matlab replacement .
Convert the list expressions to generator expressions in ` neighbors ` and ` sum ` , by using parenthesis instead of square brackets , so that you're not allocating a list every time .
@USER if you could translate it easily , I'd really appreciate that .
Using mathematical notation what I am trying to implement is a fast summation ` \sum_{k=k0}^{kN } f ( k , x )` , where ` k0 ` and ` kN ` are integers , and ` x ` is a vector , and ` f ` can be any general function of ` k ` and ` x ` like the ones given above .
For the operation you are performing , you can simply sum all elements from ` k0 ` to ` kN ` to get a scalar , which must be used to scale ` x ` for getting the desired output .
One need not use a loop at all by using the standard result for the sum of an arithmetic sequence .
Here ` n = kN - k0 + 1 ` is the number of items to sum .
If one needs to use Theano for some reason other than performance ( e.g. to obtain gradients , or as part of some larger symbolic computation ) then one can compute the same result without using sum or scan , just as in numpy .
The following code implements the original numpy and Theano approaches , and compares them with Divakar's numpy approaches ( and my Theano version of the arange sum approach ) plus my numpy and Theano approaches that use the standard sum of an arithmetic sequence result .
If you can vectorize the function ` f ` then the computation can be performed much more efficiently ( in time but perhaps not space ) in both numpy and Theano by computing the sum along the ` x ` axis of the vectorized result .
For example , if you want ` output += exp ( k*x )` then you can achieve this in numpy without an explicit loop like this : #CODE
I gave that specific example , but I am trying to solve a more general problem , where the increment to output can be any general function of ` k ` and ` x ` , say ` output += x**k ` , ` output += exp ( k*x )` or ` output += ( x-k ) **2 ` .
There is also a ` translate ` .
With this I found I need to ` reshape ` as well .
Since all the fields are ints , it possible to ` view ` these arrays as regular 1 or 2d ones , and use numeric indexing .
What you did with ` minimize ` is to minimize the sum of the three equations which is not equivalent to " minimize c3 in ( the ) set of equations " .
You defined a function ` func ( x )` that is the sum of the 3 right-hand-sides of your equations .
Performing this operation in a for loop is highly impractical , since I need to be able to scale this for large arrays and a for loop over the three indices ijk scales in O ( N^3 ) .
I create a numpy array of size l and and fill it with zeros and ones .
So you can replace this line with ` S = np.zeros ( l )` ( creating an array of length l full of zeros .
How can I derive this transpose manually ?
If you pass the permutation ` ( 2 , 1 , 0 )` , then the element that was at position ` [ x , y , z ]` will now be in position ` [ z , y , x ]` , or in position ` [ z , x , y ]` for permutation ` ( 3 , 0 , 1 )`
Efficiently filling NumPy array using lists of indices
Note that ` column_data ` is sorted and unique .
` column_data ` is sorted and unique .
I have tried with multivariate_normal and ` cov = [[ sigma1 , 0 ] , [ 0 , sigma2 ]]` , but what I get is 2 different N by M matrices , the first one with variance sigma1 and the second with variance sigma2 .
but considering a column , since the variance of the sum of independents gaussians is the sum of their variances , you have increased the variance .
No , it is just the sum of the original variance ( 0 ) and the variance of b ( 5 )
cov = np.diag ( np.ones ( M ))
a = np.random.multivariate_normal ( mean , cov , N )
The second variance is the sum of the variances , or equivalently the sum of squared stddevs ( 1 and 3 in my example )
You can update the list in-place with append : #CODE
I think the human eye has it " easy " to pick up the correct path since the long straight lines stand off from the rest of the data .
A general way to do this is to construct a truth table and then use that to select indices from ` x ` or ` y `
` numpy.where ` is more powerful than this , in that it can broadcast one or both of input arrays to the same shape as truth table .
However it gives me a error saying " could not broadcast input array from shape ( 96 , 96 , 3 ) into shape ( 3 , 96 , 96 )" , I know it means if I initialize the data like np.zeros (( 1 , 96 , 96 , 3 )) , it will be ok .
so the only way is to transform it after resize the image .
As a part of my project , I am trying to implement parallelized normalization operation on a bulk of matrice by using a map function with the matrix to processed and vectors encapsulating min and max value of each dimension as input variables .
But , when I try to reshape list of single 125 numbers ( no tuple elements ) to a numpy array , no such error occurs .
Any help on how to reshape list of tuples to a numpy array ?
Alternatively , just include the 3 elements in the reshape : #CODE
Take the sum over ` a [ i ]` out of the loop , just like you did for ` b `
I see for example a diagonal where ` i == j ` #CODE
There are functions like ` np.triu ` that give you their indices .
I want to reference a numpy array of matrices with two arrays of indices i and j .
Machine stalls because M [: , i , j ] is simply too large from all the throw away values ( all i care about is the diagonal ) .
I don't think ` einsum ` does anything for you - you are just using it as an alternative to ` diagonal ` .
The other generates a ` ( COUNT , COUNT )` matrix , and extracts the diagonal ` ( COUNT , )` array from that .
I'm wondering if the particular use of ` ravel ` and ` reshape ` in this situation are essentially the same , or if there are significant differences , advantages , or disadvantages to one or the other , provided that you know the number of axes of X ahead of time .
is a simpler way to use reshape .
` reshape ` and ` ravel ` are defined in ` numpy ` C code :
In the same file , ` reshape ` punts to ` newshape ` .
So identifying where reshape ( to 1d ) and ravel are different would require careful study .
It times the same as ` reshape ` .
I don't recommend using this method , but it may clarify what is going on with these reshape / ravel functions .
Clearly ` ravel ` takes a direct route , ` reshape ` a more general one .
@USER : I want these to flatten and count the number of occurrences .
( Because of the mixed types in your object array , ` numpy.unique ` won't work in Python 3 ; see below for a work-around . ) For example , in the following , ` values ` is an array containing the unique values in ` a ` , and ` counts ` is a corresponding array holding the number of times the value occurs in ` a ` .
Start with integer ones before trying floats .
Learn what it produces when there aren't any matches , and when there are multiple ones .
To get the probabilities divide by the sum : #CODE
I'm trying to get the indices to sort a multidimensional array by the last axis , e.g. #CODE
And I'd like indices ` i ` such that , #CODE
So , ` a.argsort ( 1 ) + ( np.arange ( M ) [: , None ] *N )` basically are the linear indices that are used to map ` b ` to get the desired sorted output for ` b ` .
The same linear indices could also be used on ` a ` for getting the sorted output for ` a ` .
and I had find out the unique id using this code #CODE
Now I want to print out the unique id with x and y , also the popularity but not with name and need to ignore the column without x or y
It's working when I use np.hstack instead of np.append , but I would like to make this faster , so use append .
Looking at the source for ` histogram ` in the same file , the parameter becomes ` defaultlimits ` , and you find a call to ` numpy.histogram ` , where it becomes ` range ` .
Assign a unique id from 1 to N to all network nodes , without assigning any colors yet .
About ` valid_colors ` and ` not_valid_colors ` you should think a bit more because with numpy arrays you cannot append things : the array have fixed length , so you should fix a maximum size before .
It essentially produces an image gradient where the largest gradient can be found at the edges of your image .
Vectorizing a sum over a function of unordered pairs ?
I'd like to sum a function f over unordered pairs ( n , m ) with n and m distinct , where n , m range from -N to N .
@USER : sin is an odd function , so the sign change from flipping n , m inside the sin cancels the sign change from flipping n , m in ( n-m ) .
since i want the ones that is not equal to the one that is already existed .
No need to create the array before appending , ` axis=0 ` will allow you to append row wise .
From then on you specify ` z ` as the original array and append the other ` array ` as such : #CODE
How can I efficiently translate " terrain " numpy array into networkx graph ?
I can build the graph G manually iterating over elements of array A and checking whether pieces of land are connected or not ( pixels are considered connected only if they have a common edge , so each pixel of land can be connected at most to 4 , and no diagonal connections are allowed ) .
In that class instance , there will be a dictionary object unique to that sample key .
If you simply want a dictionary of dictionary , where the keys for the outer dictionary are the indexes and the keys for the inner dictionaries are columns and the value are the corresponding value at that index-column ( or dictionary of classes containing dictionary ) .
If you want the values of the outer dictionary to be of type ` class Sample ` , though I hardly doubt that is useful at all , then you can do - #CODE
You can use a nested list comprehension to strip the items with ` str.strip() ` method : #CODE
This works , however I've edited my post to be a more accurate representation of the data , instead of applying strip I need to apply a regex that captures the desired data , would it be possible to capture the data that I want and ignore the rest of the data that's already formatted properly ?
But How can I apply the similar method to a mat ?
For example , you can write ` A.A [: , 1 ] 0.5 ` ( the ` .A ` attribute returns an ` ndarray ` view of the matrix ) , or ` ( A [: , 1 ] 0.5 ) .A1 ` ( the ` A1 ` attribute returns a flatten ` ndarray `) .
But what about selecting column vectors if I don't want to transpose the matrix first and then apply the above method ?
Essentially I want to take the the pairwise dot product of the rows , then divide by the outer product of the norms of each rows .
It sounds like you need to divide by the outer product of the L2 norms of your arrays of vectors : #CODE
Based on the precalculated values thus far , you would have the column indices for all rows in ` p ` .
Note that these column indices would be a large ndarray containing all possible column indices and inside our code , only one would be chosen based on per-iteration calculations .
Using the per-iteration column indices , you would increment or decrement ` X0 ` to get per-iteration output .
Get row indices : #CODE
You can get the indices with : #CODE
I have various time series , that I want to correlate - or rather , cross-correlate - with each other , to find out at which time lag the correlation factor is the greatest .
When I correlate a time series that starts in say 1940 with one that starts in 1970 , pandas ` corr ` knows this , whereas ` np.correlate ` just produces a 1020 entries ( length of the longer series ) array full of nan .
This is what I get when I correlate with pandas and shift one dataset : #CODE
When I correlate a time series that starts in say 1940 with one that starts in 1970 , pandas ` corr ` knows this , whereas ` np.correlate ` just produces a 1020 entries array full of ` nan ` .
I just need to shift for seeing the max correlation within one year .
Since your array will be very sparse , a ` scipy ` sparse matrix could save on memory , since it only stores the nonzero elements .
But it has to also store that element's coordinates as well , so storage per nonzero element isn't as compact .
But no good for changing sparsity ( adding nonzero elements ) .
You can flatten the matrix and then sort it : #CODE
The last set of calculations can be done with ` dot ` #CODE
But we want just the diagonal of the last ` dot ` .
A simpler sum of products is better : #CODE
Slews of zeros ?
The reason i was having installation issues with pip , was down to the fact ( thanks pv ) that i was using the command :
One approach is to separately compute the absolute value and the sign as follows : #CODE
Or do I have to totally roll my own ( and probably in a very slow python loop instead of fast numpy or pandas code ) ?
This range is wide enough so that then min value minus the width and the max value plus the width will capture all x values .
This seems closer to " roll my own " than " supported functionality " though .
This is not really the answer I was looking for ( I can roll my own without help in writing the code ) , but I gave credit since you're the only person who took the time to write an answer .
Python how to find unique entries and get the minimum values from a matching array
I have a numpy array , ` indices ` : #CODE
All of the rows in ` indices ` have a match in the ` distances ` array .
The problem is , there are duplicates in the ` indices ` array , and they have different values in the corresponding ` distances ` array .
I would like to get the minimum distance for all triplets of indices , and discard the others .
As written above , the input order of the indices will not necessarily be preserved ; keeping the original order would require a bit more thought .
Briefly , I have a stack of image taken at different focal depth .
Function ` get_fmap ` creates a 2d array where each pixel will contains the number of the focal plane having the largest log response .
It creates a log mask #URL use from Python as ( for example ) ` Vips.Image.logmat ( 2 , 0.1 )` .
Try : ` x = Vips.Image.logmat ( 2 , 0.1 ); x.matrixprint() ` to see the log mat that vips makes .
I couldn't quite understand the equation regarding the log kernel .
The log kernel looks similar , but I'd check the one that vips is making ( obviously ) .
Each has 30,000 ids : [[ 1 ,..., 12 , 13 ,..., 30000 ] , [ 1 ,.., 43 , 45 ,..., 30000 ] ,..., [ ... ]]
First flatten your ndarray to obtain a single dimensional array , then apply set() on it : #CODE
Each has 30,000 ids : ` [[ 1 ,..., 12 , 13 ,..., 30000 ] , [ 1 ,.., 43 , 45 ,..., 30000 ]]`
The current state of your question ( can change any time ): how can I efficiently remove unique elements from a large array of large arrays ?
Update : as @USER pointed out in his comment , my dummy example is biased since floating-point random numbers will almost certainly be unique .
Those are almost guaranted to be unique .
A couple of earlier ' row-wise ' unique qestions :
vectorize numpy unique for subarrays
Count unique elements row wise in an ndarray
In a couple of these the count is more interesting than the actual unique values .
If the number of unique values per row differs , then the result cannot be a ( 2d ) array .
If you just want to flatten the image to integer values you can use the skimage rg2gray ( img ) function .
Numpy : how to add / join slice objects ?
It allows you to perform more complex slices along the first axis - you can concatenate multiple slices with commas : ` np.r_ [ 5:10 , 100:200 : 10 , 15 , 20 , 0:5 ]` .
With multiple slices you have to concatenate somewhere , either before indexing or after .
' roll ' uses the same method ( s ) .
@USER if you're after performance , it seems like ` roll ` and ` vstack ` are much faster than using index ranges ( which makes sense ) .
I have a 2D histogram that I generate with numpy : #CODE
How can I normalize the bin counts by the area in each log bin ?
Thanks for showing the broadcast method tho too ...
Certainly possible with ` broadcasting ` after adding with ` m ` zeros along the columns , like so - #CODE
You won't notice the difference as you are using it on an array of all zeros .
Hi , my question is how to convolve the matrix with the filter .
I would like to substitute ( only ) unmasked values of an array ( x ) by ones without changing masked values .
You can also try to put a threshold : if your max in below whatever good value , then it is probably not your laser but a noisy point ...
To find the minimum indices ignoring NaNs , you can use ` np.nanargmin() ` #CODE
Please see above append to question .
Where ` jj ` is an entry of the filelist that records where each tile beblongs .
I want to reshape the numpy array as it is depicted , from 3D to 2D .
It looks like you can use ` numpy.transpose ` and then reshape , like so - #CODE
To get back original 3D array , use ` reshape ` and then ` numpy.transpose ` , like so - #CODE
If I do that by putting the below lines at the relevant place my code ( lines 16 and 17 ) the interpreter complains about a ` ValueError ` because it somehow tries to broadcast the ` guess ` to ` len ( x )` or ` len ( y0 ` ( i.e. operands could not be broadcast together with shapes ( 0 ) ( 40 ) .
Then , do differentiation along the rows , giving us all zeros for duplicate rows , which could be extracted using ` ( sorted_array == 0 ) .all ( 1 )` .
You can use ` boolean indexing ` with ` np.in1d ` to select the rows excluded in the give indices list .
plotting a row of 3 plots using matplotlib and numpy but getting " IndexError : too many indices for array "
I tried to create a plot but keep getting an ` IndexError : too many indices for array ` .
I get the error : ` IndexError : too many indices for array `
Trying to access it with two indices should give an error : #CODE
Obviously sum of each row equals to 1 .
Float64 is not an absolute limit ; it's a representation choice .
I was wondering if it could be possible that conditional statements work in a similar way of numpy.where in the sense that they show only the indices of such conditions ?
` maxlen ` is some number ( likely the max of all of the ` len ( second_index )`) or just something simple like ` 1000 ` .
You can use a list or array of indices rather than slice notation in order to select an arbitrary sequence of indices in the final dimension : #CODE
What this lambda function does is strip the double quotes characters from each entry ( or if it's empty or ` nan ` set it as ` -999 `) and then convert the entry into a float .
What you'd then do is use the ` convolve ` function : #CODE
If you want " wrapping " behaviour like xnx's answer has , use the ` mode= " wrap "` argument to ` convolve ` : #CODE
( This doesn't give quite the same result as the ` convolve ` approach suggested in the other answer because it handles the edges of the array differently . )
[ concatenate ] ( #URL ) ?
@USER ` concatenate ` is not the solution as it merge the way I don't want to .
Possible duplicate of [ Find unique rows in numpy.array ] ( #URL )
You could convert those rows to a 1D array using the elements as two-dimensional indices with ` np.ravel_multi_index ` .
Then , use ` np.unique ` to give us the positions of the start of each unique row and also has an optional argument ` return_counts ` to give us the counts .
@USER Can you also time the ` unique ` example where its combined with ` view ` ?
@USER so changing to min should work for positive and negative numbers ?
@USER I would think so , again as I said not tested , so not 100% sure , but the idea with ` a.max ( 0 )` was to give enough " separation " between consecutive tuples to be considered / simulate as indices of a multidimensional array .
I keep getting ` ValueError : invalid entry in coordinates array ` , the only way I could get it to work was to use ` clip ` instead of raise but not sure what implications that may have
@USER , a related question came up again and out of interest I was trying to compare creating a mapping of non unique elements to counts , it just would not work when there were negative numbers and it was beginning to drive me mad as to why
M.sparsity.colidx holds " Column indices array of CSR data structure " , while M.sparsity.rowptr holds " Row pointer array of CSR data structure " , ok , so I have all needed coordinates to reconstruct the CSR matrix , but where does this CSR data structure live ?
@USER thank you , but values contains all data ( with zeros ) .
Suppose we have an array of indices of another numpy array : #CODE
You can also flatten ` a ` with ` .ravel() ` , index into ` b ` and reshape back to ` a's ` shape - #CODE
That is the 2 columns of ` a1 ` do not provide two indices ( which would require a 2d ` b `) .
Why not load the file as a 1d array of ` f4 ` , and reshape it after ?
Not sure what the -1 in the reshape is though , I am now using ` np.fromfile ( filename , np.float32 , M*N ) .reshape ( M , N )`
Ideally you could pose it in a more general way ( e.g. * " How can I generate random sequences where each unique item follows each other unique item an equal number of times ? " * ) .
Vectorization of cumulative sum in python
I'm trying to vectorize / broadcast ( Not sure what it is called formally ) my code in order to make it faster , but I can't quite get it .
What I want with this code is basically the absolute sum of l1 for adding each element from l2 to all numbers in l1 .
The trick is to first combine the two one-dimensional arrays into a single two-dimensional array , and then sum over that .
If you have a vector of shape ` ( a , 1 )` and you broadcast it with an array of shape ` ( b , )` , the resulting array will be shape ` ( a , b )` .
You can try to reshape the analytical expression to get rid of this numerical problem ( specifically , loss of precision ) , but it won't be easy .
If you're OK with a private copy instead of sharing the original array , note that the first step is to flatten the array , e.g. ` x_np = x_np.flatten() ` , because your example above doesn't work .
So I was looking into the np.random.norm() function and I understand mean and std .
` numpy ` has its own functions , like ` any ` and ` max ` and ` min ` ( these are just the ones that come to mind -- this is not meant to be exhaustive ) , which shadow built-ins .
You can always import specific functions directly ( ` from numpy import sin , cos ` etc . ) , and sometimes that makes some formulae look prettier .
Math percentile on histogram bins .
Percentiles are what they are yes , but the last ones are not correct due to the lack of points on the edge bins so maybe a good extrapolation could be a solution and that's the reason for proposing to delete the edge bin values and guess them with a regression ( which maybe could not be the best solution of course )
We can then get the histogram counts : #CODE
For each row in the array , I want to count the instances of True , and create a new item-wise array of the same shape as the original array in which each entry is the sum of all previous True instances in the row .
Basically you can use ` ndarray.cumsum ` along the second axis and prepend with all zeros column at the start .
A bit more efficient technique would be to initialize an output array with all zeros and then insert cumsumm-ed values into it , like so - #CODE
It's short for cumulative sum and you can give it an " axis " to work on , in your case try running : #CODE
The sum of each mix should be 100 .
numpy - resize array filling with 0
I would like to reshape and resize it to a new array with ` shape = ( 3 , 8) ` , filling the new values in each row with ` 0 ` .
Initialise an array of zeros : #CODE
You need to make sure your new X and Y ranges go over the same range as the old ones , just with a smaller stepsize .
So it is probably best for me to simply shift all indices to positive , as suggested by all_m ...
Thank you , I see now that I can solve now my problem quite simply by reindexing to positive indices .
I didn't know that there are problems with negative indices .
Instead , use ` numpy.sqrt ` which broadcasts the sqrt operation across every element in the array .
Finding largest indices of non-zero elements along each axis
Get the indices of non-zero elements with ` np.nonzero ` and stack them up in columns with ` np.column_stack ` and finally find the ` max ` along the columns with ` .max ( 0 )` .
Looks like there is a built-in function ` np.argwhere ` for getting indices of all non-zero elements stacked in a ` 2D ` array .
` argwhere ` is just ` np.nonzero ( A ) .T ` , the nonzero indices rearranged .
reshape the resulting array : #CODE
Since the inputs are 2D arrays , you can stack them along the third axis with ` np.dstack ` and then use ` np.nansum ` which would ensure ` NaNs ` are ignored , unless there are ` NaNs ` in both input arrays , in which case output would also have ` NaN ` .
After ensuring equality , I would like the order to be as random as possible yes , assuming there is more than 1 unique solution to this problem
@USER i don't think it does . that said , i'm replicating results in a published paper , and , should my results be subtly different from the ones reported , i don't want this to be the reason .
I would like to remove the duplicates in the list number and concatenate the corresponding data in cost and account together for each duplicate .
Is this just a reshape ?
Suppose aa = array ([[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]]) and suppose I append array ([[ 7 , 8 , 9 ] , [ 10 , 11 , 12 ] , [ 13 , 14 , 15 ]]) to aa .
Buffer the returns data inside module in a numpy array ( ret : shape = ( num_days ( D ) , num_instrs ( N )) with indices mapped to a ring buffer ( data for date di goes into di% ret.shape [ 0 ]) .
` groupby ` -wise you could do ` df.groupby ( " Planets ") [ " Rank "] .ffill() ` or ( what I might do , because it doesn't depend on where the non-nan value is ) ` df.groupby ( " Planets ") [ " Rank "] .transform ( max )` .
@USER the transform ( max ) trick is neat .
A completely different approach would be to use a dictionary comprehension to map the unique planets : #CODE
Translate reshape from Matlab to Python
I'm using numpy and I don't know how translate this MATLAB code to python : #CODE
but I don't know how translate all of the rows .
That means you need to index ` A ` with ` B - 1 ` , and then reshape your result as desired .
One potentially confusing piece : the ` -1 ` in the ` reshape ` method is a marker that indicates numpy should calculate the appropriate dimension to preserve the size of the array .
I need a list of the indices corresponding to where the array is equal 1 , 2 , 3 , ...
it should be possible to speed up the index search quite a lot , but I haven't been able to transfer the methods proposed there to my problem of getting the actual indices .
As an add-on : I want to store the indices later , for which it makes sense to use np.ravel_multi_index to reduce the size from saving 3 indices to only 1 , i.e. using : #CODE
So that funky ` [: , None , None , None ]` is ` reshape ( ..., -1 , 1 , 1 , 1 )` , is that right ?
You might make some progress by using a sparse matrix to store the locations of the unique values .
This takes advantage of fast code within the sparse matrix constructor to organize the data in a useful way , constructing a sparse matrix where row ` i ` contains just the indices where the flattened data equals ` i ` .
We can get the linear indices ( so similar to ` find ` in MATLAB ) corresponding to ` i = [ 0 .. N ]` with a call to ` numpy.argsort ` over the flattened array : #CODE
But then we get a single big array ; which indices belong to which ` i ` ?
We just split the indices array based on the counts of each ` i ` : #CODE
If you still need the 3D indices somewhere , you can use ` numpy.unravel_index ` .
I think the fastest way to use ` fromiter ` is to flatten the ` combinations ` with an idiomatic use of ` itertools.chain ` : #CODE
But it isn't a ` np.array ` , and does not have a ` reshape ` method .
You can squeeze a bit more performance out of ` np.fromiter ` by specifying the size of the final array , which can be computed using ` scipy.special.binom ( 6 , 2 )`
An alternative way to get every pairwise combination of ` N ` elements is to generate the indices of the upper triangle of an ` ( N , N )` matrix using ` np.triu_indices ( N , k=1 )` , e.g. : #CODE
in the template try using the dot notation ` {% for el in instance %} ` ` Name is {{ el.name }} " {{ el.offset }} "` ` {% endfor %} `
How about a roll , followed by vector operations on a view : ` x [ 1 :] ` .
And functions like ` append ` , ` insert ` and ` delete ` are probably late addons , conveniences for programmers who are more familiar with lists .
I ask because I eventually want to make sure this works by printing out the max value of the modified ` Late ` column , and make sure it shows 6 ( or any number less than 46 ) , not 48
when I try this and then print out the max value of ` df [ ' Late ']` , it gives me the correct value , not 48 .
However , when I try to display a histogram , it still looks like it did before , showing values for 46 and 48 : ` hplot = trainDF [ ' Late '] .hist ( bins=10 );
In my computer the histogram looks normal .
matplotlib and numpy - histogram bar color and normalization
1- I have a 2D histogram w 1D histograms along the x y axes .
These histograms total up their respective x and y values , while the main histogram totals the values in logarithmic x-y bins .
I've used pcolormesh to generate the 2D histogram ... and I've generated a color bar in the range vmin=1 , vmax=14 ...
I also want to color the 1D histogram bars according to the same normalization .
Look at x-axis histogram values around 10^4 ( or 10^6 ) ...
They are colored at the 1 / 2 way point in the color bar , not at the log scale point .
Maybe I should use np hist , but then I don't know how to do a matplotlib.bar plot with log scale and colored bars ( again , mapping the colors I use for the 2D hist ) .
I just realized I create norm , but don't use it !
I normalize the heights of the histogram rectangles by the bin width ( dbx ) and the comoving volume of my simulation ( cmvol ) .
I suggest you read more about broadcasting which is very often useful to vectorize computations in numpy : interestingly enough , ` a.transpose() - b ` wouldn't have raised an error ( you'd need to transpose the result again to obtain your desired output ) .
` a.transpose() - b ` would require another transpose to get the desired output , right ?
you may want to read the chapters in pandas docs about ` merge ` and ` join ` .
If records are aligned , you can truncate each table by minimal length of the two ; if no alignment is assumed , you should look at ` join ` method .
Read your files into Pandas with ` pandas.read_csv ` and join on the `' gene '` column .
One approach would be to get the places of shifts , where the numbers change and use those indices to split the input array into subarrays .
For finding those indices , you can use ` np.nonzero ` on a differentiated array and then use ` np.split ` for splitting , like so - #CODE
I want to create a function that slices depending on the parameters ` a ` and ` b ` ( where ` a ` and ` b ` are indices ) .
How to translate list operations like count , find and zip to numpy
Next up , I am assuming that you have ` new_gr ` initialized with zeros of shape ` ( height , width )` .
I like that broadcasted indices with one column array and another row array and also the fact that it's closer to original code .
How can I append this to find the nearest , smaller value ?
Here it's necessary to flip the order of your example array and then make the indices returned into negative integers for the fancy-indexing .
Update : replaced empty with zeros
I want to calculate the sum of different combinations of these numbers .
Currently ` Final_Product ` is of the shape ( 306 , 3743 , 138 8) , so I can just reshape to get there .
I think it's a succinct version of what ` dot ` does here .
Instead of ` dot ` you could use ` tensordot ` .
Note the ` transpose ` at the end to put the axes in the correct order .
Like ` dot ` , the ` tensordot ` function can call down to the fast BLAS / LAPACK libraries ( if you have them installed ) and so should be perform well for large arrays .
The ` zeros ` creates a ` ( 10 , 10 )` array , where each element has ` dtype ` defined by ` np.dtype ([( ' x ' , float ) , ( ' y ' , float )])` .
Do you mean Python core dumps ?
` sumC= numpy.sum ( numpy.absolute ( A ) , axis=0 )` treats ` A ` like an array ( it works with list ` A ` because internally ` absolute ` converts it to an array ) .
If you are seeking strict diagonal dominance :
If you are seeking non-strict diagonal dominance :
One option is to figure out which ones you want to delete , remove them from a flattened array , and then reshape it back to the correct shape .
Such a mask could easily have produced an array that couldn't be reshape back to 2d .
@USER That question doesn't include the part about not including the main diagonal .
@USER Yes , good eye .
I would simply like to add a column and populate with it with the ` cum . sum ` of the last three values of the ` mean ` column : #CODE
This is what is happening in your case , and noted from the comments , since the amount of ` unique ` values for the index are only ` 1695 ` , we can confirm this by testing the value of ` len ( df.index.unique() )` .
To populate an array , that is add to an array , you commonly use the ` append ` method and don't use assignment .
Your code should be changed to add the append method in order to save each files contents , I'll assume ` a ` is a list : #CODE
Too many indices for array
But I am getting the error for the last line : ` mat1 [ x , y , z ] = mat [ x , y , z ] + ( R**2 / U**2 ) **pf1 [ l , m , beta ]` : #CODE
If you however use ` np.convolve ` with the parameter `' same '` it returns an array which has the maximum length of f or g ( i.e. max ( |f| , |g| ) ) .
Furthermore you have to be sure that the dimensions of the matrices and the indices you use are correct , for example :
In this case they get to the range of the ` mat ` matrix .
Also be sure that the last variable indices you calculate ( l and m ) are within the dimensions of ` pf1 ` .
A small easy optimization you could do is instead of calculating the sin and cos several times , create a variable storing the value : #CODE
But now I get mat1 [ x , y , z ] = mat [ x , y , z ] + ( R**2 / U**2 ) **pf1 [ l , m , beta ]
But because the indices should be integers , hence the truncation .
I suggest you check those out : they work like the respective ` plot ` s , with a bit more , and they take care of the ` log ` s for you .
It's usually even better : they keep the * log scale * while preserving your original variables : if your ` x ` goes from 1 to 1000 then the final ` xticklabel ` s will not write ` log ( 1000 )` , but ` 1000 ` , and the ticks will be chosen logarithmically .
This is not the same as plotting ` log ` vs ` log ` , so it's up to your needs .
In that case , a workaround would be to get the set of " inverse " indices and use ` np.bincount ` to count the occurrences : #CODE
In that case , calling ` np.unique ( ..., True )` would correspond to ` np.unique ( ..., return_index=True )` which will give you the indices of the unique values rather than their counts as the second return variable , and therefore your CDF will be incorrect .
I'm trying to generate a vector ` [ a ( 1 ) , a ( 2 ) ,... a ( 100 )]` where ` a ( i )` is ` sin ( i )` if ` sin ( i ) 0 ` , and ` 0 ` otherwise .
What I came up with is creating a lambda expression and applying it through ` numpy.fromfunction ` , as such : ` np.fromfunction ( lambda i , j : sin ( j ) if np.sin ( j ) 0 else 0 , ( 1 , 100 ) , dtype=int )` , but this produces an error #CODE
you don't have to create zeros array , comparing using ` np.where ( b > 0 , b , 0 )` is enough
Finally , replace the ones that had no matches along that axis with the corresponding values in ` a ` .
` a ` has shape ` ( 2 , )` and it is broadcast over the second dimension .
numpy stack
As above recipe takes more than 30 minutes to build the stack from scratch , I am looking for optimization .
@USER You should use Anaconda for installing packages from SciPy stack ( IPython , NumPy , Matplotlib , ... )
python zero indexes . this means that the " first " element of an 869 element array is at 0 , and the last is at 868 . hence , referencing 869 will fail . use the search feature next time ; this question could have been fixed by simply looking at python documentation , or previous questions , rather than posting on stack .
when should i use hstack / vstack vs append vs concatenate vs column_stack
Taking a look at ` hstack ` we can see that it's actually just a wrapper around ` concatenate ` ( similarly with ` vstack ` and ` column_stack `) : #CODE
There's usually a better way to go than taking outer products .
I feel like it's not good advice to say " there's a better way to go than taking outer products " when you have no idea of what the users application is .
Sometimes there isn't anything better , because what you want is an outer product .
Unless you're writing a library's outer product function , you generally want to compute some function of the outer product , and you can frequently compute that without going through an explicit outer product .
Good point ... updated to use outer
Yep , that looks _almost_ like what OP wants -- I think the only thing left to do is to flatten it ( in the correct order ) to get exactly what OP wants .
outer is specifically multiplication . numpy , in general , is extremely fast ( relative to other python options , not speaking of other languages ) .
That includes outer products and inner ( matrix ) products , as well as element by element operations .
Second , what the small area data should sum to , at the subregional level .
don't know much about svm models , but maybe it's the ` gamma ` paramater ?
The absolute sign in the mean absolute error is in each entry in the sum , so you can't check whether ` mae 0 ` after you summed it up - you need to put it inside the sum !
Where ` np.absolute ( matrix )` calculates the absolute value element-wise .
I have a list of tuples , and I want to concatenate them to values stored in a numpy array .
Double sum in Python
I am now programming on BFGS algorithm , where I need to create a function with a doulbe sum .
I want to get a function that , for each instance ` X [ i ]` in ` X ` , and for each ` W [ j ]` in ` W ` , return a function of the sum of ` numpy.dot ( X [ i ] , W [ j ])` .
For example , ` X [ 1 ]` dot ` W [ 2 ]` shoulde be ` 2*3+2*3+2*3+2*3 `
When do you decide which parts of X and W you want to sum ?
The VALUE you name ` Func ` in your edit is computed by ` sum ([ np.dot ( x , w ) for x in X for w in W ])` or , more efficient , ` np.einsum ( ' ij , kj- ' , X , W )` .
Please do not mind the details of the formula , what I want to say is that , I use two ` sum ` here and just using ` i in rage ( 5 )` and ` t in range ( 3 )` to tell the code do the sums .
I was wondering whether there is a opposite of ` numpy.where ` ( going from booleans to indices ) which goes from indices to booleans ; for example ` numpy.there ` .
The ` zeros ` approach is much faster than the ` in1d ` , and faster than the sparse version .
At best , the in1d can have O ( val*log ( val ) + len ( A ) * log ( len ( A ))) ( sorting both first and then a linear walk through both arrays incrementing at least one index ) .
That is , a list of size n becomes an numpy.array of size n which than still takes O ( n log ( n )) to sort , even when the sorting is implemented in C .
I'm sorry , but a loop implemented in C and that same loop implemented in Python both should have the sample runtime complexity , even though Python may be a constant factor slower , at some point an O ( n ) Python implementation is going to be faster then a O ( n log ( n )) C implementation .
Wheels fix part of the problem , but my experience is that tools like ` pip ` / ` virtualenv ` are just not sufficient for installing and managing a scientific Python stack .
NumPy : matrix dot product incompatible with MATLAB
The MATLAB operation ` ang_stv '` is a conjugate complex transpose , while NumPy ` swapaxes ` is not .
I missed that the ' operator is the conj transpose , and .
' is simple transpose .
` * ` is not the dot product in MATLAB
* ` in MATLAB is * not * a dot-product , even though it has a dot .
'` operator in MATLAB means simple transpose , and `'` operator means transpose with complex conjugation .
+y ` , the dot means apply element by element .
Is there a ( cheap ! ) way to convert all ' signalling ' nans to quiet ones ?
python generate histogram / pareto chart on keywords
How do I generate a list / dictionary that I can plot into a histogram / Pareto chart , such as :
Are those numbers counts of occurrences of each unique word ?
To solve this , one way would be to store the indices corresponding to the mask and then use indexing .
Now , when I have somehow large input data ( larger than my memory ) , I need to fill the histogram chunk by chunk .
You could stack the individual histograms and give all of them the same face collor , so it would look like a single bar chart ...
Plotting the histogram directly with plt.hist and plotting with plt.plot + np.histogram should produce the same results .
Not too familiar with Python and need to translate some code .
I don't know what ` myNumpyArray [: , 0 ]` means and get compile error ` IndexError : too many indices ` .
Then I'm trying to assign some values to C via some formula ( ` C ( x )= ( exp ( -|x|^2 )` in this case ) .
Step #1 Get all combinations corresponding to all indices being calculated with ` np.array ( it.multi_index )` in the code .
Step #2 Perform the L2 norm calculations across all combinations in a vectorized manner .
Step #3 Finally do ` C ( x )= ( exp ( -|x|^2 )` in an elementwise manner .
So it looks like you just wan't to apply your operation to the indices of each element ?
I want to get back ` [ 4 , 5 ]` because the interval between the first set of ones is 4 indices , the interval between the second set has nans and is thus ignored , and the interval between the third and fourth ones is 5 indices .
You don't need to install anything , just dump Tifffile.py in your project .
I don't know a good way to sort or search that whole thing by the eigenvalues to pull out the correct eigenvectors ( all eigenvectors with eigenvalue 1 normalized by the sum of the vector entries . ) My thinking is to get the indices of the eigenvalues that equal 1 , and then pull those columns from the array of eigenvectors .
Your solution of searching the eigenvalues for the ones you want seems plausible enough .
w , vl = eig ( P , right=False , left=True ); tol = 1e-15 ; v1 = vl [: , abs ( w - 1 ) < tol ] .T .conj() ; assert numpy.allclose ( v1.dot ( P ) , v1 )
You linked to How do I find out eigenvectors corresponding to a particular eigenvalue of a matrix ? and said it doesn't compute the left eigenvector , but you can fix that by working with the transpose .
I think if you wanted a more general solution including sum constraints , you'd need to use minimise with explicit constraints / bounds in the following form , #CODE
From minimise , the sum is correct but the value of ` x ` is not quite right with ` np.dot ( A , x )= [ 69.75001902 , 59.25005706 ]` .
I want to optimize four parameters : ` alpha ` , ` beta ` , ` w ` and ` gamma ` .
You need to be taking the derivative of ` func ` with respect to each of the elements in your concatenated array of ` alpha , beta , w , gamma ` parameters , so ` func_grad ` ought to return a single 1D array of the same length as ` x0 ` ( i.e. 22 ) .
In particular , ` W ` is a ` ( 3 , 4 )` array , but ` df_w ` only returns a ` ( 4 , )` vector , and ` gamma ` is a ` ( 4 , )` vector whereas ` df_gamma ` only returns a scalar .
If you don't pass a gradient function to ` minimize ` then it will try to approximate it using first-order finite differences .
Either ` gamma ` is a ` ( 4 , )` vector or ` df_gamma ` is a scalar - it wouldn't make sense for both to be true .
I used chardet to detect the encoding of the string and tried to decode it as follows but no glory #CODE
What happens if the array contains only negative numbers and zeros ?
I need to do the same thing , but to sort only the non-zero numbers ; this method will return zeros as the highest values ...
So I am creating a grid of ones and zeros using numpy : #CODE
Positive smoothing factor defined for estimation condition : ` sum (( w [ i ] * ( z [ i ] -s ( x [ i ] , y [ i ]))) **2 , axis=0 ) = s Default s=len ( w )` which should be a good value if ` 1 / w [ i ]` is an estimate of the standard deviation of ` z [ i ]` .
I would like to sum the columns to yield : #CODE
In order to do that you can use the sum method of numpy .
You could use ` np.random.choice ` with the optional argument ` replace ` set as ` False ` to have ` unique random ` IDs from ` 0 ` to ` m* m-1 ` that could be set to ` ones ` in a ` zeros ` initialized ` 2D ` array using ` linear indexing ` with ` np.put ` .
Unfortunately , finding the zeros of a nonlinear multivariate function is far from easy , so you have to get your hands dirty sooner or later .
They both have zeros along some hyperbolic-like curves .
this shows a quite different picture , with a finite support for the zeros of the radial component .
I screwed up the original plots , so the second component has a bit different zeros as I thought .
The problem is when you try to append values to any of the empty numpy arrays nothing happens .
How can I append values to my 2x2 numpy array one at a time ?
If you would like to add the elements one at a time you would have to add them to a temporary 1D array in groups of 4 and then reshape the temporary array into a ( 2 , 2 , 1 ) shape and then append the full temporary 2x2 numpy array to the empty one , essentially going from a 2 , 2 , 0 shape to a 2 , 2 , 1 shape .
You might want to consider using lists for parts of your code where your data structures need to grow dynamically , or find a way to preallocate your array and then fill it instead of using ` append ` .
It's superficial similarity to list append is deceiving .
Your reshape changes the shape , but not the number of elements - still 2*2*0=0 .
` np.append ` is just an alternate front end to ` concatenate ` .
` append ` is for Python users who persist in thinking in list terms .
When using my real data , arrays are generally ~200 mb ( 5,000 * 10,000 instead of 3 * 3 ) , and there are hundreds of unique IDs .
My " gut instinct " is that this could be done faster if I didn't have to iterate over every unique ID value and repeat the lookup and operations , but I have no idea if that's actually true or how I would go about it .
These numeric IDs would be useful both in ` mean ` and ` std ` calculations .
Now , for mean calculations , those numeric IDs could be used as `" weights "` for binning with ` np.bincount ` , giving us the sum of data elements corresponding to each ` ID ` .
For ` std ` calculations , one way would be to sort the IDs such that the identical IDs are placed consecutively .
Your std approach ( which could be generalized to other np functions ) might be too memory intensive , as the IDs have the potential to be highly asymetrical .
I find this approach to clear and straightforward , and it resulted in a 40X speedup when running a test with a 5000 * 5000 array and 500 unique IDs .
After creating a ` numpy ` array , I'm looking to append post creation : #CODE
' numpy.int32 ' object has no attribute ' append '
Is there any way to create a set of arrays you can later append additional values to with numpy ?
Internally , I believe , ` tensordot ` uses same sort of ` transpose ` and ` reshape ` as you do .
In this case , there are numerous ways to reshape and transpose and after many trials I found out the listed one to be the most optimized .
There's only 2 levels of looping , and sum expression is different , but I think it has the same issues : #CODE
Therefore ` 1 + shape [ 1 ]` is adding one to the second dimension of ` X ` , giving the size of a new 1-D array filled with zeros when you do ` np.zeros ( 1 + X.shape [ 1 ])` .
Furthermore , numpy doesn't seem to have a log function that accepts an arbitrary base .
But it can't handle the large number in the first place without FIRST taking the log at base r .
Since ` ln ( a*b ) = ln ( a ) + ln ( b )` , ` ln ( c**d ) = d*ln ( c )` , and ` ln ( exp ( e*f )) = e*f ` , you can rewrite your function as : #CODE
The issue is that NP doesn't seem to have a direct way of calculating an arbitrary base log .
However , I found that @USER ' s solution works great ( happened to help a colleague who's working on a similar project in Matlab , too ! ) , so the log function isn't necessary .
For example , it's not clear to me the ` A ` and ` b ` is before or after adding * ones * ( 1s ) .
SLSQP will handle both nonnegativity constrains and the sum constraint . nnls will solve the problem more efficiently .
If you view the condition that ` sum ( x )= 1 ` as a third equation ( see linked [ question ] ( #URL ) ) then it can be solved exactly with ` x =[ - 59.9125 , - 99.525 , 160.4375 ]` .
@USER I agree that x =[ 0 , 0 , 1 ] does not satisfy Ax =b , but the problem we are solving is to minimize the norm of Ax-b subject to x > 0 and sum ( x )= 1 constraints .
Do you mean that x =[ 0 , 0 , 1 ] does not minimize the norm when you say it is clearly incorrect ?
These are algebraicly equivalent , and usually D is a suitable diagonal matrix .
You can , for instance , choose D to have the inverses of b on the diagonal , which will make Db =[ 1 , 1 ] ^T .
What I want to do is to concatenate them horizontally resulting in #CODE
But fundamentally it is syntatic sugar for ` concatenate ` .
The issue is that since ` m ` has only one dimension , its transpose is still the same .
You need to make it have shape ( 1 , 3 ) instead of ( 3 , ) before you take the transpose .
Trivially , ` M * x == 0 ` would be true for a vector of all zeros , but I assume that's not what you want .
I would use the pandas DatetimeIndex to accumulate the values for each date .
Lining up the sizes of the trailing axes of these arrays according to the broadcast rules ,
` from numpy.numpy.dual import eig `
2 ) Counted the unique occurrences of cell values #CODE
print ( x ) prints all unique cell values .
How many total unique values do you have in these rows that will result in new columns ?
I only ask , because if you have thousands of rows with thousands of unique values , you're going to end up with an exponential amount of columns and it'll become unwieldy rather quickly regardless of the technique for creating the new columns .
We can verify that object in the ` fields ' field of ` registers [ 0 ]` is the ` regfield ` array by looking at their ids .
Question 1 : How can I automatically fill in my missing data with the corresponding values of X and Y and two zeros ?
That means you initialize an aggregator to an empty list prior to a ` for ` loop , and each time through the loop , append to the aggregator .
The referred question is correct -- if you work through the calculation for the residual sum of squares and the total sum of squares , you get the same value as sklearn : #CODE
The residual is the difference between the predicted and observed value and its variance is the sum of squares of this difference .
If I understand , you want to substract each pixel the min of its 3-horizontal neighbourhood .
Avoid underflow using exp and minimum positive float128 in numpy
` w ( i ) / ( sum ( w ( j ))` where ` w ` are updated using an exponential decreasing function , i.e. ` w ( i ) = w ( i ) * exp ( -k )` , ` k ` being a positive parameter .
When working with exponentially small numbers it's usually better to work in log space .
For example , ` log ( w*exp ( -k )) = log ( w ) - k ` , which won't have any over / underflow problems unless k is itself exponentially large or w is zero .
Then , when doing the sum , you factor out the largest term : #CODE
` np.logaddexp ` is also a ufunc , so you can use ` np.logaddexp.reduce ` to sum along an axis of an array
Python xray - is it possible to append a table ?
I've been using the ` .append() ` method to concatenate two tables ( with the same fields ) in pandas .
Xray doesn't have an append method because its data structures are built on top of NumPy's non-resizable arrays , so we cannot append new elements without copying the entire array .
Hence , we don't implement an ` append ` method .
For what it's worth , pandas has the same limitation : the ` append ` method does indeed copy entire dataframes each time it is used .
I guess I could do the same via several ` numpy.swapaxes ` and ` numpy.newaxis ` ( for broadcast dimensions ) , ` numpy.reshape ` but is there some simpler or more direct way , just like ` dimshuffle ` ?
The function ` numpy.transpose ` permits any permutation of the axes of an array .
In numpy , this can be achieved using a combination of ` transpose ` and ` reshape ` .
Note that in numpy , care is taken to make ` transpose ` return a view on the data whenever possible .
Numpy advanced indexing using a 2D array of row indices without broadcasting the output
I have an ndarray ` array ` with ndim 3 , and some indices ndarray ` idxs ` with ndim 2 , which specify indices for the first dimension of ` array ` .
Create a 2D grid of indices corresponding to row indexing : ` idxs [ i1 , i0 ]` and use a ` N x 1 ` array for column indexing .
When indexed into ` array ` like that , the column indices would be ` broadcasted ` to the shape of the row indices .
The corresponding set of column indices into ` array ` are just the sequence integers between 0 and ` idxs.shape [ 0 ] - 1 ` ( which happens to be the same as ` array.shape [ 1 ] - 1 ` in this case , but need not be in general ) .
` idxs.T ` is interpreted as a 2D array of row indices , so if ` array ` was an ` ( m , )` 1D array then ` array [ idxs.T ]` would have shape ` ( m , n )` ( since you're sampling multiple times from each row ) .
To collapse the ' existing ' column dimension you need to give it another 1D vector of indices .
Also , how do you plan to sum the list of arrays along the 0-th dimension ?
The argument ` axis = 0 ` is necessary because otherwise ` sum ` will add up all the elements in the 2D ` sample ` array rather than adding each column separately .
What one could do is implementing his / her own linear solver based on Gauss elimination , which relies on sum , subtraction , multiplication and division , all of which are well-defined and executed exactly with fraction objects ( as long as the numerators and denominators fit in the data type , which I think is arbitrarily long ) .
where _F denotes the frobenius norm
But I don't want to use mean but I would like something of the type mean squared error or gradient solution .
I would like to quickly convert every string in the column to a list of integers , where a unique integer represent each character in the string .
All indices of a numpy array should be in a single ` [ ]` separated by commas ( ie ` tab [ i , j ]`) , if you provide just one index you get an array with the first dimension equal to that index ( a row in this case ) .
Probably the easiest approach to installing Numpy ( and the rest of the Python ' scientific stack ' including SciPy an many others ) is to use Continuum Analytics ' " Anaconda " tools .
I have read that local variables are handled better by python functions ; how does this translate to class constructs ?
This is basically ` modulo operation ` and thus could be simulated with ` np.mod ` to give us ` index2 ` as column indices at each iteration .
Then , we index into the ` k-th ` row of ` mat2 ` and use column indices from the previous step to get all elements needed for our purpose .
These elements are compared with the threshold ( ` -1e10 ` in our case ) , giving us a mask , which is used to select elements from that row and set the corresponding ones into the output array ` mat1 ` after scaling with ` mat2 [ k ] [ index1 ]` .
Also , as mentioned in the comments , if ` mat2 ` has all its values initialized to ` -2e10 ` , then ` mat2 [ k ] [ index2 ] -1e10 ` would never be ` true ` , so in that particular case , ` mat1 ` would keep its ` zeros ` .
Next up , if you need the indices that would pass the condition , use ` np.where ` or ` np.argwhere ` on the mask .
Query Pandas DataFrame distinctly with multiple conditions by unique row values
I did not have any luck with ` .groupby ( " user ")` , because it mixed users although they are unique row values .
Maybe a better workflow solution is to dump the DataFrame into a MongoDB instead of pursuing a solution with Pandas to perform the analysis in this case .
To change the interval , translate the x values from [ -1 , 1 ] to [ a , b ] using , say , #CODE
How can I collapse these 3 matrices into one megastructure that is the same indices and columns labels ( m x n matrix ) .
In addition , dot product is [ matrix multiplication ] ( #URL ) .
Here the code - it is fitting a Lasso to my dataset , performing model evaluation in the outer loop , model selection in the inner loop ( where it is finding optimal alpha values for the lasso ): #CODE
There has to be an inner loop for finding the optimal parameter , and an outer loop with a left out fold for model evaluation .
Do you need each element of the sum , or the * partial * sum up to a given index ?
If I understand correctly and you want to plot the partial sum of your series , then you can use #CODE
This will define a ` lambda ` named ` parsum ` , which accepts the ` n ` values for which it will compute the partial sum up to the given ` n ` .
I actually am having a similar problem with an dot product using a pairwise sum , which should definitely give me a speed up but is actually between 300% and 1000% slower than numpy.dot .
I personally could not confidently construct a CUDA code that could do a vector add , or elementwise multiply , or dot product , that was any faster than a CPU , assuming this is the only function being performed , and the timing methodology is as you have shown here , i.e. including the data transfer cost .
Hi I am running the fft function in the simulator on iOS platform
How ever with numpy fft I get this result : #CODE
Thanks for the reference , but the question would remain : How to find all variables with identical ids ?
IDs are unique to each object ; there are no two objects with identical IDs .
Plus ` shares_memory ` doesn't check ids , which what your title specifies .
find max , min from a .txt -file which has multiple columns
I want to find the max / min . values using a Python script .
Or do you have any idea how to start an alternative script which puts out the max .
/ min . values ?
When ` b ` is added to that , broadcasting applies , which in effect does an " outer sum " ( i.e. adds all the pairwise combinations ) , forming an array with shape ( 3 , 4 ): #CODE
One possible and only memory not time intensive solution is using ` np.repeat ` and ` np.resize ` to repeat and resize the arrays a and b to the size of the resulting shape first and then simply add those two arrays .
you can use outer method of ufunc : #CODE
I've been attempting this using numpy's mgrid , to create n dimensional arrays , flattening them , and then making arrays that use the same elements from each array to create new ones .
For the special case where k=2 it's often faster to use the indices of the upper triangle of an n x n matrix , e.g. : #CODE
if I wanted to keep the first 10% of an array as it is and change the remaining 90% of the array to zeros ?
I also tried just plotting the log of the runtime vs . the log of the problem size , but looking for a subtle change in slope is non-trivial ( because of cache effects , etc . as the problems get larger ) .
operands could not be broadcast together with shapes ( 31 , 31 ) ( 0 , )
print " c " when you run it with ` cov = False ` or plot your input data along with the fit and tell us what you find , please .
In #URL compares and contrasts ` reshape ` and ` newaxis ` .
I'm not sure it can be explained any more the OP's ` reshape ` version can .
If you want to continue using ` reshape ` ( which is also fine for this purpose ) , it's worth remembering that you can use -1 for ( at most ) one axis to have NumPy figure out what the correct length should be instead : #CODE
and I am now iterating through the numpy array and converting those numbers to the ones corresponding in the dictionary like this : #CODE
@USER Well in this case , ` searchsorted ` is basically looking for places or indices where elements from ` message ` exists in the keys of ` codes ` .
Those indices are then used to index into the values of codes to give us the string NumPy array output .
It has to convert them to absolute values
I guess one solution is to manually zoom in on sections of it using the GUI and save / join those images , but I would really prefer to not do something like that !
At the very least it would probably make more sense to truncate it ( see the ` truncate_mode ` parameter ) .
What does the log say ?
thx . but the log is too long .
Finally , the dot product : #CODE
How comes the result of the last dot product is exact ?
In fact , using a different root for the zs ( say sqrt ( 2 )) , yields perturbated results .
because I need to be able to define the indices in a separate step unfortunately .
It is not returning unique values for each time .
Below code I use to dump arrays to a csv .
I want to truncate the date so that i have only last monday of each timestamp #CODE
I tried doing it using concatenate but I get errors .
How to set the min and max of the original array in sklearn.preprocessing.MinMaxScaler
You could create an array of zeros ( ` np.zeros `) , then replace the rows with your ` a ` and ` b ` .
This is a more direct approach ( even with more than 2 elements , you could always do ` max_len = max ([ len ( row ) for row in I ])`) .
@USER , yes it may be faster but you would still have to then fill all the values with zeros for the shorter arrays , so I think you would lose that benefit .
From the docs : "` empty ` , unlike ` zeros ` , does not set the array values to zero , and may therefore be marginally faster .
Initializing to zeros handles the final ` else ` case .
The same basic idea should apply , but perhaps instead of using a boolean masking array , using the valid indices themselves would be better ...
I created a new empty folder , placed three ` csv ` files in it named ` file_1.csv ` , ` file_2.csv ` , and ` file_3.csv ` , each of which has the value ` 1 ` , ` 2 ` , and ` 3 ` as unique value ( without header ) .
Looks like a starting point is to reshape it , for example #CODE
Using @USER ' s transpose : #CODE
I think you can achieve your desired result using two ` reshape ` operations and a ` transpose ` : #CODE
@USER ' s first ` reshape ` operation gives us this : #CODE
The easiest way to achieve this sort of thing is to first ` transpose ` the
Finally I can use ` reshape ( 2 , -1 )` to collapse the array over the last two dimensions .
The normal way to create a sparse matrix is to construct three 1d arrays , with the nonzero values , and their ` i ` and ` j ` indexes .
The coordinates don't have to be in order , so you could construct the arrays for the 2 nonzero mask cases and concatenate them .
Even through ` deltax ` has lots of zeros , it is probably fastest to make it dense .
Or I could concatenate the ` res1 ` and ` res2 ` , etc and make one sparse matrix : #CODE
Here I choose the masks so the nonzero values don't overlap , but both methods work if they did .
It's a delibrate design feature of the ` coo ` format that values for repeated indices are summed .
In the code that I just added , I used ` np.where ` to get the coordinates of the nonzero items .
You have two indices ( i=current_date and j=temp_date , presumably with some mapping -- right now temp_date isn't sorted , so it's not clear ) and the value those indices specify .
Another option would be to resize ` temp_date ` so that for every ` current_date ` , there are 500 ` temp_dates `
Edit : if you have duplicate indices , unstacking won't work , and you probably want a ` pivot_table ` instead : #CODE
Now extract the numpy array , and reshape to [ nrows x ncols x 1 ] as you specified in the question : #CODE
I get ` ValueError : Index contains duplicate entries , cannot reshape ` when trying to unstack .
Create block diagonal numpy array from a given numpy array
I would like to arrange them into a bigger array having the smaller ones on the diagonal .
It should be possible to specify how often the starting matrix should be on the diagonal .
So if I wanted this array 2 times on the diagonal the desired output would be : #CODE
but the problem that i didn't know how to arrange and reshape my array
It sounds as though your real problem is that you don't understand how to reshape numpy arrays .
or by calling ` reshape ` on the output : #CODE
Do you want the indices where the values are less than 200 , or the values less than 200 ?
Shows indices ... got it .
` numpy.where ` returns the indices .
fast way to transpose and concat csv files in python ?
I am trying to transpose multiple files of the same format and concatinating them into 1 big CSV file .
This is my code to transpose and merge all csv files into 1 big file : #CODE
The 4 arguments are self and the 3 explicit ones .
So in the above , the correctly classified objects are on the diagonal , but if you look at , say , row 3 , column 0 , it looks like two " class 3 " objects were misclassified as " class 0 " objects .
If I set the tolerance for each column as ` col1 = 1 ` , ` col2 = 2 ` , and ` col3 = 10 ` , I would want a function such that it would output the indices in ` a ` and ` b ` that are within their respective tolerance , for example : #CODE
You can see by the indices , that element 2 of ` a ` is within the tolerances of element 1 of ` b ` .
I recently had to get the last set status for certain items , labeled with ids .
I found this answer : Python : How can I get Rows which have the max value of the group to which they belong ?
However I do not need to get all max values , only the last one .
I think you have to provide the bounds like ` ( ( min , max ) , ( min , max ) )`
Therefore you can observe a pattern : there are ` s-1 ` number of overlapping mini ` 2x2 ` ones matrices .
I was thinking of creating a ` 2x2 ` ones matrix and then by using a dynamic referencing ( for loop ? ) like ` B [: -1 , : -1 ] = ones_matrix ` , where ` B ` is the zeros matrix of size ` sxs ` .
And I can't figure out a way to do that dynamically for ` n ` -sized zeros matrix .
Python , how to reshape a " linear " data set format into a matrix-like data set format
Within each array values are unique , and each is initially unsorted .
I would like the indices to each that yield their sorted intersection .
Then the sorted intersection of these is ` [ 4 , 5 , 11 ]` , and so we want the indices that turn each of x and y into that array , so we want it to return : #CODE
Use ` np.unique ` to get the unique values and corresponding indices in ` x ` and ` y ` separately : #CODE
Find the intersection of the unique values using ` np.intersect1d ` : #CODE
Finally , use ` np.in1d ` to select only the indices that correspond to unique values in ` x ` or ` y ` that also happen to be in the intersection of ` x ` and ` y ` : #CODE
I initially missed the fact that in your case both ` x ` and ` y ` contain only unique values .
Here's a more realistic test case , where ` x ` and ` y ` both contain unique ( but partially overlapping ) values : #CODE
How do I append the list into this output file ?
I thought that perhaps imsave was stretching the values so that the max value showed up as 255 in the image , but I have other images created with imsave that have a max below 255 .
How to use a dictionary to translate / replace elements of an array ?
There are only 26 pairs in the dictionary ` transdict ` , but there are hundreds of letters in the numpy array I must translate .
` AttributeError : ' numpy.ndarray ' object has no attribute ' translate '`
I get a broadcasting error : ` ValueError : shape mismatch : objects cannot be broadcast to a single shape `
Fine ... but you might wait around for other ideas , especially the pure numpy ones .
Is there a way to now actually create a matrix in a similar fashion , in which the diagonal values are in themselves matrices .
Suppose also I would like to have some other matrix of the same sizes as the ones before , but this matrix would run the diagonals above and below ` A ` like so : #CODE
[ ` Create block diagonal numpy array from a given numpy array `] ( #URL ) might help ?
The key is to use the ` np.eye ( N , M=N , k )` , where ` k ` specifies the diagonal of the identity matrix .
The clip I modified did nothing with the keys , so I'm ignoring those ( so far ) .
However , then you will have to be careful to append new data coming from the next chunk to ` pairs ` .
This gives you an array of first index indices and then column indices ( this is the least efficient part of this numpy version ): #CODE
You can use the same trick in pandas but with stack to get the index / columns natively : #CODE
Here , S.row and S.col are arrays of row and column indices , S.data is the array of values at those indices .
` Splot ` is a ` N / 2 + 1 x 1 ` 2D matrix , yet you are trying to squeeze 101 elements into a single element in this matrix .
Could you translate for us ?
To find the indices of the ` True ` items you simply need to use ` nonzero() ` .
Why not using argmin on the absolute value of z1 and z2 ?
With this option , the result will broadcast correctly
I have the indices of the positions of non-zero elements in a sparse matrix in python in the form #CODE
How does using just the nonzero elements change the final answer ?
` np.nonzero ` gives you the indexes of the nonzero elements .
for what it's worth , ` nonzero ` returns the array I copied from your post at the start : #CODE
The sparse matrix also has a ` nonzero ` method : #CODE
I found out you can use boolean array indices in python so the following does what I wanted to achieve : #CODE
Is there a good way to resize numpy structured dtypes without having to rebuild the entire dtype every time ?
The basic idea is that I want to resize the data type of a specific name .
` reshape ` and ` transpose ` affect how it handles the elements of ` X ` , but do nothing internally to those elements .
The ones that I've examined end copying data field by field from old to new - and recursively working its way down nested dtypes if needed .
Option 2 : I iterate only over the lower triangular part ( without diagonal ) , then add the transpose ( because covariance matrices are symmetric ) and then add the diagonal .
cov ( Xs , Xt ) = min{s , t } ?
If you are familiar with Gaussian processes , the covariance matrix of Brownian motion for example has entry ( i , j ) take the value min ( i , j ) .
You can use ` vstack ` / ` hstack ` / ` column_stack ` / ` tile ` and many other similar functions for this .
In other words , operate directly on arrays of the indices of your output .
Since your cumulative percentile values are increasing linearly , and since the size of the array is evenly divisible by 5 , a trivial solution for the example you gave would be to just split ` my_cumulative_percentile ` into 5 equal chunks , e.g. ` np.split ( my_cumulative_percentile , 5 )` , or to get the corresponding indices , ` np.split ( np.arange ( my_cumulative_percentile.shape [ 0 ]) , 5 )` .
The following will split an array ( ` arr `) into a list of arrays ( ` chunks `) where the sum of each array in ` chunks ` is roughly equal .
We can observe the split indices are not equally spaced : #CODE
On ` Machine2 ` , I have the following cron job run every hour + 5 min : #CODE
Transpose then reshape : #CODE
Edit : alternatively , you can reshape then transpose : #CODE
I've not timed , but maybe something like ` sum ( f ( x , x+1 ) for x in range ( n-1 ))` will actually be faster ?
@USER actually your code is not producing the desired result , as the function needs to accumulate results
You have 2 equations in 3 unkowns so there is no unique solution .
Take dot product of first and middle entry , second and middle+1 entries until middle-1 and last entry python / numpy
I have a 81x990000x3 array and for each of the 81 entries ; I need to sum the dot product of the first entry with the 495000th entry ( the middle ) , the second with the 495001st entry and so on until the 494999th entry with the last entry .
Can you divide your array and two and than dot product them ?
If I took two arrays , 495000x3 and a 3x495000 array and used the dot product the array would be 495000x495000 and that gives " array is too big .
Doubtless you can even remove the outer loop as well ( though in your case it probably won't speed things up much ): #CODE
No need to transpose the last array , by doing so you risk a copy .
numpy max vs amax vs maximum
( Similarly for ` min ` vs . ` amin ` vs . ` minimum `)
On the second question : I guess ` amax ` * could * be made into a ufunc , although the main purpose of ufuncs is to allow operations to be broadcast between arrays .
There seems little need to make ` max ` a unary ufunc .
I have written a simple bit of code to loop through all dicoms in a directory , get the pixel counts and pass them to a numpy array and flatten the array .
@USER - The repeated linear search ` data [: , 0 ]= =i ` can be avoided though , to lower the complexity from O ( n**2 ) to O ( n log ( n )) or even O ( n ) .
I don't see a file in the question , not even something labeled as a clip from a file .
the line df [ ' ID '] = '"' + df.ID + '"' gives me the error TypeError : cannot concatenate ' str ' and ' numpy.ndarray ' objects .
Here the ` squeeze ` function is squeezing out a dimension , to convert the one-column group summary stats ` Dataframe ` into a ` Series ` .
Spyder console lock-up caused by " IndexError : too many indices for array "
The code was part of a function being used to resize some arrays .
Numpy cuts zeros at end of a number
I've noticed that when a python list ( of float numbers ) is converted into a numpy array , trailing significant zeros are removed .
Do you know why numpy cuts out the trailing zeros ?
Any idea how to force numpy to keep the zeros ?
Once you convert it to the numeric value , the trailing zeros are not recoverable .
The reason I want the zeros is that I have a list of len ( 20000 ) file names with a sample name : AN-1.20780-BC .
Obviously , if during the sorting the zeros are removed I end up with NEW-1.2078-BC instead of NEW-1.20780-BC .
My initial idea was to convert the dataframe to a matrix using ` df.as_matrix() ` , pick out the unique IDs , an compute distances from the matrix .
If you need to get the length of a total path for each unique pair you could do #CODE
you can calculate the difference between ` x ` and ` y ` by using the ` diff ` function which will produce ` na ` for the first columns but you can drop it easily using ` dropna ` function .
I just was obsessed on understanding the ` numpy.ma ` class , specifically on creating arrays filled by zeros and ones .
I'm doing this by creating a zero array and looping over it to increment each element , and by building a list of zeros with .append() and then looping over it a second time to increment each element .
It was also about small NumPy arrays , rather than large ones .
` TypeError : Required argument ' mat ' ( pos 2 ) not found `
I have found very nice solution ( and vectorized version ) for binomial tree and I am trying to change it for a trinomial case .
I'm not using binomial or BS price in this case , because my assigment is to do it with trinomial tree .
I am trying to calculate the running median , mean and std of a large array .
That ` cumsum ` trick is specific to finding ` sum ` or ` average ` values and don't think you can extend it simply to get ` median ` and ` std ` values .
One approach to perform a generic ` ufunc ` operation in a sliding / running window on a ` 1D ` array would be to create a series of 1D sliding windows-based indices stacked as a 2D array and then apply the ` ufunc ` along the stacking axis .
For getting those indices , you can use ` broadcasting ` .
For running ` median ` and ` std ` , just replace ` np.mean ` with ` np.median ` and ` np.std ` respectively .
In order to estimate mean and standard deviation of a given sample set there exists incremental algorithms ( std , mean ) which helps you to keep the computational load low and do it online estimation .
In fact you never under- or oversample test set , you might however , in some cases - add weights to particular classes to make a correction for true priors ( which might be different from the empircal ones ) or due to cost sensitive learning .
There's a good chance that it can be modified to accept a 2D array as the ` X ` parameter and broadcast over the columns .
This ` T ` and ` X ` broadcast together just fine , for example ` T*X ` works .
` vectorize ` can make it easier to ' broadcast ' values , but it does not speed things up much .
By contour I mean a line going from left to right and passing through the points where intensity gradient is maximal .
The ` #column ` cannot be determined unless I use ` reshape ` .
I tried numpy where but it returns the indices , not the values .
Finding the median of a histogram
A have a dataframe like this which represents a histogram , with each bin size being .003 .
I want to find the median value in the histogram , but I am unsure how .
The median should be where half of the area of the histogram lies to the left , and half of the area to the right .
My histogram looks like this :
the histogram looks like this :
Aren't you simply calculating the median of the bin edges and discard the information about the histogram ?
2 ) Iteratively sum the normalized counts until you have accumulated more than 0.5 , return the average between the last and this value as approximation to the median .
I understand , I think I understand what you were hinting at , find the cumsum and then write a function to iterate through the value column until .51 of the cum sum is met and that should be approximately the median
It's fairly easy to roll your own file-reader in Python , rather than having to rely on the constraints of ` numpy.loadtxt ` : #CODE
If you give it ` dtype=None ` ( which tells it to figure out the data type from the file ) , it returns a third column containing all zeros .
Python , scale float range [ 0 , 1 ) to [ min , max )
to another range where 0.0 from this example is transformed to min and 0.8 is transformed to max .
Cython and numba have no awareness of the internal workings of numpy , and can't do anything to optimize numpy functions - you would have to write your own low-level loops over the array to compute the norm .
Simply generating the array takes 1 / 4 the time the norm takes .
And most of the time spent in ` svd ` , which produces an ( N , 8 ) array : #CODE
So if you want to speed up the ` norm ` , you have look further into speeding up this ` svd ` .
` svd ` uses ` np.linalg._umath_linalg ` functions - that is a ` .so ` file - compiled .
` np.linalg.norm ( A , ord=2 )` computes the spectral norm by finding the largest singular value using SVD .
However , since your 8x8 submatrices are Hermitian , their largest singular values will be equal to the maximum of their absolute eigenvalues ( see here ): #CODE
The spectral norm of a hermitian matrix is the maximum of the absolute values of the eigenvalues , whether or not the matrix is positive definite .
If your data don't consist of integer indices between 0 and the number of unique values , you can convert them into this format using ` np.unique ` : #CODE
After calling that , you can sum the rows .
First , create an array ` p ` of positive values whose rows sum to 1 : #CODE
` entr ` uses the natural logarithm , so to get the base-2 log , divide the result by ` log ( 2 )` .
Change your definition of ` signal ` to ` signal = zeros ( time.size )` .
I want to get all unique combinations of a numpy.array vector ( or a pandas.Series ) .
No because It doesn't provide unique combinations !
If you mean vector array , it is unique because it's elements are random float64 numbers .
For each unique element in FIDO , we want to calculate the average color of that region within the base images .
Note , the original version gets faster if there are a small number of unique values in ` FIDO ` .
Also , for this application , most of the time is actually spent traversing the data , so I wanted to be able to update the sum rather than just count .
IndexError : arrays used as indices must be of integer ( or boolean ) type `
N-1 ` range , where ` N ` =num unique FI DO: #CODE
NOTE : The cython approach is much faster , as instead of iterating the number of unique FIDO ` N ` and for each of them scan the image ( size ` M = Width x Height `) this only iterates the image ONCE .
` ValueError : operands could not be broadcast together with shapes ( 38890 ) ( 40000 ) `
IndexError : arrays used as indices must be of integer ( or boolean ) type `
Your array has a lot more zeros than ones .
If the last n - rnk ( A ) coefficients in x ' are to be zeros , this imposes that
It's been a while since I've spent a ton of time with numpy , but IIRC , you should be able to drop at least the outer loop by using ` Ellipsis ` : #CODE
This actually doesn't work as is , in the original post , the np.sum() produces a single value , which is replicated into the data_new array in indices #URL With the ellipsis , the tmp should produce a 1d , shape [ Nt , 1 ] array , which would then be put into data_new in a space of shape [ Nt , jend-jstart +1 ] , so I think I would have to manually replicate the 1d array
The .ravel will only flatten the data and produce a scalar , but really the 1d array that normally results has distinct values and needs to be replicated .
original zeros must remain .
First we figure out how many numbers we need to flip ( trying our best to get close to the right value ) then we randomly choose ` n ` of the nonzero indices , and finally we set them to zero .
I don't get empty indices list with ` n < 0 ` ( ` np.__version__ == ' 1.8.2 '`)
I get empty indices with n=0 , -2 , -3 , -4 ,... etc , but not -1 !
If you do not need to keep the existing zeros as zeros , and simply want the entire array to have an average of 20% 1's , could you not go through your array with a " for " loop , and for each element call randint ( 1 , 5 ) .
If however you wish to retain all the original zeros , that means you want to reduce the number of 1's to 40% of the number there now , so go through the array , if the number is 1 , call randint ( 1 , 5 ) and if it returns 1 or 2 , retain the original 1 , else change it to zero .
Also , why does numpy's implementation of ` np.outer ` not match the definition of outer multiplication on Wikipedia ?
Your understanding of the relationship between inner and dot / transpose seems correct .
You cannot write the outer product in terms of np.dot for matrix , at least in an easy way .
Numpy doesn't really do the outer ( tensor9 product , what numpy does is to transform your matrix in a vector and them it makes the outer product of those vectors .
An easier way to see how the outer product work is to use vectors instead of matrix .
The outer product is defined on vectors , not matrices .
The Wiki outer link mostly talks about vectors , 1d arrays .
Notice that the Wiki outer does not involve summation .
Inner does , in this example ` 5 ` is the sum of the 3 diagonal values of the outer .
` dot ` also involves summation - all the products followed summation along a specific axis .
Some of the wiki outer equations use explicit indices .
I'm looking to understand outer though !
@USER ` np.outer ` just ravels n-dimensional arrays and then takes the conventional 1D outer product .
N-dimensional ` outer ` isn't well defined .
where ` np.where ` returns an array of row indices that satisfy the
Understanding gradient of gradient descent algorithm in Numpy
I'm trying to figure out the python code for multivariate gradient descent algorithm , and have found several several implementations like this : #CODE
From the definition of gradient descent , the expression of gradient descent is :
` hypothesis - y ` is the first part of the square loss ' gradient ( as a vector form for each component ) , and this is set to the ` loss ` variable .
` xTrans ` is the transpose of ` x ` , so if we dot product these two we get the sum of their components ' products .
I tried some other transpose thing that didn't work , not sure why I didn't try that one .
How do I reshape it to ( 73257 , 1024 ) so that every image is unrolled in a row ?
You need to swap axes , moving the 73257 dimension to the front , then reshape .
As suggested in the comments , first get every image in a column , then transpose : #CODE
I already adressed in my original question how I attempted to solve my problem ( histogram and pdf ) and also stated in the final two paragraphs that I was mainly interested in finding those sample points .
" list indices must be integer , not float "
[ edit ] changed numbers to avoid confusing ID with indices
The first two columns of ` d ` are the indices into ` set1 ` at which you want to place the values in the third column of ` d ` , after a small correction .
If the first column of ` d ` are not indices ...
In the general case where the first column of ` d ` are not indices , you'll need to look up the location of each entry of ` d [: , 0 ]` in ` a ` .
What if the first colum of ` d ` is not the indices but rather the ID , in such a way that the first entry of the row in ` d ` has to be equal to the first entry of ` set1 ` .
I would like to reshape this into an array of shape ` 208*208 ` but in chunks of ` 26*26 ` .
since ` 43264=208*208 ` you could reshape your array to a shape of ` 26*208*208 ` but not ` 208*208 * 26*26 `
Use reshape to split it into blocks of 5 #CODE
and use transpose to reorder dimensions , and in effect reorder those rows #CODE
If ` x ` is already a numpy array , these transpose and reshape operations are cheap ( time wise ) .
The idea is that I have to use the unique dates inside a for loop to do things .
In that case you would need to make ` x ` and ` y ` the unique x , y coordinates for each column / row in the grid , then reshape ` u ` and ` v ` so that they each have dimensions ` ( ny , nx )` .
So I just tried it now , and strangely enough my first assignment of ` tmp ` ( ` tmp = ( N11+N00+N10+N01 ) .values `) came out as all zeros , so I had to re-enter the line , which produced the correct values .
The premise of the routine is to convolute the fourier transform of exp ( -i*PhaseAngle ) with your interferogram .
-Take the real ( one sided ) fft of the resulting data
EDIT : A lot of what this question boils down to is how should I arrange my signal ( that has a known , constant , phase error of 0.3 ) before applying the fft such that the imaginary part of my result is a constant 0.3 ?
And further , what type of fft in python should I use to achieve this result ?
You can take the " real " fft as python describes it , because I'm only interested in the positive frequency information in the fft domain .
My understanding is that the only difference between numpys fft and rfft is that the fft gives you both the negative and positive frequency information , while the rfft only gives the positive half .
Something else that I just discovered is that the fft is dropping data points .
I think the error is somewhere in the fft or phase angle generator part of the code , but I'm not sure .
Assuming each ` Location ` has a unique ` Identifier ` , a join using the `' inner '` strategy should do what you're asking for : #CODE
I have tried applying an unsharp filter , using histogram equalization , and just using grayscale into edge detection .
or transpose : #CODE
Objective : to create a unique and random number for each row within the constructed field ' Birth ' .
Each row in the matrix is considered a unique " value " , and you want to count how many times you see each " value " .
reshape list of numpy arrays and then reshape back
I want to reshape this list of arrays into a numpy vector and then change each element in the vector and then reshape it back to the original list of arrays .
Of course , you lose the information about your array sizes when you flatten , so you need to store them somewhere ( here , in ` dims `) .
In turn , beta variate could be sampled as pair of gamma variates : #CODE
So small parameters are moved from being first in gamma ( if you sample Dirichlet directly via gamma variates ) to being second .
So , when you would be doing ` max_absolute ` , it would give ` NaN ` as ouptut , instead of the actual ` max ` .
The following code will load the seismic traces , transpose them , and plot them .
Objective : To format ` [ ' Birth Month ']` with leading zeros
Mine had the diagonal of 1s , and lots of ` nan ` on the off diagonal .
Your ` norm ` function returns a 1x1 sparse matrix .
For this pair , this ` dot ` is 0 , but it still a 1x1 sparse matrix : #CODE
Rewriting the ` norm ` to handle sparse matrices correctly is one .
I only use ` ndarray ` when the normal ` zeros ` , ` ones ` , ` empty ` don't work .
Bottlepy return an numpy image after resize an image
You can use column indices ( letters ) like this : #CODE
You could just as easily read that file with pure python ; e.g. something like ` with open ( ' tile ') as f : a = [ line.strip ( ' \n ') for line in f.readlines() if not line.startswith ( ' # ')]`
( my text editor is set to strip trailing blanks , hence the ragged lines ) .
I think ` np.loadtxt ( " tile " , dtype =b ytes , delimiter= " \n ") .astype ( str )` might work , but I agree completely with the overkill point .
My objective is , for every unique ` id ` , I would like to know the ` min ` and ` max ` dates so that I could calculate the time delta in days .
I am currently using ` pandas.groupby ` to calculate the arrays of max and min dates : #CODE
@USER : I , for one , was not aware that the numbers after the ` X ` were supposed to be indices .
Here's an example of using your ` coeffs ` to fill in an array , with value indices derived from the dictionary keys : #CODE
The keys in this dictionary are ` ( i , j )` tuples , the indexes of the nonzero elements .
The interpretation of the numbers as indices was nice .
I am not sure why I can not concatenate multiple dimensions of an array together using the the numpy.concatenate function .
Is there a way I can get the 2 arrays to concatenate both axes to yield a shape of ( 2400,240 0 ) ?
I am confused by these words " concatenate multiple dimensions of an array together " .
The real question is how many times you want to concatenate each array and in which dimension or if you want to zero pad : #CODE
Which would be accomplished by using numpy to create an array of zeros and concatenating in the appropriate direction on both A and B before concatenating the results together .
i receive the following error : ` ValueError : could not broadcast input array from shape ( 417,417 ) into shape ( 417 , 1 )`
I was thinking of algebraic versions of the cross and dot products in the accepted answer .
But this clip loads multple files and joins them .
Your formula is a bit difficult to extrapolate to higher dimensions , but the derivation can be used to re-express it in terms of dot products , which would make it 100% applicable to the original question .
Which of course loses the indices .
I get a list of 4096 values which I reshape into a 64x64 array then plot the array .
This all worked fine and I got the figure that I was looking for , and made sure that it worked with my stack cleared .
However , now when I run it without changing anything other than the range of the integration , also with a cleared stack , the figure doesn't generate properly .
Instead of being a full figure window , the window appear and the image I want is situated in the bottom left corner , really small and I cannot resize it , zoom in , move it about or anything .
Next the remaining 1808 values are put into the buffer for the inplace division but the first row has already changed : ` arr [ 0 ]` is now simply a view of a row of ones , so the values in the latter columns will just be divided by one .
This compact representation might be all you need , but if you want the full square matrix you can allocate a zeros matrix of the right size and copy ` arr ` into it using ` np.triu_indices_from ` , or you can use ` scipy.spatial.distance.squareform ` : #CODE
I think you can just stack the valid numbers and nan's back together : #CODE
Your choice of kernels will greatly affect SVM performance in scikit-learn , as well as parameters gamma and C .
Or I could turn ` xxx ` into a structured array , and append that : #CODE
Finding indices of matches of one array in another array
A conatains unique values and B is a sub-array of A .
But that is shape ( 3 , 1 ); so reshape : #CODE
However , python will try to evaluate the dot product of x and y , since now they are both np arrays and It will report #CODE
For instance if f ( a , b ) = np.sum ( sin ( a * b )) .
In this case , you'll want to add a new axes to give numpy some hints about where to broadcast : #CODE
If you'd like the sums along the second axis , you can sum like this : #CODE
I want to make a ` Dictionary of Dictionaries ` where the outer key is column " a " , the inner key is column " d " , and the value is " e " .
But for what you want to do in this specific situation , you can transpose , then gather 0 , 2 , 4 , and then transpose back .
The point is that SWIG has no way of knowing what size the argout array ` sum ` will be .
In your case the function ` cancel ` allocates the memory for the array ` sum ` , assuming the caller has some way of figuring out the size .
I'm using the built in Python method ` min ` : #CODE
Also ` min ` doesn't understand array-like comparisons hence your error
One NumPythonic vectorized solution would be to create sliding windows across the entire length of the input array ` measurement ` stacked as a 2D array , then index into the array with those indices to form a 2D array version of ` measurement ` .
` I ` is the imaginary unit ` sqrt ( -1 )` .
The median could be calculated with bisection ; sort the ` ( key , count )` pairs by key , sum the counts , and bisect the half-way point into a accumulated sum of the counts .
@USER : then your option is to calculate the mean yourself : ` sum ( key * count for key , count in counter.iteritems() ) / sum ( counter.itervalues() , 0.0 )` for Python 2 .
The mean is just the sum of all numbers divided by their count , so that's very simple : #CODE
I am trying to find the indicies in a python numpy array that is 700x700 in order to find the number with the same indices in a separate array .
My ` lat_end ` and ` lon_end ` are the indices that I am finding in the ` temp_mask ` array .
Also the calls to ` numpy.roll ` have been replaced by taking the modulus of the indices .
So I can't reach my aim ... and I can't do a reshape when I begin to dimensions ( ... , ... , 1 ) #CODE
So I know that it is supposed to be ` ( 2 , 3 , 4 )` I can easily reshape the result .
Are there always the same number of rows per unique ID , or do some IDs occur more frequently than others ?
OK , so to clarify , the output must be either a list , tuple , or np.object array , rather than an ( ids , rows , cols ) homogeneous array .
Why would you suggest an ( ids , rows , cols ) homogeneous array instead ?
I am assuming ` List_IDs ` is a list of all unique IDs from the first column .
( not just inner and outer joins ) .
That is , I want to take all the positive values , and reverse within the positive values , leaving the trailing zeros in place .
how to sum over certain row of a data frame in Python
I have a data frame ` A ` , and I would like to sum over the rows that their row index value has a number greater or equal 10 .
sum has an axis argument , pass axis=1 to sum over rows : #CODE
Thanks , but I want to sum over rows not columns .
I have added the argument ` axis=0 ` , but the thing is I only want the sum of row 2 and 3 .
You can use normal slice indexing to select the rows you want to sum over : #CODE
It would make more sense to sum over only the last three columns : #CODE
What is the most efficient way to generate all possible portfolios subject to the criteria that the sum of every combination of instruments must be = 1 ?
@USER - thanks , but this doesn't address the boundary condition that only combinations which sum to 1 are relevant and the list structure , which is a product of different minimums and maximums .
On the other hand , itertools.product will generate all 100 cases and then you have to filter out those combinations whose sum is > 100 .
For some inputs itertools.product may be faster since it is written in C , but it doesn't do any pruning of cases based on the sum of the current selections .
Aren't you checking every combination to see if the sum is greater than 1 ?
Once any partial sum is > 1 we ignore any portfolio with that initial sequence .
In python , the ` print ` statement will append a newline after each call , whereas the ` write ` method on an open file object will not .
In addition , each point should add a weight factor to the residual , thus making the sum of square reach global minimum : #CODE
Here's a good example of implementing gradient descent with numpy : #URL
So although the amount of steps used is for example 108000 , only a fraction of the simulations end up being that long , so I ended up needing to trim the zeros away from the arrays with np.trim_zeros() .
In the latter case you will probably have to sort the ` Value ` column by ascending ` Y ` and ` X ` coordinates , then reshape it into a 2D array ([ see here ] ( #URL )) .
This would be " the unique point where the weighted relative position of the distributed mass sums to zero or the point where if a force is applied causes it to move in direction of force without rotation .
` giant = max ( nx.connected_component_subgraphs ( G ) , key=len )`
I need to remove repeating vectors so that it is an array of unique vectors and to compare all of these vectors .
I know that this comparison can be done in lists , but I have not found a way to append full vectors to a list .
This is the result ( I've marked unique vectors with unique letters ): #CODE
I need the end result to be an array of unique vectors , ( so in this case it'll be vectors a-u ie , 21 vectors ) .
John , the " Find unique rows " link you mentioned also has a number of vstack solutions .
Efficiently calculating sum of squared differences
This is a loop for extracting the RGB values of two images , and calculating the sum of squared differences across all three channels .
sum gives me 1000000000000+
I assume I need to flatten my data into something like : #CODE
concatenate assumes that all the arrays are the same size , which may always be the case for you , otherwise check out something like #URL
I think with " if " and a " for loop " it is possible to do that but i dont know how to append the results ...
The ids from ` Elements ` as a vector ; make it ` int ` for easy comparison : #CODE
Similar ids from ` nodes ` : #CODE
And magically we can now match up node ids with elements ids .
Just out of curiousity , I figured how to use the Elements ids without raveling : #CODE
The absolute fastest way to do this is to write an extension module in pure C and use the buffer protocol to access the data directly .
The ` sum ` inside the ` log ` simply needs the appropriate axis specified : #CODE
It should be a whole lot faster if you didn't go the extra mile of zipping and then summing up each dimension instead of using numpy's sum function : #CODE
This gets me the sum of all red combined in original - all red combined in mutated .
@USER You did confirm on the previous question's solution that it worked with sum : ` (( Orginal - Mutated ) **2 ) .sum() ` .
Also you do you use that clip to write many arrays to a file ?
I can understand repeatedly opening a file in append mode , or performing multiple ` savetxt ` with one open file .
You are asking two things : how to sum across all rows , and how to read the last row .
You don't need to iterate to sum along each column #CODE
gives you a ( 100 , ) array , one sum per column .
Numpy reshape sub-list
But is there a way to reshape sub-arrays without doing a for-loop ?
Using a 32-bit float array the mean computed by ` sum ` is 20% off .
I am really new to numpy , so I am having some troubles understanding the dot product .
I am a little confused , because it seems that a shape of ` ( 10 , )` is not a column vector , because the transpose is the same .
yes like [ 1 ., 1 ., 1 ., 1 ., 1 . ] dot ([[ 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 .,,,, ] ,
For example , I have this 7 by 7 array of zeros : #CODE
And I want to place this 2 by 2 array of ones on row 2 and column 3 of the previous array , resulting in : #CODE
But if you've got that for some reason , you can use ` .tolist() ` to convert a numpy array to a list and simultaneously change the types to Python-native ones .
The elements in ` jagged_slice_of_a ` are the diagonal elements of ` a [: , entries_of_interest ]`
so the line ` ts [ mask ] = np.interp ( np.flatnonzero ( mask ) , np.flatnonzero ( ~mask ) , ts [ ~mask ])` simply interpolates known values onto ` NaN ` s ( which are identified in previous line , and their indices are coded in the ` mask `) .
sampling multinomial from small log probability vectors in numpy / scipy
Is there a function in numpy / scipy that lets you sample multinomial from a vector of small log probabilities , without losing precision ?
You're dividing a probability by a log probability , which makes no sense .
If you do that , you find ` a = [ 1 , 0 ]` and your multinomial sampler works as expected up to floating point precision in the second probability .
That said , if you still need more precision and performance is not as much of a concern , one way you could make progress is by implementing a multinomial sampler from scratch , and then modifying this to work at higher precision .
NumPy's multinomial function is implemented in Cython , and essentially performs a loop over a number of binomial samples and combines them into a multinomial sample .
Another way you might implement a multinomial sampler is to generate N uniform random values , then compute the histogram with bins defined by the cumulative probabilities : #CODE
The main trick is to realize that the log of uniform random deviates is equivalent to the negative of exponential random deviates , and so you can do everything above without ever leaving log space : #CODE
The resulting multinomial draws will maintain precision even for very small values in the p array .
If you dig into the source , you can trace this issue to the binomial function upon which the multinomial function is built .
The problem is that the ` binomial ` function chokes on very small values of ` p ` this is because the algorithm computes the value ` ( 1 - p )` , so the value of ` p ` is limited by floating-point precision .
Well , it turns out that for small values of p , the Poisson distribution is an extremely good approximation of the binomial distribution , and the implementation doesn't have these issues .
So we can build a robust multinomial function based on a robust binomial sampler that switches to a Poisson sampler at small p : #CODE
The first entry is nonzero and near 10 as expected !
We see that even for these very small probabilities , the multinomial values are turning up in the right proportion .
The result is a very robust and very fast approximation to the multinomial distribution for small ` p ` .
how would one adapt the cython function to work in log space ?
The whole line of code returns the percentage of subarrays in ` test_y ` that has a max value equal to the value in the corresponding position in the array returned from ` self.predictor ( test_x )` .
Lets take a simplified version of the problem , taking a cumulative sum until a given threshold is reached : #CODE
It seems the appropriate c-structure to use is std :: vector , but I can't figure out how to export the final product back into python .
Suppose I have a 3D numpy array of nonzero values and `" background " = 0 ` .
First , I would like to find the " border voxels " ( all nonzero values that have a zero within their ` 3x3x3 ` neigbourhood ) .
Second , I would like to replace all border voxels with the mean of their nonzero neighbours .
I messed one thing up : I would like to replace all border voxels with the mean of their nonzero AND non-border neighbours .
Should I set ` sqrt ( 3 )` ( 3D case ) as radius to get the 26-neighbourhood ?
2 ) I'm not using euclidean distance anywhere - the ` p= np.inf ` in ` query_ball_point ` means to use the infinity ( a.k.a. max ) norm , so ` p= np.inf , r=1 ` means to find all points that are * at most one pixel / voxel from the query point in any direction* .
My mistake was keeping the default parameters ( not infinity norm ) , so thanks again .
I would like to sum the values from the " Value " column when multiple criteria are met .
So , after that you need to reshape : ` .reshape ( -1 , n-1 )` as stated in the function ` def cut_valid_array ` .
Edit : For reference my trace log is as follows
stack just adds the column .
` column_stack ` and ` hstack ` require the same transpose .
From the numpy documentation , if you give just one array as input , it should return the indices where the array is non-zero ( i.e. " True ") :
indices where condition is True .
But if try it , it returns me a tuple of two elements , where the first is the wanted list of indices , and the second is a null element : #CODE
Indeed , to get the wanted list of indices I have to add the indexing , as in ` np.where ( array 4 ) [ 0 ]` , which seems ...
so that you need to do some indexing to access the actual array of indices : #CODE
Or if you don't like the column vector , you could transpose it again #CODE
Lots of ways of taking an array out the ` where ` tuple ( ` [ 0 ]` , ` i , = ` , ` transpose ` , ` array ` , etc ) .
A two-dimensional array has two indices , so the result of ` np.where ` is a length-2 tuple containing the relevant indices .
I have a dictionary dump out as csv as below .
you don't need to manually broadcast , simply add them and it will work `` Series ( pd.to_timedelta ( sample_size , unit= ' D ') + date )``
The expected output of scipy.optimize.check_grad is the the 2-norm of the difference between the gradient and the finite difference approximation of gradient .
I am unable to understand from the output whether my gradient is good enough or not .
Your gradient function returns a vector , lets call it ` g ` , and scipy numerical method gives you good ( although costly ) estimate ` v ` .
In other words , it is a square root of the squared errors of your gradient summed over all dimensions .
Depends on the function and gradient dimension .
When you take the log of it you get a NaN .
Whilst xnx gave you the answer as to why ` curve_fit ` failed here I thought I'd suggest a different way of approaching the problem of fitting your functional form which doesn't rely on a gradient descent ( and therefore a reasonable initial guess )
Note that if you take the log of the function that you are fitting you get the form
Which is linear in each of the unknown parameters ( log A , alpha , B )
log y = M p
Where log y is a column vector of the log of your ydata points , p is a column vector of the unknown parameters and M is the matrix ` [[ 1 ] , [ log x ] , [ x ]]`
Masked arrays allow you to continue working with many functions like ` mean ` or ` std ` #CODE
It's just a sum over k indices .
Declaring the loop indices as ` cdef int ji , jj , jk , jn ` made all the difference !
In that question , I sought to sum values in a numpy structured array based on multiple criteria , including matches in a list .
Now I am trying to figure out how to modify this if I have underscore delimited values in my array and I want to include the row in my sum if there is a partial match to the critieriaList .
Another thought - join the elements into one big multiline string , and use ` re ` .
I tried ` np.reshape ` but since the current shape of ` values ` is ( 1 , 1 , 1 ) I can't directly reshape the array in a 6x2 one .
I can reshape this easily with ` values.reshape ( 6 , 2 )` ... what error do you get ?
The outer one has dtype object .
Several of the length-1 dimensions you don't want , you're * explicitly adding * with that ` reshape ` call .
To remove dimensions of length one , the best approach is to use the ` squeeze ` method either as ` A.squeeze() ` or ` np.squeeze ( A )` , i.e : #CODE
If your ` values ` array is really what you've said , then it should also be fine to use ` reshape ` #CODE
If you're getting an error trying to ` reshape ` ` values ` , is it possible it is actually a ` list ` instead of an ` array ` ?
` reshape ` and ` squeeze ` work on a ` ( 1 , 1 , 1 , 6 , 2 )` array , but not on a ` ( 6 , 2 )` nested inside a ` ( 1 , 1 , 1 )` .
To run your ' how I did it ' clip , I have to make some guesses as to the inputs ( that almost merits a downvote ) .
But this reshape should produce a ` ( n , 1 , 1 )` array , not your ` ( 1 , 1 , 1 ,... )` array .
Singular dimensions ( value 1 ) can be removed with indexing , ` [ 0 ]` , or ` squeeze ` .
` reshape ` also removes demensions ( or adds them ) , but you have to pay attention to the total number of elements .
But reshape does not operate across ` dtype ` boundaries .
You can use cel's info if you concatenate X and Y , although that will not be efficient since you will be computing X-to-X and Y-to-Y distances along the way .
Isn't this just the square of the dot product of line XY ?
By nature this is probably a pretty brutal operation , like if you're trying to get like a Minkowski sum or some other set of vectors or something similar , that kind of necessitates actually iterating over all the points .
Numpy , replace a broadcast by iteration
Why does there appear to be a gradient ?
So calling ` flatten ` on a NumPy matrix returns another 2D matrix , albeit one with shape ` ( 1 , N )` : #CODE
If you convert the matrix to an array , then ` flatten ` will return a 1D array : #CODE
ii ) the sum of each column is 1 .
The results do not sum to 1 ( only 10 samples included below ): #CODE
I am trying to populate a surface of potential portfolios in which each asset has a unique correlation to the others .
Generating N uniform random numbers that sum to M
I'm attempting to use the itemfreq() function from scipy to count the frequencies of unique elements in numpy arrays .
Fitting a distribution given the histogram using scipy
In my case , because the histogram has integer bins of size 1 , I know that I can extrapolate my data in the following way : #CODE
And building the histogram using : #CODE
Yet , given that I already have the histogram built , I would like to avoid extrapolating the data and use orig_hist to fit the distribution , but I don't know if it is possible to use it directly in the fitting procedure .
As it happens , the histogram is enough for the former .
I might be misunderstanding something , but I believe that fitting to the histogram is exactly what you should do : you're trying to approximate the probability density .
And the histogram is as close as you can get to the underlying probability density .
There are two main problems with this histogram .
Furthermore , your histogram clearly only contains a part of the distribution .
The faint upside of introducing ` A ` is that you don't have to normalize your histogram .
Have you tried using ` curve_fit ` on the actual histogram ?
I normalized the histogram , dividing it by its sum , then tried ` curve_fit ` , and the result was the same ` 1.00000001 ` I got when using the dummy histogram of the example in my question .
Are your actual data more distributed according to Weibull ( i.e. does your histogram tend to 0 as x -> 0 , at least if that is how weibull_min looks like ) ?
The following array is an example of my data ( histogram ): ` array ([ 23 ., 14 ., 13 ., 12 ., 12 ., 12 ., 11 ., 11 ., 11 ., 11 ., 10 ., 10 ., 10 ., 9 ., 9 ., 8 ., 8 ., 8 ., 8 ., 8 ., 8 ., 8 ., 8 ., 8 ., 8 ., 8 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 7 ., 6 ., 6 ., 6 ., 6 ., 6 ., 6 ., 6 ., 6 ., 6 ., 6 ., 6 . ] , dtype=float32 )`
Also , as you suspected , the histogram is only a part my distribution .
I would define the logistic equation symbolically , compute the Lyapunov exponent directly from the definition , and do the numerical calculations as you do now , except use ` sympy.lambdify ` to convert the symbolic expressions into numeric ones that can be evaluated with numpy and plotted with matplotlib .
Athough just for the record , it appears the fastest way is probably how John La Rooy suggested ( using transpose then flatten ): #CODE
I don't have the gradient of f and I don't know the shape .
( These value lists are evidently a result of using default dictionary with a list append . It's a nice way of building a dictionary , but unfortunately , for an array we have to unpack it . ) #CODE
To flatten the list within list , use intertools chain #CODE
fft : why my main peak is lower than the side peak ?
I did the following discrete fft transform .
For each hour I am calculating the sum of ` BSs ` for that hour divided by the number of samples in that hour .
Python - interpolation 2D irregular points with Griddata / Delaunay tri
Using fftw fft correctly
My answer to your previous question [ here ] ( #URL ) already shows how to do this - you need to pass a vector of indices as the third argument to [ ` scipy.ndimage.measurements.center_of_mass `] ( #URL ) specifying which labels to process .
I am using numpy genfromtxt to read in CSV data files which I subsequently stack into a single structured numpy array .
Because of this , when I try and stack the data I get a " TypeError : Incompatible type " for the field with all missing data .
then the arrays have a common ` dtype ` and I can concatenate ( no need for the ` RF ` version ): #CODE
Then , for example , I calculate the gradient of the scalars with respect to x ( or some other processing function ) to get some array ( s ) containing scalar values .
Here I calculate use np.gradient , which returns two 2d arrays containing gradient values with respect to x and with respect to y : #CODE
Ignoring the gradient data for a minute , I want to find the contour Z1 = 0.2 #CODE
However , ` scipy.ndimage.map_coordinates ` expects array indices , and cannot be passed the x and y meshgrid arrays !
... which is basically zero , as expected for the gradient at this point on the surface .
Numpy array show only unique rows
I want to have the rows of an array , which are unique .
Contrary to numpy's ` unique ` function , I want to exclude all rows , which occur more than once .
@USER True , but once you can get the unique rows with ` np.unique ` it would be trivial to exclude the duplicated items by returning the item counts using ` np.unique ( ..., return_counts=True )` and filtering the unique row indices based on that .
I want to take some rows ( if they meet certain condition I check for in my real code ) and stack them on b .
There's no need to iterate over the rows of your array to stack them onto ` b ` , as ` vstack ` alone will work fine .
I need to iterate over the matrix because in my real code I am checking some conditions for each element of each row , and if those conditions are met , I add the rows to the stack .
Then rather than performing a ` vstack ` for each iteration , I recommend appending to a list , and doing just one stack .
Appending to a list and then performing the array join or creation is common numpy practice .
But ` rand() ` can't be optimized by ` numba ` ( in older versions of ` numba ` , at least . In newer versions it can , provided that ` rand ` is called without arguments ) .
Multiplication with the transpose of that copy does not work
The solution proposed in the other answer is to replace the ` matrix ` multiplications with the equivalent ones for ` np.array ` ( ` np.dot `) .
Then , using broadcasting , you can remove the outer 2 loops .
By using ` np.where ` we choose ( based on the ` threshold `) whether we want to get the corresponding value of ` data ` , or , again , the broadcast value of ` CM_tilde ` .
It might help in vectorizing code this way by looking at the implicit indices of your temporary variables .
I was thinking of passing 3 arrays ( vectors ) to ` SparseFeatures ` , the ` data ` , ` indices ` , ` indptr ` attributes , or the ` data ` , ` rows ` , ` cols ` attributes of the ` coo ` format .
i.e. the coordinates and values of the nonzero elements of the matrix .
replace min value to another in numpy array
Are you interested in small , almost trivial arrays that fit on a human-readable line , or big ones ( 1000s of elements ) ?
The key is to ensure that you are storing the array as a plain python list while inside your object so you can ensure you can do a yaml dump .
A possibly better alternative is to use the built-in ` dump ` functionality provided by numpy .
Assuming your object is storing only ` a_array ` as a member , you could do the following : ` a = A() ; dump ( a.a_array.tolist() , default_flow_style=False )` .
If I use ` tolist ` to turn an array into a list that ` yaml ` can dump , I get : #CODE
I demo it both as standalong yaml dump and as part of your object formatting .
In this case you can efficiently get the total sum of luminance in the square surrounding the pixel using the Integral Image .
In C++ ( compiled with g++ -o log . / log.cpp -std=c++11 -O3 ): #CODE
which has fast log functions , and when compiled as a numpy ufunc , runs in 20 ms , which is great , but the loss in accuracy is significant .
%timeit numexpr.evaluate ( ' log ( drr )')
It doesn't do ` log2 ` directly , so use ` log ( a ) / log ( 2.0 )`
@USER Since taking the log is an elementwise operation you only need the values stored in the sparse array , which you can easily access via its ` .data ` attribute .
numpy get values in array of arrays of arrays for array of indices
and indices array : #CODE
You could reshape the array to a ` 10x10 ` , then use slicing to pick the first 4 elements of each row .
Then flatten the reshaped , sliced array : #CODE
I want to broadcast over all the dimensions marked ?, of which I don't know the number a-priori .
To start off , reshape ` A ` to a ` 3D ` array with the trailing non-matching dimensions merged as one dimension , then perform the broadcasted elementwise multiplication with ` B ` and finally reshape back the product to original shape of ` A ` .
IndexError : too many indices
Simple example : I got a list called ' mylist ' and I want to accumulate the numbers inside and save them into a new list called ' mylist_accum ' .
Rows in array a represent lower and higher indices of the subset of interest in every row of b : " from the first row in b , I'm interested in the subset [ 2:7 ] , from the second row I'm interested in the subset [ 3:6 ] , and from the third and last row the subset [ 2:8 ] .
How would you produce c using indices in a ?
Why are log2 and log1p so much faster than log and log10 ?
[ this answer ] ( #URL ) in math SE seems to say that some methods reduce by ` log2 ` for calculating any log . this may mean that the implementation of the log functions of np depend , in one way or another , on log2 and / or ln ( x+1 ) .
that probably has to do with the relative simplicity of the [ taylor series of ` log ( x+1 )`] ( #URL )
I just tried to test with C code and glibc , using log2 / log10 / log and didn't see any difference really .
Looks like your numpy install is grabbing its log / log2 implementation from two different places which is odd .
bit hacks : Find integer log base 10 of an integer
bit hacks : Find the log base 2 of an N-bit integer in O ( lg ( N ))
The integer log base 10 is computed by first using one of the
techniques above for finding the log base 2 .
sped up ( on machines with fast memory access ) by modifying the log
would require a total of only 9 operations to find the log base 10 ,
I'm almost certain that any implementation of log to the base e function can be made as fast as the log2 function , because to convert one to the other you require a single division by a constant .
For double precision , in ` glibc ` , the ` log ` function is implementing a completely different algo ( compared to ` log2 `) from IBM from ~2001 , which was included in their ` libultim ` library .
In contrast , for single precision , both functions ` log ` and ` log2 ` are the same apart from division by ` ln2 ` in the ` log2 ` case , hence the same speed .
Does ` pandas ` support a ` numpy ` -like transpose ?
What is the most idiomatic way to run this function ` n ` times and stack these together to a 3 dimensional 2x3xn array ?
Proper way to " append " multi-dimensional numpy arrays ?
for very large cases you could see if ` vstack ` ( or concatenate axis=0 ) is faster .
Why is an aggregate of zeros counted as an object , and is there away around it ?
It does essentially the same thing , but always treats zeros in the input array as background , and assigns them a label of zero in the output .
For some reason the cavity is counted though it is zeros .
The problem was , when label is applied on the matrix with a background=0 the 0 values are changed to -1 and if I have a group of ones they are reduced to 0.Therefore by adding a 1 , I adjust the img object to background=0 and any group of numbers that is not 0 will be counted .
Third , how do I turn the array above ( or any array produced by a similar process , into a ` ( 4 , 3 )` array such that , in this case , three out of four rows are all zeros .
Why is vector dot product slower with scipy's sparse csr_matrix than numpy's dense array ?
I have a situation in which I need to extract a single row from a sparse matrix and take its dot product with a dense row .
This is surprising to me because I expected that sparse dot product would involve significantly fewer operations .
Also ` dot ` gets deligated to fast numeric libraries .
Just to be clear , you are testing a very sparse vector ( 1 nonzero out of 10000 ) times a dense vector of the same size .
When I vary the number of nonzero values in ` u ` ( and the dense equivalent ) , there is no variation in the timings .
This results in s 3.5 * speedup and is faster than dense dot product .
On modern processors , computing vector dot products is very fast , so the overheads dominate the computing time in this case .
Numpy sum elements in array based on its value
I have array with zeros of desired values : #CODE
This is because I was doing floor division , ` ngrps // ncols ` , rather than ceiling division , ` - ( -ngrps // ncols )` .
In that case , group by ` Type ` first , create a figure and subplots within this outer loop , then iterate over ` SourceID ` s and fill the subplots .
@USER Then what you're asking for doesn't really make sense - if ` x ` and ` y ` are the unique column and row values for an ` ( ny , nx )` 2D grid , then there must be at least ` nx * ny ` ` z ` -values .
If your ` x ` and ` y ` arrays already contain all of the locations in your grid then there's no point in finding the unique ` x ` and ` y ` values and generating another 2D grid from them using ` np.meshgrid ` .
Instead , you just want to sort your ` z ` -values by ascending ` y ` and ` x ` , then reshape them into a 2D array ([ see here , for example ] ( #URL )) .
Yes , the question / answer I linked you to shows how to reshape your ` z ` data in order to achieve this .
I have created a grid of zeros that I can increment for all values in the array : #CODE
Note that ` unique ` is probably a faster way to count occurrences of ` x , y ` pairs .
Then , use ` unique ` to return the unique values and counts for the values in ` xy ` .
` unique ( whr , return_index=True )` will find those for you as well !
It looks like that the Code didn't transpose the Data .
doesn't seem to transpose my Data .
To get the values I needed I used pop() in the append method : #CODE
For down-sampling N-dimensional images with integer factors by applying the arithmetic sum or mean , see ` skimage.measure.local_sum ` and ` skimage.transform.downscale_local_mean ` , respectively .
In Python I have a numpy array ` X ` of values ` xi ` and I would like to create an array ` F ` , where each entry is the sum over all other entries of ` X ` .
hmh , change your point of view : the first contains the sum of ` all but x1 ` , the second ` all but x2 ` and so on .
pandas Dataframe groupby , sort groups by absolute value
Hi everyone I basically want to find an efficient way to sort the grouped data by the absolute value .
Consider simply creating an absolute value column through a defined function , apply the function on a groupby , and then sorting item ascending and absolute value descending .
But then reshape commands gives an error saying ` ValueError : total size of new array must be unchanged ` .
performing outer addition with numpy
If there is a way to perform outer addition / multiplication : #CODE
All universal functions that take two input arguments have an attribute ` outer ` : #CODE
Where is my mistake , and what can I do to use the fft as intended ?
E.g. , if ` f ( x ) = sin ( x )` , then you would have a nice , simple delta func at 1 to capture the simple frequency in ` sin ( x )` .
Specifically I need basic math , sort , lexsort , argsort , bincount , np.diff , and indexing ( both boolean and with the array of indices ) .
I have the following code to minimize the Cost Function with its gradient .
It looks like it is not normalized enough , add max , min , mean values of both X and y to your question ( or maybe histograms ? )
Basically , we use ` np.column_stack ` to stack ` column-1 ` with ` column-2 ` and then again ` column-1 ` with ` column-3 ` to give us a 2D NumPy array ` arr2D ` of shape ` N x 4 ` .
Next , we reshape ` arr2D ` to a ` 2*N X 2 ` array and split along the rows with ` np.vsplit ` to give us the expected list of ` 2D ` arrays .
` ( " The input vector ' x ' must lie within the normal simplex . but sum ( x )= 1.000000x : " , array ([ 0.33000001 , 0.34 , 0.33 ]))` .
Don't sum up to 1 !
A quick solution may be only to use the first components for optimisation and calculate the last from the sum .
the condition now becomes re.search ( reg_exp , f ) , means search for the pattern reg_exp in f , and if there is a match then append filename to the list of files
narrow it to ones with 2 fields separated by dash #CODE
Add zeros as prefix to a calculated value based on the number of digits
For example , if I just wanted to sum a larger array #CODE
broadcast intersection on a numpy array
I am not interested in the diagonal of the matrix ( I substitute NaN ) .
Add zeros as prefix to a calculated value based on the number of digits
Numbers ( integers ) don't have leading zeros , just their string representation .
however you cannot use that output in any kind of calculations since converting back to a number will strip the left zeros and possibly use it as an octal number - which will cause a conversion error if there are digits that would not be valid octal i.e. 0397
The following format specification , `' { : 04} '` , says replace that with the argument ` n ` from ` format ( n )` and then format it , ` : ` , with leading zeros , ` 0 ` , printing at least 4 , ` 4 ` , digits .
I have kept on searching the problem , and even when I try to lambdify this simple expression : ` 17469169.5935065 *sin ( 0.942966379693359 *m ) *cos ( 1.5707963267949 *m ) / m ` , with " numpy " as module argument , it gives : `' float ' object has no attribute ' sin '`
Function ` f ` sum all values of column ` C2 ` , because there not depends on value of ` key2 ` .
If the underlying data in ` a1 ` and ` a2 ` are not part of one contiguous block of memory , there is no way to join them -- without copying -- into one new NumPy array , ` A ` , since the new array can have only * one pointer* , one stride , one shape and one dtype .
If yes , you can use the Linux terminal to strip quotes from the ends of the rows quickly .
The norm of error ` A.dot ( solution ) - rhs ` is quite large , however , the relative error is small .
` self.data.fill ( fill_value or self.fill_value )` - what if someone wants to fill it with zeros ?
My implementation of the class as an ndarray subclass ran into issues when I try to resize :
ValueError : cannot resize an array references or is referenced
Use the resize function .
ValueError : cannot resize this array : it does not own its data `
since ` m ` contains the set of all currently-mobile points , whereas ` k ` contains the indices of only those points that are touching other non-mobile points .
python sum all previous values in array at each index
and would like to create a new array that for each index in my_array is the sum of all the previous index values #CODE
If you google [ numpy cumulative sum ] ( #URL ) , you'll find that the first hit answers your question .
` np.cumsum ` returns an array of cumulative sums , with each sum going up to and including the corresponding element in the input array .
You want to exclude the corresponding elements from the sum , so just subtract them out .
The code basically starts assigning sum with 0 and putting it into the first index of ` summed_array ` , and then it adds each respective index of ` my_array ` to sum .
Then , it comes back to the ` for ` loop and assigns each index of ` summed_array ` with the previous ` sum ` .
` numpy ` has std , mean , average functions separately , but does it have a function that sums up everything , like ` summary ` does in ` R ` ?
Not that I can't install ` pandas ` , just thought that if ` numpy ` has statistics methods like ` mean , std , var ` , etc . why wouldn't it have a method to aggregate these statistics ?
to get a list with the indices of equal arrays but since I am no expert in python there could be a better solution ( it seems to work ... )
According to my code the function should consist out of ` n1*b ( x ) +n2*b ( x ) +n3*b ( x )` if all ` b ` -values are zero , and out of ` n1*a ( x ) +n2*a ( x ) +n3*a ( x )` if all ` b ` -values are ones ( including mixed results ) .
Whatever `` f ( x )`` is , there will not be a unique solution to `` n1 * f ( x ) + n2 * f ( x ) + n3 * f ( x )`` .
I have code , which you can see below , which returns the error " Index Error : Arrays used as indices must be of integer ( or boolean ) type " , which is a problem as the data I have is made up of strings .
@USER , you can do all this using the csv mod , numpy itself or better again pandas if you have it installed , do you actually want an array at all ?
@USER , no worries , if you want to keep each row separated from each other you can append instead of extending , extending will give you a flat list of values which may or my not be what you want
That is here if ` threshold ` is either 2 , 3 or 4 hence demanding at least 2 , 3 or 4 consecutive zeros in order to leave them unchanged .
running % of total ( the sum of all % values up to and including that row )
This is a little better ( yields the sum and % columns ) but :
Just compare the time taken to compute the sum , and the percent .
But what if I use a vector in my function and the very nasty sum operation .
` vec [: , None , None ] - x [ None ]` then gets broadcast out to shape ` ( 10 , 7 , 9 )` .
Finally , we compute the sum over just the first dimension using ` np.sum ( ..., 0 )` to yield a ` ( 7 , 10 )` array that looks like this : #CODE
This is why I got the error : " : shape mismatch : objects cannot be broadcast to a single shape " .
You could either use ` x [ None ] .T ` and broadcast over the last dimension , as in your suggested edit , or you could insert extra dimensions in ` vec ` and broadcast over the first , as in my update .
if the min value is ` - 3.40282e +38 ` , you probably have some errors :D
Note that the results will differ if the minimum value is not unique but it is not clear from your question which output you desire in this case .
More scientifically , I think the reverse fft should be able to generate an array with specific autocorrelation , but I've never done this or looked at how !
Actually you don't need all of that ` ravel ` ing and ` reshape ` ing - ` colors [ labels ]` will work fine ( it's more or less equivalent to ` colors.take ( labels , axis=0 )`)
No , I meant why not fit the analytic solution ( ` exp ( -c / 2*t ) * ( A*cos ( w*t ) +B*sin ( w*t ))` with ` w=sqrt ( k-c ^2 / 4 )`) to the simulated noisy data ?
Any meaningful comparison should look like ` max ( abs ( answ- [ 2 , 10 ]) / abs ( answ )) tol ` or something similar .
if you have troubles differentiating noisy data , you could convolve with a gaussian kernel : #URL
But , how to append ` n columns ` of ` np.arange ( 1 , 4 )` to ` A ` ?
Do you want to append rows to ` A ` or create ` A ` that looks like the one in your question ?
As mentioned in docs you can use concatenate #CODE
It was not obvious to me what would happen here , since 252+10 > 255 ( the max for uint 8) .
Result for ` filter_indices ` ( filtered indices in original unfiltered data ): #CODE
The easiest way to solve the problem is to transpose ` data ` array : #CODE
You might get a more helpful answer if you asked another question explaining the full problem you are trying to solve - maybe there is a way to avoid computing ` exp ( x )` altogether ?
It will avoid the overflow warnings generated by ` 1.0 /( 1.0 + exp ( x ))` .
` trace ` and ` sum ` also fill with 0 , but ` prod ` fills with ` 1 ` .
Finally sum along the second axis for the final output .
min [ min{Xi , Xj } , min{Yk , Yl } ] for all i , j , k , l
Can you roughly indicate what the performance of the ` reshape ` and ` einsum ` methods are compared to my original ` for-loop ` ?
Dimension mismatch : array ' cov ' is of shape ( 1 , 1 ) , but ' mean ' is a vector of length 2
ValueError : Dimension mismatch : array ' cov ' is of shape ( 1 , 1 ) , but
I can't seem to be able to change the shape of ` cov ` from ` ( 1 , 1 )` .
But I can't seem to figure out how to update ` cov ` such that it isn't ` ( 1 , 1 )` .
The sum of the first row is 6 , not 12 .
So if you want to sum in first row it'll be across columns and you need to change axis to 1 : #CODE
Or you could do it with sum method of the numpy array : #CODE
With ` out = jsd_vectorized ( A , I )` Is there a way to make the index ` ids =o ut [: , 1 :] ` as integer ?
How to solve nonlinear equation without sympy ( max and min ) ?
How can I resolve the equation like ` x * max ( x , 15 ) + max ( x , 45 ) / x = 10 ` with python libraries ?
Then , I will add zeros to the short rows to equal them to the longest one , so that I can iterate them as a 2D array .
I am thinking to add zeros at end of the short rows to equal them to the longest one .
I am trying to stack a Numpy 2d array ( effectively a matrix , although it is representing a table of data from a FITS lightcurve file ) .
Numpy append ones to matrix
I tried it first without transposing the matrix , but it seems that ` np.ones ( m )` always produces a row vector that I couldn't transpose ( I tried doing ` np.ones ( m ) .T )` .
@USER Yeah you can do that to define a 2D array with ` ones ` along the first axis and the second axis being a singleton , instead of a " non-tuple " case where it's defined as 1D array with ones along the only axis .
Think that would be ` np.ones ` instead and also since OP wants to prepend ` ones ` , so I guess - ` np.c_ [ np.ones ( X.shape [ 0 ]) , X ]` ?
You need to use some kind of concatenation depending on the shapes of the inputs ( features and ' NEW ') you are looking to concatenate .
How to do specify level depth ( or axis ) for outer in Python 2.7 ?
The key thing is to tell outer to stop breaking things into components at a certain level : #CODE
The latter dictionary is what I would like to translate ` a ` into .
For convenience , I have an array ` offsets ` that indicates the indices ` a ` of the starts of the groups .
If float , the parameter represents a proportion of documents , integer absolute counts .
If float , the parameter represents a proportion of documents , integer absolute counts .
You can't change the typing of the array in-place ( unless I'm grossly mistaken ) , but you can floor .
You can't change the typing of the array in-place , but you can truncate ( or floor if you prefer ): #CODE
You need to swap the second and third axes first , and then reshape : #CODE
Especially with numpy and sympy this will cause collisions ( for example both provide a ` diff ` function ) .
i'm trying to reshape a array in Python and fill it with mean values .
To correct those i inserted some missing ones randomly .
It works perfect if I don't append the starting coordinates .
Doesn't work for me - I get a ' figure of 8 ' if I don't append the starting coordinates .
I get this problem if I append coordinates : #URL
I am planning to later append in smaller video segments over the blank canvas .
It seems that the frames made of array of uint8 zeros is not enough to write a valid video .
yes , it does , but in above picture i don't want display all contours detected but only the ones around the no.s
L2 matrix rowwise normalization gradient
Im trying to implement L2 norm layer for convolutional neural network , and im stuck on backward pass : #CODE
numpy dot product in steps
I am trying to split up my dot product into steps .
PS : [ Ke ] is always going to be a 4x4 matrix and the [ k_small ] is a diagonal matrix which can have any value for the diagonal .
Why do you flatten this matrices ?
I had to flatten ` X ` and ` Y ` such that ` A ` becomes a 2D array , which is the format needed by the ` lstsq ` solvers .
So if ' a ' satisfies the condition at indices 1 , 2 , 4 , 7 , I want to have access to the array at indices 2 , 3 , 5 , 8 .
Use ` numpy.where ` to get the indices where ` idx ` is ` True ` : #CODE
Like just hard coded indices ?
For background , the points will always fall within a cylinder of fixed radius , and height = max z value of the points .
[ Numpy n-dimensional histogram ] ( #URL )
round all your values to integers ( rasterize your data ) probably with some ` floor ` function ;
compute the histogram of each " cell " ( several functions from numpy / scipy or numpy can do it );
First , you can create group numbers by comparing the value column to zero and then taking a cumulative sum of these boolean values .
How do I check which dot is in which white block ?
1 ) find white connected components ( your white blocks ) , 2 ) Create a new all zeros label image 3 ) set in the label image a different integer value ( label ) for points that belongs to the same white CC ( you can easily do this with ` setTo ` and the mask of the white CC ) 4 ) find the centroid for each green and red CC 5 ) Check the value in the label image for the centroid position .
I have a little script for calculating the Fourier Transform of a square wave which works well and returns the square wave correctly when I invert the ` fft ` using ` numpy.fft.ifft() ` .
I am not sure if I am doing the " sort fftfreq by absolute value " part right here .
That's LaTeX notation saying that the discrete Fourier transform is a linear combination of complex exponentials ` exp [ 2 pi i m k / n ]` where ` n ` is the total number of points and ` m ` is the discrete time index .
In your notation this would be ` exp [ 2 pi i m k / N ]` because you're using ` N ` to mean the total number of points .
Use ` exp ` , not ` sine `
Suppose you take the DFT of a signal ` exp [ 2 pi i ( N + p ) m / N ]` .
In fact , you get the same thing that you would get if you took the DFT of ` exp [ 2 pi i p m / N ]` .
Now suppose we have a time domain signal ` cos ( 2 pi p m / N )` .
` ( 1 / 2 ) [ ( exp ( 2 pi i p m / N ) + exp ( -2 pi i p m / N ) ]` .
Instead , you want to sort fftfreq by absolute value and then sum up complex exponentials with those frequencies .
But anyway , I did ` freqs2 = np.abs ( freqs )` and then ` freqs2 = np.sort ( freqs2 )` then updated my for loop as you suggested , using ` exp ` instead of ` sin ` .
You can contact me if you look at my physics stack exchange user profile and follow the link to my professional website .
@USER please also note the existence of the [ signal processing stack exchange ] ( #URL ) .
Thanks for letting me know of that stack exchange too .
Notice that , in this case , the gradient of the vector field is an identity tensor field .
Find indices of mutually exclusive elements between two arrays in python
Now , I am trying to figure out how to get the indices of the elements of the exclusive array , ` ex ` for both ` a ` and ` b ` .
About the performance too lazy to measure , but I expect my method will be ~50 times faster : RNGs are fast , branch misprediction is typically 20 cycles , sqrt is also about 20 cycles , while a single sin or cos is 200-300 cycles .
Note that in the rejection-based implementation I first generated ` npoints ` samples and threw away the bad ones , and I only used a loop to generate the rest of the points .
choose ` theta ` = rand ( 0 , 360 ) .
choose ` phi ` = 90 * ( 1 - sqrt ( rand ( 0 , 1 ))) .
choose ` theta ` = rand ( 0 , 360 ) .
choose ` radius ` = sqrt ( rand ( 0 , 1 )) .
Code : ` figure ; N=5000 ; theta=2*pi*rand ( 1 , N ); phi =p i / 2* [ sqrt ( rand ( 1 , N / 2 )) , -sqrt ( rand ( 1 , N / 2 ))]; plot3 ( cos ( phi ) . *cos ( theta ) , cos ( phi ) . *sin ( theta ) , sin ( phi ) , ' . ') ; axis equal ; axis vis3d `
@USER hm , yes , that doesn't look right . what do ` rand ( 1 , N )` and ` rand ( 1 , N / 2 )` return ?
this approach definitely assumes that the value inside the sqrt is a uniform distribution in the range [ 0 , 1 ] .
` rand ( 1 , N )` and ` rand ( 1 , N / 2 )` produce vectors of length ` N ` and ` N / 2 ` ( respectively ) , each element uniform on ` [ 0 , 1 ]` .
My formula is in error ; Phi should be chosen as ` 90 * ( 1 - sqrt ( rand ( 0 , 1 ))` .
I believe I solved the mystery :) The probability density you're using is not uniform , but proportional to ` ( 1-2 / pi*phi ) / cos ( phi )` if you measure ` phi ` in radians .
As for the disk : in that case ` sqrt ( x )` should be the exact solution , but i haven't checked that .
@USER I first thought that it was simply due to the structure of the matrix ( very few unique elements being permuted ) , but I couldn't get a zero-determinant matrix using random matrix elements in the same pattern ( badly conditioned yes , singular no ) .
I know how to loop over the each row to append 1 to matching elements of each columns .
Are you looking to also create log Returns ?
I don't know exactly what a log return is , but I'm getting percent profit , appending it to a list , and sum ( list ) / len ( list ) after all the stocks have run . puzzler 30 mins ago
You should include your ` textvariable ` within the ` Entry() ` call , not after it ( ` append ( Entry ( master , textvariable=xyz ))` rather than ` append ( Entry ( master ) , textvariable=xyz )`) .
I'm trying to understand why numpy's ` dot ` function behaves as it does : #CODE
does what I expect : the matrix multiply is broadcast
this is actually explained in the numpy docs for dot and matmul
` matmul ` differs from ` dot ` in two important ways .
Stacks of matrices are broadcast together as if the matrices were elements .
it is a sum product over the last axis of a and the second-to-last of b : #CODE
This resembles the inner ( dot ) product .
In case of vectors , ` numpy.dot ` returns the dot product .
Arrays are considered collections of vectors , and the dot product of them is returned .
For N dimensions it is a sum product over the last axis of ` a ` and the second-to-last of ` b ` : #CODE
If either argument is N-D , N > 2 , it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly .
Expressed this way , the ` dot ` equivalent , ' ij , lkjm -> ilkm ' , looks just as natural as the ' matmul ' equivalent , ' ij , lkjm -> lkim ' .
I want to create a numpy array in which each element must be a list , so later I can append new elements to each .
I have looked on google and here on stack overflow already , yet it seems nowhere to be found .
If your primary concern is with the speed of append operations then you can't do much better than a regular Python list-of-lists , since appending to a list is very cheap compared with array concatenation .
AttributeError : ' numpy.ndarray ' object has no attribute ' append ' in python 3.4.x . apparently an array of empty objects needs to be created first , followed by a fill .
It is not a clone of list append .
Well , I need to start a vector / array with N elements , each element will have a list of elements and they may vary size , and these lists I will append to .
And you can then append values to each of those lists : #CODE
It's a little unclear from the question and comments whether you want to append to the lists , or append lists to the array .
It isn't a substitute for list append .
You need to construct another object array to concatenate to the original , e.g. #CODE
You can reshape , transpose , etc an object array , where as creating and manipulating a list of lists of lists gets more complicated .
I am new to torch7 , and I can't find a way to get the some non contiguous indices of a tensor based on another tensor .
` ValueError : operands could not be broadcast together ` when attempting to plot a univariate distribution from a DataFrame column using Seaborn
Has this happened because my data contained large number of zeros ?
The problem was that ` ad ` had a lot of zeros .
And if ` a ` is dominated by enough zeros , the IQR will be zero as well , e.g. #CODE
so your intuition that the high number of zeros might be playing a role was right , I think .
Theano : Discrepancy vs Numpy when taking matrix dot with CPU vs GPU
An absolute difference of that size can be normal if the relative difference is small .
I'm trying to figure out a better way to unwrap the output of numpy's arctan function .
Since ` np.tan ` and ` np.arctan ` both return an array , even if the input is a list , your ` unwrap ` can be written as : #CODE
numpy makes it easy to translate python objects into numpy ndarrays , and will even pick an appropriate resulting data type if one is not specified : #CODE
Can you post raw input data and code to reproduce your error , the error seems reasonably clear , the dimensions of your data , indices and columns don't match you need to post more info
Thus , you are dividing the mean / std , and then only after the subtraction is applied .
One easy strategy to try to get a distribution more normally is to use a log transformation . numpy.log() might do the trick here , but only if the original distribution is not too skewed .
but I i do ` c [ 0 , :] ` i get the error message ` list indices must be integers not tuples `
Every time you concatenate two numpy arrays together , the result is a new copy of the array .
If you must do your appending inside a loop then I suggest you use a regular Python list , and convert it to a numpy array outside the loop ( unlike numpy arrays , you can append to Python lists in place without generating a copy ) .
You can concatenate arrays in ` numpy ` .
numpy 3D dot product
I have two 3dim numpy matrices and I want to do a dot product according to one axis without using a loop : #CODE
I see ... what if I generate a matrix using indexes calculated similar to einsum and build a bigger matrix using these indices to feed into my theano function ?
do you know a similar solution for np.divide instead of dot product ?
Generating gradient map of 2D array
Now I want to find the gradient of this array .
I want to learn how can I use this or any other way to create a gradient map that shows the change in gradient of the 2D array .
` varray ` is the 2D array I want to create gradient map of .
I know there should be clever way to use ` x gradient ` and ` y gradient ` generated by ` np.gradient() ` but I cannot think of it .
Using magnitude of gradient as suggested by @USER Physicist below .
On right : Image of the gradient map .
If you are looking for the magnitude of the gradient , you can just do #CODE
If , you want to plot the gradient as a vector map or stream plot , do something like #CODE
hey thanks for suggestion , I used what you said with magnitude of the gradient but I am not getting the result as close to above .
I have two 3dim numpy matrices and I want to do a dot product according to one axis without using a loop in theano .
" c " , is defined as dot product of " a " and " b " : #CODE
@USER of course :) I need theano for gradient calculation .
I tried a couple of transpose shapes , but it didn't work and raised an exception of dimension mismatch .
do you know a similar solution to divide instead of dot product ?
Besides the 0 based indexing , I used ` np.ix_ ` to reshape the indexes .
In this case , where 2 dimensions don't roll anything , ` slice ( None )` could replace those : #CODE
are given , the entropy is calculated as S = -sum ( pk * log ( pk ) ,
sum ( pk * log ( pk / qk ) , axis=0 ) .
In our case , we are doing these entropy calculations for each row against all rows , performing sum reductions to have a scalar at each iteration with those two nested loops .
Now , the catch here is that ` stats.entropy() ` would sum along ` axis=0 ` , so we will feed it two versions of ` distributions ` , both of whom would have the rowth-dimension brought to ` axis=0 ` for reduction along it and the other two axes interleaved - ` ( M , 1 )` ` ( 1 , M )` to give us a ` ( M , M )` shaped output array using ` broadcasting ` .
What does the ** None ** do and why do you transpose those two matrices ?
That transpose is needed because internally that function sums across axis = 0 , which is to replicate the sum of all elementwise multiplied elements that we would get in the loopy code of the question with each iteration .
Hey @USER , each matrix represents a gradient update for a distributed word2vec training algorithm .
Each matrix , with dtype of float32 , is about 3GB after gzip plus cPickle dump .
Assuming you have some flavour of scipy.sparse matrix , you could save the values and the indices in separate numpy arrays ( e.g. using ` np.savez `) or in an HDF5 file .
You don't need the second reshape .
The goal is to concatenate each of these arrays together into a list ( or numpy array ) and then save the results .
and I then thought I could concatenate this into a list , i.e. #CODE
Each output of the function is unique and should be labeled appropriately
I should make it clearer above , but each function output is unique --- I have to input several parameters in order for it to run correctly .
There's a significant overhead that is not dependent on the number of rows or the number on nonzero elements in a particular row .
Selecting a row then requires looking up the relevant indices , and constructing a new sparse matrix with selected indices and data .
` lil ` format may easier to understand , since the data and indices are stored in sublists , one per row .
First , you probably need list of unique IDs , right ?
How can I compute the area under a histogram after a certain value ?
I am using this to compute the area under the whole histogram .
However I cannot find resources that tell how to calculate the area under the histogram after a value or within a particular interval .
The gradient of this objective function is calculated as :
Now I have my LogisticRegression class which has the sigmoid function to calculate the sigmoid , the loglikelihood function to calculate the loglikelihood , and gradient to calculate the gradient .
So I got these three errors : 1 . divide by zero error , 2 . overflow encountered in exp 3 . invalid value encountered in multiply .
Do you guys think I did something wrong in calculating the loglikelihood and gradient ?
I need to find the cardinal and diagonal neighbors of any node in a two dimensional array .
Do you need the indices of those 8 neighbors or just the elements themselves ?
this is the error when i call datarr [ 0 ] : IndexError : too many indices for array
You may use broadcasting of numpy arrays over the diagonal of the array : #CODE
Each different query I issue has the same pattern , though with different absolute values .
@USER each unique query produces a different excessive initial allocation .
We have thousands of unique queries on a given day .
@USER ller Each unique query produces a one-time allocation at that line of code .
As a work-around , you could simply encode the lines before before passing them to ` genfromtxt ` .
The confusion was actually with ` pickle ` dumps of ` numpy ` arrays .
80% of the time is spent on ` repeat ` and ` extract ` - if only there was a way to broadcast the ` extract ` :( ...
Edits 7 8 : Actually , you can broadcast the extract using ` as_strided ` from ` numpy.lib.stride_tricks ` , but it doesn't seem to help much performance-wise :
However , if ` len ( a )` is a power of two we can replace the expensive mod operation with ` ( len ( a ) - 1 )` , which does end up being a bit faster than the ` extract ` version ( now about 4.9x ` np.sort ` for ` a=randint ( 0 , 1e8 , 2**20 `) .
I suppose we could make this work for non-power of two lengths by zero-padding , and then cropping the extra zeros at the end of the sort ... however this would be a pessimisation unless the length was already close to being power of two .
Now I want to remap the values according to its indices which are written in tuples in a dictionary .
How do I join these two array of arrays together ?
EDIT : Suppose I have all the array's to be of ` len ( 20 )` inside ` b ` , then when I concatenate I get the shape of ` final_array ` to be ` ( 4 , 20 )` .
suppose I have all the array's to be of ` len ( 20 )` inside ` b ` , then when I concatenate I get the ` shape ( 4 , 20 )` .
The behavior of ` concatenate ` is clearer when the inputs are themselves arrays .
What does it mean to join ` a ` and ` b ` together ?
It's just as unique as the ` ith ` row of a many row matrix .
Most of the unique ` matrix ` activity is defined in : ` numpy / matrixlib / defmatrix.py ` .
My initial thought was to use something like b = np.cumsum ( a [ a == 1.0 ]) , but I don't know how to ( 1 ) modify this to reset at zeros or ( 2 ) quite how to structure it so the output array is the same shape as the input array .
Your matrices look rather like vectors and additionally only contain zeros and ones .
If not , you could use the sequence of zeros and ones and interpret it as the bit sequence of an integer .
Now we would like to make the emptied cells ( the ones containing zeros ) editable , i.e. convert them into text fields .
( Matrix multiplication in numpy is done with ` dot `)
What I understand is that if you know the ` dtype ` and the ` count ` , it allows preallocating memory to the resulting ` np.array ` , and that if you don't specify the optional ` count ` argument that it will " resize the output array on demand " .
` np.fromiter ` is mainly used by people who are trying squeeze out some speed from iterative methods of generating values .
The code would be lot more complicated if it decided ' on the fly ' how to convert the ` value ` ( and all previously allocated ones ) .
Numpy loadtxt remove leading zeros ( convert ) and define spaces delimiter
I have files with XYZ coordinates generated by some software in the way of fixed column widths including spaces and leading zeros : #CODE
Update : Thanks to the answer below I realized leading zeros are not the issue with numpy.loadtxt - it automatically strip them off .
You can use regex to remove leading zeros , but must be careful of various situations : #CODE
Now ` idx ` will be a list of all the indices ` ( x , y , z )` where the assertion fails .
Create pandas DataFrame using unique labels sourced from filenames
Then I find the unique labels using : #CODE
Then I create a Series of the unique labels using : #CODE
So now I would like to create a DataFrame which has the unique labels as the column headers with all the data items sorted into their respective columns .
It is the outer product of vectors : #CODE
i tried using df.pivot_table but i get an error that says : contains duplicate entries , cannot reshape .
When I use this my NaNs will be deleted from the list , but strange to say I do not have the zeros either in the list .
try adding the ` .replace ` after the ` .strip ( ' \t ')` before the ` .split ` in your data append .
It then needs to use those array points in Z-space along with the slice location ( in this case 0 ) and the energy deposition at those array indices and interpolate to find the correct energy deposition at that z-location of the slice .
` std = RMS ( data - mean )` .
In this case : ` std = sqrt (( 0.5 ^2 + 0.5 ^2 ) / 2 ) = sqrt ( 0.25 ) = 0.5 `
@USER , thank you , I just got a bit confused with sample and population std .
Set the optional ` ddof ` parameter to ` 1 ` to get the population std : #URL
` ddof ` modifies the divisor of the sum of the squares of the samples-minus-mean .
Then that is append to T_SLR
If I have multiple columns of categorical variables is there an easy way to encode the categorical variables in a one hot scheme in such a way that I could encode a new data frame with the same encoding scheme .
I would like to encode all the categorical data in a one hot scheme all in one go .
But it will not encode a future data frame with the same structure consistently .
Eg ., what if I want to encode data which happens to only have " Female " entries in gender .
Note that I have set ` max_distance ` at 5.0 but the longest distance possible is ` sqrt ( 2 )` so that should not replace any entry by zero .
Earlier derivatives are all either sine or cos functions as expected .
However , I cant just use a matrix full of zeros , since I could afterwards not say which elements of the matrix are the " default " zeros and which ones i might have inserted myself .
Here a single scalar has been broadcast to update the entire subarray .
We can resize A to be able to fit the shape 2 entry : #CODE
The transpose is because the slice of B is 1-dimensional shape ( 3 long ) rather than a 2-dimensional shape like A ( which is 1 wide and 3 high ) .
How to combine two arrays to one by their indices in the result ?
And their indices in the result array : #CODE
You then provide these two arrays ( say ` inds1 ` and ` inds2 ` as the indices of the original array ( so ` data2 = data [ inds1 , inds2 ]`) .
They can then be broadcast together , and the singleton dimensions will do exactly what you need .
` , and leading singleton dimensions are also implicitly assumed during broadcast , so you can simply use #CODE
2.I am not clear on min , max values .. what does rectangle coordinates denote ?
Depending on the template matching method used , the brightest / darkest ( max / min ) points on this result matrix are your best matches .
To correctly handle the duplicate indices , you'll need to use ` np.add.at ` instead of ` += ` .
( The values in the corners correspond to the diagonal elements . )
There may be function that creates the union ` zdt ` from the individual ones , but I'm not going to dig around for that now .
I now show how to join the ` dtype.descr ` .
If you transpose your data so that the ` index ` you show below makes up the ` columns ` and your ` xticks ` make up the new ` index ` , you should get what you are looking for .
How to apply log to an numpy sparse matrix elementwise
but I am getting the following error when I apply the log : #CODE
the error is related with the fact that feats it's a sparse matrix I would appreciate any help with this , I mean a way to apply the log to a sparse matrix .
sorry I agree with you , instead of this i want to apply the log to all the enters of the matrix that are distinct of zero .
If you want to only take the log of non-zero entries and return a sparse matrix of results , the best way would probably be to take the logarithm of the matrix's ` data ` and build a new sparse matrix with that data .
in struct ' s ' , access field value ' a ' using dot notation ( i.e. s.a )
Random indices from masked array
I have a 2D MaskedArray ` X ` and I want to randomly select 30 non-masked elements from it and return their indices ` idx ` .
The goal is that I could use the indices to read / set values efficiently later in my code : #CODE
I was able to generate a list of indices from a flattened array with masked values removed , ` idx = np.random.choice ( X.count() , size=30 )` , but I have no idea how to map that back to indices from the original array
Use the ` labels ` to sort , I.e. reorder , the rows , and ` add.reduceat ` to sum them by groups .
Find the indices that will sort ` lbls ` ( and rows of ` arr `) into contiguous blocks : #CODE
For example , I want to express the values generated ( in the new column ) as a fraction of the sum of all these values in a new column .
I want that ` flights ` becomes an array , so that I could retrieve data using indices e.g. ` flights [ 0 ] [ 3 ]` , ` flights [ 1 ] [ 5 ]` .
How can I convert ` result ` into a row with columns , each time I append ` flights ` ?
And also in the case of " array of indices " : #CODE
If you're using a pandas dataframe , setting the type using [ ` df [ column ] .astype ( ' category ')`] ( #URL ) and using the [ ` get_dummies `] ( #URL ) method will one-hot encode text columns for you as well .
@USER , yes , but if you get additional dataframe of the same structure - you should be able to encode it with the same encoding you used in train set .
A polynomial fit for the start of the data , where the motor is accelerating to max speed .
A linear fit for when max speed is reached .
cuda gemm transpose with numpy
If the codes translate to actual values through simple functions , that would be a more effective way to do the translation .
Your codes look like indices , so you could use some numpy index tricks to get your result : #CODE
I am trying to append values from a newly created array to a previously created array , then run a while loop to do this 10,000 times .
I am trying to append the newly created values from the new_positions and new_velocities array to the partpos and partvel array , but am getting the following error : #CODE
You should initialize it with float zeros or ` dtype= ' float64 '` .
If you want to ` append ` or otherwise concatenate on ` axis=1 ` , you need to start with arrays that have at least 2 axes .
On the first iteration , we initialize ` Powers ` to be a singleton 2D array with 1 column , then we append from there .
If you want to have zeros in the beginning , turn it to zero explicitly : #CODE
I'm trying to fit data to probability distribution ( gamma function in my case ) .
I want to create a 2D histogram using this data .
Then , you can create a 2D histogram in the standard way using ` hist2d ` : #CODE
Only length-1 arrays can be converted to Python scalars with log
Notice that when ` log ` refers to ` numpy.log ` , then ` - 1.0 *sum ( prob* np.log ( prob ))` can be computed without error : #CODE
but when ` log ` refers to ` math.log ` , then a TypeError is raised : #CODE
The code you posted does not produce the error , but somewhere in your actual code ` log ` must be getting bound to ` math.log ` instead of ` numpy.log ` .
Much better , thanks :) I realized that the ploting is not trivial : you'd have to plot the resulting fit together with a histogram computed from your data ...
I would compute a histogram with either [ ` numpy.histogram `] ( #URL ) or [ ` scipy.stats.histogram `] ( #URL ) , then plot together the histogram with the ` pdf ` of your fitted powerlaw .
When I plot the pdf curve , it is showing a sharp gradient .
I changed the code to be y= 100 * exp ( - 0.0482 * X ) but still got a name error message
y= 100 * exp ( - 0.0482 * x )
Use ` dot ` : #CODE
Alternatively to the well known ` dot ` function you could use numpy.matmul if you have numpy version > = ` 1.10.0 ` : #CODE
That is the exact problem with your ` square_list() ` function as well , it is returning a list , not the sum of the list elements .
If you want plain sum : #CODE
If you want the group sum of squares : #CODE
It's been doing that for as long a I can remember ( a few years ( ! ) at least ) :) The apply still happens in python so if you have a lot of small groups the dummy_column + groupby sum will blow apply out of the water .
Perhaps you want the sum : #CODE
Note : it may be faster to create a dummy column for the squares then a " clean " groupby sum .
Currently , what you are doing with the ` np.where ` method is to return the indices where has lat > 37.5689 and lat <3 7.6689 , if you do not have any indices that fulfill such a criteria an empty array will be returned ,
` numpy.where ` will return a length 2 tuple where the 2 elements are : an array of indices for the rows and an array of the corresponding indices for the columns that satisfied the condition .
` numpy.ravel ` will flatten the tuple of the 2 arrays into a single array and you will no longer have 2 distinct arrays for the row and column indices .
To preserve the indices : #CODE
` [ 204 , 204 , 205 , 205 ]` are the row indices and ` [ 145 , 146 , 145 , 146 ]` are the corresponding column indices were the conditions where satisfied .
To get the values from the lat array , using these indices , you can do : #CODE
EDIT : A way for one to see the indices as row , column pairs : #CODE
More generally in Python ( and even without using Numpy ) , how to find k_0 such that ` D [ k_0 ] = min D [ k ]` ?
A more Pythonic way of implementing the same algorithm you're using is to replace your loop with a call to ` min ` with a ` key ` function : #CODE
I didn't know this min / key pattern !
Usage of the builtin ` min ` is the way for this : #CODE
You're computing a slightly different value ( the smallest sum , squared , rather than the sum * of * squares ) .
( meaning to compress the code as much as possible in memory , so I can get better speed improvement while still doing the same operations )
Output : ` im [: , : , :] ` - Numpy array representing a RGB image with all 3 color channels , hence the need to clip the range ( 0-255 ) .
Your code transforms the values in ` ra ` by performing linear interpolation to get their corresponding row indices within ` cr ` .
You can generate column indices via : #CODE
The other approach is to load each file , collect the arrays in a list , and concatenate them .
` skimage.transform.downscale_local_mean ` can only perform integer scaling factor ( and pads image with zeros if the factor is non-integer ) .
You are creating strings from ` tbdata [ k ] [ ' a '] and [ ' b '] ` len ( NameA5 )` times , just create them as temporary variables in the outer loop .
Finding indices of non-unique elements in Numpy array
I would also like to store either an array of indices to discard , or to keep ( a la ` numpy.take `) .
I use this solution to combine each row of ` a ` into a single element , so that we can find the unique rows using ` np.unique ( , return_index=True , return_inverse= True )` .
Then , I modified this function to output the counts of the unique rows using the index and inverse .
From there , I can select all unique rows which have ` counts == 1 ` .
Like that you both delete the duplicates and get a list of indices to discard .
That will avoid a python function call when you do sqrt and exp , replacing it with a direct c call ( which should be a lot faster ) .
To eliminate this , you can use numpy arrays to preallocate the data , and use its ` repeat ` and ` tile ` functions to generate the ` i , j , k ` values : #CODE
Getting indices of a specific value in numpy array
This is how I am getting the indices of all 0's in the array : #CODE
( array ([ 0 , 1 , 2 , 4 , 5 , ..., 21 , 22 , 23 , 24 , 25 ]) , ) is one method . slice the where with [ 0 ] just to get the indices
thanks @USER , what does the [ 0 ] at the end of nonzero do ?
@USER , ` nonzero ` returns a tuple of arrays , one for each dimension of ` arr ` .
Personally I wouldn't keep the dict around , though , since the fact that it holds references to all of the arrays would prevent any individual unused ones from being garbage collected later on in your script .
The same dot product takes ~650ms on my laptop when numpy is linked against multithreaded OpenBLAS , but ~16 seconds when linked against the reference CBLAS implementation .
But still the numpy dot is the best .
Also , I want to know the sum of squared distances ( Cost of K-Means ) .
When I'm expecting 1 min .
@USER everything including reduce . its not 10 min , its actually 30min ..
I want to read a mat file back in python but I have trouble going back to a graph , because the mat file gives a numpy.ndarray type file and I need a sparse matrix to reconstruct my graph .
Something similar to ` @USER ' s ` solution but without the ` transpose ` .
I'm using ` ogrid ` ( or ` np.ix_ `) as a compact way of generating 2 3d arrays that broadcast with ` b [: , None , :] ` to produce a set of ` ( 3 , 2 , 2 )` indices .
Sorting a pandas series in absolute value
I want to sort this series in descending order such that the highest absolute value feature should come on top .
I even wrote up my own std function : #CODE
There's a difference : Excel's ` STDEV ` calculates the sample standard deviation , while NumPy's ` std ` calculates the population standard deviation by default ( it is behaving like Excel's ` STDEVP `) .
To make NumPy's ` std ` function behave like Excel's ` STDEV ` , pass in the value ` ddof=1 ` : #CODE
` d [: , None , None ]` has shape ` ( 2 , 1 , 1 )` which is broadcast across your ` c ` array of shape ` ( 2 , 2 , 2 )` to multiply each block of ` c ` by the corresponding element of ` d ` .
I am trying to change values in the original array by giving relative indices to the masked array : #CODE
This happens because using a boolean array or a sequence of indices is " fancy " indexing .
However , if you really need to , you can do it by translating the fancying indexing into specific indices that you want to modify .
First we create an array of " flat " indices of the same shape as our array .
Now we can extract the indices that the fancy indexing would extract : #CODE
What does ` dot ` say about arrays that are larger than 2d ?
This really isn't a ` dot ` case .
The ` einsum ` expression does not sum anything .
As an alternative solution , if you use ` einsum ` , you can avoid having to reshape the array for the dot product : #CODE
We can't get rid of the outer Python loop easily , but we can make some improvements .
( I'd also stop the loop at sqrt ( prime_max ) , but I tried to focus on improvements in NumPy usage rather than algorithmic improvements . )
Tensorflow Tensor reshape and pad with zeros
Is there a way to reshape a tensor and pad any overflow with zeros ?
` vstack ` and ` hstack ` both use ` concatenate ` .
You could call ` concatenate ` directly , which might shave a % or two off .
And ` concatenate ` works a lot faster , too
Worked like a charm , and is very flexible should I want to plug another function than the ` cos ` one .
I recently moved to Python 3.5 and noticed the new matrix multiplication operator ( @ ) sometimes behaves differently from the numpy dot operator .
How can I reproduce the same result with numpy dot ?
You can't get that result out of dot .
The ` @ ` operator calls the array's ` __matmul__ ` method , not ` dot ` .
` matmul ` differs from ` dot ` in two important ways .
Stacks of matrices are broadcast together as if the matrices were elements .
The last point makes it clear that ` dot ` and ` matmul ` methods behave differently when passed 3D ( or higher dimensional ) arrays .
If either argument is N-D , N > 2 , it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly .
For N dimensions it is a sum product over the last axis of a and the second-to-last of b
In the second branch , minimum is less than zero , so adding the abs ( min ) shifts the data up to zero ...
Oh ... normalized means that the max is 1.0 and the min is 0.0 .
` tile ` would work , so would indexing , or even : #CODE
I need to find all grid points within a region defined by lat / lon indices and calculate an average over the region to leave me with a 2-dimensional array .
I now need to find those grid pints within a 4D array which is 38x5000x30x20 ( give or take ) if the areas where square I would use numpy.where to find indices between east and west and north and south then slice the array with those ranges but I'm at a loss on how to best approach finding these irregular shapes .
So this code so far will just create a simple histogram of the radiation counts centred at zero , so the background radiation is ignored .
rand ( 4 , 1 ) random.randint ( 1 , 4 ) or numpy.random.randint ( 1 , 4 ) ?
I would like to loop on this list and when the sign changes to get the absolute difference between the maximum ( minimum if it's negative ) of the first interval and maximum ( minimum if it's negative ) of the next interval .
The ` min ` / ` max ` is computed for each group and stored in a list ` minmax ` .
It would be straightforward to retrieve max / min values of intervals , and then do the calculation .
It contains strings of space separated pairs of indices ( int ) and values ( float ) separated by semicolon .
For multi-dimensional arrays you need to reshape the array afterwards .
The left column are the indices of ` test ` and the right the result of ` argsort ` , this means the index ` 0 ` goes to ` 0 ` , ` 1 ` to ` 1 ` , ` 2 ` to ` 3 ` , ...
I think there's probably a duplicate somewhere but I'm not sure that's the right one ( e.g. the duplicate will show how to use ` tile ` or ` repeat `) .
First , reshape your array to a shape of ` ( 4 , 1 )` , then pad 4 columns on the edge : #CODE
You can get the transpose by either using ` numpy.tile ` or the transpose operator : #CODE
Thank you for your comment .There are many zeros in resulting matrix I read the documentation but could not find way on how to create sparse matrix of such a huge matrix is there any other document that i should refer that will solve the Question ?
So far I've been able to accurately identify the petri dish rim edge with a hough circle finder and morphological gradient transform of image .
This sorts by ` -a [: , 1 ]` , then by ` a [: , 0 ]` , returning an array of indices than you can use to index ` a ` .
Do any of the standard python packages ( or even some non-standard ones ) implement these ?
So ` np.float64 ( ' nan ') is np.float64 ( ' nan ')` is ` False ` whereas ` np.nan is np.nan ` is ` True ` because it is one unique nan stored in numpy module .
However @USER , can you try with ` max_features= ' sqrt '` , just in case some bug .
I am trying to get my 3 lists to join and when I do python says that the new list only contains 1 item .
In that case , you could concatenate the lists using addition : #CODE
How to calculate mean and max increase , decrease of time series in python
So you can get max increase with #CODE
If the data is big using numpy will be the way , and it will actually be easier , just make " a " be a numpy.array then you can get max increase with : #CODE
From there , you can take the mean , max etc .
well , your problem doesn't seem well-formed then , as your arrays have different dimensions , and the only way you could concatenate them would be to make a 1xN array , which I'm guessing is not what you want .
Better to append to a list .
In that case , you could flatten your positional axes , use a single loop , and then reshape the regression results back to 3D .
Now if you want the dot product , I beleive it's in numpy ( or scipy )
If you need 1x2 vector you could use ` reshape ` : #CODE
Then you could use numpy ` dot ` method for multiplication : #CODE
But ` dot ` also works without reshaping : #CODE
If you want to use the dot product , in the matricial sense , you should use the ` .dot ` method , which does exactly that : interpret its inputs as vectors / matrices / tensors , and do a dot product .
Ie slope should go from max value to min value rather than min value to max value .
Python numpy - IndexError : too many indices for array
You have to append 0 to y : #CODE
Efficiently slice windows from a 1D numpy array , around indices given by second 2D array
I want to extract multiple slices from the same 1D numpy array , where the slice indices are drawn from a random distribution .
Note that ` inds ` will always be 2D in my case , but after getting the slices I will always be summing over one of these two dimensions , so an approach that only accumulates the sum across the one dimension would be fine .
Memory usage is identical as my own looped approach ( I profiled using ` memory_profiler `) , whereas Divakar's broadcasting approach uses double memory ( it stores all indices + the data ) .
It's guessing all zeros regardless of the onehot that I created for the 2 classes ( 1 or 0 ) .
When you argmax it on axis 1 , you're always going to get a vector full of zeros ( because there's only one element on that axis , so of course it's the maximum element ) .
Please be mindful that for extremely huge input array cases if the ` b ` elements are such small fractions , because of cummulative operations , the initial numbers of ` b_rev_cumprod ` might come out as ` zeros ` resulting in ` NaNs ` in those initial places .
Lastly , I create a histogram of all the differences , in a range I am interested in .
Example of the histogram
Looking at the histogram after .
Then append ` min ( abs ( r_p - l_c ) , abs ( r_c - l_c ))` to your result .
Is there any convenient way to normalize each row by row's maximum ( divide by row's max )
3 ) define the diagonal ( which is the sum of the row placed diagonally )
Its time complexity is O ( m ) , while the ` searchsorted ` method is O ( log ( m )) , but that will only matter for much larger ` m ` .
And this works to fetch via individual indices : #CODE
Is there a simple way to do this , or do I need to generate them separately and concatenate ?
You need to concatenate , either before or after indexing .
To make an index I first need to get a unique list of words in all the sentences .
I'll use ` sum ( split_sentences , [ ])` instead .
Create a numpy array according to another array along with indices array
I have a numpy array ( eg ., ` a = np.array ([ 8 ., 2 . ])`) , and another array which stores the indices I would like to get from the former array .
My code throws a ' Y [ i ] is not in list ' ValueError , even though this list is composed of the unique values in Y .
There is also a parameter that determines whether to flatten in row-major or column-major order or preserve the ordering .
What's the ufunc to do floor division in numpy with python3.x ?
In python 2.x , I can use the universal function ` numpy.divide ` to do floor division .
So what is the universal function in numpy with python3 to do floor division ?
It's kind of silly to call the explicit functions , when you can just use ` / ` and ` // ` ( for true and floor division respectively ) .
For my example , I will re-build the ` min ` function ; please , don't tell me about some Numpy function for computing that ; this is a mere example !
Generating the indices ` idx ` is what takes most of the time here .
While ` _ , idx = np.unique ( g , return_index=True )` is an elegant way to get the indices , it is not particularly quick .
The reason is that ` np.unique ` needs to sort the array first , which is O ( n log n ) in complexity .
Then I am using the numpy histogramdd to create a multi dimensional histogram : #CODE
@USER : It would be better to start off with a numpy array in the first place , but for some reason I am unable to append to one using np.append , so this was my shady workaround
You can use reshape , to change the shape of each individual array .
The inner arrays only need to be ` ( 1 , 3 )` if I want to join them with ` np.concatenate ` .
For python2 the ` translate ` is a little different : #CODE
And do you want the indices of the two matching elements , or just their value .
Both indices and values .
Well , if you have the indices , you have the values automatically .
You can read Numpy's gradient code at #URL and use the algorithm therein .
Compare rows of 2 pandas data frames by column and keep bigger and sum
I would also like to see example how to add additional column SUM for sum of Value columns from both data frames .
Edit : I just realized after testing the example from the first answer that the data frames I have are missing completely the rows with ids where Value was null .
So could be please also included how to make them same size before comparison - adding rows with missing ids from each other with IDs and zeros ?
. #URL RuntimeWarning : overflow encountered in exp
Now applying ` np.where ` to this condition gives a tuple with matching indices .
For a 2D array we would have gotten a tuple with two elements , holding the indices along the first and second dimension .
We take these indices out of the tuple : #CODE
The only explanation I can infer is the ` inv ` method sharing some resources .
Or take the max : #CODE
The arrays have different ids .
Note that here we have broadcasting , which " simulates " the outer product of two vectors , and so you might think it's matrix multiplication .
Here , ` B * A ` gives you a proper dot product .
I found that str also truncate after 10th decimal . str ( 1.123456789123456789 ) = ' 1.12345678912 '
Since I am sending the result to ` join ` , using a generator is better here .
@USER And since the generator is the only parameter for ` join ` , you can remove the extra brackets ( answer updated ) .
Here's one approach that starts off by converting the input numeric array to a string array with ` .astype ( ' S4 ')` conversion and then does one level of loop comprehension to join all strings in each row with `" " .join ( item )` , where ` item ` is each row from the string array , like so - #CODE
` In [ 2 ]: def magic_func ( indices , values , size ): arr = np.zeros ( 10 ); arr [ indices ]
But how can I find the indices of the k-smallest values ?
Note : in my use case A has between ~ 10 000 and 100 000 values , and I'm interested for only the indices of the k=10 smallest values .
In fact , if ` H.shape = ( 2000,200 0 )` , the array of eigenvalues ` Es ` contains only zeros , apart from the last element , which equals 24 times the length of the array : #CODE
I am defining the tuple as ` tup1 ` , not ` cond `
I could access the indices of the matrix ( i , j ) via #CODE
I presume that if you are looking for where all 3 conditions prevail , you are looking for a sum of 3 .
The dimensions do not broadcast together properly .
Logical combinations of the boolean masks are easier to produce than combinations of the ` where ` indices .
To use the indices I'd probably resort to ` set ` operations ( union , intersect , difference ) .
It is still easier to combine tests in the boolean array form than with ` where ` indices .
This becomes an even greater mess for pairs diagonal
week1 ( weekn ) must fit the first ( last ) week of the log array .
Tip : you can get rid of loop by np.tile() the log and np.repeat() the week .
Actually there is a much easier way to know which week the log entry belongs without np.tile and np.repeat .
The first sum is supposed to be much slower than the second .
I apologize for such a dump question .
MASL I think your question is fine , there is no such thing as dump questions .
You need to call the NumPy array's ` sum ` method , not the plain Python builtin ` sum ` function , in order to take advantage of NumPy : #CODE
You're using the python ` sum ` on the numpy array instead of numpy's sum : #CODE
I think your best bet is to convert both the SFrame and SArray to numpy arrays and use the numpy ` dot ` method .
I think this can be done with an SFrame ` apply ` as well , but unless you have a lot of data , the dot product route is probably simpler .
If you now sum values in each row you will get your first element of vector : #CODE
Well , what are the shapes of ` image ` and ` kernel ` when you pass them to ` convolve ` ?
So you are expecting a list of unique columns and their counts ?
The counts appended to the unique columns of ` a ` .
I have the upper triangle entries ( including diagonal ) of a symmetric matrix in a flat list ( concatenated rows ) , and I want to use them to fill in the full matrix , including the lower triangle .
Your symmetric matrix as 3 distinct parts , upper and lower triangle and diagonal .
with an assignment of ` utri ` directly to the transpose of ` ret ` : #CODE
` np.tri ` is the base ' tri ' function .
How else does one locate the indices where 3 is located ?
I know a similar question was already asked for C# , but is there a way for me to append individual , 1D vectors ( with possibly different lengths ) together to form a multidimensional array ?
Say a function that counts the appearances of ones at each index of an array : #CODE
map a sum function to each element : CPU 16s GPU 140s
Arrays should be constructed using ` array ` , ` zeros ` or ` empty ` ( refer
Always prefer ` zeros ` over ` empty ` , unless you have a strong reason to do otherwise .
` empty ` , unlike ` zeros ` , does not set the array values to zero ,
This loop can take from few hours up to a couple of days and I can see that performances degrade with time ( I log the progress and I can see is much slower towards the end ) maybe due the growing sizes of lists ( ?? ) .
We flatten the arrays and stack them together , to form a single big array of shape ( 2,900 ) .
It's column stack that requires equal length strings .
So I tend to accumulate long ` ipython ` histories .
Assuming you mean what I think you mean this is a simple way to do it ( it's a bit shorter to use append but for large arrays always preallocate using numpy ) .
Get count of all unique rows in pandas dataframe
Then you could use ` reshape ` method of numpy array
Assuming that all of your data is binary , you can just sum the columns .
To be safe , you then use ` count ` to get the total of all non null values in the column ( the difference between this count and the previous sum is the number of zeros ) .
I have a little problem with the function gradient ( numpy ) .
You need that gradient of F inside the gradient of T , otherwise , the step size is just the array values of F .
You'll want to strip your datetime64 of time information before comparison by specifying the ' datetime64 [ D ]' data type , like this : #CODE
( Put three arrays in the tuple to concatenate , rather than two .
You might also have to strip newlines from the input lines .
list mean with max length : #CODE
I tried speeding things up by avoiding the ` for ` -Loops , but couldn't figure out how to broadcast the arrays properly so that I only need a single function call .
I had a similar issue , and my solution was to dump doctests .
How can I sum each element of ` List1 ` with each element inside each list in ` List2 ` ?
What is the best way to combine these functions so I do not need to make multiple passes back through the data to concatenate the results ?
Compared to all the list comprehension operations , ` concatenate ` is relatively cheap .
Look at the guts of ` np.vstack ` , etc to see how those use comprehensions along with concatenate .
In the case , numpy tries to " broadcast " them to the same shape .
If the matrices _aren't_ the same shape , numpy tries to [ broadcast ] ( #URL ) the arrays to the same shape .
There exists a numpy array ` multidimensional_array ` which has either has all integers and no zeros , or one zero among many integers : #CODE
@USER , if ` zeros ` is an empty list , the ` for ` loop will be skipped over .
If you append zeros_list then is should be : #CODE
Python Pandas - sum a boolean variable by hour
I want to sum the boolean variables by hour , so I have something that looks like : #CODE
` df.resample ( ' 1H ' , how= ' sum ') .fillna ( 0 )`
This is especially true for packages like NumPy and others from scientific stack .
I do that by calling an ' event ' a max or min of the data .
The list ` i ` in your code doesn't store the number of peaks per chunk but the peaks indices .
So , to sum it all up I think this is what you want .
Do they all have unique IDs ?
All will have unique id's , I originally implemented it as dict .
How about using a sorted list of the ids ?
Logical indices in numpy throwing exception
Umm ... matlab returns the full matrix in the latter case , not the diagonal values .
As I understand it , numpy will translate your 2d logical indices to actual index vectors : ` arr [[ True , False ] , [ False , True ]]` would become ` arr [ 0 , 1 ]` for an ` ndarray ` of shape ` ( 2 , 2 )` .
broadcast them to the same shape .
If they cannot be broadcast to the
both return the two first diagonal elements , so not a submatrix but rather a vector .
And then what I wrote above holds true : the logical values are translated into indices , and the result is expected to be a vector .
I don't know what functionality numpy has for this , but since your text file happens to be valid JSON , you could just load it as JSON , flatten it , and then convert the result to a numpy array .
The ids usually 6 digits and don't have order .
There is a ` scipy.sparse ` module that represents arrays with lots of zeros , but it can't handle ' objects ' , just real numbers .
Do you want to resize it ?
Once you have ` pillow ` installed , this answer should help : How do I resize an image using PIL and maintain its aspect ratio ?
However , scipy gives me error , here's a pastebin with the pip log .
My guess from viewing that log is that you ran out of memory .
each field is collected as bytes . float ( b'1 \n ') return 1.0 , but float ( b '" 8210 "') give an error . the converters option allow to define for each field ( here field 0 ) a function to do the proper conversion , here converting in string ( decode ) and removing ( strip ) the trailing `"` .
theano - use tensordot compute dot product of two tensor
I want to use tensordot to compute the dot product of a specific dim of two tensors .
I want to do a dot use A's third dim and B's second dim , and get a output whose dims is ( 3 , 4 )
So you should probably indicate that you want the diagonal of the result .
That's what ` dot ` is doing , as well .
Maybe the examples ' indices ?
That would just generate indices for training and validation folds .
You would subset your data with those indices and call ` fit ` and ` predict ` on the subsets , with Source removed , and then score it using the Source column .
The values of ` W ` and ` b ` always seem to increase , probably because the values of ` cross_entropy ` are always a vector of all zeros .
This vector must always sum to 1 .
The contents of the sum are ( double-precision ) complex numbers , so would have to be stored using 16 bytes per item .
My intuition may be off , but through numpy's conventions for the output of the fft and inverse-fft , respectively , shouldn't the image in real space be concentric circles - centered in the middle ?
I have to draw several ( structurally similar ) objects ( 300 / 400 max ) whose each one consists in :
How to copy contents of one numpy array to another while maintaining indices ?
Say I want to copy the contents of one array over another array , meaning the indices of the copying array are maintained and overwrite the first .
You can compare the input 3D array with the threshold ` x ` and then sum along the first axis with ` ndarray.sum ( axis=0 )` to get the count and thereby the percentages , like so - #CODE
I am thinking of operations like Ising spin renormalization where you divide a matrix into blocks and return matrix where each block is replaced by its sum , average or other function .
Or you could reshape it so all the values in one block are in one axis : #CODE
Now that all values in one block are segregated into one axis , you can sum along that axis , or take its mean or apply some other function : #CODE
Sample run to calculate blockwise ` sum ` , ` average ` , ` std ` , etc .
In Python 2.7 , integer division does ` floor ` : #CODE
' stack ' a bunch of copies of this image .
I'm trying to JPEG compress a greyscale image in Python with Numpy .
Sorry , forgot to reshape to match my input shape , see above .
@USER Hadn't come across gradient before , but it looks good for avoiding the need to calculate the drop one by one .
I think this is very close to what I need , only the coding is not as expected e.g. given a window of [ 3 , 2 , 0 ] , [ 1 , 6 , 9 ] , [ 4 , 4 , 4 ] , the output for the central pixel ( 6 ) is 4 ( i.e. South ) , but should be 128 ( NE ) , as with the direction coding here #URL Possibly the cause is the gradient array seems to have half its data set to zero .
I needed to make a few edits to run , but it's still not quite there - the diagonals are still coming out with zero gradient difference .
I've written a function which pulls the elements from non-uniform distribution and returns indices of elements of input array , as if they were pulled from uniform distribution .
Using my function I can pull elements from this distribution and get indices as if they were pulled from uniform distribution : #CODE
This generates ` n ` values from the unique set , then uses ` searchsorted ` to look for those values in the original array , and returns their indexes .
This is a part of a bigger problem , the key is selecting indices as if elements were uniformly distributed .
All articles are unique , and I want to pull 10000 unique articles so that each topic is about 33.33 % politics , 33.33 % sports and 33.33 entertainment .
You can kill the loopy portion which I am assuming is the most time consuming part there by extending the use of ` np.unique ` to incorporate ` return_inverse=True ` that would give us unique numeric labels for each unique string in ` x ` .
Those numeric labels could then be used as indices to lead us to a vectorized calculation for ` element_freq ` .
For instance , a ` sum ` calculation reaches the limits of the ` int16 ` variable ( all the AC column values are positive ): #CODE
ValueError : Shape of passed values is ( 14258 , 2 ) , indices imply ( 14258 , 11 )
I have written the following snippet in order to stack 3D arrays along a fourth axis , to apply rotations along this axis .
In the new version of Numpy ( 1.10.0 ) there is a ` stack ` function for ndimensional stacking .
` np.array ` ' stacks ' arrays along a new first axis ; transpose can move the new axis to the end : #CODE
I could also use ` concatenate ` if I first add a new axis to each component #CODE
The ` concatenate ` variants , ` vstack ` , ` hstack ` , ` dstack ` all use some sort of iteration over the ` args ` list to adjust their dimensions in preparation for concatenation .
But why stack / concatenate in the first place ?
And if you are going to ' stack ' arrays , and then iterate over them , it make more sense to do that along the 1st axis , not the last .
( MATLAB places the ' outer ' axis last , numpy places it first ) .
@USER As discussed in the comments earlier , those are for the * bad * places that are set later on by ` zeros ` , when ` np.where ` chooses based on the mask .
2 ) Get unique IDs for all elements and display them alongside input elements - #CODE
You can just loop through your array and when the current element is different from the previous then increment your counter and append the count to the current element like so #CODE
How to quickly grab specific indices from a numpy array ?
But I don't have the index values , I just have ones in those same indices in a different array .
Is there some ` NumPy ` method than can quickly look at both of these and extract all values from ` a ` whose indices match the indices of all ` 1 `' s in ` b ` ?
You could use ` compress ` : #CODE
Python numpy running sum of repeated trues
I would like to determine the running sum of true values , but reset to zero when an element is false .
I am getting a dimension mismatch error when I try to append values even though I have made sure that both arrays have the same dimension .
Why are you making a 1d array of zeros ?
` np.append ` is not the same as the list append .
Same for ` concatenate ` .
You can't append or concatenate it to an array of float zeros ( your original ` freq_177 `) .
If you must iterate and collect values , I suggest using list append , e.g. #CODE
Returns : append : ndarray
Note that append does not occur in-place : a new array is allocated and filled .
I was looking at Scipy.org for function append but all it says is that it appends values to the end of an array .
But , instead dot between two vectors , I want the op is the function to compute cosine similarity .
A generator expression does not build the list but hands a so called generator to ` join ` .
Not possible , the data looks like a downsampled histogram of a 2d detector showing some high energy peak , the peak you want to preserve is the sharpest peak of your data , hence not possible .
In the real data you can try medfilt from scipy.signal or fft filtering but in this
Detecting comic strip dialogue bubble regions in images
I have an grayscale image of a comic strip page that features several dialogue bubbles (= speech baloons , etc ) , that are enclosed areas with white background and solid black borders that contain text inside , i.e. something like that :
I'm open to any suggestions , including ones in OpenCV , etc .
I need to form a stack of three dimensions and extract a single dimension vector to compute a covariance matrix ( the red vectors in the picture ) .
@USER : To stack a collection of ( identically-shaped ) 2d arrays into a 3d array , you can either use [ ` numpy.dstack `] ( #URL ) , or in NumPy 1.10 and later , [ ` numpy.stack `] ( #URL ) .
And the log is : #CODE
Somehow my indices became corrupted , but indptr is fine : #CODE
How can I append these two as one array with shape ` ( 480 , 640 , 4 )` ?
Otherwise , to use ` append ` or ` concatenate ` , you'll have to make ` B ` three dimensional yourself and specify the axis you want to join them on : #CODE
Here , I think you want ` fv ` .
The most efficient way is likely to use ' np.empty() ' to allocate the space / memory for your end dataset and then load data & broadcast within that using slice indexing .
You can use boolean masks and draw random indices from an integer array which is as long as yours .
This gives you the indices for the selection : #CODE
Get all not selected indices : #CODE
You can make any number of row-wise random partitions of ` A ` by slicing a shuffled sequence of row indices : #CODE
If you don't want to change the order of the rows in each subset , you can sort each slice of the indices : #CODE
I'd like to add them along rows of b and concatenate , so that I end up with : #CODE
Python Array issue - string indices must be integers not tuple
I am being hit with a type error ` string indices must be integers , not tuple `
Furthermore , use ` rfft ( data , n=n_pow2 )` instead of ` fft ( data )` and multiply your resulting mean by 2 , where `` n_pow2=2** ( ceil ( log2 ( len ( data )))`` pads data with zeros to the next power of 2 .
For proper scaling of power values that have been computed as ` abs ( ` fft coefficients `) **2 ` , you will need to multiply by ` ( 2.0 / len ( data )) **2 ` ( Parseval's theorem )
So for simplicity , ensure that ` uF ` is strictly less than ` max ( freqs )` .
When you do fft the frequencies ranges from -pi / timestep to pi / timestep ( assuming that frequency is defined as w = 2*pi / t , change the values accordingly if you use f =1 / t representation ) .
Nth position is max frequency now you can easily determine your range #CODE
The justification is the Poisson summation formula , which states that sum of squares is not changed by a FFT ( or : the power is preserved ) .
I'm trying to do something along the lines of a masked broadcast , where only certain values are broadcasted .
This just returns the same thing a regular broadcast would , namely : #CODE
I was trying multiply data1 with 255 but I didn't try combination of min - values .
Or I could reopen the file the second time in append mode .
I think I understand your problem and in these cases , I usually find it easier to make a list and append it to the existing dataframe .
I'd guess the sum is exactly 1 and the answer to your question is yes .
For a quick check ( e.g. that your distribution is normalized ) , just sum the values of ` f ` and multiply by the grid spacing : #CODE
That's why ` A ` can be array , matrix or sparse - anything that implements ` dot ` .
But avoiding that will require changes to the Fortran code , changing ` B ` ( array , dimension N ) to a pair of smaller arrays , one representing the nonzero values , the other their indices .
and also keep |x| small , a.k.a. regularization , in the L1 or L2 norm
The indices of a slice ` slice ( start , stop [ , step ])` can be often represented by ` range ( start , stop , step )` ( or ` range ( *slice ( start , stop , step ) .indices ( length ))` when taking the underlying dimensions into account ) .
just for reference : ` range ( start , stop , step )` doesn't always produce the correct indices as it may also produce indices out of range , to create a valid range [ ` slice.indices() `] ( #URL ) needs to be used : ` range ( *slice ( start , stop , step ) .indices ( length ))` .
` np.ix_ ` ( and ` ogrid `) reshape indexes so they can be broadcast .
That is already a good start . len ( xrange() ) should be inefficient Could one use floor (( stop - start ) / step ) instead ?
I suspect you just have to go through the tedium of a analyzing each dimension , to build up either a new slice or an array of indices .
First dimension - make an array of the whole range of indices , and slice them sequentially : #CODE
For the third , let's do that same as with the 1st , but taking into account the offset between outer and inner lists : #CODE
` numpy ` is default C order , and prefers to append them on the left .
Except ( 1 ) it would be nice to append , not prepend , ( 2 ) it requires allocation of a new array .
My suspicion is that when I produce the error by initially generating col1 by iterative addition that compounds the errors in the later array indices .
One of the functions I am working on queries the catalogue and returns all the information for a given star id ( or set of star ids ) .
I get the ` TypeError : cannot perform accumulate with flexible type ` error when I try your list for ` delimiter ` .
I can mask out the -9999 values again , but as I will be applying the argmax function across axis 0 of the gradient array , the retention of the original values causes issues .
` gradient ` is not masked , so only gets the data part .
With np.nan in place , If I try gradient [ 0 ] .argmin ( axis=0 ) , the result is [ 1 0 0 0 ] , which I think means nan is being considered .
First I convert lower values of ` df ` to ` NaN ` by ` where ` and ` numpy.triu ` and then ` stack ` , ` reset_index ` and set column names : #CODE
I the only thing to watch out for is if you have any ` NaN ` values that you want to preserve in the upper triangle ( ` stack ` will drop them all ) .
I thought of that as well , however I don't know that it can be made to work when you have a dtype with nested levels , ie ` dtype= [( ' firstGroup ' , [( ' homework ' , int ) , ( ' classwork ' , int )]) , ( ' roomsOfTheHouse ' , [( ' bathroom ' , [( ' sink ' , str ) , ( ' tub ' , str )]) , ( ' kitchen ' , [( ' floor ' , str ) , ( ' counter ' , str )])]`
I've answered a lot of structured array questions , and some masked array ones , but have never explored their combination .
I can do the same comparison with the original ` a ` by converting it to numeric array - by using a ` copy ` , ` view ` and ` reshape ` .
All the examples on the net simply reshape the list into an array and save them as an image !
Regardless of whether your example is correct , I feel the need to note that ** any nonzero scalar multiple of an eigenvector is an eigenvector** .
Any nonzero scalar multiple of an eigenvector is an eigenvector .
Using python load images from directory and reshape
I want to load same images from directory and reshape them using reshape function using python .
Load the images from a directory , then reshape them with Python .
Assuming that you have scipy installed and assuming that with " reshape " you actually mean " resize " , the following code should load all images from the directory ` / foo / bar ` , resize them to 64x64 and add them to the list ` images ` : #CODE
If you need a numpy array ( to call ` reshape `) then just add ` images = np.array ( images )` at the end ( with ` import numpy as np ` at the start ) .
find max coordinates xMax , yMax in list
fill it with zeros
For large lists this numpy solution may be faster , but probably not for small ones .
I could calculate the ` cnt ` list with the same sort of logic as in ` onestep ` ( or ` onestep1 `) , except accumulate a count value rather than sublist .
By default it will adjust the L2 ( Euclidean ) norm of each row of features to be equal to 1 .
This is not the same as making the elements sum to 1 : #CODE
Since the sum of ` custom ` is much greater than 1 , the cumulative probability of drawing a particular integer reaches 1 before you get to the end of the probability vector : #CODE
What you ought to in order to normalize ` custom ` is divide by its sum : #CODE
why is dot product in dask slower than in numpy
a dot product in dask seems to run much slower than in numpy : #CODE
The problem is caused essentially by matrix size , not by dot and the chunks I believe have a major role in this .
The calculation of the dot product in dask runs much faster when adujusting the chunks : #CODE
The top values are my ( calculated ) values , and the bottom values are the NASA ones .
The other problem , I think , is that your rotation matrices are the transpose of what they should be , per the document you described ( see below ) .
It is : ` - ( sin ( omega ) *cos ( Omega ) +cos ( omega ) *cos ( i ) *sin ( Omega ))` .
Note that since ` x ` has shape ( 3 , 3 , 3 ) and ` xmax ` has shape ( 3 , 3 ) , the expression ` x == xmax ` causes NumPy to broadcast ` xmax ` up to shape ( 3 , 3 , 3 ) where the new axis is added on the left .
How to replace all zeros in numpy matrix with corresponding values from another matrix
If you are doing the transformation in-place ( e.g. fixing zeros in a matrix ) , the first option is best .
and I want to apply the following permutation : #CODE
At end of operation I want to have one hot vector which will have 3^ 7-1 zeros and one 1 values .
It worked after tiny change ` shape = max ( a ) + 1 ` .
Error Message : ` ValueError : Shape of passed values is ( 475 , 243 ) , indices imply ( 83 , 243 )`
if numpy.dot is implemented by underlied BLAS gemv / gemm ( even dot ) , the return value should be equal . but the testcase shows the proposition is negative .
Your question asks if they're * equal * ( they are , see my first comment ) , but you comment asks if ` dot ` is * implemented * using gemv or gemm ; those questions are not the same .
If there's a constraint such that every row _does_ have the same number of elements , then you can flatten and reshape .
With ` sum ` I find how many elements there are in each row , and then use this to ` split ` the flattened array into sublists .
And yes , the diagonal elements ` A [ i , i , : , :] ` are all 0s .
using pandas and numpy to parametrize stack overflow's number of users and reputation
Instead we have something similar to a histogram .
What we've plotted is similar to a histogram , in that it's raw counts of the number of users at a given reputation level .
Our data is basically an estimate of the complimentary cumulative distribution function ( CCDF ) , in the same sense that a histogram is an estimate of the probability distribution function ( PDF ) .
First off , note that ` log ( 1 ) -- 0 ` .
Thank you @USER for pointing out to where the raw data is , the log graph looks very interesting .
Suppose that I have a np.array of ` shape ` ` ( 3 , 3 )` that contains cells filled with 3 zeros , or with any other specified value .
I want to mask the cells that are identical to ` value ` , the mask should hide all the cells filled with 3 zeros ( or any other specified ` value `) , so :
if you just don't need zeros you could similar to @USER : ` example_array [ example_array ! = 0 ]` which will give you ` array ([ 1 ., 2 ., 3 . ])`
One question was about the `'` and ` {} ` syntax in MATLAB ( transpose and cell ) .
In addition it defines some tensor products - multi dimensional versions of the matrix or ` dot ` product .
As far as I am concerned I know the result of multiplication of a matrix with its inverse must be an identiy matrix namely , its main diagonal being ones and having zeros elsewhere .
I'm getting ones on the main diagonal , fine , but not zeros elsewhere .
and when I try ` np.allclose ( x1.dot ( inv ( x1 ) , np.eye ( 4 ))` I'm returned true .
Why the result of x1.dot ( x1_inv ) doesn't seem what I would expect it to be ( an identity matrix with ones on main diagonal and zeros elsewhere ) ?
If the distinction is too confusing stick with ` concatenate ` or one of the ` stack ` functions .
I am trying to sum a list of NumPy vectors in a list .
( In this example it's a list of 2 items , but in my case the list can be of any size . ) How to sum them into a new vector ?
I'm not aware of an easy replacement for numpy's ` roll ` , but scatter seems like a decent candidate .
Thanks , what about numpy's roll ?
Oups , I added an edit for roll .
For a 2-dimensional tensor ` x ` shifted by ` n ` columns you could use the following pattern ( and extend it to n-dimensional tensors ): ` s = x : size() [ 2 ]; y = torch.Tensor() : resizeAs ( x ); y [ {{} , { 1 , n }} ] = x [ {{} , { s-n +1 , s}} ]; y [ {{} , { n+1 , s }} ] = x [ {{} , { 1 , n}} ]; ` where ` s ` is the number of columns , the first ` y [ ... ]` command copies the last ` n ` columns to the first ` n ` of ` y ` and the second command copies the remaining first columns of ` x ` into the last ones of ` y ` .
Because of this fact , you could also use ` hstack ` or ` concatenate ` to achieve the same thing ( they also coerce the lists to arrays that have the correct shape for what we want ) .
The basic ` concatenate ` works because ` np.array ( a )` produces a ` ( 5 , 1 )` array .
` hstack ` and ` column_stack ` also join on the ` 1 ` axis , adjusting dimensions if needed .
But if I double the size of the array to ` ( 1200 , 800e3 )` I hit the swap , and it takes ~ 2.7 min to create ` db ` ;( #CODE
You could give it an object that compares the elements of your 2 arrays ( in what ever way is appropriate given the ` dtype `) , and then test for a nonzero count .
I know how to do it via ` SQL ` using ` left join ` and aggregate functions but I do not know how to replicate this using ` Python ` / ` pandas ` .
The problem is - it is only returning the rows with an order ( 1st row and 3rd ) but not the other ones ( 2nd row ) #CODE
I want to create an ` out ` vector that consists of ones where the corresponding row in ` db ` matches the ` mask ` and ` target == 1 ` , and zeros everywhere else .
I have defined a ` vline ` function that applies a ` mask ` to each array line using ` np.array_equal ( mask , mask vector )` to check that vectors 101 and 111 fit the mask , then retains only the indices where ` target == 1 ` .
Numpy unique on each row and not on all elements
And I need to have unique run on rows , rather than elements , so that my output is : #CODE
What is the numpy-ish way to get a darray from this file , and then some matplotlib way to plot diff vs . datetime ( doesn't matter if diff ` n-1 ` or ` n+1 `) ?
Python / pandas / numpy : ' StringMethods ' object has no attribute ' capitalize '
correctly works and produces my array I am looking for , however in my local python anaconda distribution of the same version of 2.7.6 , I get `' StringMethods ' object has no attribute ' capitalize '`
From what I've read on #URL this capitalize string method is " locale-dependent " which I'm not quite sure what that means but I can only conclude this is the reason for the error .
The error message suggests that the result of the ` str ` ` StringMethod ` as applied to a ` pandas ` ` Series ` does not have the attribute capitalize - so I would be looking for issues with the pandas version rather than with the ` python ` ` string ` methods .
NumPy precision when doing dot product
Currently I prefer to use the second to computed the dot product since it seems that it can learn better weights .
Is it caused by different implementations of dot ( matrix , vector ) and dot ( vector , vector ) ?
The reason it looks so big is that the summation of the dot product amplifies it .
It's that summing in the dot product that makes it * seem * so big - just the cost of doing business using float32 .
Apparently ` dot ` follows different code paths depending on whether the first argument is 1D or 2D , and the different paths result in slightly different numerical errors .
oh yeah . the more common idiom is to use ` strip ` to get rid of the the ` \n ` but I suppose that's no better than your solution . btw the reason why you would use readlines instead of read is in case the file is too big to fit in memory .
Your stack trace doesn't match your code .
The other is a word like ' log ' .
It's important to provide the full stack trace on SO when asking for help debugging these kinds of problems .
This code was originally posted here : Is there a builtin bitset in Python thats similar to the std :: bitset from C++ ?
I need a very big matrix ( 2.000.000 x 20.000 ) filled with zeros ( int ) .
At the moment I need about 10 min to create the final matrix and about 1h to write it to the disk .
But here I format the 2 fields separately , and then join them for the write .
I could have used ` %s ` to format the strings , but ` join ` works just as well .
Actually I wouldn't even have to join the int array and string array into one structured array .
Both tensors matrices the first tensor containing 100 10x9 matrices and the second containing 100 3x10 matrices ( which I have just filled with ones for this example ) .
My aim is to multiply the matrices as the line up one to one correspondance wise which would result in a tensor with shape : ` ( 100 , 3 , 9 )` This can be done with a for loop that just zips up both tensors and then takes the dot of each but I am looking to do this just with numpy operators .
I tried using np.tensordot , np.einsum ( read here #URL that it is supposed to do the job but I did not get Einsteins indices correct ) also in the same link there is some crazy tensor cube reshaping method that I did not manage to visualize .
the two ` j ` sum and cancel out .
Reshape to 3d , ` T1.reshape ( 300 , 2 , 24 )` , and after reshape back ` R.reshape ( 100 , 3 ,... )` .
Or is it like ` k ` and unique to one of the inputs .
Then you have a chance at doing the same for the outer ones .
Looking at the docs , it seems that ` itertools.product() ` accepts any iterable , including generators such as the ones I've suggested above .
Thank you , generator improves the efficiency of inner loop , but I think improve the efficiency of outer loop is more significant
Next step would be reduce the outer iterations .
For example start with ` np.eye ( N )` , and just iterate on the lower tri indices : #CODE
Replacing ` nonzero ` with boolean indexing : #CODE
Since the number of nonzero values for each row of ` annot ` can differ , the number of terms that are summed for each ` score ` also differs .
Given you image is just a stack trace , please just post the text in a code block
There is an ` accumulate ` function in ` itertools ` module : #CODE
Find cumsum of subarrays split by indices for numpy array efficiently
Given an array ' array ' and a set of indices ' indices ' , how do I find the cumulative sum of the sub-arrays formed by splitting the array along those indices in a vectorized manner ?
Also , if I had the same problem , but a 2D array and corresponding indices , and I need to do the same thing for each row in the array , how would I do it ?
You can use ` np.split ` to split your array along the indices then using python built in function ` map ` apply the ` np.cumsum() ` to your sub arrays .
If you want to such results you need to add 1 to your indices : #CODE
@USER How you want to split a 2D matrix with ` indices ` array ?
I have an array of arrays as so : indices = np.ndarray ([[ 3 , 8 , 14 ] , [ 4 , 9 , 17 ]]) corresponding to an array with two rows .
There is essentially no difference in performance between ` np.hstack ( map ( np.cumsum , np.split ( array , indices )))` and a regular ` for ` loop or generator expression , e.g. ` np.hstack ( np.cumsum ( a ) for a in np.split ( array , indices ))` .
Besides , that you wasn't aware of this point , about the benchmark you should try to benchmark with large datasets to find the difference , and about the generator exp you are wrong because each generator exp would be converted to a generator function by python and calling the ` __iter__ ` and ` __next__ ` attributes would makes the code to takes more time than even a regular loop .
Even for a 1000000-long array with 100 indices I see a difference of about 13 ms in runtime between using ` map ` and a generator expression .
You can introduce differentiation of originally cumulatively summed array at ` indices ` positions to create a boundary like effect at those places , such that when the differentiated array is cumulatively summed , gives us the indices-stopped cumulatively summed output .
This should be an almost vectorized solution , almost because even though we are calculating linear indices in a loop , but since it's not the computationally intensive part here , so it's effect on the total runtime would be minimal .
@USER What's the shape of 2D array and number of elements in ` indices ` in your actual 2D case ?
300*100000 is the shape of array and number of indices is between 3000 to 20000 for each row , so that's 900000 to 6000000 indices in total .
Can we ravel this , perform the operation on modified indices , then unravel it again ?
In this question the stack command is used to literally stack the arrays together .
The latter ( tested on numpy version 1.8.2 ) produces an array of two objects while stack produces a single numpy array .
Same as you did with ` d3 ` only you have to reshape ` d4 ` into a 3-d array :
` d3 = concatenate ([ d3 , d4.reshape ( 1 , 18 , 1 8)]) `
I am now iterating over industries.T ( the transpose of industries ) which means industry is a 20-element array with the same industry code in each element .
Is there a way to achieve this in numpy purely based on ` roll ` , ` hstack ` etc ., i.e. without using any ` for ` loops ?
As in that MATLAB question , if your kernel is x-y separable then you could express it as two 1D vectors of weights , then construct two separate Toeplitz matrices as above and compute two separate dot products along different axes of your image array .
The rest of your code should then work as you expect ( note that it's not necessary to call ` sum ` ; just ` print ( " nan : %d " % dfv [ np.nan ])` suffices ) .
Python / numpy : Most efficient way to sum n elements of an array , so that each output element is the sum of the previous n input elements ?
Is there a scipy / numpy method to get the indices for nearest interpolation ?
However , I realized that the spatial structure of the data is constant in time ( Eulerian ) , so once I figure out which indices correspond to this or that point for interpolation for , say , the first time step , I could just obtain those indices , and the same indices should be able to create the same shaped , interpolated data for other times .
` scipy.interpolate.NearestNDInterpolator ` is built on top of [ ` scipy.spatial.cKDTree `] ( #URL ) , which can return nearest-neighbour indices as well as distances
` cKDTree.query ` will return the nearest-neighbour indices as well as the corresponding distances for a given set of input coordinates .
Each row is basically a permutation of the numbers from 1 to 5 .
The zeros need to be filled now .
So fill the zeros in that order , 5 for first zero , 1 for the second .
Here , I iterate over the grouped lists and save the index ` k ` to determine which indices to use from index_list .
Note that this only works when the indices are used from the largest to the smallest , since otherwise the positions shift .
The outer list knows nothing about the nature of the inner lists .
I am thinking of re-sample all the time columns into uniformly separated ones based on the starting point .
Index Numpy tensor without having to reshape
How can I access an element along the 0th axis and still maintain 3 dimensions without needing to reshape .
Is this possible without needing to reshape ?
Now what I would like to do is to use indices as a mask on a to replace every max element with say -1 .
You can use ` indices ` to index into the last dimension of ` a ` provided that you also specify index arrays into the first two dimensions as well : #CODE
I am trying to find the element value that has ` max ` count . and if there is a tie , I would like all of the elements that have the same ` max ` count .
You can use np.unique to get the counts and an array of the unique elements then pull the elements whose count is equal to the max : #CODE
un are the unique elements , cnt is the frequency / count of each : #CODE
` cnt == cnt.max() ` will give us the mask to pull the elements that are equal to the max : #CODE
the outer set of ( ) doesn't make a difference .
If you want to collect both ` y ` and ` err ` , but in separate lists , use ` zip* ` to repack them ( something like the numpy transpose ) .
I would like to only calculate the distance vector as opposed to the entire matrix and then selecting the relevant diagonal .
` coslat ` is the array of cosines of the latitudes , and ` coslat [: -1 ] *coslat [ 1 :] ` is the vectorized version of the expression cos ( ? 1 ) cos ( ? 2 ) in the Haversine formula .
Python given numpy array of weights , find indices which split array so that sum of each split is less than value
I need to find the smallest array of indices such that when w is split by these indices , the cumsums of split arrays less than the corresponding capacities in c .
I need to find the smallest number of indices ' i ' so that the cumsums of split arrays less than the corresponding capacities in c .
An approximation of the required indices is also fine .
In this case , I just have to ensure that each split array has to have sum less than a given value ( somewhat similar to bin packing ) .
I find the indices as follows .
This gives me the indices .
One thing to note is that this might give me wrong indices , because cumulative sums are calculated from the beginning .
dividing cumsum by 5 and taking ceil gives me [ 1 , 1 , 2 , 2 , 2 ] which would correspond to splitting indices of [ 1 , 4 ] .
But the actual splitting indices are [ 1 , 3 , 4 ] .
I initially did it using a for loop , then I approximated it by finding the cumulative sums , dividing them by c , rounding them up , finding the diffs , and then getting the indices of the 1s .
The region I'm interested in is in the beginning ( of ` autocorr `) , so I think I can't simply truncate the input arrays
If you're really only interested in the first N elements in the result then you definitely can truncate your input array - you just need to make sure that you correlate at least the first N elements in ` a ` with the last N : #CODE
So , to save on memory and hopefully better performance , you can create unique paired IDs with ` np.triu_indices ` and modify the earlier listed approach , like so - #CODE
By picking where this slope changes from positive to negative you can find out the indices of the peaks in your original array .
I didn't bother to make a list of the time , but since you hace the indices ...
In my book , list comprehensions are very ' pythonic ' ; nested ones even more so .
The first step is convert the arrays in new [ 0 , 1 ] array and sum ( i.e. 3 = data in each array , 2 =d ata only in two array , 1 =d ata in olny one array , 0=no data ) #CODE
the second is sum all original array and then dived by dd #CODE
When I sum a value with nan I got nan .
Why bother with ` nonzero ` ?
The solution that you had before with idx and nonzero was more intuitive to me .
Or ` np.count_nonzero ` , which ` nonzero ` uses to preallocate its output arrays .
I am wondering how to combine two array of different shape to a new array but not change the original shape because the reshape information is import to future process .
Python list do not support the broadcast features of ` numpy ` arrays ... you have to write explicitly things like ` [ x [ 0:3 ] for x in c ]` .
The answer is there : Increment given indices in a matrix
I think the question is not about incrementing the indices .
The question is about incrementing the value of the array at a given set of indices ( based on the example given in the question ) , and based on the phrase " increment specific locations of an array " .
How to reassign numpy indices after linear transformation
Suppose I have rotated all the indices of a numpy array by an angle ( matrix mult with rotation matrix ) .
These rotated indices are in a tensor of dimensions ( width_img*height_img , 2 ) ( assume width= height for this case ) where img is the numpy array .
Is there a way of of using these indices to rotate the image ? as in reasigning them some how ?
I have used einsum to rotate all indices myself and verified it works its just the reasignment thats not working out .
i'd like to fit a sin wave by using neural net and python numpy , but my program can't fit the sin wave , i i think i miss something in backpropagation
You might consider sigmoid , tanh or relu .
How to concatenate pandas.DataFrames columns
Are all the values in ` Distances1 ` , ` Distances2 ` , etc . unique ?
And are all the values in ` Distances1 ` , ` Distances2 ` , etc . unique ?
The distances are not unique .
You could ` stack ` the frame like so : #CODE
I try to transpose a large numpy matrix using matrix.T .
I think it transpose . but the 3 dots are just presentation ( prevent printing huge matrix on screen )
You'd have this problem whether you are trying to do the transpose or not .
It's not about transpose .
The idea was to generate the matrix to be able to transpose it .
