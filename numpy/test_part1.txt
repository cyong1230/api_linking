OLS solution using pinv / svd #CODE
The bad days are eliminated , and the good ones are kept .
From this I've gathered that ` len() ` will return the number of rows , so I can always us the transpose , ` len ( A.T )` , for the number of columns .
shape mismatch : value array of shape ( 1000 , ) could not be broadcast to indexing result of shape ( 1000,3,255,255 )
@USER Well in this case , ` searchsorted ` is basically looking for places or indices where elements from ` message ` exists in the keys of ` codes ` .
@USER -- I had a hard time remembering how ` translate ` and ` maketrans ` work for quite a while too , but I've gotten used to it .
Edit : if you're using a version of numpy > = 1.8.0 , then ` np.linalg.eigvals ` operates over the last two dimensions of whatever array you hand it , so if you reshape your input to an ` ( n_subarrays , nrows , ncols )` array you'll only have to call ` eigvals ` once : #CODE
` reshape ` returns a view of the original array , not a copy , so the conversion to 3D only requires altering the ` shape ` and ` strides ` attributes of the array , without having to copy any of the actual data .
sum this new array along particular axes ; and then maybe
Calling ` reshape ` returns a view , so it doesn't incur any big copying costs or anything like that .
so at some point in the execution it will max my memory .
If ` len ( vals )` is too long , do you want to truncate ` vals ` ? and if ` len ( vals )` is too short , do you want to fill the rest of the array with ` 0 ` s ?
Note that extension to even higher combinatorics should be trivial , along the lines presented ; but keep an eye on the n used in that case .
These functions return a list , which I convert to a numpy array and then sum over .
Can it be because of the many zeros in the initial table ?
I also understand that sum ( A , axis=1 ) will sum each row .
But what I really want to do , is to bin ` array [: , 1 ]` by day ( as derived by the unix timestamps in array [: , 0 ]) , and plot these as a stacked histogram , with each ( colored ) stack representing a day .
It's interesting to see that when I go back to ` nloop=1000 ` , ` nreps=3 ` I actually see a slightly * greater * rate of cache misses for the row sum ( 17% vs 13% ) , even though it's faster than the column sum .
You can concatenate arrays in ` numpy ` .
If you are 100% sure that l2 would only be one column then you can reshape that array to make it one dimensional before doing the subtraction .
You won't be able to create a 2D array that way , and @USER method of returning a 1D array that you reshape afterwards is a sure go .
I have a square matrix A ( could be any size ) and I want to take the upper triangular part and place those values in an array without the values below the center diagonal ( k=0 ) .
You can mimic this behavior with a simple function to flatten a list : #CODE
So ` popt ` , according to the documentation , returns * " Optimal values for the parameters so that the sum of the squared error of f ( xdata , popt ) - ydata is minimized " .
And I'd like indices ` i ` such that , #CODE
But , ` resize ` looks like it just might be the thing I'm looking for ...
` rfft ` , apart from repeated terms excluded , and an almost 2x speed-up , returns the exact same you would get from ` fft ` .
Plus , if I have 4 dimensions , I thought I should have 4 eigenvalues and not 150 like the eig gives me .
If I run your code to generate ` d ` and ` dx ` with ` eig ` I get the following : #CODE
In other words- it is not just taking a min or max .
D [ I+1 , J+1 ] = map ( norm , x [ I ] -y [ J ]) + np.minimum ( np.minimum ( D [ I , J ] , D [ I , J+1 ]) , D [ I+1 , J ]) ?
` dot ` just has tighter code for a specific combination of dimensions .
numpy sum does not agree
Since you are only adding many ` 1 ` s you can convert ` diff ` to ` bool ` : #CODE
It isn't mathematically possible to represent 0 on a log scale , so the first value will have to either be masked or clipped to a very small positive number .
possible duplicate of [ Efficiently count the number of occurrences of unique subarrays in NumPy ? ] ( #URL )
Or you could mask the x value as well , so the indices were consistent between x and y #CODE
Here the ` outer ` method of the ` multiply ` ufunc is used to create the new 20x20 array .
I have a 3D numpy array consisting of 1's and zeros defining open versus filled space in a porous solid ( it's currently a numpy Int64 array ) .
You are attempting to broadcast a 4-D array together with a 3-D array .
Scipy NDimage correlate : unbearably slow
I know that I can reshape the array to a 100 x 2 array of grid points : #CODE
You probably could get ` append ` to work , but it just does a step by step concatenate , which is slower .
This produces a random permutation of each column's indices .
As it happens , the histogram is enough for the former .
I see how the symmetry of the trace lets you replace the final ` dot ` .
In that question , I sought to sum values in a numpy structured array based on multiple criteria , including matches in a list .
Assuming I understand you , and you're looking for the indices where either the element is True or the next element is True , you can take advantage of ` shift ` and use #CODE
to delete the lines that had zeros in them !
Fill scipy / numpy matrix based on indices and values
It looks like a vector product followed by a sum along the resulting array .
The trick is that this convolve function can be used in-place so the double for loop : #CODE
But this reshape should produce a ` ( n , 1 , 1 )` array , not your ` ( 1 , 1 , 1 ,... )` array .
For an extreme example , consider a sequence that consists of 9 zeros followed by the result of a coin toss , 9 zeros and another coin toss , etc .
If so then ` np.array ( a )` is a 2d array , and you can sum over ` axis=1 ` .
I am trying to create a lat / lon grid that contains an array of found indices where two conditions are met for a lat / lon combination .
This NAMBE is the absolute difference between a base vector and another vector , divided by the base vector and multiplied by a hundred , in pseudo-code notation : #CODE
this my code to and i want to use histogram data to plot scatter where y axis is counts center from the histogram , is there any direct command or way to do this ?
Please compile with ` cython -a ` , then show us the C code that the ` a [ 0 ] += sum ` line turns into .
The revised question is still a duplicate , see [ this question ] ( #URL ) , and [ this question ] ( #URL ) for finding the indices .
ValueError : operands could not be broadcast together with different shapes in numpy ?
There are thousands of numbers below the ones shown here .
Assuming you want to align all the arrays to the left , and pad to the right with zeros , then you could first find the maximum length with #CODE
How to do the same If I want to apply norm column-wise to a matrix ?
The easiest approach is to reshape to data to a long format using ` .stack ` , which can be be passed straight into rolling mean .
It's pretty low-level , and mostly focused on how to address the more difficult problem of how to pass C++ data to and from NumPy without copying , but here's how you'd do a copied std :: vector return with that : #CODE
` std = RMS ( data - mean )` .
This generalized diagonal would be defined as those elements of the array whose 0th and 2nd index coincide , and would have shape ( 3 , 3 , 7 ) .
I have a given array ` [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 2 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 2 , 1 , 0 , 2 , 3 `] ( arbitrary elements from 0-5 ) and I want to have a counter for the occurence of zeros in a row .
To see the benefits of this , you need to use ` z , p , k = butter ( output= ' zpk ')` and then work with poles and zeros instead of numerator and denominator .
In that case you would " extrapolate " zeros to the left and the right .
can numpy interpret column of indices like matlab does
To get the diagonal elements you can get their indices with ` np.triu_indices ` ( or , for the lower triangle , ` np.tril_indices `) and then index by them .
The question states that the input array is of shape ` ( 128 , 36 , 8) ` and we are interested in finding unique subarrays of length ` 8 ` in the last dimension .
What does work , however is nesting append and concatenate #CODE
( ` b ` will be broadcast along ( ? ) the first axis ) #CODE
As he points out , the ` [ 0 ] [ 1 ]` element is what you'd want for ` cov ( a , b )` .
returns ` 1 ` , making the sum not commutative !
But as I have a log of values ( 10000+ ) , this will be quite slow .
@USER - good point . anyway , ` diff ` works on python lists too .
It will also work if they are both arrays that can be broadcast .
It's column stack that requires equal length strings .
In the end it is usually not too complicated , especially if you use [ ` mgrid `] ( #URL ) or similar to get the indices .
The absolute error will be at most 1 / 2 ULP , 2 -150 .
AttributeError : ' Add ' object has no attribute ' log ' Python
Or , you could initialize an array of all zeros if you know the size of the array ahead of time .
Are you checking shape or number of nonzero values ?
Something like ` eigvals , eigvecs = la.eigh ( mat )` ` principal = eigvecs [: , eigvals.argmax() ]` ` if ( principal > = 0 ) .all() or ( pricipal <= 0 ) .all() : print ' all the same '` ?
I also want bins to have a width of .5 so that I can have a bin from 10.5 to 11 or 24 to 24.5 etc ... because otherwise , python outputs the histogram with the bins random and undetermined .
Maximum is always bigger than the minimum ( more to the right on a 1d axis , not by absolute value ) .
should give the sum of the columns .
Suppose , You wanna check how many times you will get six if you roll dice 10 times .
With this option , the result will broadcast correctly
Do you mean ` indices = np.where ( a == a.max() )` in line 3 ?
The problem I have much later on in the code is that if one of these parameters isn't in the ASCII file it throws errors up so I have to keep adding in ones I don't need .
` append ` adds them to the end of the list , which is exactly what you want .
I have two 3dim numpy matrices and I want to do a dot product according to one axis without using a loop in theano .
you have at most 4 in that dimension ( see your reshape line ) , so the index it will count are 0 and 2 ( 1 and 3 are skipped , and 3 is the last element ) .
Once we have the indices to sort ` data ` , to get a sorted copy of the array it is faster to use the indices than to re-sort the array : #CODE
I hope this will help you perform your transpose and column-wise operations
It is better to specify that I'm looking for something that performs the log-sum-exp trick , doing a simply succession of exp elem-wise , summing the rows and doing a log elem-wise is trivial in ` scipy.sparse ` .
Scipy uses ` int32 ` to store ` indptr ` and ` indices ` for the sparse formats .
But not able to plot it as a graph ( something like a histogram ) ... that is the problem .
It gave error testing doesnot have attribute append as its of None Type .
In both cases , you can access individual elements by indices , like ` R [ 0 ]` ( which would give you a specific object , a ` np.void ` , that still gives you the possibility to access the fields separately ) , or by slices ` R [ 1 : -1 ]` ...
I think you can have a sum over a sliding window ( or a rolling window ) or a mean over a sliding window .
I got your point and I find it more logical , but when trying the code you've suggested to get rid of the second error I got another error : ` AttributeError : flatten `
` dot ` does many things under the hood , it is apparent that ` np.dot ( A , x )` is not calling BLAS and is somehow defaulting over to numpy's internal GEMM routine .
Below is some code which uses a callback to print out the current azimuthal and elevation angles , as well as append them to a list for further use later .
It's super alex , here to answer NumPy questions in the blink of an eye :)
Your solution of searching the eigenvalues for the ones you want seems plausible enough .
If d is larger than 8 or 9 , then bases will be sufficiently long that you probably would be better off going with the other version using the dot product .
Interesting , in R ` Reduce ( ' + ' , s )` yields the same sum as in python .
I'm not sure which indices i need to change to achieve the minimum and not the maximum values .
The dimension of ` result ` has been set earlier to the correct dimension , so can check it , but it would be nice to only use the length of ` indices ` to determine it .
Alternatively , what about applying the same function without indices along the depth axes ?
Here's an O ( n log n ) algorithm for your problem .
You need to add axes to ` coeffs ` so it will broadcast in the dimension ( s ) you want .
If you want to search for a certain rank on B randomly , you need to start off with a valid B with max rank , and rotate a random column j of a random B i by a random amount .
I want to save some histogram data in a csv file .
I want to read a mat file back in python but I have trouble going back to a graph , because the mat file gives a numpy.ndarray type file and I need a sparse matrix to reconstruct my graph .
numpy makes it easy to translate python objects into numpy ndarrays , and will even pick an appropriate resulting data type if one is not specified : #CODE
This ` T ` and ` X ` broadcast together just fine , for example ` T*X ` works .
I assume the number produced in exp is too big to fit in a ` float64 ` .
I have a numpy matrix A and I need a function that will count ( A [ i , j ] / sum of all elements in i-th column ) - A [ i , j ] / sum of all elements in j-th row
This also works if , instead of a single index , you provide an array of indices : #CODE
How to solve nonlinear equation without sympy ( max and min ) ?
Bivariate Legendre Polynomial Fitting to find orthogonal coefficents
For numpy this list includes ' linalg ' , ' fft ' , ' random ' , ' ctypeslib ' , ' ma ' , and ' doc ' last I checked .
I have a big n-square diagonal matrix , in the scipy's sparse DIA format
To find the most frequent value of a flat array , use ` unique ` , ` bincount ` and ` argmax ` : #CODE
The funny thing is in the above function If i pass an extra argument and just divide sum by it , then the times are the same again .
are the same as the ones posted in the examples of this web page .
How to remove rings from convolve healpix map ?
With the information of the full stack trace report the bug to the ubuntu team .
fastest way to get lookup table indices with numpy
Well , a few more , anyway : ` cos ` , ` pi ` , ` diag `
I implemented a LOWESS smoother ( which is the curve you see ) with a tight fit to eliminate noise , since the real waveforms have a non-trivial noise component , and then tried doing a rolling max with a window over the data , but I can't get anything solid .
But sum function from numpy doesn't suport " 1:3 "
( the ` np.nonzero ` should return a tuple with one element , an array of indices ) .
Can the " small values of derivative " be small with respect to the sin curve ?
6 columns , 92370574 rows , 2496502 locations , 37 months each , unique amounts for each value .
Note that where possible , ` reshape ` will give you a view of the array .
Here you append only a REFERENCE to your only one existing ` energy ` array .
And you can combine the summation and multiplication into a dot product : #CODE
For example , ` a ` is generated from ` a = z [ z ! =0 ]` ; ` a ` then changes through some processing , and now I need to insert ` nan ` s where there were originally zeros .
I frequently use the numpy.where function to gather a tuple of indices of a matrix having some property .
I suspect the original formula was right but you didn't encode it right in Python .
This gets me the sum of all red combined in original - all red combined in mutated .
` p2 = einsum ( ' nk , nk -> n ' , p1 , delta )` is the pairwise dot product of the rows of ` p1 ` and ` delta ` .
I did the reshape , just so that both arrays are same shape , but I do not think you really need the reshaping , with the list comprehension the shape of array you get is ` ( length of string , )`
I made a typo in the norm , fixed now .
Also , I expect the positions of the zeros to be relatively sparse ( ~1% of all bit positions ) .
Slicing arrays with meshgrid / array indices in Numpy
( An nonzero exit status usually indicates an error on Unix style systems . A couple programs are different , e.g. , ` diff ` . ) Try examining the ` stderr ` produced by the subprocess to see what error messages are printed there .
To achieve exactly what you are asking for I would apply a ` [ 3x3 ]` box-filter on the image and than I would resize the matrix using nearest neighbor interpolation .
Is there a quick way to reshape my ` csr_matrix ` without copying everything in it ?
The catch is that I need to keep the colors exactly the way they are ( background : I'm resizing a map where provinces are color-coded ) , and so I cannot just perform a resize with bicubic interpolation , because that will also interpolate the pixel colors while smoothing .
You should " flatten " the array of arrays first . unfortunately , there's no builtin method , see #URL
then concatenate the saved objects whit this code : #CODE
For something like a dot product , pandas ` DataFrames ` are generally going to be slower than a numpy array since pandas is doing ** a lot more stuff ** aligning labels , potentially dealing with heterogenous types , and so on .
I want to pass an array of indices and column names and get a list of objects that are found in the corresponding index and column name .
From this you would expect the total sum to be ` 100,679,697 = 200* ( 1,000,000 - 499,097 ) + 499,097 `
The histogram way is not the fastest , and can't tell the difference between an arbitrarily small separation of points and ` 2 * sqrt ( 2 ) * b ` ( where ` b ` is bin width ) .
} for n=1 , 2 , 3 , 4 , 5 , 6 ( using Sum ( c_n exp ( i 2 pi n x ) ) as Fourier series ) .
I think I can t just simple sum the " seq * " array , because instead of a chord I will get noise .
I presume you want to transpose first : #CODE
Oh , that's interesting you can do it with stack .
In this case , using numpy outer operations allow you to compute the multiplications and sums at the ` C ` loop speed .
The most efficient way is likely to use ' np.empty() ' to allocate the space / memory for your end dataset and then load data & broadcast within that using slice indexing .
Ok , with your histogram I get at least the total number of each pair .
This is because python's sum is basically summing a for loop over the object .
Then the entire shape changes from ( x , y ) to merely ( x , ) and I get ' too many indices ' errors when I try to use masks .
If reps has length d , the result will have dimension of max ( d , A.ndim ) .
I want to do this by dividing each histogram by its maximum value so all the distributions have the same scale .
An obvious path would be to transpose the array so that the indices that I am selecting would come up first .
Now , for mean calculations , those numeric IDs could be used as `" weights "` for binning with ` np.bincount ` , giving us the sum of data elements corresponding to each ` ID ` .
However , what I need is a string containing all the elements in the list linked by ' ; ' , not the list itself , so it seems like I have to sum all the elements in asString with another iteration ?
the output I need : ` S = [ 2 , 5 , 8 , 11 , 14 ]` I thought something like : ` S1 = np.array ( L [: ] [ 1 , 0 ])` should work but whatever I try I have the error like : ` TypeError : list indices must be integers , not tuple ` .
I need it because in the next part I will sum up this large np.array with some delta_array that has the same shape .
Used reshape to make rows into columns .
I understand that you could create an array of zeros and iteratively change the values in each column , but I also understand this is not an efficient method .
I'm trying to implement the univariate gradient descent algorithm in python .
numpy glossary says the sum along axis argument ` axis=1 ` sums over rows : " we can sum each row of an array , in which case we operate along columns , or axis 1 " .
It also prints out the new indices signature .
At first , your ` result ` does not look like a complex FFT output , because you calculated the absolute values of the FFT .
When you do fft the frequencies ranges from -pi / timestep to pi / timestep ( assuming that frequency is defined as w = 2*pi / t , change the values accordingly if you use f =1 / t representation ) .
debug performance diff of Same code on nearly same cpu / ram
The HTML file generated by Cython indicates that the bottleneck is the dot products ( which is expected of course ) .
` numpy.unique ` with ` return_index=True ` will give you a list of indices to take from .
I forgot exactly why , but there is a good reason why you calculate it as the ratio between these two averages , instead of directly averaging ` fft ( y ) / fft ( x )` .
Do you really want this ' roll ' ?
By adding a nonzero number at the end of the array , you can still use np.nonzero to get your desired outcome .
which simply sorts the terms and then takes the ones which aren't equal to the previous one .
4 : I am not sure about the indices , by writing couple of code lines I just able to get cluster indices based on fclusterdata .
Matlab gives me a norm = 2 for your matrix .
I first generated a labelled array of unique IDs for each discrete region , calculated sizes for each ID , masked the size array to focus only on size == 1 blobs , then index the original array and set IDs with a size == 1 to 0 : #CODE
absolute ( a - b ) = ( atol + rtol * absolute ( b ))
Then I reshape this to form a 2D numpy array .
n=5 ( min length of sequence )
I have written a function which contains nested loops and a conditional statement ; the purpose of the loop is to return a list of indices for the nearest elements in array x when compared to array y .
I also want to color the 1D histogram bars according to the same normalization .
If you are calling it with an empty matrix for [ low , high ] it will just use whatever the max and min values in the array are .
Creating a class deriving from ` ndarray ` and overriding indexing such that the absolute indices are used .
One solution is to sort both arrays ( adding an index column so that the sorted arrays still contains the original indices ) .
Use ` reshape ` : #CODE
What's wrong with the normal div / mod operations ?
You can use ` argmin ` to find the False values , and this will be faster and take less memory than using nonzero , but this is linear in the length of ` a ` .
I'd like it to be like 8x10^8 or .8x10 ^9 to save space instead of putting all those zeros .
The one I pointed out in a comment to other answer as to encode the binary representation of the array as a Base64 text block .
due to broadcasting , you don't need to repeat duplicate indices , thus : #CODE
Maybe ` flatten() ` the original array , then use your 1D solution , finally calculate the real nD indices using the original shape ?
Multiply the coefficients and sum #CODE
Note that the diagonal is always zero since ` mahalanobis ( x , x )` equals zero for
I will have a go at the numpy 2d histogram in the next days .
For example , 1851 is four times and 1852 is 5 times , when i put the interval ( 1851,185 2 ) it will sum up and give out put as 9 .
NumPy has the efficient function / method ` nonzero() ` to identify the indices of non-zero elements in an ` ndarray ` object .
possible duplicate of [ NumPy min / max in-place assignment ] ( #URL )
Once the tree structure has been built , go back and collect all the branches and leaves into the array structure and by definition , they will be unique .
I wrote the following code but the output only contains the ids ( single column ) .
Maximum is always bigger than the minimum ( more to the right on a 1d axis , not by absolute value ) .
Note that ` unq_count ` doesn't count the occurrences of the last unique item , because that is not needed to split the index array .
If yes , you can use the Linux terminal to strip quotes from the ends of the rows quickly .
Which works fairly well , except it's somewhat cumbersome to do interactive work since you constantly have to remember which array dimensions correspond to which axes , and which parameteres correspond to which indices along that certain axis , etc .
The append method for a numpy array returns a copy of the array with new items added to the end .
I have a 2-D NumPy array and a set of indices the size of which is the first dimension of the NumPy array .
I want to get the norm of this array using numpy .
The only problem here is that I think it will append directly to the column , when I would prefer it to append to a new column .
So the function might be something like ` def some_function ( arr )` and it returns the indices in arr that meet a series of conditions .
However , because columns indices are both 0 , it adds a_ to the end of the dataframe column , resulting in a single column .
The value held in your tensor ` x ` at position with indices ` [ 0 , 0 , 1 , 1 ]' ?
You don't need to import string , and you don't need to loop through all the lines and append text or count the characters .
The transpose of the transpose of a matrix == that matrix , or , [ A^T ] ^T == A .
The last timestamps in ` a ` that are smaller than or equal to each item in ` b ` would be ` [ 0 , 4 , 6 ]` , which correspond to indices ` [ 0 , 2 , 3 ]` , which is exactly what we get if we do : #CODE
Currently I am looping through the arrays and using numpy.dstack to stack the 1000 arrays into a rather large 3d array ... and then will calculate the mean across the 3rd ( ? ) dimension .
[8 0 :] [ 105 :] it returns me an array with the corresponding values of allpix but when I set [8 0:1 : 200 ] [ 105:1 : 200 ] it returns me the original one roipix ( i.e with only zeros ) without any change .
transpose simply returns a ` view ` if possible -- So it's a fast operation -- That said , if you * can * just switch the way you index your array , that's probably the fastest you can do if you're operating one element at a time .
If you strip all these out and just call lapack in your for loop ( since you already know the dimensions of your matrix and maybe know that it's real , not complex ) , things run MUCH faster ( Note that I've made my array larger ) : #CODE
First , you have a binomial response : having or not having a particular behavior .
The call to ` np.sqrt ` , which is a Python function call , is killing your performance You are computing the square root of scalar floating point value , so you should use the ` sqrt ` function from the C math library .
A little annoying :) which is one reason why I almost never use the ` trace ` feature ; seeing _all_ syscalls is often more instructive , and ` grep -v ` can remove ones I don't want to see after the fact .
What's the ` dot ` product involving your ' point ' ?
It's probably ( ? ) those two that are slowing you down ( mostly ` sum ` vs ` numpy.sum `) .
I would like to perform dot ( A , A.T ) where certain indices are omitted : #CODE
I know need the sum of the values for each label and column .
Now to this I want to add a new column at beginning with all ones
This would call the function ` np.loadtxt ` which would load the file ` GPBUSD1d.txt '` and transpose ( " unpack ") it .
I have a quite large numpy array of one dimension for which I would like to apply some sorting on a slice inplace and also retrieve the permutation vector for other processing .
Can anyone explain me why does this happen ? and what do i do if i have keep the frame from getting changed when i write a composite clip into a video ?
You can't change the typing of the array in-place ( unless I'm grossly mistaken ) , but you can floor .
I have a 2D histogram that I generate with numpy : #CODE
Your calculation is essentially a dot ( matrix ) product .
Finally I just transpose the dataframe to get ids as rows and categories as columns .
Are the operations you're doing simple enough ( eg , dot product etc ) that you could simply implement them yourself ?
As a side note , in Matlab you'd better use the [ more efficient approach ] ( #URL ) ` mat = bsxfun ( @USER , mat , sum ( mat , 1 ))`
The following way of obtaining the unique elements in all sub-arrays is very fast : #CODE
Can't use dot , though , because doing that will add extra dimensions .
Whats the best way to plot a histogram of this data with minute bins and 10-min bins ?
You can't use the numpy reshape for a simple reason : you have data duplicity in your original array ( time and positions ) and not in the result you want .
So it does not make much sense to me to reshape it to a " 1d-matrix " .
PS : For the curious ones , this is a variant of the prize-winning solution to the famous NetFlix million prize problem .
Now create 5-bit bitstrings from each integer and join them together : #CODE
It would probably be just as much work to translate the top Matlab routine from Maurits .
In the particular case of your example , where your unique values are sequential integers , you can use ` find_objects ` directly .
Every second row of the ft consists completely of zeros .
The i-th element of the output is the sum of all the ` data ` elements corresponding to " id " ` i ` .
I'd like to add two numpy arrays of different shapes , but without broadcasting , rather the " missing " values are treated as zeros .
axis=1 refers to working on rows in this 2d case ( axis=0 , in contrast , would be getting you the max in each column )
There are many other ` ufunc ` , and other iteration modes - ` accumulate ` , ` reduceat ` .
All diagonal elements will be of the form ` s_i ** 2 / s_i ** 2 == 1 ` .
I get a vector of zeros and ones , of length 1200 .
Note that in many geometries d ( x , y ) = 0 iff x = y , so you may want to skip that check , and deal with the zeros later on .
Take the sum over ` a [ i ]` out of the loop , just like you did for ` b `
@USER In the example above , I get the following error : Non-broadcastable operand with shape ( 100 ) doesn't match the broadcast shape ( 100,100 )
However , I cant just use a matrix full of zeros , since I could afterwards not say which elements of the matrix are the " default " zeros and which ones i might have inserted myself .
is calculated such that all but the diagonal #CODE
Now in that sorted array , the first element would always be the unique one and that ` diff ` would have reduced the length by 1 .
To compute the number of unique elements in a numpy array , you can use ` unique ( x ) .size ` or ` len ( unique ( x ))` ( see ` numpy.unique ` ) .
To make sure they appear as an ordered pair I'd have to find the indices where ` x ` and ` y ` appear in ` cluster ` and compare them , which seems very clunky and inelegant , and I'm certain there must be a better solution out there .
Or would that basically require implementing the outer loop in Cython ?
For a tensor it is not clear how to define an inverse or a transpose .
Second , you are doing transpose the hard way .
I would like to compute the following sum .
I've modified the answer to show how to get this sum in numpy .
I am thinking of re-sample all the time columns into uniformly separated ones based on the starting point .
The older arrays that are filtered I could then add to the yearly sum and delete , so that I do not have more than 16 ( 4+12 ) time steps ( arrays ) in memory at all times .
sum ( triple ) = 1
Where does log ( b , 2 ) come from ?
` numpy.indices ` calculates the indices of each axis of the array when " flattened " through the other two axes ( or n - 1 axes where n = total number of axes ) .
( The values in the corners correspond to the diagonal elements . )
I tried using the scipy.stat module by creating my numbers with ` np.random.normal ` , since it only takes data and not stat values like mean and std dev ( is there any way to use these values directly ) .
until you reach a solution of size k , add to the solution the point for which the sum of distances from it to all the points already in the solution is the greatest .
Where ' reverse cumulative sum ' is defined as below ( I welcome any corrections on the name for this procedure ):
The asymptotic complexity of both of the ` matrix_rank ` and ` det ` calls are therefore O ( n^3 ) , the complexity of LU decomposition .
One way to achieve what I need is to use a code like the following ( see e.g. THIS related answer ) , where I digitize my values and then have a j-loop selecting digitized indices equal to j like below #CODE
I think the np.std() is just universal std .
Column sum with matrix from txt file ?
Golub and Van Loan also provide a way of storing a matrix in diagonal dominant form .
I see no reason why ` numpy ` would need to make a copy for an operation like this , as long as it does the necessary checks for overlaps ( though of course as others have noted , ` resize ` may itself have to allocate a new block of memory ) .
Any chance you know how I can display this as a normal 3D histogram with bars .
I found another stack question about this here , but I am not entirely sure how it was resolved , I'm still a little confused .
Maybe ` floor ( arange ( 0 , 10 , 0.1 ))` ?
or , if you want to sum over a column , i.e. , the index that changes is the 0-th #CODE
In python , I would like to convolve the two matrices along the second axis only .
` view ` is basically taking your two coordinates as a single variable that can be used to find the unique coordinates .
Optimize NumPy sum of matrices iterated through every element
You also incur a performance penalty by constructing a new NumPy array each time in the inner loop and using ` in ` to check the sum ( which is ` O ( n )` in complexity ) .
Keep in mind that machine precision for a 32-bit double is ~ 10^-16 , which will be an absolute limiting factor .
This takes advantage of fast code within the sparse matrix constructor to organize the data in a useful way , constructing a sparse matrix where row ` i ` contains just the indices where the flattened data equals ` i ` .
Each counter is basically a histogram .
Then I try and resample the ` DataFrame ` to daily using ` df.resample ( " 1D " , how= " sum ")` .
@USER To add to hpaulj's comment , [ advanced indexing ] ( #URL ) ( e.g. with an array of integer indices ) always returns a copy of the data rather than a view , so you need to take into account the additional cost of allocating a new array when you do ` A [: , subset ]` , as well as the fact that you are indexing non-contiguous blocks of ` A ` .
Getting linearized indices in numpy
Note that this approach will give you the exact proportion of zeros / ones you request , unlike say the binomial approach .
That is the exact problem with your ` square_list() ` function as well , it is returning a list , not the sum of the list elements .
For example , if you want to sum along the last dimension of the array , you would do : #CODE
For a sparse csr matrix ( X ) and a list of indices to drop ( index_to_drop ): #CODE
Also , if there is then I could just append to the b and c arrays each time instead of overwriting and starting from scratch each loop .
Use ` multiprocessing.Process ( target = somefunc , args = ( sa , )` ( and ` start ` , maybe ` join `) to call ` somefunc ` in a separate process , passing the shared array .
This leads to the situation that in the end I have an array with tail of zeros .
Take a look a the concatenate function .
The simplest way to deal with this is by making your array twice as large along every dimension , padding with extra zeros , and then discarding the extra data .
Get the column which have maximum sum value from matrix
Unlike Joe Kington's answer , the benefit of this is that you don't need to know the original shape of the data in the ` .mat ` file , i.e. no need to reshape upon reading in .
Also , to start with , rather than ` pd.Series.sum ` - just use `' sum '` - the code should take a faster path .
( The code won't even get through one loop for me because I'm using Python 3 and so the ` max ` fails -- in 2 it'll just give an answer that I doubt the OP intends . )
If you're multiplying each element of ` y ` by every element of ` X ` , just multiply all the elements of ` X ` together first , then use multiply the array ` y ` by this number and sum : #CODE
but I think , finding the local max can be simplified to : #CODE
Can this be vectorised as a dot product ?
The axis are showed clearly but no histogram appears on the plot .
I would like to resize the ones (( 12 , 12 )) such that it's new size will be 5x5 .
@USER ` swapaxes ` seemed to be indistinguishable from ` transpose ( 0 , 2 , 1 )` .
Do gradient actually compute really a gradient ?
Just multiply everything by the x raised to the absolute value of the largest negative exponent and use the normal polynomial class .
I would suggest to first program it with ` np.nditer ` and then translate it into C .
As you can see , using the join function on the list ( ` binary_list `) works properly , but on the equivalent numpy array ( ` binary_split_array `) it doesn't : we can see the string returned is only 72 characters long instead of 80 .
Does this mean that ` numpy.random.RandomState ( seed=None )` is called every time you call ` rand ` ?
@USER .B . the above question is significantly different from mine ; it asks for both min and max , and it is for 2D matrix
This will join the rows and write them to a new csv : #CODE
The reason I have ` -det ( mat )` in the energy function is because the simulated annealing algorithm does minimization .
My biggest issue is that further down the code I divide my spectra by a template spectrum constructed from the sum of all my spectra in order to analyse the differences and since I do not have enough decimal places I am getting rounding errors .
Also is ` x ` unique ?
If you wanted to automatically generate levels like the ones I have used , you can consider this piece of code : #CODE
` plt.hist ` is making a histogram of the values .
Pandas append filtered row to another DataFrame
Again , the code notes that set of combinations is not unique ; but it does have a unique subset , namely [[ 2 3 ] , [ 0 1 ]] , which as you just revealed , you do consider a valid combination .
Note that in the last one , your list ( if you are setting a column ) would have to be a list within a list ( i.e. the outer list acts as a row , the inner lists act as columns ) .
In short , this creates a vectorized version of the python function , that can be broadcast element-wise on an array .
That concatenate action should be pretty fast .
This produces a big band of zeros in the internal covariance matrix calculated by ` gaussian_kde ` , making it singular and causing the routine to fail .
If you want to pass in the transpose , you'll need to set ` rowvar ` to zero .
You can override this behavior by using the arguments ` vmin ` and ` vmax ` ( or ` norm `) of ` imshow ` .
I then must sum these values , using the expression :
gaussian sum filter for irregular spaced points
Differences between matrix multiplication and array dot
Here I collect 5 ( 3 , 3 ) arrays and join them into one .
@USER , ` cs ` is sorted and ` searchsorted() ` exploits that to do a binary search - only ` O ( log ( len ( weights )))` comparisons are needed .
If we sum the elements of the array it will give 15 .
Think ` flatten ` without the copy .
In your case it looks like the weight arrays will have the same dimension as ' A ' , so you reshape them accordingly and multiply dx and dy by their individual weight vectors .
Python program can not import dot parser
Does this mean the standard error of the gradient or intercept ?
I would think a density map ( 2D histogram ) would be more informative .
The second creates a 2D ` numpy.array ` of 1 row and 3 columns , filled with zeros : #CODE
Now I have the start and end indices for my mcp process .
In other words , I need to find the indices in my time array that satisfy that condition ( 0-240 ) , and then apply those indices to the amplitude array in a way that outputs the mean and st dev .
Your array has a lot more zeros than ones .
Also , the algo has a lot of matrices manipulation ( fft , filters , etc . ) , so using numpy / scipy should result in faster run time .
First , extract the inverse indices and counts of each unique item .
Essentially I want the code to go through the list of indices I have and find the index of the lowest value .
You can broadcast that into an array using expressions , for example #CODE
Sorry , my question more specifically was how I can get the indices of the True booleans in the array ?
If I use the above test on the absolute values of the angles to be tested , everything
` np.arange ( K ) [ None , None , :] + k1 ` is ( L , 1 , K ) , so we need to tile it #CODE
I have the diagonal elements what I need are the off diagonal ones
The returned gradient hence has
To be honest , I think the ` masked_array ` constructor ought to broadcast the mask against the array internally - I might consider making a pull request .
" In the first case the gradient is 1 mV / ms , in the second case it is 50 mV / ms .
If True , uses the old behavior from Numeric , ( correlate ( a , v ) == correlate ( v , a ) , and the conjugate is not taken for complex arrays ) .
Why don't you just compress the files with the built-in ` gzip ` module ?
So you need to write some function that convert a poly parameters array to a latex string , here is an example : #CODE
In your example , the square root is calculated by evaluating the the module and the argument of your complex number ( essentially via the log function , which returns log ( module ) + i phase ) .
I am trying to run hstack to join a column of integer values to a list of columns created by a TF-IDF ( so I can eventually use all of these columns / features in a classifier ) .
The only indices in my code are " index " , which is an integer from a " range " generator ; and " id " , which is pulled from a list of integers .
How to pass these ` norm ` and ` cmap ` parameters in matplotlib to ` plt.show ` or ` imshow() ` ?
You need to call the NumPy array's ` sum ` method , not the plain Python builtin ` sum ` function , in order to take advantage of NumPy : #CODE
Forget about the C stack , numpy objects can't use it .
If the sum of the ' dot ' becomes too large for the ` dtype ` , it can be negative , producing a ` nan ` when passed through ` sqrt ` .
This is similar to those Knapsack and subset sum problems .
You can use the append function as he has defined .
This can be particularly tricky when trying to append to a numpy array quickly .
I have a question regarding to the ` fft ` and ` ifft ` functions .
and arrays filled with zeros : #CODE
As others have hinted at your signals must have a large nonzero component .
The polynomial results will still expose logarithmical growth ( since ` log x^n = n * log x `) , but the exponential curve will transform into a proper straight line ( ` log exp ( x ) = x `) .
Or , better just directly index into the output array with the row and column indices - #CODE
So for now , I just changed the max ( z ) to a number that I know is the max ( 1567 ) .
The ` add ` operation does not do the same thing as ` join ` .
You don't specify ` x ` or ` y ` , and your ` mat [: , i+1 ]` indexing will not work with a structured array .
This is because in some cases it's not just NaNs and 1s , but other integers , which gives a std > 0 .
